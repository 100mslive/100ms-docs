[
    {
        "title": "Frequently Asked Questions",
        "link": "/flutter/v2/debugging/faq",
        "keywords": [],
        "headings": [
            "Flutter version compatibility",
            "Not getting event updates after hot reload/restart",
            "Issues while using `hmssdk_flutter` with flutter 3.0.x",
            "Is there any limit to the number of HMSVideoView on-screen at a time ",
            "Do you have any implementation with popular State Management libraries -",
            "Not able to get room updates after joining the room",
            "Join room with muted audio/video ",
            "Get `onPeerUpdate` in preview",
            "Getting updates multiple times in the room",
            "Can I create a room using API?",
            "Receiving too many logs from SDK ",
            "Do I need to do anything to handle poor internet connection?",
            "How do I implement Raise Hand, polls in application ?",
            "Why do I see videos getting stuck or frozen?",
            "What is the maximum allowed duration for a session?",
            "I want to suggest a new feature."
        ],
        "content": "  This page lists down frequently asked questions. If you want to add a new question or edit an older one, feel free to  send us a PR (https://github.com/100mslive/100ms-docs/blob/main/docs/flutter/v2/debugging/faq.mdx).   Flutter version compatibility HMSSDK works with flutter 3.3.x or above.   Not getting event updates after hot reload/restart This is caused because the platform channel needs to be reinitialized again, hence this is intended behaviour. The solution for this is to re-run the app. Practices for faster development :   Perform the intended changes.   Leave the room.   Perform hot reload/restart and rejoin the room.   Verify the changes. The permanent solution for this is in pipeline, we will update once it's done.   Issues while using hmssdk_flutter with flutter 3.0.x Flutter versions 3.0.0 to 3.0.5 had issues related to Platform View. Refer:  Android Platform View issue (https://github.com/flutter/flutter/issues/107313) &  Flutter Platform View bug (https://github.com/flutter/flutter/issues/103630) These were resolved in Flutter versions 3.3.0 & above. Please update the Flutter version to 3.3.0 or above.   Is there any limit to the number of HMSVideoView on-screen at a time  HMSVideoView internally uses SurfaceView in android and UiKitView in iOS. It is recommended to render at most 3 to 4 videos on a Single page/screen of the app and rest should be paginated for optimum performance.   Do you have any implementation with popular State Management libraries  Please find the implementations below:    Provider (https://github.com/100mslive/100ms-flutter/tree/main/example)    Bloc (https://github.com/100mslive/100ms-flutter/tree/main/sample%20apps/bloc)    Getx (https://github.com/100mslive/100ms-flutter/tree/main/sample%20apps/getx)    Mobx (https://github.com/100mslive/100ms-flutter/tree/main/sample%20apps/mobx)    Riverpod (https://github.com/100mslive/100ms-flutter/tree/main/sample%20apps/riverpod)   Not able to get room updates after joining the room To listen to the room updates please attach HMSUpdateListener as:  dart class Meeting implements HMSUpdateListener     Meeting()      hmsSDK.addUpdateListener(updateListener);       ...     You can find more details about HMSUpdateListener  here (../features/update-listeners)   Join room with muted audio/video User can join the room with muted audio/video by default. Please find the docs  here (../features/mute setting-video-and-mic-off-while-joining)   Get onPeerUpdate in preview User can get onPeerUpdate in preview the docs can be found  here (../features/preview get-on-peer-update-and-room-state-in-preview)   Getting updates multiple times in the room Please ensure removing the HMSUpdateListener while leaving the room.  dart hmsSDK.removeUpdateListener(updateListener);   You can find more details about HMSUpdateListener  here (../features/update-listeners)   Can I create a room using API? Yes,please find the link  here (/server-side/v2/Rooms/create-via-api)   Receiving too many logs from SDK Logs can be turned OFF using the hmsLogSettings parameter of HMSSDK . More info about this can be found  here (../features/error-handling setting-log-levels-in-sdk)   Do I need to do anything to handle poor internet connection? Not much, just turn on a flag in dashboard, and show a proper UI when a video gets degraded/unsubscribed. More details  here (../features/auto-video-degrade-restore).   How do I implement Raise Hand, polls in application ? You can do using  peer metadata (../advanced-features/peer-metadata-update use-cases).   Why do I see videos getting stuck or frozen? If you have enabled subscribe degradation from the dashboard, the SDK might go in the degradation mode on poor internet connection turning off some videos to ensure good call quality. When this is done, a flag on the track will be turned on to let the UI know. The UI should treat it similar to the track turning off for purpose of displaying avatar etc. More details  here (../features/auto-video-degrade-restore).   What is the maximum allowed duration for a session? The maximum allowed duration for a session on the 100ms platform is 12 hours.   I want to suggest a new feature. Awesome, we're always looking out for new ideas and features. Please reach out to us over  discord (https://100ms.live/discord) ",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/debugging/faq"
    },
    {
        "title": "Frequently Asked",
        "link": "/javascript/v2/debugging/faq",
        "keywords": [],
        "headings": [
            "What does HMS stand for in the SDK name?",
            "Can I listen to webhooks on server side?",
            "How do I record a room?",
            "How do I debug blank video tile while rendering?",
            "Why is the video not auto-playing on page load?",
            "(Angular) Why is video not auto-playing even though muted is set as true?",
            "How do I debug no audio coming in the room?",
            "Is it possible to do RTMP out, live stream a room to YouTube, Twitch, Wowza?",
            "What should I do to hide the beam tile showing up in 100ms web-app for browser based recording/streaming?",
            "How do I make the beam bot join with a custom role for dashboard web-app?",
            "Why does YouTube dashboard shows that the video bitrate is less than the recommended bitrate when using RTMP Out?",
            "Can I get HLS out for a room?",
            "How do I join an API created room from dashboard web-app?",
            "Do I need to do anything to handle poor internet connection?",
            "Can I store extra information with a peer?",
            "How do I implement Raise Hand?",
            "Why do I see videos getting stuck or frozen?",
            "Do you have UI components?",
            "Can I use the SDK with NextJS, Angular, Svelte, VueJS etc.?",
            "I want to suggest a new feature.",
            "I'm facing an issue, how do I reach out?",
            "Can I create room using API?",
            "Can I disable a room?",
            "Is it possible to create and manage roles using APIs?",
            "Does the SDK remembers input/output device selection for future joins?",
            "Can I implement custom events to broadcast or sent to a specific person in the room?",
            "How can I access the user id field used while creating the token after joining?",
            "How can I implement break out rooms?",
            "Can I locally mute a remote audio track?",
            "Can I process video before sending over to others in the room?",
            "How to disable console logs if I'm using the web SDK?",
            "What is the maximum allowed duration for a session?",
            "How can I get the HLS URL to enable live stream playback?",
            "I get type errors related to WebRTC Stats, for example, 'Cannot find name RTCInboundRtpStreamStats'."
        ],
        "content": "  This page lists down frequently asked questions. If you want to add a new question or edit an older one, feel free to  send us a PR (https://github.com/100mslive/100ms-docs/blob/main/docs/javascript/v2/debugging/faq.mdx).   What does HMS stand for in the SDK name? Hundred(100) Milliseconds ðŸ˜Š.  Why? (https://www.nngroup.com/articles/response-times-3-important-limits/)   Can I listen to webhooks on server side? Yes, please check  webhooks (/server-side/v2/introduction/webhook).   How do I record a room? We have two types of recordings available,  SFU (/server-side/v2/Destinations/recording) and  Browser (/server-side/v2/Destinations/rtmp-streaming-and-browser-recording). You can also start the latter from the  SDK (../features/rtmp-recording).   How do I debug blank video tile while rendering? Start with making sure that attach video is being called for the correct track and video element. Also ensure that there is no bug leading to detach call just after or around the same time as attach. These calls when done will also show up in the redux DevTools  extension (../debugging/debugging redux-devtools-integration). Some things we have seen in the past    The role was not subscribed properly in the dashboard's templates section    React  Calling detach as a cleanup function of the same useEffect which calls attach and has video track as dependency. Instead of this   please have a separate useEffect with no dependencies to call detach on component unmount.    React  Not using the  key (https://reactjs.org/docs/lists-and-keys.html) field properly while rendering the list of components displaying   the track. This should ideally be the trackId or peerId-trackType , where track type is video or screen.    Angular  Not using the  trackBy (https://angular.io/api/core/TrackByFunction) field properly while rendering the list of components displaying   the track. This should ideally be the trackId or peerId-trackType , where track type is video or screen.   Why is the video not auto-playing on page load? For the video to auto-play please make sure these fields are set on the video element  auto-play , muted , playsinline . Please check the docs for  render video (../features/render-video) for more details.   (Angular) Why is video not auto-playing even though muted is set as true? Angular 2+ is sometimes not able to translate the muted field correctly. Instead of setting the muted and auto-play property as <video muted> doing <video  muted =\"true\"> should work. Please check this  Stack Overflow answer (https://stackoverflow.com/questions/48856562/chrome-android-video-autoplay-inside-angular-2-component) for more details.   How do I debug no audio coming in the room?   Check that the role is being  subscribed to (../foundation/templates-and-roles subscribe-strategies) properly in  dashboard's (https://dashboard.100ms.live/) templates section.   If your web-app doesn't require a user click to join the room, you might run into auto-play issues. Browsers don't allow a website to play audio if user hasn't interacted with the page till that point in time. Fortunately, we have inbuilt support to detect and resolve this given in more details  here (../features/error-handling handling-autoplay-error).   If you're using the  setVolume (../advanced-features/volume-control) API, it's possible that even though the audio is available it has been locally muted.   Is it possible to do RTMP out, live stream a room to YouTube, Twitch, Wowza? Yes, you can achieve it both from  server-side APIs (/server-side/v2/Destinations/rtmp-streaming-and-browser-recording) or  SDK (../features/rtmp-recording).   What should I do to hide the beam tile showing up in 100ms web-app for browser based recording/streaming? You can use a viewer role which doesn't have any publish permissions.   How do I make the beam bot join with a custom role for dashboard web-app? You can append a query param in the end of the URL for the custom role  <custom_role_url>?skip_preview=true . This will tell the web-app to skip preview screen and join directly.   Why does YouTube dashboard shows that the video bitrate is less than the recommended bitrate when using RTMP Out? You can safely ignore this, this will happen if there is no activity happening on the URL being streamed. For example, there is nobody in the room with their video turned on.   Can I get HLS out for a room? Not yet, but we're working on this.   How do I join an API created room from dashboard web-app? You won't see the join room button on the dashboard, but it's possible to form an URL which you can use. The format is https://<subdomain>.app.100ms.live/preview/<room_id>/<role> , For example https://myroomlink.app.100ms.live/preview/123456/teacher . All of these, the subdomain, room_id and role are available on the dashboard.   Do I need to do anything to handle poor internet connection? Not much, just turn on a flag in dashboard, and show a proper UI when a video gets degraded/unsubscribed. More details  here (../features/sub-degradation).   Can I store extra information with a peer? Yes you can store  peer metadata (../advanced-features/peer-metadata) for a peer. The initial value can be provided at the time of join, and can be modified post join.   How do I implement Raise Hand? You can do using  peer metadata (../advanced-features/peer-metadata).   Why do I see videos getting stuck or frozen? If you have enabled subscribe degradation from the dashboard, the SDK might go in the degradation mode on poor internet connection turning off some videos to ensure good call quality. When this is done, a flag on the track will be turned on to let the UI know. The UI should treat it similar to the track turning off for purpose of displaying avatar etc. More details  here (../features/sub-degradation).   Do you have UI components? Not yet, but it's work in progress for react. Do let us know on discord if you want to sign up for beta and we'll hit you up soon.   Can I use the SDK with NextJS, Angular, Svelte, VueJS etc.? Yes, the core SDK is framework agnostic, you can follow the  JS Quickstart (../guides/javascript-quickstart) to learn the basics. The quickstart guide is with vanilla JS and doesn't assume any framework.   I want to suggest a new feature. Awesome, we're always on the lookout for new ideas and feature. Please reach out to us over  discord (https://100ms.live/discord).   I'm facing an issue, how do I reach out? Please see  reaching out (../debugging/debugging reaching-out).   Can I create room using API?  Yes (/server-side/v2/Rooms/create-via-api).   Can I disable a room?  Yes (/server-side/v2/Rooms/disable-or-enable). You can also do this while ending the room using the  SDK (../features/end-room).   Is it possible to create and manage roles using APIs?  Yes (/server-side/v2/policy/template-object).   Does the SDK remembers input/output device selection for future joins? Yes, just make sure you pass the rememberDeviceSelection as true in the  join config (../features/join).   Can I implement custom events to broadcast or sent to a specific person in the room? Yes, you can do so using our  messaging system (../features/chat custom-events).   How can I access the user id field used while creating the token after joining? It will be available as peer.customerUserId for any peer in the room.   How can I implement break out rooms? This can be done using  roles (https://www.youtube.com/watch?v=aO0KA2w03io).   Can I locally mute a remote audio track?  Yes (../advanced-features/volume-control).   Can I process video before sending over to others in the room? Yes, you can write  custom video plugins (../plugins/custom-video-plugins).   How to disable console logs if I'm using the web SDK? Please follow  setting log level (../features/log-level).   What is the maximum allowed duration for a session? The maximum allowed duration for a session on the 100ms platform is 12 hours.   How can I get the HLS URL to enable live stream playback? You can get the HLS URL in several ways based on whether you use 100ms client SDKs or a custom integration for playback; please check the  HLS guide (./../foundation/live-streaming live-stream-playback) for more information.   I get type errors related to WebRTC Stats, for example, 'Cannot find name RTCInboundRtpStreamStats'. The minimum supported version of TypeScript for the SDK is '4.4'. You could update to any version above 4.4 or if you are on an old version of TypeScript and cannot do a major upgrade you can set skipLibCheck: true in your tsconfig file. ",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/debugging/faq"
    },
    {
        "title": "Frequently Asked Questions (FAQ)",
        "link": "/react-native/v2/guides/faq",
        "keywords": [],
        "headings": [
            "Could not invoke HMSSDK.build or HMSManager.build",
            "\"trackId\" is undefined (HMSView is rendering blank view)",
            "Unable to Join Meeting or getting error on join / preview functions of HMS Instance",
            "Run the Example app",
            "UI components to test all the features and functionalities",
            "Customize Track Settings",
            "BLUETOOTH_CONNECT permission error/warning",
            "Using HMS with another WebRTC library",
            "Expo Support",
            "Change Streaming Video Aspect Ratio",
            "How to get HLS Stream URL?",
            "Video Player to play HLS Streams",
            "How to Mute/Unmute Specific or All Remote Peers?",
            "How to Mute/Unmute Specific or All Remote Peers only locally?",
            "Restrict Remote Peer from Speaking after their Audio is Muted",
            "Enable PIP Mode automatically when user leaves app",
            "Enable PIP Mode in iOS device.",
            "Using HMSSDK Instance in nested components without passing as props",
            "How many HMSView can be rendered?"
        ],
        "content": "  This page lists down frequently asked questions. If you want to add a new question or edit an older one, feel free to send us a PR.   Could not invoke HMSSDK.build or HMSManager.build   error (https://user-images.githubusercontent.com/56931905/174788439-53a39a54-847e-46f7-9104-7524b1547992.jpg) This error generally appears in development mode due to hot reloading. When the peer has joined the room and then the app is hot reloaded from the terminal, the peer is still in the room and when he tries to join back this error occurs. To make sure this error does not occur you have to remove your peer from the room.   To avoid you can add instance.leave() function on the unmounting of the Home Screen, so whenever the app is hot reloaded which leads to unmounting of the Home screen the leave function is called.   js section=CouldNotInvokeHMSManagerBuild sectionIndex=1 const onLeavePress = async () =>     await instance     ?.leave()     .then((d) => console.log('Leave Success: ', d))     .catch((e) => console.log('Leave Error: ', e));  ; useEffect(() =>     return () =>       onLeavePress();    ;  ,   );     If this error occurred you can either join through web app and remove the peer which is still present due to hot reloading or you can kill the app and rebuild it.   \"trackId\" is undefined (HMSView is rendering blank view)  Peer objects can have undefined trackId . If you are trying to use trackId directly from Peer object, then you might endup with undefined trackId problem. We recommend using ON_TRACK_UPDATE event for listening to track updates. When you receive TRACK_ADDED update type on this event, you can save received track and peer objects. Then you can use trackId from track object to show video in HMSView . This way your trackId will never be undefined.  > Note: ON_TRACK_UPDATE event is emitted for both \"Audio\" and \"Video\" tracks. For rendering video usecase, you only need to consider events received for \"Video\" tracks. Some Useful links:   Render Video of Peer (https://www.100ms.live/docs/react-native/v2/features/render-video)   HMSTrackUpdate Event Listener (https://www.100ms.live/docs/react-native/v2/features/event-listeners-enums hms-track-update)   Unable to Join Meeting or getting error on join / preview functions of HMS Instance This problem can happen due to many reasons. To self-serve, We recommend you to check if:  you are using correct authToken and username .  > authToken must contain correct roomId and role . role should be lowercase.   Also check out  Auth Token and Security Guide (https://www.100ms.live/docs/react-native/v2/foundation/security-and-tokens).  you are calling static build function on HMSSDK class correctly. <strong>Do not create an instance of HMSSDK class yourself with new keyword</strong>.    build function returns instance of HMSSDK class and also sets up SDK on native side.    js  import   HMSSDK   from '@100mslive/react-native-hms';  const hmsInstance = await HMSSDK.build();     Also check out  Join Room Guide (https://www.100ms.live/docs/react-native/v2/features/join)  Meeting Joining link is not disabled   Run the Example app To run the Example app on your system, follow these steps  1. Clone  100ms React Native Package repository (https://github.com/100mslive/react-native-hms) 2. In the project root, run npm install  3. Go to the example folder, cd example  4. In the example folder, run npm install  5. To run on Android, run npx react-native run-android   You can run example app on Android Emulator using \"deviceId\" option, run npx react-native run-android deviceId <device_id_here>  6. To run on iOS   a. First run Pod Install in iOS folder, cd ios && pod install && cd ../   b. Set the Correct Development Team in Signing & Capabilities in Xcode Build Settings to run on an actual iPhone or iPad. Apple Development Team Signing is not required for running the app on Simulators.  c. Run the command npx react-native run-ios   You can run example app on iOS Simulator using \"simulator\" option, run npx react-native run-ios simulator <simulator_name_here> .   UI components to test all the features and functionalities 100ms React Native package does not provide UI components except HMSView . However, We have created UI for testing all features of React Native SDK in our  example app (https://github.com/100mslive/react-native-hms/tree/main/example) and  sample apps (https://github.com/100mslive/react-native-hms/tree/develop/sample-apps). You can use UI from these apps to quickly test features and experiment in your apps.   Customize Track Settings You can customize Video and Audio track settings of Local Peer with HMSTrackSettings , HMSVideoTrackSettings and HMSAudioTrackSettings classes. while setting up HMSSDK instance you can pass instance of HMSTrackSettings to build function available on HMSSDK class. Refer to  Track Settings Guide (https://www.100ms.live/docs/react-native/v2/advanced-features/track-settings) for more info.   To customize more settings (other than defined on above mentioned classes) like video quality, aspect ratio, screenshare quality.   You can change all these and more in  100ms dashboard (https://dashboard.100ms.live/templates). Check out  Templates and Roles Guide (https://www.100ms.live/docs/react-native/v2/foundation/templates-and-roles)   BLUETOOTH_CONNECT permission error/warning On Android 12 devices, new Bluetooth permissions have been added to interact with other nearby Bluetooth devices. To fix this error  1. Declare BLUETOOTH_CONNECT permission in AndroidManifest.xml file.   xml <uses-permission android:name=\"android.permission.BLUETOOTH\" android:maxSdkVersion=\"30\"   <uses-permission android:name=\"android.permission.BLUETOOTH_SCAN\"   <uses-permission android:name=\"android.permission.BLUETOOTH_ADVERTISE\"   <uses-permission android:name=\"android.permission.BLUETOOTH_CONNECT\"     2. The BLUETOOTH_CONNECT permission is runtime permission. Therefore, you must also request user approval at runtime before you join a call or display a preview in your app, like we do for Camera and Audio Permissions.   We suggest using  react-native-permission (https://www.npmjs.com/package/react-native-permissions) to acquire permissions from both platforms.  Check out official  Android Bluetooth Permissions (https://developer.android.com/guide/topics/connectivity/bluetooth/permissions) page.  You can also check  Our Android Integration Guide (https://www.100ms.live/docs/react-native/v2/features/integration for-android) and permission related code in our  Quickstart Sample App (https://github.com/100mslive/react-native-hms/tree/develop/sample-apps)   Using HMS with another WebRTC library WebRTC is a core dependency of 100ms SDKs. While building your Real Time Audio Video apps, developers tend to utilize multiple libraries. So it can happen that some another package also has WebRTC as a dependency. In this scenario, your app might emit compile time errors, crash at run time or have unexpected behaviours. This usually happens due to multiple WebRTC instances within the app. To avoid these issues, it's recommended to only add 100ms package & remove any other packages that also depend on WebRTC. 100ms provides a rich set of features which you can easily customize to build your ideal Audio Video experiences.   Expo Support Yes, Expo is supported by 100ms React Native SDK. You can follow our  Expo Setup Guide (https://www.100ms.live/docs/react-native/v2/features/integration expo-setup) to complete your setup.   Change Streaming Video Aspect Ratio Default Aspect Ratio of Streaming Video is 16:9. This can show many Peer Tiles in your streaming video. You can change Aspect Ratio of Streaming video from   100ms Dashboard (https://dashboard.100ms.live/dashboard) > Templates > Select a Template > Destinations tab > scroll down to Live Streaming > \"Customize stream video output\"  You may want to change default ratio as per your use case. for example, If you have only one Tile in your stream video, then you can make video aspect ratio as same as Peer Tile acpect ratio. This will give your Stream Viewers very nice watching experience.   How to get HLS Stream URL? HLS Stream URL is available in Room object. Check out below code snippet    js // you can check if hls stream is running const isStreaming = room.hlsStreamingState?.running; // you can access various variants of running hls stream const hlsStreamingVariant = room.hlsStreamingState?.variants 0 ; // on stream variant, you have access to stream url const sreamURL = hlsStreamingVariant.hlsStreamUrl; // Showing running stream in a video player <VideoPlayer url= hlsStreamingVariant.hlsStreamUrl      You can get Room object by hmsInstance.getRoom method and HMSUpdateListenerActions.ON_ROOM_UPDATE event    js // Initially get room object from getRoom method const room = await hmsInstance.getRoom(); // Add listener to receive Room Updates hmsInstance.addEventListener(  HMSUpdateListenerActions.ON_ROOM_UPDATE,  (data:   room: HMSRoom; type: HMSRoomUpdate;  ) =>     // Updated Room object   const room = data.room;   if (data.type === HMSRoomUpdate.HLS_STREAMING_STATE_UPDATED)      console.log('HLS Streaming state has changed')        );   To know how to start or stop HLS Streaming, check out  HLS Streaming Docs (https://www.100ms.live/docs/react-native/v2/features/hls)   Video Player to play HLS Streams  react-native-video (https://www.npmjs.com/package/react-native-video) is the most reliable package to play videos on React Native apps. We are also using this package in our example app to play HLS Streams.   How to Mute/Unmute Specific or All Remote Peers? 100ms have changeTrackState APIs to request mute or unmute remote peers. Refer to  Change Track State (https://www.100ms.live/docs/react-native/v2/features/change-track-state) API docs to learn more. To Mute all Remote Peers at once, you can refer  here (https://www.100ms.live/docs/react-native/v2/features/change-track-state mute-all-remote-audio-tracks).   How to Mute/Unmute Specific or All Remote Peers only locally? When you mute audio or video of remote peer locally, you won't be able to hear or see the remote peer but it will be still audible and visible to others. 100ms have \"Playback Allowed\" APIs to mute or unmute remote peers locally. Refer to  Playback Allowed (https://www.100ms.live/docs/react-native/v2/features/playback-allowed) API docs to learn more. To locally mute all Remote Peers at once, you can refer  here (https://www.100ms.live/docs/react-native/v2/features/playback-allowed local-mute-all-remote-peers-audio).   Restrict Remote Peer from Speaking after their Audio is Muted Once you  Mute a Peer (https://www.100ms.live/docs/react-native/v2/features/change-track-state), they can unmute themselves. To prevent peers from un-muting themselves, you should  Change their Role (https://www.100ms.live/docs/react-native/v2/features/change-role) to a Role which has less  publishing permissions (https://www.100ms.live/docs/react-native/v2/foundation/templates-and-roles publish-strategies) as per your use case instead of muting the peer.   Enable PIP Mode automatically when user leaves app Currently, Enabling  Picture in Picture (PIP) mode (https://www.100ms.live/docs/react-native/v2/features/pip-mode) automatically (that is without calling any function) is not supported. We recommend enabling Picture in Picture (PIP) mode (by calling enablePipMode function)   while app is active  , on a button click or programmatically.   Enable PIP Mode in iOS device.  Picture in Picture (PIP) mode (https://www.100ms.live/docs/react-native/v2/features/pip-mode) is not supported in iOS devices due to lack of permission of using  multitasking-camera-access (https://developer.apple.com/documentation/bundleresources/entitlements/com_apple_developer_avfoundation_multitasking-camera-access?language=objc discussion) entitlement. We are working on making this work. Thanks for your patience.   Using HMSSDK Instance in nested components without passing as props   Don't call HMSSDK.build function just to get the hmsInstance in nested components without prop drilling to use various APIs.    HMSSDK.build creates and returns new instances of SDK each time you call it. It is not returning the previously created SDK instance. We recommend using State Management solutions like Redux or Context API to save your originally created hmsInstance into store and use this stored instance in your nested components.   How many HMSView can be rendered? We recommend rendering separate HMSView for each trackId . It means If you have 50 peers with trackIds in a room, then you will render 50 HMSView for each peer. This can have an impact on your apps' performance. Therefore, since ideally maximum 4-6 HMSView should be visible at a time because of small screen size of mobile devices. You can use  FlatList (https://reactnative.dev/docs/flatlist) to render HMSView for large list of peers, this way HMSView that are not in visible area will never be mounted and HMSViews that goes out of visible area will be unmounted. By using FlatList you are improving your apps' performance and rendering separate HMSView for each trackId . > Pro Tip: You can use key prop with trackId as value to bind HMSView with trackId. Example  <HMSView key= trackId   ...    ",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/faq"
    }
]
