[
    {
        "title": "Show Audio Levels",
        "link": "/android/v2/advanced-features/audio-level",
        "platformName": "Android",
        "objectID": "/android/v2/advanced-features/audio-level",
        "keywords": [],
        "content": "  The concepts explained here are: 1.  Getting Audio Levels for all speaking peers ( audio-level). 2.  Creating a UI that shows the loudest speaking users ( active-speaker-views). 3.  Dominant Speaker  the loudest speaker ( dominant-speaker).   Audio Level The audio level for every speaking peer can be retrieved by registering an instance of HMSAudioListener in hmsSDK.addAudioObserver like so.   s id=\"add-audio-observer\" items=  'Kotlin', 'Java'       id='add-audio-observer-0'>   kotlin hmsSDK.addAudioObserver(object : HMSAudioListener       override fun onAudioLevelUpdate(speakers: Array<HMSSpeaker>)        Log.v(\"TAG\",          \"Active Speakers are: $ speakers.map   s -> \"$ s.peer?.name  $ s.level \"    \"      )           )    </Tab>   id='add-audio-observer-1'>   java hmsSdk.addAudioObserver(new HMSAudioListener()         @Override       public void onAudioLevelUpdate(@NonNull HMSSpeaker   hmsSpeakers)           StringBuilder allSpeakers = new StringBuilder(\"Active speakers are: \");         for( HMSSpeaker speaker : hmsSpeakers)            String name;          if(speaker.getPeer()  = null )             name = speaker.getPeer().getName()            else             name = \"\"                      allSpeakers.append(name)             .append(\" \")             .append(speaker.getLevel())             .append(\" n\");                   Log.d(\"tag\", allSpeakers.toString());              );   </Tab>  Here are the properties on the HMSSpeaker class:  trackId : String . The track id of the audio track for this speaker.  level : Int . The level of the audio. It may vary from 0-100. A higher value indicates a higher speaking volume.  hmsTrack : HMSTrack?. The audio track corresponding to this speaker. It may be null when the speaker who was speaking loudly, leaves.  peer : HMSPeer?. The peer who was speaking. This may be null if the peer has left before the update information has reached the other person.   Active Speaker Views To maintain an active speaker view, such as the default view in the  open source advanced sample app (https://github.com/100mslive/100ms-android), you need to keep track of who the active speakers are over time. We'll be using the input from HMSAudioListener as mentioned above and then building something that attempts to show all the active speakers while minimizing re-ordering the list. This is achived in the sample app by means of the  ActiveSpeakerCache (https://github.com/100mslive/100ms-android/blob/b02ad19b7ad1bcc3b9973a2a6a4e1a36c121f477/app/src/main/java/live/hms/app2/ui/meeting/activespeaker/ActiveSpeakerCache.kt),  ActiveSpeakerHandler (https://github.com/100mslive/100ms-android/blob/b02ad19b7ad1bcc3b9973a2a6a4e1a36c121f477/app/src/main/java/live/hms/app2/ui/meeting/activespeaker/ActiveSpeakerHandler.kt) and then piping the input from the HMSAudioListener into the handler as demonstrated  here (https://github.com/100mslive/100ms-android/blob/0ebfc6527bafbda7236d5245dedb5fefb262bfcc/app/src/main/java/live/hms/app2/ui/meeting/MeetingViewModel.kt L126).   Dominant Speaker  The dominant speaker is the speaker who's the loudest at any given moment. There's a callback for this in the onPeerUpdate callback for HMSUpdateListener . Here's the method signature for onPeerUpdate: override fun onPeerUpdate(type: HMSPeerUpdate, hmsPeer: HMSPeer)  When the dominant speaker has changed, there will be updates of the type:  HMSPeerUpdate.BECAME_DOMINANT_SPEAKER When a new person becomes the dominant speaker.  HMSPeerUpdate.NO_DOMINANT_SPEAKER When the current dominant speaker stops talking and there's no one to replace them. The hmsPeer parameter contains the person these updates apply to. "
    },
    {
        "title": "Camera Controls",
        "link": "/android/v2/advanced-features/camera-controls",
        "platformName": "Android",
        "objectID": "/android/v2/advanced-features/camera-controls",
        "keywords": [],
        "content": "  To ues the camera controls, you can use CameraControl class to access it's functionality.   Minimum Requirements   SDK version 2.5.7 or higher The latest HMSVideoView SDK version is:     (https://img.shields.io/badge/dynamic/xml.svg?label=100ms&color=blue&query=%2F%2Fmetadata%2Fversioning%2Flatest&url=https%3A%2F%2Frepo1.maven.org%2Fmaven2%2Flive%2F100ms%2Fandroid-sdk%2Fmaven-metadata.xml) (https://jitpack.io/ 100mslive/android-sdk)    Getting the camera controls instance. The CameraControl instance can only be found in HMSLocalVideoTrack (local video track), otherwise the getCameraControl() method will return null .   kotlin val videoTrack : HMSVideotrack = hmsPeer.videoTrack val CameraControl = (videoTrack as? HMSLocalVideoTrack)?.getCameraControl()     Focus   Check if tap to focus feature is available Checks if tap to focus is supported by current facing camera.   kotlin cameraControl.isTapToFocusSupported()     Set tap to focus.   kotlin View's coordinate system (x, y) (0,0)    ( viewWidth , 0)                                                   (0, viewHeight )   ( viewWidth ,  viewHeight )  @param viewX should be the point in view's X coordinate @param viewY should be the point in view's Y coordinate @param viewWidth should be the current view's width @param viewHeight should be the current view's height cameraControl.setTapToFocusAt(     viewX: Float,     viewY: Float,     viewWidth: Int,     viewHeight: Int,     scalingType: ScalingType = ScalingType.SCALE_ASPECT_FILL   )      Zoom   Check if zoom feature is available Checks if zoom feature is supported by current facing camera.   kotlin cameraControl.isZoomSupported()     Minimum zoom level Gets minimum zoom value in for current camera session. If not supported  ZOOM_DEFAULT_VALUE  is returned   kotlin cameraControl.getMinZoom()     Maximum zoom level Gets maximum zoom value in for current camera session. If not supported  ZOOM_DEFAULT_VALUE  is returned   kotlin cameraControl.getMaxZoom()     Set zoom level  Sets the current zoom, will not zoom if its outside the zoom bounds   kotlin cameraControl.setZoom(zoomValue: Float)     Reset zoom level  Resets the current zoom   kotlin cameraControl.resetZoom()   > 💡 Tap to focus and zoom controls doesn't work well together. If you want to focus, you will have to reset zoom.   Flash   Check if flash feature is available Checks if the current facing camera has a flash device.   kotlin cameraControl.isFlashSupported()     Is flash enabled Checks if the camera flash is enabled.   kotlin cameraControl.isFlashEnabled()     Enables or disable camera flash Enables or disable camera flash.   kotlin cameraControl.setFlash(enable: Boolean)     Capture at the highest resolution offered by the camera Takes the highest quality picture from the local video stream irrespective of publisher's resolution. You need to provide a file object where the desired image is to be saved. If it's on the external directory proper, app permission has to be taken before.   kotlin @param savePath Path where Image is to be save @param callback  Boolean.true  if image was successfully captured and saved in the file provided,  Boolean.false  otherwise. cameraControl.captureImageAtMaxSupportedResolution(   savePath: File,   callback: (isSuccess: Boolean) -> Unit )     Wiring camera controls with gestures Here's an example how you can wire camera controls with gestures.   Tap to focus is done on touch  zoom is done using pinch gesture   very high resolution image camera capture is done using doube tap gesture.   kotlin var lastZoom = cameraControl.getMinZoom() val gestureDetector = GestureDetectorCompat(context, object : GestureDetector.SimpleOnGestureListener()     override fun onDown(e: MotionEvent?) = true   override fun onSingleTapUp(event: MotionEvent): Boolean       if (cameraControl.isTapToFocusSupported())       cameraControl.setTapToFocusAt(         event.x,         event.y,         viewWidth = width,         viewHeight = height       )     return true       override fun onDoubleTap(e: MotionEvent): Boolean       val cachePath = File(context.cacheDir, \"images\")     cachePath.mkdirs()     val imageSavePath = File(cachePath, \"image.jpeg\")     cameraControl.captureImageAtMaxSupportedResolution(imageSavePath)   isSuccess ->     if (isSuccess)         val fileSaveUri = FileProvider.getUriForFile(         context,          use your own provider ,         imageSavePath       )                return true      ) val scaleGestureDetector = ScaleGestureDetector(   context,   object : ScaleGestureDetector.SimpleOnScaleGestureListener()       override fun onScale(detector: ScaleGestureDetector): Boolean         if (cameraControl.isZoomSupported())           lastZoom  = detector.scaleFactor         cameraControl.setZoom(lastZoom)         return true               return false          ) videoView.setOnTouchListener   _, event ->   var didConsume = scaleGestureDetector.onTouchEvent(event)   if ( scaleGestureDetector.isInProgress)       didConsume = gestureDetector.onTouchEvent(event)       didConsume     "
    },
    {
        "title": "Stats for HLS Player",
        "link": "/android/v2/advanced-features/hls-stats",
        "platformName": "Android",
        "objectID": "/android/v2/advanced-features/hls-stats",
        "keywords": [],
        "content": "    Stats Information Statistics for an on-going stream can be viewed by user for debugging purposes. It can also be used as a standalone dependency if required and has no dependency on 100ms SDK.   Add dependency   Adding the Player-Stats plugin and SDK dependency to your app-level build.gradle .     (https://img.shields.io/badge/dynamic/xml.svg?label=100ms&color=blue&query=%2F%2Fmetadata%2Fversioning%2Flatest&url=https%3A%2F%2Frepo1.maven.org%2Fmaven2%2Flive%2F100ms%2Fandroid-sdk%2Fmaven-metadata.xml) (https://jitpack.io/ 100mslive/android-sdk)   s id=\"sdk-imports\" items=  'Maven Central', 'Jitpack'       id='sdk-imports-0'>   json 5-8 :settings.gradle section=AddSDKDependencies sectionIndex=1 dependencies   // See the version in the badge above. // There are separate libraries for the sdk, virtual background and hls-player-stats. //   add just the ones you need. def hmsVersion = \"x.x.x\" implementation \"live.100ms:android-sdk:$hmsVersion\" implementation \"live.100ms:hls-player-stats:$hmsVersion\"     </Tab>    id='sdk-imports-1'>   json 2,4-6 :build.gradle section=AddSDKDependencies sectionIndex=2 dependencies   def hmsVersion = \"x.x.x\" // See the version in the jitpack badge above. implementation \"com.github.100mslive.android-sdk:lib:$hmsVersion\" implementation \"com.github.100mslive.android-sdk:stats:$hmsVersion\"     </Tab> <Note>   Note     This Plugin supports Exoplayer versions 2.15.0 and above. </Note>   Instantiate PlayerEventsCollector An Existing exoplayer instance can be directly attached to PlayerEventsCollector as follows :   s id=\"instanstiate-PlayerStatsPlugin\" items=  'Kotlin', 'Java'       id='instanstiate-PlayerStatsPlugin-0'>   kotlin       // an existing exoplayer instance       val exoplayer : Exoplayer        // this is an optional configuration provided while instantiated while creating the object, where       // @param eventRate is the rate in milliseconds at which client wants to recieve the events.(default set to 2000 i.e 2 seconds)       val initConfig = InitConfig(eventRate = 2000)       playerEventsCollector = PlayerEventsCollector(exoplayer : exoplayer)    </Tab>   id='instanstiate-PlayerStatsPlugin-1'>   java       // an existing exoplayer instance       Exoplayer exoplayer;        // this is an optional configuration provided while instantiated while creating the object, where       // @param eventRate is the rate in milliseconds at which client wants to recieve the events.(default set to 2000 i.e 2 seconds)       InitConfig initConfig = new InitConfig(2000);       PlayerEventsCollector playerEventsCollector = PlayerEventsCollector(exoplayer,initConfig);    </Tab>   Listening to updates from PlayerEventsCollector  To start listeneing updates from the plugin , listener can be attached as follows :   s id=\"addListener-PlayerStatsPlugin\" items=  'Kotlin', 'Java'       id='addListener-PlayerStatsPlugin-0'>   kotlin      playerEventsCollector.addListener(object : PlayerEventsListener           override fun onEventUpdate(playerStats: PlayerStats)             // playerStats object contains information for all the stats provided by the plugin.           // more information for this object is mentioned at the end of the document.                  )    </Tab>   id='addListener-PlayerStatsPlugin-1'>   java       playerEventsCollector.addListener(new PlayerEventsListener()           @Override         public void onEventUpdate(@NonNull PlayerStats playerStats)                              );   </Tab>  To stop listeneing updates from the plugin , listener can be removed as follows :   s id=\"removeListener-PlayerStatsPlugin\" items=  'Kotlin', 'Java'       id='removeListener-PlayerStatsPlugin-0'>   kotlin       playerEventsCollector.removeListener()    </Tab>   id='removeListener-PlayerStatsPlugin-1'>   java       playerEventsCollector.removeListener();    </Tab> <Note>   Note     removeListener() must be called when view listening to it is destroyed </Note>   Interpretting the values :   Name  Description  Unit  Usage            bandwidthEstimate  The current bandwidth, as estimated by the player  bytes per second  Use this to show the current network speed of the user   bytesDownloaded  The total bytes downloaded within the given poll duration  Bytes  Use this to calculate the total Bytes downloaded in a session or to show the network activity   bitrate  bitrate of the current layer being played  bytes per second  Use to show a bitrate chart to the user   bufferedDuration  An estimate of the total buffered duration from the current position  ms  This can be used to show how much data is already buffered   video width  The width of the video  pixels  Used to know the resolution being played   video height  The height of the video  pixels  Used to know the resolution being played   video frameRate  The frame rate  frames per second  Used to know the FPS   droppedFrames  The number of dropped frames since the last call to this method  Int  Used to calculate the total number of dropped frames   distanceFromLiveEdge  Distance of current playing position from live edge  Int  Used to calculate how far current player position is from live edge of content.  "
    },
    {
        "title": "HLS Timed Metadata",
        "link": "/android/v2/advanced-features/hls-timed-metadata",
        "platformName": "Android",
        "objectID": "/android/v2/advanced-features/hls-timed-metadata",
        "keywords": [],
        "content": "  HLS Timed Metadata feature helps you synchronise certain events with the HLS stream. This can be useful for showing interactive quizzes / product overlays etc.   Requirements  100ms Android SDK version 2.5.6 or higher  Active HLS stream   Sending HLS Timed Metadata To add HLS timed metadata cue to the currently running HLS stream use setHlsSessionMetadata API like this:  s id=\"send-hmsTimedMetadata\" items=  'Kotlin', 'Java'       id='send-hmsTimedMetadata-0'>   kotlin val metadata = HMSHLSTimedMetadata(             payload = payloadText,             duration = durationText           ) hmsSDK.setHlsSessionMetadata(arrayListOf(hmsHLSTimedMetadata), object : HMSActionResultListener       override fun onError(error: HMSException)         Log.d(TAG, \"hls metadata sending failed\");           override fun onSuccess()         Log.d(TAG, \"hls metadata sent successfully\");        )   </Tab>   id='send-hmsTimedMetadata-1'>   java HMSHLSTimedMetadata metaDataModel = new HMSHLSTimedMetadata(payload,duration,metaData); ArrayList<HMSHLSTimedMetadata> metadataModelList = new ArrayList<HMSHLSTimedMetadata>(metaDataModel); metadataModelList.add(metaDataModel); hmsSDK.setHlsSessionMetadata(metadataModelList, new HMSActionResultListener()     @Override   public void onSuccess()       Log.d(TAG, \"hls metadata sent successfully\");       @Override   public void onError(@NonNull HMSException e)       Log.d(TAG, \"hls metadata sending failed\");      );   </Tab>   Receiving HLS Timed Metadata   Getting metadata from exoplayer  s id=\"recieve-hmsTimedMetadata\" items=  'Kotlin', 'Java'       id='recieve-hmsTimedMetadata-0'>   kotlin     val hlsManifest = exoPlayer.currentManifest as HlsManifest?     val windowIndex = exoPlayer.currentMediaItemIndex     val timeline = exoPlayer.currentTimeline     val window = timeline.getWindow(windowIndex, Timeline.Window())     val currentAbsolutePosition = (window.windowStartTimeMs + exoPlayer.currentPosition)     val META_DATA_MATCHER = \" EXT-X-DATERANGE:ID= \"(?<id>. ) \",START-DATE= \"(?<startDate>. ) \",DURATION=(?<duration>. ),X-100MSLIVE-PAYLOAD= \"(?<payload>. ) \"\"     var duration = 0;     var tagStartTime = 0L;     String payload = \"\";     hlsManifest?.mediaPlaylist?.tags?.forEach         if (it.contains(\"EXT-X-DATERANGE\"))           val pattern = Pattern.compile(META_DATA_MATCHER)         val matcher = pattern.matcher(it)         if (matcher.matches())             try               // X-100MSLIVE-PAYLOAD : this contains payload passed in metadata from setHlsSessionMetadata()              payload = matcher.group(4).orEmpty()             // DURATION : this contains duration that was passed in metadata from setHlsSessionMetadata()             duration = matcher.group(3).orEmpty().toLongOrNull()?.times(1000) ?: 0             // START-DATE : this contains start time for metadata             val startDate = matcher.group(2).orEmpty()             val formatter = SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")             formatter.timeZone = UTC             tagStartTime = formatter.parse(startDate).time             // Syncing of metadata with current player position.             if (tagStartTime <= currentAbsolutePosition && currentAbsolutePosition  tagStartTime <= duration)                 Log.i(TAG,payload);                           return@lastOrNull tagStartTime <= currentAbsolutePosition             catch (e: Exception)               e.printStackTrace();                                     false         </Tab>   id='recieve-hmsTimedMetadata-1'>   java     HlsManifest hlsManifest = (HlsManifest) exoPlayer.getCurrentManifest();     int windowIndex = exoPlayer.getCurrentMediaItemIndex();     Timeline timeline = exoPlayer.getCurrentTimeline();     Timeline.Window window = timeline.getWindow(windowIndex, new Timeline.Window());     long currentAbsolutePosition = (window.windowStartTimeMs + exoPlayer.getCurrentPosition());     String META_DATA_MATCHER = \" EXT-X-DATERANGE:ID= \"(?<id>. ) \",START-DATE= \"(?<startDate>. ) \",DURATION=(?<duration>. ),X-100MSLIVE-PAYLOAD= \"(?<payload>. ) \"\"     String payload = \"\";     long tagStartTime;      hlsManifest.mediaPlaylist.tags.forEach(new Consumer<String>()         @Override       public void accept(String tag)           if (tag.contains(\"EXT-X-DATERANGE\"))             Pattern pattern = Pattern.compile(META_DATA_MATCHER);           Matcher matcher = pattern.matcher(tag);           if (matcher.matches())               try                 String payload = matcher.group(4);               long duration = Long.getLong(Objects.requireNonNull(matcher.group(3)))   (1000);               String startDate = matcher.group(2);               SimpleDateFormat formatter = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\");               formatter.setTimeZone(TimeZone.getTimeZone(\"UTC\"));               tagStartTime = formatter.parse(startDate).getTime();               if (tagStartTime <= currentAbsolutePosition && currentAbsolutePosition  tagStartTime <= duration)                   Log.i(TAG,payload);                               catch (Exception e)                 e.printStackTrace();                                                  );   </Tab> > Please Note : above step has to be repeated periodically to sync meta-data with playback.  "
    },
    {
        "title": "Peer Metadata Update",
        "link": "/android/v2/advanced-features/peer-metadata-update",
        "platformName": "Android",
        "objectID": "/android/v2/advanced-features/peer-metadata-update",
        "keywords": [],
        "content": "  Looking for persistent state that can be set on a peer and updated anytime, for everyone in the room? Peer metadata is it. Metadata can be set initially in the HMSConfig object that's passed into the join method. This section will show you how to: 1.  Read Peer Metadata ( reading-metadata). 2.  Respond to when a remote peer changes its metadata ( responding-to-updates). 3.  How to set a peer's metadata ( updating-metadata). The HMSPeer object prior to 2.2.1 contained customerDescription a read-only string whose value wouldn't change throughout a call. This has been replaced with metadata . The value is initially assigned as before but now can be changed by the peer who owns it. Any peer can change the metadata for themselves. Currently one peer cannot change another peer's metadata. To change their own metadata value the peer should call changeMetadata on their HMSSdk instance. The data may be any arbitrary string though it cannot be changed to null.   Reading metadata To read metadata, read the metadata value on any HMSPeer instance.   Responding to updates Whenever a remote peer's metadata is updated a callback will be received in onPeerUpdate(update : HMSPeerUpdate, peer : HMSPeer) of HMSUpdateListener where the update value will be of type HMSPeerUpdate.METADATA_CHANGED . When this callback is received the UI for that peer should be updated as well.   Updating Metadata Here is how a peer can set their own metadata to a random string. In this case the string is stringified json.  s id=\"change-metadata\" items=  'Kotlin', 'Java'       id='change-metadata-0'>   kotlin val newMetadata = \"  \"ms \": 100  \" hmsSdk.changeMetadata(newMetadata, object : HMSActionResultListener     override fun onSuccess()         override fun onError(error: HMSException)        )   </Tab>   id='change-metadata-1'>   java String newMetadata =\"  \"ms \": 100  \"; hmsSdk.changeMetadata(newMetadata, new HMSActionResultListener()     @Override   public void onSuccess()         @Override   public void onError(@NonNull HMSException e)        );   </Tab> "
    },
    {
        "title": "Show Network Quality For Peers",
        "link": "/android/v2/advanced-features/peer-network-quality",
        "platformName": "Android",
        "objectID": "/android/v2/advanced-features/peer-network-quality",
        "keywords": [],
        "content": "    Network Quality Information The network quality for every peer can be retrieved by registering an instance of HmsUpdateListener in hmsSDK .   Limitations The network quality information per peer is calculated by the server by observing how well the peer can download other peer's videos. > ⚠️ If the peer is not subscribing to any videos network quality updates will NOT be sent for them to anyone in the call. This depends only on what videos that peer is seeing, so it's possible in a large call that some people have on-screen only degraded tiles, or tiles where others have videos off and the peer who can't see the videos will not have any network quality information about them sent to others.   Reading Network Quality Information Whenever peer network quality changes an update will be fired to onPeerUpdate(update : HMSPeerUpdate, peer : HMSPeer) of HMSUpdateListener where the update value will be of type HMSPeerUpdate.NETWORK_QUALITY_UPDATED .  > 💡The HMSPeer object represents a person in the call.  It could look something like this:  s id=\"peer-quality_update\" items=  'Kotlin', 'Java'       id='peer-quality_update-0'>   kotlin override fun onPeerUpdate(type: HMSPeerUpdate, hmsPeer: HMSPeer)     when (type)       HMSPeerUpdate.NETWORK_QUALITY_UPDATED ->         // values can be accessed using hmsPeer.networkQuality?.downlinkQuality         val quality = hmsPeer.networkQuality?.downlinkQuality                    </Tab>   id='peer-quality_update-1'>   java   @Override void onPeerUpdate(HMSPeerUpdate type, HMSPeer hmsPeer)     switch (type)       case NETWORK_QUALITY_UPDATED:       HMSNetworkQuality quality = peer.getNetworkQuality();             if(quality  = null)           int networkQuality = quality.getDownlinkQuality();               break;     // ....           </Tab>   Interpreting the Values peer.networkQuality?.downlinkQuality will be a value between 0 and 5.  -1 -> Network check failure  0 -> Very bad network or network check failure  1 -> Poor network  2 -> Bad network  3 -> Average  4 -> Good  5 -> Best"
    },
    {
        "title": "Adaptive Bitrate (Simulcast) ",
        "link": "/android/v2/advanced-features/simulcast",
        "platformName": "Android",
        "objectID": "/android/v2/advanced-features/simulcast",
        "keywords": [],
        "content": "  Simulcast enables  Adaptive Bitrate (../foundation/adaptive-bitrate) (ABR) in video conferencing scenarios. This means 100ms SDKs can upgrade or downgrade video quality for remote tracks based on user preferences or network conditions.   Minimum Requirements   SDK version 2.5.2 or higher   Simulcast enabled in room template   100ms SDK Simulcast APIs You interact with the simulcast feature by setting HMSRemoteVideoTrack 's setLayer function. By default the layer is set to high i.e HMSLayer.HIGH Let's look at the track interface:   kotlin class HMSRemoteVideoTrack   /      Public api to set layer to  HMSLayer.HIGH ,  HMSLayer.LOW ,  HMSLayer.MEDIUM    /  fun setLayer(HMSLayer: HMSLayer) \t/      Public API to get layers definition   /  fun getLayerDefinition() : List<HMSSimulcastLayerDefinition> \t/      Public API to get the current selected HMSLayer   /  fun getLayer() : HMSLayer   data class HMSSimulcastLayerDefinition( \tval resolution: HMSVideoResolution, \tval layer: HMSLayer ) enum class HMSLayer   \tLOW, MEDIUM, HIGH     Here getLayerDefinition() property holds an array of currently available layers for the track and layer property is the currently selected layer. Within HMSSimulcastLayerDefinition you get its resolution and the layer name such as high/medium/low.   Disabling Auto Simulcast You can read more about it  here (https://www.100ms.live/docs/android/v2/features/render-video disabling-auto-simulcast)"
    },
    {
        "title": "Set Track Settings (Video/Audio)",
        "link": "/android/v2/advanced-features/track-settings",
        "platformName": "Android",
        "objectID": "/android/v2/advanced-features/track-settings",
        "keywords": [],
        "content": "  You can customize local peer's Audio & Video track settings while creating instance of 100ms SDK.  These settings are a optional parameter and meant to be passed in the build function as trackSettings parameter which is a HMSTrackSettings object.   kotlin HMSSDK     .Builder(application)     .setTrackSettings(trackSettings) // track setting     ...     .build()    You can set the quality and description of the Audio tracks with setiings like initialState() , setUseHardwareAcousticEchoCanceler() etc   kotlin val trackSettings = HMSTrackSettings.Builder()     .audio(       HMSAudioTrackSettings.Builder()         .setUseHardwareAcousticEchoCanceler(true/false)         .initialState(MUTED/UNMUTED)         .build())    Similarly, for Video tracks you can use like  cameraFacing() , resolution() , forceSoftwareDecoding() etc   kotlin val trackSettings = HMSTrackSettings.Builder()    .video(     .disableAutoResize(true/false)     .forceSoftwareDecoder(true/false)     .initialState(MUTED/UNMUTED)     .build())     cameraFacing Property specifies which camera to open while joining. It can be toggled later on. Default value is set as HMSCameraFacing.FRONT   kotlin HMSCameraFacing.FRONT HMSCameraFacing.BACK     forceSoftwareDecoding This can be used when alot of video is rendered at a single time. It is known that hardware decoder on certain phones don't tend to work well with large grids. This may cause adverse effect like phone heating up, use this flag only when required. Default value is set as false   kotlin HMSVideoTrackSettings.Builder() .forceSoftwareDecoder(true) ...     disableAutoResize The SDK intelligently downscales the resolution when, publisher's bandwidth is flaky or is cpu bound. This resuls in a low resolution to the viewers. But if the viewers are persistent they want highest resolution at all times, then this setting comes in handy. Default value is set as false   kotlin HMSVideoTrackSettings.Builder() .disableAutoResize(true) ...     initialState This keeps the inital state for a particular role or when a new role is assigned. Usecase : user might want to turn on/off before joining the call. Default value is set as HMSTrackSettings.InitState.UNMUTED   kotlin .initialState(HMSTrackSettings.InitState.MUTED) ... HMSTrackSettings.InitState.MUTED/ HMSTrackSettings.InitState.UNMUTED     setUseHardwareAcousticEchoCanceler This setting use's the phone's Acoustic echo Cancellation instead of relying on the SDK's software based implementation. Default value is set as true   kotlin HMSVideoTrackSettings.Builder() .setUseHardwareAcousticEchoCanceler(true) ...   Here's a sample implementation of adding track settings while initializing 100ms SDK    kotlin val hmsTrackSettings = HMSTrackSettings.Builder()     .audio(       HMSAudioTrackSettings.Builder()         .setUseHardwareAcousticEchoCanceler(true)         .initialState(HMSTrackSettings.InitState.MUTED)         .build()     )     .video(       HMSVideoTrackSettings.Builder().disableAutoResize(false)         .forceSoftwareDecoder(true)         .initialState(HMSTrackSettings.InitState.MUTED)         .build()     )     .build() HMSSDK     .Builder(application)     .setTrackSettings(hmsTrackSettings)     .build()   "
    },
    {
        "title": "Stats for WebRTC",
        "link": "/android/v2/advanced-features/webrtc-stats",
        "platformName": "Android",
        "objectID": "/android/v2/advanced-features/webrtc-stats",
        "keywords": [],
        "content": "  Sometimes you need a way to capture certain metrics related to a call. This may be helpful if you want to tailor the experience to your users or debug issues. Typical metrics of interest are audio/video bitrate, round trip time, total consumed bandwidth and packet loss. 100ms SDK provides this data via dedicated delegate callbacks found in HMSStatsObserver . These will be called with a fixed interval of one second after a room has been joined. You can get stats on a per track basis ( fun onRemoteAudioStats(audioStats: HMSRemoteAudioStats, hmsTrack: HMSTrack?, hmsPeer: HMSPeer?) ) or as an overall summary ( fun onRTCStats(rtcStats: HMSRTCStatsReport) ). To begin getting callbacks for statistics, call hmsSDK.addRtcStatsObserver(HMSStatsObserver) with your implementation of HMSStatsObserver . > 💡 Note: Gathering stats takes several ms and can be a large operation if requested on a room with thousands of peers. Consider whether the additional load is necessary. Here is the full list of callbacks:   kotlin /// This callback provides stats for a local audio track. fun onLocalAudioStats(audioStats: HMSLocalAudioStats, hmsTrack: HMSTrack?, hmsPeer: HMSPeer?) /// This callback provides stats for a local video track. fun onLocalVideoStats(videoStats: HMSLocalVideoStats, hmsTrack: HMSTrack?, hmsPeer: HMSPeer?) /// This callback provides stats for a remote audio track. fun onRemoteAudioStats(audioStats: HMSRemoteAudioStats, hmsTrack: HMSTrack?, hmsPeer: HMSPeer?) /// This callback provides stats for a remote video track. fun onRemoteVideoStats(videoStats: HMSRemoteVideoStats, hmsTrack: HMSTrack?, hmsPeer: HMSPeer?) /// This callback provides combined stats for the session. fun onRTCStats(rtcStats: HMSRTCStatsReport)     HMSRTCStatsReport This class will contain the combined stats for the room.   kotlin class HMSRTCStats   \t// Total bytes sent in the current session. \tval bytesSent: Long \t// Total bytes received in the current session. \tval bytesReceived: Long \t// Total packets received in the current session. \tval packetsReceived: Long \t// Total packets lost in the current session. \tval packetsLost: Long \t// Total outgoing bitrate observed since previous report. \tval bitrateSent: Double \t// Total incoming bitrate observed since previous report in Kb/s. \tval bitrateReceived: Double \t// Average round trip time observed since previous report in seconds. \tval roundTripTime: Double   class HMSRTCStatsReport   \t// Combined audio + video values \tval combined: HMSRTCStats \t// Summary of all audio tracks \tval audio: HMSRTCStats \t// Summary of all video tracks \tval video: HMSRTCStats       HMSLocalAudioStats This class contains stats related to local audio track.   kotlin data class HMSLocalAudioStats( \t// Round trip time observed since previous report. \tval roundTripTime: Double?, \t// Total bytes sent by this track in the current session. \tval bytesSent: Long?, \t// Outgoing bitrate of this track observed since previous report in Kb/s. \tval bitrate: Double? ) : HMSStats.HMSLocalStats()     HMSLocalVideoStats This class contains stats related to local video track.   kotlin data class HMSLocalVideoStats( \t// Round trip time observed since previous report. \tval roundTripTime: Double?, \t// Total bytes sent by this track in the current session. \tval bytesSent: Long?, \t// Outgoing bitrate of this track observed since previous report in Kb/s. \tval bitrate: Double?, \t// Resolution of video frames being sent. \tval resolution: HMSVideoResolution?, \t// Frame rate of video frames being sent (FPS). \tval frameRate: Double? ) : HMSStats.HMSLocalStats() data class HMSVideoResolution(  var width: Int,  var height: Int, )     HMSRemoteAudioStats This class contains stats related to remote audio track.   kotlin data class HMSRemoteAudioStats( \t// Packet Jitter measured in seconds for this track. Calculated as defined in section 6.4.1. of RFC3550. \tval jitter: Double?, \t// Total bytes received by this track in the current session. \tval bytesReceived: Long?, \t// Incoming bitrate of this track observed since previous report in Kb/s. \tval bitrate: Double?, \t// Total packets received by this track in the current session. \tval packetsReceived: Long?, \t// Total packets lost by this track in the current session. \tval packetsLost: Int, ) : HMSStats.HMSRemoteStats()     HMSRemoteVideoStats This class contains stats related to remote video track.   kotlin data class HMSRemoteVideoStats( \t// Packet Jitter measured in seconds for this track. Calculated as defined in section 6.4.1. of RFC3550. \tval jitter: Double?, \t// Total bytes received by this track in the current session. \tval bytesReceived: Long?, \t// Incoming bitrate of this track observed since previous report in Kb/s. \tval bitrate: Double?, \t// Total packets received by this track in the current session. \tval packetsReceived: Long?, \t// Total packets lost by this track in the current session. \tval packetsLost: Int? \t// Resolution of video frames being received. \tval resolution: HMSVideoResolution? \t// Frame rate of video frames being received (FPS). \tval frameRate: Double? ) : HMSStats.HMSRemoteStats()     Hierarchy Here's the hierarchy for the sealed classes for stats   kotlin sealed class HMSStats     sealed class HMSLocalStats : HMSStats()   sealed class HMSRemoteStats : HMSStats()     "
    },
    {
        "title": "Release Notes",
        "link": "/android/v2/changelog/release-notes",
        "platformName": "Android",
        "objectID": "/android/v2/changelog/release-notes",
        "keywords": [],
        "content": "description: Release Notes for 100ms Android SDK   v2.5.7  2023-01-20   Added   Send requested_by in acceptChangeRole message   Added CameraControls interface to HMSLocalVideoTrack  Check details about this  here (https://www.100ms.live/docs/android/v2/advanced-features/camera-controls)   Fixed   captureBitmap API of the local HMSVideoView now returns the bitmap at the publishing resolution of local peer   Sometimes HMSTrack was in-correctly removed from HMSPeer if a RemotePeer un-publishes a HMSTrack and re-publishes it again   v2.5.6  2023-01-11   Added   New API in SDK to send Timed Metadata while streaming to hls viewers   Util method added to get HMS SDK and webrtc version   Added joined_at field to Local and Remote peers   AutoSimulcast is enabled by default while using HMSVideoView   Local Stats for all the layers   Fixed   isScreenShared() API always returning false   Multiple setLayer call in HMSVideoView    Changed   onJoin notification will come before onPeerUpdate when user joined a room. Earlier it used to be sent later   Prefer layer is now request response instead of notify   onLocalVideoStats of HMSStatsObserver now has a List of HMSLocalVideoStats instead of single object to accomodate for Simulcast layers   v2.5.5  2022-12-17   Fixed   TRACK_REMOVE update not being sent for screen-share to a new peer who joined the room when screen-share was already in progress   v2.5.4  2022-12-15   Added    Auto Simulcast (https://www.100ms.live/docs/android/v2/migrations/surfaceview-migration)  HMSVideoView when used, changes the simulcast layer based on its size automatically to reduce bandwidth consumption.    Bulk role change (https://www.100ms.live/docs/android/v2/features/change-role bulk-role-change)  A list of roles an be provided to change several peers with that role to a single one with one API call.   Fixed   Only a single update is sent for a local peer when their role is changed instead of two.   Deprecated   The method changeRole is now changeRoleForPeer . No functional changes.   v2.5.3  2022-12-7   Added   HMSVideoView  Easy to use abstraction over SurfaceViewRenderer   Fixed   Simulcast bug where the layers are not added in the HMSRemoteVideoTrack   Fix for audio capture not picking the correct sampling rate of mic automatically   v2.5.2  2022-11-25   Added   Simulcast.   Quality limitation reasons on the local video track.   Extra Analytics.   Current and past sdks from 2.4.0 are also deployed on Maven Central along with jitpack.   Fixed   A bug where degraded tracks would not recover in a certain situation.   Consistent naming for HLS player stats across platforms.   v2.5.1  2022-10-28   Added   Updated webrtc to fix hardware simulcast resolution not changing between layers.   Screenshare now gets unpublished if the role of the peer is changed to one that can't.   Changed   Messages that are sent via REST api now arrive with a null sender peer, where previously they were not sent at all. senderPeer in HMSMessages can be null now.   v2.5.0  2022-10-14   Added   SFN module that can be used to receive periodic player stats about HLS playback   Option in HMSVideoTrackSettings to force device to use software decoder   v2.4.9  2022-09-30   Added   Session Metadata API(alpha version)   Added better Telemetrics for analytics   Initial states(Muted/Unmuted) for local video and audio track can be mentioned while joining using the HMSVideoTrackSettings and HMSAudioTrackSettings in the builder of HMSSDK    Changed   Deprecated ROOM_NAME_UPDATED and ROOM_ID_UPDATED types of HMSRoomUpdate as they are no longer used   Updated WebRTC to m104 from m97   Fixed   SDK calling leave() on being removed from the room leading to peer.leave.failure webhook along with peer.leave.success   LetterBoxing(Black borders on top and bottom) observed when sharing screen in landscape mode   NPE fix in performance measurement while calling join and no websocket connection   Send offline analytics events to the right environment   v2.4.8  2022-08-12   Fixed   removeSink will only stop the download of a videoTrack if all the sinks attached to it are removed   Stop camera capturer on calling leave on Preview page   Setting the correct audio output device when a Bluetooth headset is in connecting state   v2.4.7  2022-07-28   Fixed   Calling switchCamera API leads trigerring of onSuccess callback twice   onRoomUpdate with type HMSRoomUpdate.ROOM_PEER_COUNT_UPDATED not getting called when peer count changes in the room   Peer not able to publish tracks when updated to webrtc from HLS if rejoins after a reconnection in webrtc Mode    Changed   HMSHLSConfig is now an optional parameter while calling startHLSStreaming and stopHLSStreaming   The meetingUrl parameter while creating HMSHLSMeetingURLVariant instance for HMSHLSConfig is optional now. If nothing is provided HMS system will take the default meetingUrl for starting HLS stream   changeRoleForce permission in HMSRole is now removed and no longer used   recording permission in HMSRole is now broken into  browserRecording and rtmpStreaming   streaming permission in HMSRole is now hlsStreaming   v2.4.6  2022-07-15   Fixed   Checking for targetApiVersion along with Device SDK version while initializing Bluetooth manager   v2.4.5  2022-07-14   Fixed   peer_list_time calculation   Audio device switch issues   Bluetooth issues in HLS mode   Removed additional Bluetooth permission in SDK for android 30 and above    Added   Join response time to analytics   Support to enable or disable webrtc's AutoResize option while publishing video track   v2.4.4  2022-07-07   Fixed   Web-Socket reconnection not working for non webrtc peers   v2.4.3  2022-07-01   Fixed   cameraFacing field in HMSVideoTrackSettings not getting updated on calling switchCamera API   Issue of 'peer is not joined' error while joining and SDK reconnection   Automatic proguard rules were fixed. No app-side proguard configuration is required for production builds with 100ms.    Added   APIs to stream device audio in different modes   APIs to view and change the output speaker selected by the SDK to playout   Analytics events to measure retry counts of JOIN calls   v2.4.2  2022-06-22   Fixed   Webrtc not able to find Camera from Android Camera Manager leading to CantAccessCaptureDevice error while calling preview or join API   Assertion Crash in Network Monitor while trying to create new instane of HMSSDK while a call is in progress   Chances of Multiple socket connection getting created incase there is network disconnection in preview screen and join gets called during that time    Added   setAudioMode API to change the Audio out mode manually between in-call volume and media volume   Retry on 5xx and 429 errors in all RPC calls   Analytics events to measure peer-list time   Join optimization by not sending offer for non webrtc peers    Changed   Made TrackError while switching camera non-terminal   Removed recreation of Lists happening in SDK, leading to slowness while calling certain APIs in big rooms. Affected APIs by this change are : getPeers() , getRemotePeers() , getPlugins()  now return List instead of Array   v2.4.1  2022-06-14   Fixed   Made SDK compatible for compileSDKversion 30 by reducing version of Room lib to 2.3.0    Added   Analytics events to measure join and preview time   Retry on 5xx errors in internal join message   v2.4.0  2022-06-09   Fixed   Update the width and height of screen-share video track being published based on the rotation of device   Crash fix on Android 12 if app does not provide Bluetooth Connect permission. SDK now sends a non-terminal error   Crash fix on Android 12 on calling the HMSScreenCaptureService.getStopScreenSharePendingIntent API while starting screenshare   Un-necessarily sending HLS_RECORDING_STATE_UPDATED in onRoomUpdate callback when HLS Streaming is enabled    Changed   Socket close from cancel, which ensures the leave api does not wait for server response hence initiating internal teardown immediately    Added   Offline storage of Analytics event. Events that cannot be send immediately via websockets would be persisted in a db and will be sent later when connection returns   Remove height/width check for rtmp and browser recording in SDK   RemoveRtcStatsObserver API is added   v2.3.9  2022-05-18   Fixed   Screen Share not starting in highest resolution   Serverside Subscribe degradation fix  marking track not degraded after removeSink is called   IO Exception Crash in Android 12, in the alarm manager of Log Util   NPE Crash fix in webrtc stats monitor   Crash in LogUtils    Changed   serverReceiveTime data type in HMSMessage is not of type Long instead of Date   v2.3.7  2022-05-09   Fixed   A crash in some rare cases where analytics events were being sent while the room was ending.    Added   sessionId is a new parameter in HMSRoom. It is updated with the current sessionId when a room is joined.   2.3.6  2022-04-28   Fixed   Fixed bug: Local peer being null in stats callbacks   Fixed bug: onSuccess callback not called from preview / join if network goes off BEFORE ws-connect and comes back after sometime    Added   Server Side Subscribe Degradation   2.3.5  2022-04-20   Fixed   Crash fix: InvalidTokenFormat exeception. This would be thrown in the onError callback instead   Fixed bug: Sending TRACK_DEGRADED when not required in some cases    Added   Apps can now add resolution(w h) when starting RTMP Streaming   SDK has the capability to write logs in the filesystem of devices. Apps can enable or disable while initializing the SDK   2.3.4  2022-03-28   Fixed   HLS recording status wasn't always updated when stopped.   Rare crash when cameras are unavailable and it seemed to the app like none exist.    Added   Network Quality in preview.  Network quality (../features/network-quality-reports) reports can now be requested at the preview screen. Use the returned value to determine if you should suggest people's internet is too slow to join with video etc.   Network Quality during calls.  Peer Network Quality (../advanced-features/peer-network-quality) updates are now received during the call. Use this to show how strong any peer's internet is during the call.   2.3.3  2022-03-17   Fixed   Leave not finishing if SDK is in reconnection state. Hence all join calls after that was getting queued up if called on the same HMSSDK instance   Improved subscribe degradation so that new add sinks are handled properly when SDK is already in degraded state   Crash fix on starting/stopping HLS where HlsStartRecording was null    Added   Added Hls Recording to initial PeerList   Ignore room-state updates after joining   2.3.2  2022-03-14   Fixed   ERROR Analytics events not being sent   2.3.1  2022-02-25   Fixed   NPE in OnPeerUpdateManager when sometimes room-sate message comes before policy-change message. Ignoring the first room-sate message in such cases   2.3.0  2022-02-23   Added   HlsRecordingConfig to HMSHLSConfig object   onPeerUpdate and onRoomUpdate callbacks in 'HMSPreviewListener' to get info about the room at Preview screen   Server will send a timeout error if a peer stays on preview page for too along   Calling preview API again without a leave call would result in an error    Changed   Synced leave function   Make leave API call to server sync instead of async    Fixed   Socket is now cancelled instead of close, which was causing queued messages to be processed after  teardown leading to crashes and Failed to set remote offer sdp: Called in wrong state: closed error   NPE in sendMessage API   2.2.9  2022-02-08   Added   Local track updates like mute/unmute will be send as well via onTrackUpdate   Added startedAt and stoppedAt field for Browser and SFU recording    Changed   Upgraded kotlin version to 1.6.10   Upgraded webrtc version to m97   customerUserID can be null    Fixed   Crash Fix when joining without mic permission   Stop screenshare on calling leave() API if screenshare is going on   Potential fix for ClassCastException on accesing localPeer while publishing   Fix for HMSException: Network connection lost crash when connection failed while joining   Improved time taken for YUV conversion for virtual background   Fixed background image overlay   2.2.8  2022-01-24    Added   Added APIs to support of Virtual Background   Use the following dependency along with main 100ms SDK dependency to support VB plugin         implementation 'com.github.100mslive.android-sdk:virtualBackground:2.2.8'        Changed   Gradle Dependency to add core 100ms SDK features has changed to following         implementation 'com.github.100mslive.android-sdk:lib:x.x.x'       2.2.7  2022-01-18    Changed   Downgrading androidx.core:core-ktx to 1.6.0 to support apps with compile sdk version < 31.   2.2.6  2022-01-14    Added   Added analytics to collect whether device is using h/w or s/w AEC   Added isTerminal property in HMSException  Use this property to determine whether peer can continue in the room or not   Added analytics to collect info about could not find FRONT camera issue   Added analytics events for phone call interruptions   Queue-ing analytics events which maybe getting missed as websocket not present    Fixed   Bug fix  All Remote peers get muted for local peer if the local peer receives a phone call at a moment when there is no one else in the room   onReconnection will not be send continuously & will be sent only in the start. It was being sent continously while SDK was retrying.   NPE crash fix in OnPeerUpdateManager and SDKStore.getRoom   2.2.5  2021-12-29    Fixed   OOM Crash due to printing of SDKStore object in HMSSpeaker   Clear local track mute status when the sdk is cleared   Fix for java.lang.IllegalStateException: No camera device found for FRONT crash  Now the peer will be able to join the room but with no videoTrack    Added   An optional HMSActionResultListener is added to switchCamera API  errors will be sent back to app if some errors come up   Send the local Screenshare Track to app in onTrackUpdate   Added new analytics event videopresence.failed for cases where SDK cannot find a camera to capture    Changed   Device permission denied errors are not terminal errors anymore.   Clients will get a onJoin or onPreview callback even if audio or video tracks cannot be created by the SDK due to some issues in the capture device.   onError callbacks will also be called along with the above callbacks   2.2.4  2021-12-20    Fixed   RTMP updates are fired once server notifies   switchCamera API does not need a Coroutine scope to be called from    Added   APIs to start/stop HLS Streaming (beta version)   HLS_STREAMING_STATE_UPDATED enum in HMSRoomUpdate to notify changes about HLS streaming status in HMSRoom   Beta version of getStats() API   startScreenShare API now take an optional Notification from app which can be used to stop the screenshare   2.2.3  2021-12-03    Fixed   UninitializedPropertyException crash while calling acceptChangeRole due to socket not getting initialized   Permission Errors are non terminal now    Added   Subscribe Degradation events being sent to analytics   2.2.2  2021-11-25    Added   changeMetadata API to change the metadata in HMSPeer . This change will be updated to all and will be persistent    Fixed   HMSRoomUpdate will be trigerred for the local peer who initiated a start or stop Recording   The room's Recording status will be checked upon reconnection of a peer   Screenshare cannot be unpublished while in reconnection mode   setVolume() API on RemoteAudioTrack can now take in value from 0.0 to 10.0    Fixed   Analytic fixes   Fix issue in self role change which was calling roleChange twice   Remove token from URL Params   2.2.0  2021-11-19    Fixed   Analytic fixes   Fix issue in self role change which was calling roleChange twice   Remove token from URL Params    Added   Add Audio Recording errors to Analytics   Add more instrumentation tests   2.1.9  2021-11-11    Added   Beta version of screenshare API   Leave API now takes in an optional parameter to return success and error callbacks    Fixed   Class cast exception crash fix if End Room API was called by server where requested_by peer is null   2.1.8  2021-11-03    Added   getRoom API to HMSSDK which will return a HMSRoom instance that the user has joined, NULL otherwise    Fixed   Crash fix  IllegalArgumentException: Receiver not registered on calling leave immediately after joining   Already JOINED HMSException will now be coming in onError of HMSUpdateListener instead of a throwing a fatal Exception   RTMP Recording fixes   2.1.7  2021-11-01    Changed The timeouts for all websocket operations on the server have been increased from 5 seconds to 11 seconds. If a join was failing before for lack of feedback, it has a larger margin within which to succeed now.   2.1.6  2021-10-28    Breaking Changes  requestedBy: HMSPeer in HMSChangeTrackStateRequest is now nullable. It could be mute if the person tried to mute or unmute someone and immediately left the room.    Fixed   Check for Role change of remote peers after reconnection   Return correct value of isPlaybackAllowed   HMSTrack is null for local peer in HMSSpeaker   onSuccess callback for the acceptChangeRequest method was never being called. This will now be called appropriately.   This request can safely be ignored because the SDK already muted the peer but may be helpful in recognizing that the mute came externally.   A check like the following is now required in your onChangeTrackStateRequest :   kotlin       if (details.track.isMute  = details.mute)          // Process ChangeTrackMuteRequest              Added   Added customerId and description to HMSPeer's tostring().   HMSChangeTrackStateRequest is now sent even when peers are muted. Previously, if a remote peer requested a peer to mute, there would be no ChangeTrackStateRequest   The way packet loss is measured for track degradation is changed   2.1.5  2021-10-27    Fixed   Crash fix for \"MediaStream has been disposed\"  that could happen when multiple peers leave and join a large room   2.1.4  2021-10-18    Fixed   Videos send by Pixel 3 device are distorted   UninitializedPropertyAccessException crash fix on leave   Using subscribe degradation as set on the local peer's role    Added   HMSTrack obj in HMSSpeaker   2.1.3  2021-10-04    Fixed   Reconnection improvements  reconnection time has reduced   Analytics fix    Added   Ability to start/stop a RTMP Streaming or Recording   Ability to change the mute status of one or many remote HMSTrack for all peers of a particular role, or all tracks of a particular source, type or source AND type.   Ability to use either software or hardware acoustic echo cancellation from the application layer    Updated   Updated to latest m94 release of webrtc   2.1.2  2021-09-20    Fixed   Donot degrade videos on reconnection   SendTrack updates after reconnection if a new peer joins while SDK is reconnection   Muting new tracks that are getting added when SDK mutes all remote tracks on interruption   2.1.1  2021-09-10    Fixed   SDK Reconnection on network disconnection improvement   NPE Crash fix on preview screen in case of SDK reconnection   Fix: onPreview getting was called twice incase of SDK reconnection   2.1.0  2021-09-03    Fixed   Subscribe Degradation improvements   sendBroadcast API now accepts any type of HMSMessageType   Concurrent modification Exception Crash fix   2.0.9  2021-08-27    Added   Support for self role change    Changed   Updated webrtc lib to latest   Bug fix(can reuse the same instance of hmsSDK to rejoin a room)   Few memory leak   IMP > With this release no need to add any dependancy to webrtc library. Anyone migrating from previous versions, PLEASE REMOVE the webrtc dependancy from your app's build.gradle   2.0.8  2021-08-17    Added   Sending private and group messages APIs   Remove peer from a room API   Remote mute peer API   End room API    Changed   send(message:) API renamed to sendBroadcastMessage   2.0.7  2021-08-06    Fixed   Bug fixes in subscribe degradation   Bug fixes in role change   Bug fixes in onReconnection handling   Bug fix in incoming phone call handling   2.0.6  2021-07-27    Fixed   Bugs in change role    Added   Support for subscribe degradation   2.0.5  2021-07-20    Added   HMSSpeaker contains HMSPeer instead of peerId   Support for changing of roles   2.0.4  2021-07-09    Fixed   Crash fixes on leave and network change   Audio Routing to earpiece on interruption   No audio in BT headset on connecting disconnecting error   Check if role allows publishing in preview   Release camera on leave    Added   SDK will handle all interruptions like incoming voice and VoIP calls   Pass HMSRole to client via HMSPeer   2.0.2  2021-06-30    Fixed   Made hmsSDK.getLocalpeer() nullable   Fix logging bug   Fix bug where ontrackUpdate came before RTCTrack   2.0.1  2021-06-29    Added   Retry when join call returns server error in 5xx or 429   isTerminal property to analytics    Fixed   NPE Crash on SDKStore.getLocalPeer()   Unsupported notification message crash   Call onError callback in webrtc subscription failure   2.0.0-beta02  2021-06-24    Added   Preview API   Enabled Opus dtx for audio   Added retry to INIT api call   2.0.0-beta01  2021-06-16    Added   Proper error messages from client and server   Handling of bad network conditions by retrying inside SDK   Audio Manager is part of the SDK   Callbacks for onReconnecting and onReconnected   Added Analytics to track issues   2.0.0-alpha04  2021-06-04    Added   Simplified SDK interfaces that can power a fully-featured video conferencing application     join     leave     listeners for tracks/peers/other data   Features not covered in this release     Handling network disconnections (disconnect from wi-fi to 4g ends the call )     Handling subscription network degradations     Handling second order publish network degradations     Detailed analytics     Recording   Please contact @akash goswami in your Slack channel if you have any questions"
    },
    {
        "title": "Writing HmsLogs in Local Storage",
        "link": "/android/v2/debugging/log_utils",
        "platformName": "Android",
        "objectID": "/android/v2/debugging/log_utils",
        "keywords": [],
        "content": "    Save Logs from SDK in Local Storage Logs from hmsSDK can be saved as a file in local storage by setting the settings from HmsLogSettings in Builder of hmsSDK instance. In this , whenever there is a log printed from hmsSDK it will be saved in a seperate file.  > 💡 Each new meeting will have a seperate session log with file name as session-log-$ room_id  file, where \"$ room_id \" is a placeholder    Implementation Details HmsLogSettings must be passed as a parameter in hmsSdk builder object during instantiation. Params:  maxDirSizeInBytes  max dir size in bytes to be maintained for /logs default  set to 10mb. <br   > 💡 Note : if the directory size exceeds the maxDirSizeInBytes provided by client, then it will delete the oldest files next time user starts a meeting to make room for new ones. <br  <br   A background service is also started once a day to make sure that size of directory never exceeds the desired size.  isLogStorageEnabled  set to true if /logs are to be saved else false default  false. <br   level  set log level for logs to observe default  HMSLogger.Debug <br     s id=\"save-log-dir\" items=  'Kotlin', 'Java'       id='save-log-dir-0'>   kotlin    private val hmsLogSettings : HMSLogSettings = HMSLogSettings(LogAlarmManager.DEFAULT_DIR_SIZE,true)   val hmsSDK = HMSSDK     .Builder(application)     .setLogSettings(hmsLogSettings)     .build()   </Tab>   id='save-log-dir-1'>   java    HMSLogSettings hmsLogSettings = new HMSLogSettings(LogAlarmManager.DEFAULT_DIR_SIZE,true, HMSLogger.LogLevel.DEBUG);   HMSSDK hmssdk = new HMSSDK.Builder(application)   .setTrackSettings(hmsTrackSettings)   .build();   </Tab> > 💡 whenever there is a crash in hmsSDK , it will be logged in the file named as crash-log-$ room_id  where $ room_id  is a placeholder.   Fetching the Log Directory From SDK It could look something like this:   kotlin import live.hms.video.utils.LogUtils     s id=\"fetch-log-dir\" items=  'Kotlin', 'Java'       id='fetch-log-dir-0'>   kotlin    val dir : File = LogUtils.getDirPath(context = context)   </Tab>   id='fetch-log-dir-1'>   java    File dir = LogUtils.getDirPath(context)   </Tab>"
    },
    {
        "title": "Audio Mode Change\r",
        "link": "/android/v2/features/audio-mode",
        "platformName": "Android",
        "objectID": "/android/v2/features/audio-mode",
        "keywords": [],
        "content": " -\r API to change Audio Mode manually. The most common use-case would be toggle between (in-call volume/media volume) for in call volume  AudioManager.MODE_IN_COMMUNICATION  media volume  AudioManager.MODE_NORMAL \r \r  Switch audio mode\r \r  kotlin\r fun setAudioMode(audioMode : Int)\r  \r \r \r   Types of audio mode.  Audio Modes in Android (https://developer.android.com/reference/android/media/AudioManager MODE_IN_COMMUNICATION) \r   MODE_NORMAL  Normal audio mode: not ringing and no call established. (common one )\r   MODE_RINGTONE  Ringing audio mode. An incoming is being signaled.\r   MODE_IN_CALL  In call audio mode. A telephony call is established.(common one )\r   MODE_IN_COMMUNICATION  In communication audio mode. An audio/video chat or VoIP call is established.\r   MODE_CALL_SCREENING  Call screening in progress. Call is connected and audio is accessible to call screening applications but other audio use cases are still possible\r \r invoke this method with appropriate option, to switch audio mode manually\r \r \r  s id=\"switch-device\" items=  'Kotlin', 'Java'    \r \r   id='switch-device-0'>\r \r  kotlin\r     hmsSDK.setAudioMode(AudioManager.MODE_NORMAL)\r  \r </Tab>\r   id='switch-device-1'>\r \r  java\r     hmsSDK.setAudioMode(AudioManager.MODE_NORMAL);\r  \r \r </Tab>"
    },
    {
        "title": "Audio Output Routing\r",
        "link": "/android/v2/features/audio-output-routing",
        "platformName": "Android",
        "objectID": "/android/v2/features/audio-output-routing",
        "keywords": [],
        "content": " -\r \r Audio Routing can be helpful in the case when user wants to switch output to device other than the default focussed one by SDK.\r \r  Switch audio focus to another device\r \r \r  kotlin\r fun switchAudioOutput(audioDevice: AudioDevice)\r  \r \r \r   Types of audio routing supported.\r   SPEAKER   route audio to device speaker\r   EARPIECE  route audio to device earpiece\r   WIRED    route audio to connected wired device\r   BLUETOOTH  route audio to connected bluetooth device\r   AUTOMATIC  automatic routing\r \r invoke this method with appropriate option , to switch the output to that device\r \r audioDevice is the  AudioDevice  type through which audio should be routed.\r \r  s id=\"switch-device\" items=  'Kotlin', 'Java'    \r \r   id='switch-device-0'>\r \r  kotlin\r     hmsSDK.switchAudioOutput(AudioDevice.SPEAKER_PHONE)\r  \r </Tab>\r   id='switch-device-1'>\r \r  java\r     hmsSDK.switchAudioOutput(AudioDevice.SPEAKER_PHONE);\r  \r \r </Tab>\r \r  Adding a listener\r \r  kotlin\r fun setAudioDeviceChangeListener(audioManagerDeviceChangeListener: AudioManagerDeviceChangeListener)\r  \r add a listener to get updates whenever a device is removed or attached to the device.\r audioManagerDeviceChangeListener  listener to be passed to be able to listen to device updates.\r \r callback methods : \r 1. onAudioDeviceChanged  this method is invoked when output device has changed.\r 2. onError  this method is invoked when there is an error with switching to output device.\r \r \r  s id=\"device-change-listener\" items=  'Kotlin', 'Java'    \r \r   id='device-change-listener-0'>\r \r  kotlin\r \r     hmsSDK.setAudioDeviceChangeListener(object :\r       HMSAudioManager.AudioManagerDeviceChangeListener  \r       override fun onAudioDeviceChanged(\r         device: HMSAudioManager.AudioDevice?,\r         listOfDevices: MutableSet<HMSAudioManager.AudioDevice>?\r       )  \r        \r        \r \r       override fun onError(error: HMSException?)  \r         HMSLogger.d(TAG, \"error : $ error?.description \")\r        \r      )\r \r  \r </Tab>\r   id='device-change-listener-1'>\r \r  java\r \r \r     hmsSDK.setAudioDeviceChangeListener(new HMSAudioManager.AudioManagerDeviceChangeListener()  \r       @Override\r       public void onAudioDeviceChanged(HMSAudioManager.AudioDevice audioDevice, Set<HMSAudioManager.AudioDevice> listOfDevices)  \r         \r        \r \r       @Override\r       public void onError(HMSException e)  \r \r        \r      );\r \r \r  \r \r </Tab>\r \r \r  Get current focussed device\r \r  kotlin\r fun getAudioOutputRouteType() : AudioDevice\r  \r \r invoke this method get current device type through which audio output is being routed.\r \r  s id=\"fetch-current-device\" items=  'Kotlin', 'Java'    \r \r   id='fetch-current-device-0'>\r \r  kotlin\r     hmsSDK.getAudioOutputRouteType()\r  \r </Tab>\r   id='fetch-current-device-1'>\r \r  java\r     hmsSDK.getAudioOutputRouteType();\r  \r \r </Tab>\r "
    },
    {
        "title": "Audio Share (Beta)\r",
        "link": "/android/v2/features/audio-share",
        "platformName": "Android",
        "objectID": "/android/v2/features/audio-share",
        "keywords": [],
        "content": " -\r \r This feature is the analog of screen capture, but for audio.\r There may be cases where the application needs to stream music which is either stored in the device locally or from some other app present on the device in the room where the peer is joined.\r \r Examples of such use cases can be a FM like application where the host would want to stream music while also interacting with others in the room or a host in a gaming app who would want to stream music from their device in the room along with their regular audio track.\r \r  How does audio share work\r \r > The Audio share option only works in Android 10 and above.\r \r 100ms SDK uses the  MediaProjection (https://developer.android.com/guide/topics/media/av-capture capture_audio_playback) APIs of Android to capture the device audio and stream it along with the user's regular audio track. To achieve this SDK starts a foreground service\r and starts capturing the device audio and mixes the bytes with the data collected from mic, so that the stream contains both system music and mic data.\r \r This API gives apps the ability to copy the audio being played by other apps which have set its usage\r to USAGE_MEDIA, USAGE_GAME, or USAGE_UNKNOWN. (Audio from apps like YouTube etc can be captured)\r \r  How to stream device audio from the app (without a custom notification)\r \r To start streaming device audio , app needs to call the startAudioshare method of HMSSDK , which takes in three parameters -\r The first one is HMSActionResultListener which is a callback object needed to inform about success \r or failure of the action\r The second one is an Intent which is the result data of MediaProjection permission activity;\r the calling app must validate that result code is Activity.RESULT_OK before\r calling this method.\r The third one is one of the modes of type AudioMixingMode in which the user wants to stream. This can be one out of the three available types -\r TALK_ONLY : only data captured by mic will be streamed in the room\r TALK_AND_MUSIC: data captured by mic as well as playback audio being captured from device will be streamed in the room\r MUSIC_ONLY: only the playback audio being captured from device will be streamed in the room\r \r Following is the snippet on how to use this:\r \r  s id=\"startaudioshare\" items=  'Kotlin'    \r \r   id='startaudioshare-0'>\r \r  kotlin\r // Define a result Launcher\r  var resultLauncher = registerForActivityResult(ActivityResultContracts.StartActivityForResult())   result ->\r   if (result.resultCode == Activity.RESULT_OK)  \r    val mediaProjectionPermissionResultData: Intent? = result.data\r    // Pass this intent to hmsSDK\r    hmsSDK.startAudioshare(object : HMSActionResultListener  \r     override fun onError(error: HMSException)  \r      // an error occurred\r      \r \r     override fun onSuccess()  \r      // started successfully\r      \r \r      ,mediaProjectionPermissionResultData, AudioMixingMode.TALK_AND_MUSIC)\r    \r   \r \r   // Get the MEDIA_PROJECTION_SERVICE and launch the result launcher\r   val mediaProjectionManager: MediaProjectionManager? = requireContext().getSystemService(\r         Context.MEDIA_PROJECTION_SERVICE\r        ) as MediaProjectionManager\r   resultLauncher.launch(mediaProjectionManager?.createScreenCaptureIntent())\r \r  \r \r It is important to note that this API will not work if the resultCode is NOT Activity.RESULT_OK .\r SDK will also start a foreground service with a notification when it starts capturing the audio of the device.\r \r  How to add a custom notification\r \r To take a look at how this is implemented in our sample app, click  here (https://github.com/100mslive/100ms-android/blob/3590777bf0a3677cb9b5001034ea9ed7087f9d90/app/src/main/java/live/hms/app2/ui/meeting/MeetingViewModel.kt L860).\r \r hmssdk.startAudioShare also takes an optional fourth parameter of type notification. See how to build a custom notification in the android docs  here (https://developer.android.com/training/notify-user/build-notification).\r \r It could look something like this:\r Since this is a standard Android notification the full range of customization is available.\r \r  kotlin\r \r  fun startScreenshare(mediaProjectionPermissionResultData: Intent?, actionListener: HMSActionResultListener)  \r   // Without custom notification\r   // hmsSDK.startAudioshare(actionListener ,mediaProjectionPermissionResultData, AudioMixingMode.TALK_AND_MUSIC)\r \r   // With custom notification\r   val notification = NotificationCompat.Builder(getApplication(), \"ScreenCapture channel\")\r    .setContentText(\"Screenshare running for roomId: $ hmsRoom?.roomId \")\r    .setSmallIcon(R.drawable.arrow_up_float)\r    .build()\r \r   hmsSDK.startScreenshare(actionListener, mediaProjectionPermissionResultData, notification, AudioMixingMode.TALK_AND_MUSIC)\r   \r \r  \r \r </Tab>\r \r  How to change mode\r \r To change the mode the user is streaming audio, call the setAudioMixingMode API and pass one of the modes out of\r TALK_ONLY or TALK_AND_MUSIC or MUSIC_ONLY \r \r Note that TALK_ONLY mode is equivalent to regular mode, that is without starting this API\r \r  How to stop audio sharing\r \r To stop capturing device audio and streaming into the room, call the stopAudioShare API and provide a HMSActionResultListener \r to listen to the success or error callbacks.\r \r  s id=\"stopaudioshare\" items=  'Kotlin'    \r \r   id='stopaudioshare-0'>\r \r  kotlin\r \r    hmsSDK.stopAudioshare(object : HMSActionResultListener  \r     override fun onError(error: HMSException)  \r      // an error occurred\r      \r \r     override fun onSuccess()  \r      // stopped successfully\r      \r \r      )\r \r  \r \r </Tab>\r \r > It is advisable to call leave API from the onDestroy() of the activity / fragment that\r \r   started the sharing of device audio otherwise the foreground service will be running if even user kills the\r   app from the Recents tab.\r \r > DONOT forget to add the following permission for foreground service\r \r  kotlin\r \r   <uses-permission android:name=\"android.permission.FOREGROUND_SERVICE\"  \r \r  \r "
    },
    {
        "title": "Auto Video Degrade/Restore\r",
        "link": "/android/v2/features/auto-video-degrade-restore",
        "platformName": "Android",
        "objectID": "/android/v2/features/auto-video-degrade-restore",
        "keywords": [],
        "content": " -\r \r Sometimes people have bad internet connections but everyone deserves a good meeting.\r \r When the network is too slow to support audio and video conversations together, the 100ms SDK can automatically turn off downloading other peer's videos, which may improve the audio quality and avoid disconnections.\r \r If the network quality improves, the videos will be restored automatically as well.\r \r To turn on subscribe degradation in your room, open the  templates (https://dashboard.100ms.live/templates) in the dashboard and enable it for roles there. Here's more information about  templates (/android/v2/foundation/templates-and-roles).\r \r  Responding in the app\r \r All HMSVideoTracks , within the HMSPeer 's have a variable called isDegraded .\r \r If isDegraded is true, in the UI, treat it as if the video is turned off. If the UI is not changed, the video tile will appear black.\r \r An important note here is that addSink and removeSink still need to be called on the video even if it's degraded. If this isn't done:\r 1. Subscribe degradation will not know how many videos are supposed to be shown and videos may degrade, never to be restored again.\r 2. In the case where a video was on screen and then was degraded, remove sink may not be called for it depending on your code. If this were to happen and the video was restored later you would end up downloading multiple more videos than what are being shown.\r "
    },
    {
        "title": "Change Role\r",
        "link": "/android/v2/features/change-role",
        "platformName": "Android",
        "objectID": "/android/v2/features/change-role",
        "keywords": [],
        "content": " -\r \r Role is a powerful concept that takes a lot of complexity away in handling permissions and supporting features like breakout rooms.  Learn more about roles here. (../foundation/templates-and-roles)\r \r Each HMSPeer instance has a role property which returns an HMSRole instance. You can use this property to do following:\r \r 1. Check what this role is allowed to publish. i.e can it send video (and at what resolution)? can it send audio? can it share screen? Who can this role subscribe to? (For example student can only see the teacher's video) This is can be discovered by checking publishSettings and subscribeSettings properties.\r 2. Check what actions this role can perform. i.e can it change someone else current role, end meeting, remove someone from the room. This is can be discovered by checking the permissions property.\r \r > 💡 Anyone with the role change permission can change anyone's role. Including their own role. As of version 2.0.9 of the Android SDK.\r \r In certain scenarios you may want to change someone's role. Imagine an audio room with 2 roles \"speaker\" and \"listener.\" Only someone with a \"speaker\" role can publish audio to the room while \"listener\" can only subscribe. Now at some point \"speaker\" may decide to nominate some \"listener\" to become a \"speaker.\" This is where the changeRoleOfPeer API comes in.\r \r You may choose to either:\r 1.  Single Peer Role Change ( single-peer-role-change): Change the role of a single peer to a specified one.\r 2.  Bulk Role Change ( bulk-role-change): Change the role of all peers with a certain role, to a specified one.\r \r  Single Peer Role Change\r \r To invoke the API you will need 4 parameters.\r \r   peer : An instance of HMSPeer of the peer who's role you want to change.\r   toRole : The HMSRole instance for the target role.\r   force : Whether you want to change their role without asking them or give them a chance to accept/reject.\r   hmsActionResultListener : The HMSActionResultListener that will get a success or failure callback depending on the result of the request. Note: success doesn't mean that the role was changed, just that the server accepted the request as valid.\r \r All the peers that are in the current room are accessible via getPeers method of HMSSDK instance after a successful room join.\r \r A list of all available roles in the current room can be accessed via the getRoles method of HMSSDK .\r \r Once you have all you can invoke:\r \r  s id=\"change-role-single\" items=  'Kotlin', 'Java'    \r \r   id='change-role-single-0'>\r \r  kotlin\r fun changeRoleOfPeer(forPeer: HMSPeer, toRole: HMSRole, force: Boolean)\r   hmsSDK.changeRoleOfPeer(peer, toRole, force, object : HMSActionResultListener  \r     override fun onSuccess()  \r       // The request was sent successfully\r      \r \r     override fun onError(error: HMSException)  \r       // There was an error.\r      \r    )\r  \r  \r \r </Tab>\r \r   id='change-role-single-1'>\r \r  java\r public void changeRoleOfPeer(HMSPeer peer, HMSRole toRole, Boolean force)  \r   hmsSDK.changeRoleOfPeer(peer, toRole, force, new HMSActionResultListener()  \r     @Override\r     public void onSuccess()  \r       // The request was sent successfully\r      \r \r     @Override\r     public void onError(@NonNull HMSException e)  \r       // There was an error\r      \r    );\r  \r  \r \r </Tab>\r \r \r \r If the peer accepted the request and their role changed, you will get an update in the HMSUpdateListener :\r \r  kotlin\r fun onPeerUpdate(type: HMSPeerUpdate, peer: HMSPeer)\r  \r \r with the the same peer you passed as targetPeer and a ROLE_CHANGED update type.\r \r The force parameter in changeRole , when false, is basically a polite request: \"Would you like to change you role from listener to speaker?\" which can be ignored by the other party. The way it works is the other party will first receive a\r \r  kotlin\r fun onRoleChangeRequest(request: HMSRoleChangeRequest)\r  \r \r callback in HMSUpdateListener . At which point the app can choose to show a prompt to the user asking for permission. If the user accepts, app should call\r \r  kotlin\r hmsSDK.acceptChangeRole(hmsRoleChangeRequest)\r  \r \r with the same request which it received in onRoleChangeRequest which completes the changeRole loop. Both parties will receive a roleUpdated callback so that they both can do necessary UI updates. Now the user actually becomes a speaker and the audio publishing will start automatically.\r \r Now lets imagine the newly nominated speaker is not behaving nicely and we want to move him back to listener without a prompt. This is where the force parameter comes in. When it is set to true the other party will not receive a confirmation roleChangeRequest but instead will straight away receive a new set of updated permissions and stop publishing. roleUpdated callback will still be fired so that the app can update the user's UI state.\r \r \r  Bulk Role Change\r Bulk role change is used when you want to convert all roles from a list of roles, to another role.\r \r For example if peers join a room with a waiting role and you want to change them all to viewers then you'd use this API.\r \r It takes fewer parameters than for a single peer. Here is the method signature.\r \r  kotlin\r fun changeRoleOfPeersWithRoles(ofRoles: List<HMSRole>,\r                 toRole: HMSRole,\r                 hmsActionResultListener: HMSActionResultListener)\r  \r \r 1. ofRoles is a list of HMSRole whose role should be changed.\r 2. toRole is the HMSRole they should be changed to.\r 3. hmsActionResultListener has onSuccess and onError callbacks for the result of the operation.\r \r Note that if an empty list is sent to ofRoles , no roles will be changed. This is to avoid accidentally changing roles you may not have intended such as the bots that provide recording and streaming with the roles beam .\r \r Also bulk role changes are always forced, no dialog will be given for the peer to accept it, they will just be changed immediately.\r \r Here's how the method could be called to change all guest and waiting roles to host :\r \r  s id=\"change-role-bulk\" items=  'Kotlin', 'Java'    \r \r   id='change-role-bulk-0'>\r \r  kotlin\r fun changeRoles(hmsSdk: HMSSDK)  \r     val rolesToChange : List<HMSRole> = hmsSdk.getRoles()\r       .filter  \r         it.name == \"guest\"  it.name == \"waiting\"\r        \r     val toRole : HMSRole = hmsSdk.getRoles().find   it.name == \"host\"    \r \r     hmsSdk.changeRoleOfPeersWithRoles(rolesToChange, toRole, object : HMSActionResultListener  \r       override fun onError(error: HMSException)  \r         // Error\r        \r \r       override fun onSuccess()  \r         // Roles changed successfully\r        \r \r      )\r    \r  \r \r </Tab>\r \r   id='change-role-bulk-1'>\r \r  java\r public void changeRoles(HMSSDK hmsSdk)  \r     List<HMSRole> rolesToChange = new ArrayList<>();\r     HMSRole toRole = null;\r     for (HMSRole role : hmsSdk.getRoles())  \r       if(role.getName().equals(\"guest\")  role.getName().equals(\"waiting\"))  \r         rolesToChange.add(role);\r        \r       if(role.getName().equals(\"host\"))  \r         toRole = role;\r        \r      \r     hmsSdk.changeRoleOfPeersWithRoles(rolesToChange, toRole, new HMSActionResultListener()  \r       @Override\r       public void onSuccess()  \r         // Role changed\r        \r \r       @Override\r       public void onError(@NonNull HMSException e)  \r         // Error occurred\r        \r      );\r    \r  \r \r </Tab>\r \r   Bulk Role Change Errors You may get the following errors for bulk role change:  Message                 Meaning                                                      invalid role           A role in the list of roles to change does not exist in this room.        target role clash with requested roles  the 'toRole' is also listed as one to change to 'toRole'   role does not have required permission  Peer does not have role change permission.          peer left                The peer who's role was to be changed has left.       role invalid               The 'toRole' is invalid.                 \r "
    },
    {
        "title": "Change User Name\r",
        "link": "/android/v2/features/change-user-name",
        "platformName": "Android",
        "objectID": "/android/v2/features/change-user-name",
        "keywords": [],
        "content": " -\r \r Any peer can change their own name before or after joining a room. Before joining, the name would have to be specified in HMSConfig that is passed to the join method. This document shows how the name can be changed after joining.\r \r  Responding to name changes\r \r Whenever a remote peer's name is changed a callback will be received in onPeerUpdate(update : HMSPeerUpdate, peer : HMSPeer) of HMSUpdateListener where the update will be of type HMSPeerUpdate.NAME_CHANGED .\r \r When this callback is received the UI for that peer should be updated as well.\r \r  Changing the name\r \r The peer who wants to change their name should call the following method on an HMSSdk instance.\r \r  s id=\"change-name\" items=  'Kotlin', 'Java'    \r \r   id='change-name-0'>\r \r  kotlin\r val newName = \"Ashwini Shetty\"\r hmsSdk.changeName(newName, object : HMSActionResultListener  \r   override fun onSuccess()  \r \r    \r \r   override fun onError(error: HMSException)  \r \r    \r \r  )\r  \r \r </Tab>\r \r   id='change-name-1'>\r \r  java\r String newName = \"Ashwini Shetty\";\r hmsSdk.changeName(newName, new HMSActionResultListener()  \r   @Override\r   public void onSuccess()  \r \r    \r \r   @Override\r   public void onError(@NonNull HMSException e)  \r \r    \r  );\r  \r </Tab>\r \r "
    },
    {
        "title": "Chat\r",
        "link": "/android/v2/features/chat",
        "platformName": "Android",
        "objectID": "/android/v2/features/chat",
        "keywords": [],
        "content": " -\r \r What's a video call without being able to send messages to each other too? 100ms supports chat for every video/audio room you create.\r \r You can see an example of every type of message (of the types below) being sent and displayed in the advanced  sample app (https://github.com/100mslive/100ms-android/).\r \r   Addressing messages\r \r    Broadcast messages ( sending-broadcast-messages) are sent to Everyone in the chat hmssdk.sendBroadcastMessage .\r \r    Direct messages ( sending-direct-messages) are sent to a specific person hmssdk.sendDirectMessage .\r \r    Group messages ( sending-group-messages) are sent to everyone with a particular HMSRole . Such as all hosts or all teachers or all students  hmsSdk.sendGroupMessage .\r     Learn more about  roles (../foundation/templates-and-roles) and how to create them on the backend.\r \r   Sending Chat Messages\r \r   Sending Broadcast Messages\r \r Want to let everyone in the chat know something? Call sendBroadcastMessage on the instance of HMSSDK to a send a broadcast.\r \r The parameters are:\r \r 1.  textMessage : The text of the message.\r 2.  type : The type of the message, default is HMSMessageType.CHAT .\r 3.  hmsMessageResultListener : An instance of HMSMessageResultListener .\r    onSuccess will be called when the server receives the request. The onSuccess contains a HMSMessage which has the updated serverReceiveTime a date containing the server timestamp.\r    onError will be called with an instance of HMSException detailing what went wrong.\r \r > 💡 Note that the callback only lets you know if the server has received your request for the message or if there was some error.\r >\r >It does not convey whether the message was delivered to or read by the recipient.\r >\r >Also it's important to make a new callback per message because it will only contain the results of that particular call for sending a message.\r \r  s id=\"broadcastmsg\" items=  'Kotlin', 'Java'    \r \r   id='broadcastmsg-0'>\r \r  kotlin\r hmssdk.sendBroadcastMessage(textMessage, HMSMessageType.CHAT,\r   object : HMSMessageResultListener  \r    override fun onError(error: HMSException)  \r     \r \r    override fun onSuccess(hmsMessage: HMSMessage)  \r     \r  )\r  \r </Tab>\r \r   id='broadcastmsg-1'>\r \r  java\r hmssdk.sendBroadcastMessage(textMessage, HMSMessageType.CHAT,\r   new HMSMessageResultListener()  \r     @Override\r     public void onSuccess(@NonNull HMSMessage hmsMessage)  \r       \r      \r \r     @Override\r     public void onError(@NonNull HMSException e)  \r \r      \r    \r );\r  \r </Tab>\r \r   Sending Direct Messages\r \r Got secrets to share? Send a message directly to a single person in the chat with a direct message. Call sendDirectMessage on an instance of HMSSDK .\r \r The parameters are:\r \r 1.  textMessage : The text of the message.\r 2.  type : The type of the message, default is HMSMessageType.CHAT .\r 3.  peerTo : The HMSPeer instance who should receive message.\r 4.  hmsMessageResultListener : An instance of HMSMessageResultListener .\r    onSuccess will be called when the server receives the request. The onSuccess contains a HMSMessage which has the updated serverReceiveTime a date containing the server timestamp.\r    onError will be called with an instane of HMSException detailing what went wrong.\r \r > 💡 Note that the callback only lets you know if the server has received your request for the message or if there was some error.\r >\r >It does not convey whether the message was delivered to or read by the recipient.\r >\r >Also it's important to make a new callback per message because it will only contain the results of that particular call for sending a message.\r \r  s id=\"directmsg\" items=  'Kotlin', 'Java'    \r \r   id='directmsg-0'>\r \r  kotlin\r fun sendDirectMessage(textMessage : String, recipient : HMSPeer)  \r   hmssdk.sendDirectMessage(textMessage, HMSMessageType.CHAT, recipient, \r     object : HMSMessageResultListener  \r       override fun onError(error: HMSException)  \r        \r \r       override fun onSuccess(hmsMessage: HMSMessage)  \r        \r        )\r  \r  \r \r </Tab>\r \r   id='directmsg-1'>\r \r  java\r   private void sendDirectMessage(String textMessage, HMSPeer recipient )  \r     hmsSdk.sendDirectMessage(textMessage, HMSMessageType.CHAT, recipient, \r       new HMSMessageResultListener()  \r           @Override\r           public void onSuccess(@NonNull HMSMessage hmsMessage)  \r             \r            \r \r           @Override\r           public void onError(@NonNull HMSException e)  \r \r            \r      ); \r    \r  \r \r </Tab>\r \r \r   Sending Group Messages\r \r Need to call attention to all the hosts? All the teachers? All the developers? Call sendGroupMessage on an instance of HMSSDK .\r \r The parameters are:\r \r 1.  textMessage : The text of the message.\r 2.  type : The type of the message, default is HMSMessageType.CHAT .\r 3.  hmsRolesTo : The List<HMSRoles> of all  roles (../foundation/templates-and-roles) which should receive the message.\r 4.  hmsMessageResultListener : An instance of HMSMessageResultListener .\r    onSuccess will be called when the server receives the request. The onSuccess contains a HMSMessage which has the updated serverReceiveTime a date containing the server timestamp.\r    onError will be called with an instane of HMSException detailing what went wrong.\r \r > 💡 Note that the callback only lets you know if the server has received your request for the message or if there was some error.\r >\r >It does not convey whether the message was delivered to or read by the recipient.\r >\r >Also it's important to make a new callback per message because it will only contain the results of that particular call for sending a message.\r \r  s id=\"groupmsg\" items=  'Kotlin', 'Java'    \r \r   id='groupmsg-0'>\r \r  kotlin\r \r private fun sendGroupMessage(textMessage: String, recipients : List<HMSRole> )  \r   hmssdk.sendGroupMessage(message.message, HMSMessageType.CHAT, recipients, object : HMSMessageResultListener  \r    override fun onError(error: HMSException)  \r     \r \r    override fun onSuccess(hmsMessage: HMSMessage)  \r     \r \r    )\r   \r \r  \r \r </Tab>\r \r   id='groupmsg-1'>\r \r  java\r \r private void sendGroupMessage(String textMessage, List<HMSRole> recipients)  \r   hmsSdk.sendGroupMessage(textMessage, HMSMessageType.CHAT, recipients, \r       new HMSMessageResultListener()  \r         @Override\r         public void onSuccess(@NonNull HMSMessage hmsMessage)  \r           \r          \r   \r         @Override\r         public void onError(@NonNull HMSException e)  \r   \r          \r    );\r  \r \r  \r \r </Tab>\r \r   Receiving Chat Messages\r \r When you called hmsSdk.join(config, hmsUpdateListener) to join a room, the HMSUpdateListener implementation that was passed in had the callback fun onMessageReceived(message: HMSMessage) .\r \r This where you'll receive new messages as HMSMessage during the call. It contains:\r \r  kotlin\r \r data class HMSMessage internal constructor(\r  val message: String,\r  val type: String,\r  val recipient: HMSMessageRecipient = HMSMessageRecipient(),\r  var serverReceiveTime: Date,\r  var sender: HMSPeer\r )\r  \r \r   message: Content of the text message or the text description of the raw message.\r   type: Type of message sent. Default value is HMSMessageType.CHAT .\r   recipient: The intended recipient(s) of this message as a HMSMessageRecipient .\r   serverReceiveTime: timestamp of when the messaging server receives this message. Update the time in your own messages when this comes back from the server in HMSUpdateListener.onMessageReceived for accurate ordering of your own messages.\r   sender: The HMSPeer who is sending this message.\r \r Identifying Senders:\r \r The sender of a message is always contained in the sender field of HMSMessage. This lets you get the name and peer id for any message sender.\r \r Message Body:\r \r The body of the message is in message as a String.\r \r Time:\r \r The time the message was sent is contained in time as a Java Date.\r \r   Putting together a list of chat messages.\r The UI is completely up to you to decide  You'll also need to hold onto all the received messages if you want to display history.\r \r   Identifying who the message was for\r \r The HMSMessageRecipient contained in the recipient field of HMSMessage lets you know who the message was for.\r \r The HMSMessageRecipient contains:\r  kotlin\r class HMSMessageRecipient internal constructor(\r   var recipientPeer: HMSPeer? = null,\r   var recipientRoles: List<HMSRole> = mutableListOf(),\r   var recipientType: HMSMessageRecipientType : HMSMessageRecipientType\r )\r  \r \r recipientPeer: Only contains a peer when a specific single peer is being direct messaged.\r \r recipientRoles: Only contains values when a group message is being sent to many roles.\r \r recipientType: Will be HMSMessageRecipientType.BROADCAST for a message being sent to everyone. If this is true, the other two fields will be null and empty respectively.\r \r HMSMessageRecipientType.PEER will be set when it's a direct message.\r \r HMSMessageRecipientType.ROLES will be set when it's a message to one or many roles.\r \r A good way to map this to your own app is a class like Recipient below. You could choose to copy this file into your code for the mapping.\r \r \r  kotlin 17-23 :RecipientMapping.kt\r \r sealed class Recipient  \r   \r   object Everyone : Recipient()  \r     override fun toString(): String = \"Everyone\"\r    \r   \r   data class Role(val role : HMSRole) : Recipient()  \r     override fun toString(): String =\r       role.name\r    \r   \r   data class Peer(val peer : HMSPeer) : Recipient()  \r     override fun toString(): String = peer.name\r    \r \r   companion object  \r     fun toRecipient(message : HMSMessageRecipient) : Recipient =\r       when(message.recipientType)  \r         HMSMessageRecipientType.BROADCAST -> Everyone\r         HMSMessageRecipientType.PEER -> Peer(message.recipientPeer  )\r         HMSMessageRecipientType.ROLES -> Role(message.recipientRoles.firstOrNull()  )\r      \r    \r  \r  \r \r So that if you need to convert the message to your own class you can set the recipient like:\r \r  kotlin 7 \r override fun onMessageReceived(message: HMSMessage)  \r   ChatMessage(\r     message.sender.name,\r     message.serverReceiveTime,\r     message.message,\r     false,\r     recipient = Recipient.toRecipient(message.recipient)\r   )\r  \r  "
    },
    {
        "title": "Echo Cancellation\r",
        "link": "/android/v2/features/echo-cancellation",
        "platformName": "Android",
        "objectID": "/android/v2/features/echo-cancellation",
        "keywords": [],
        "content": " -\r \r The 100ms Android SDK automatically applies the best known settings to cancel echos from devices. However some android devices have issues with their hardware echo cancellation and benefit from relying only on software for it.\r \r While we continually update the SDK with the latest known devices which have the issue, you may run into one we haven't tested with yet.\r \r If you find that a certain device echos when it joins a meeting despite this, turning off its hardware echo cancellation may solve the problem. This is because if hardware echo cancellation is supported by the device, the SDK prefers hardware over software.\r \r Echo cancellation settings can only be applied before a meeting a joined. The 100ms Android SDK provides a way to turn this off in the builder function.\r \r > 💡 Note this option must selectively be turned on for devices based on their  models (https://developer.android.com/reference/android/os/Build) disabling hardware echo cancellation for all devices will result in other devices echoing which didn't before.\r \r  s id=\"echocancellation\" items=  'Kotlin', 'Java'    \r \r   id='echocancellation-0'>\r \r  kotlin\r val faultyHardwareCancellationModels: HashSet<String> =\r   hashSetOf(\"phone 1\", \"phone 2\") // Get this from Build.MODEL for the device.\r \r private val useHardwareEchoCancellation =  faultyHardwareCancellationModels\r                       .contains(Build.MODEL)\r \r private val hmsTrackSettings = HMSTrackSettings.Builder()\r   .audio(\r     HMSAudioTrackSettings.Builder()\r     .setUseHardwareAcousticEchoCanceler(useHardwareEchoCancellation).build()\r   )\r   .build()\r \r  val hmsSDK = HMSSDK\r   .Builder(application)\r   .setTrackSettings(hmsTrackSettings) // SDK uses HW echo cancellation, if nothing is set in builder\r   .build()\r  \r \r </Tab>\r \r   id='echocancellation-1'>\r \r  java\r private HashSet<String> faultyHardwareCancellationModels = new HashSet<String>()  \r   add(\"phone 1\"); // Get this from Build.MODEL for the device.\r   add(\"phone 2\"); // Get this from Build.MODEL for the device.\r   ;\r \r private boolean useHardwareEchoCancellation =  faultyHardwareCancellationModels\r     .contains(Build.MODEL);\r \r HMSTrackSettings hmsTrackSettings = new HMSTrackSettings.Builder()\r   .audio(new HMSAudioTrackSettings.Builder()\r     .setUseHardwareAcousticEchoCanceler(useHardwareEchoCancellation).build()\r   )\r .build();\r \r HMSSDK hmssdk = new HMSSDK.Builder(application)\r   .setTrackSettings(hmsTrackSettings)\r   .build();\r  \r \r </Tab>\r "
    },
    {
        "title": "End Room\r",
        "link": "/android/v2/features/end-room",
        "platformName": "Android",
        "objectID": "/android/v2/features/end-room",
        "keywords": [],
        "content": " -\r \r Done with talking and it's time to end the video call room for everyone not just yourself? You may be looking to end the room.\r \r   Permissions\r \r Can't let just anyone end the video call room. First you need to create a  role (../foundation/templates-and-roles) with the permissions to end a room.\r \r The permission to end a room is called PermissionsParams.endRoom and you should check for that within the HMSRole of the peer to see if they have it.\r \r Here's how to check whether the local peer has the permission to end the room:\r \r  s id=\"endroom\" items=  'Kotlin', 'Java 8', 'Java 7'    \r \r   id='endroom-0'>\r \r  kotlin\r fun isAllowedToEndMeeting(): Boolean  \r   return hmsSDK.getLocalPeer()  \r         .hmsRole.permission?\r         .endRoom == true\r  \r \r  \r \r </Tab>\r \r   id='endroom-1'>\r \r \r   java\r private boolean hasEndRoomPermissions()  \r   return Optional.ofNullable(hmsSdk.getLocalPeer())\r       .map(HMSPeer::getHmsRole)\r       .map(HMSRole::getPermission)\r       .map(PermissionsParams::getEndRoom)\r       .orElse(false);\r  \r   \r \r </Tab>\r \r   id='endroom-2'>\r \r  java\r private boolean hasEndRoomPermissions()  \r   HMSLocalPeer peer = hmsSdk.getLocalPeer();\r   if(peer  = null)  \r     HMSRole role = peer.getHmsRole();\r     if(role  = null)  \r       PermissionsParams permissions = role.getPermission();\r       if(permissions  = null)  \r         return permissions.getEndRoom();\r        \r      \r    \r   return false;\r  \r  \r \r </Tab>\r \r hmsSdk.getLocalPeer() will not return null as long as you're in a preview or in a meeting. Since you likely won't need to check for permissions if you're not in one it would be ok.\r \r   Ending the Room\r \r Once you're sure the peer has the permissions to end the room, they can end it by calling hmsSdk.endRoom .\r \r endRoom takes three parameters.\r \r 1.  message : The String message you want to show people when they are forced to leave. This is a good place to thank them for joining the room and informing them that it has ended.\r 2.  lock : A Boolean for whether you want to prevent anyone from rejoining the room. If false, they will be allowed to enter the room again if the client called join . If this is true, they will NOT able to join this room again.\r 3.  hmsActionResultListener : These are the callbacks that let you know whether the room was ended successfully or not. If the room could be closed, onSuccess will be called. For any errors onError will be called with the appropriate HMSException .\r \r > 💡 After calling endRoom the video calling UI should be disposed of as well since only the other peers will get the onPeerRemoved callback. The caller has to rely on the onSuccess callback for endRoom to decide when to terminate the meeting room UI locally.\r \r  s id=\"endroomcall\" items=  'Kotlin', 'Java'    \r \r   id='endroomcall-0'>\r \r  kotlin\r val message = \"Closing time. Thanks for coming \"\r val lock = false\r \r hmsSDK.endRoom(message, lock, object : HMSActionResultListener \r   override fun onError(error: HMSException)  \r    \r \r   override fun onSuccess()  \r    \r  )\r  \r </Tab>\r \r   id='endroomcall-1'>\r \r \r  java\r String message = \"Closing time. Thanks for coming \";\r boolean lock = false;\r \r hmsSDK.endRoom(message, lock, new HMSActionResultListener()  \r   @Override\r   public void onSuccess()  \r     \r    \r \r   @Override\r   public void onError(@NonNull HMSException e)  \r \r    \r  );\r  \r </Tab>\r \r   How to handle an end room callback for receivers\r \r Once the peer with adequate permissions calls endRoom , all other peers in the room will receive a callback in HMSUpdateListener.onRemovedFromRoom .\r \r The onRemovedFromRoom callback has a single parameter called HMSRemovedFromRoom with the following structure.\r \r  kotlin\r data class HMSRemovedFromRoom(\r   val reason : String,\r   val peerWhoRemoved : HMSRemotePeer?,\r   val roomWasEnded : Boolean\r )\r  \r \r > 💡 This is the same callback that will be triggered if a peer was  removed from a room (remove-peer) as well. Except that roomEnded will be true when the entire room was ended.\r \r    reason : The string message detailing why the room was ended.\r    peerWhoRemoved : The details of the peer who called endRoom .\r    roomWasEnded : True if the entire room was ended. False if only the receiving peer was removed.\r \r Clients should read this callback and show the appropriate UI."
    },
    {
        "title": "Error Handling",
        "link": "/android/v2/features/error-handling",
        "platformName": "Android",
        "objectID": "/android/v2/features/error-handling",
        "keywords": [],
        "content": "  When you make an API call to access an HMS SDK, the SDK may return error codes. ErrorCodes are returned when a problem that cannot be recovered without app intervention has occurred. These are returned as HMSException in the onError callback of the HMSUpdateListner . Following are the different error codes that are returned by the SDK . Before returning any error code, SDK retries the errors (whichever is possible ).    Error Code      Cause of the error                      Action to be taken                                               :     :               :                               1000       Generic Error                      Need to debug further with logs.                                          1003       Websocket disconnected  Happens due to network issues  Mention user to check their network connection or try again after some time.                    2002       Invalid Endpoint URL                   Check the endpoint provided while calling join on HMSSDK .                            2003       Endpoint is not reachable                Mention user to check their network connection or try again after some time.                    2004       Token is not in proper JWT format            The token passed while calling join is not in correct format. Retry getting a new token.             3000       Generic Error                      Need to debug further with logs.                                          3001       Cant Access Capture Device                Ask user to check permission granted to audio/video capture devices.                        3002       Capture Device is not Available             Ask user to check if the audio/video capture device is connected or not.                      3003       Capture device is in use by some other application    Show notification to user mentioning that the capturing device is used by some other application currently.     3005       There is no media to return               For building HMSTrackSettings either audio or video track has to be present.                    3006       Invalid Video Settings                  Simulcast cannot be started without providing video settings.                            3007       Codec cannot change mid call               Codec cannot be changed mid call.                                          3011       Mic Capture Failed                    Failed to capture mic access.                                            3012       Bluetooth Inaccessible                  BLUETOOTH_CONNECT permission missing .                                       3013       Bluetooth Inaccessible                  General Bluetooth Permission missing.                                            4001       WebRTC error                       Some webRTC error has occurred. Need more logs to debug.                              4002       WebRTC error                       Some webRTC error has occurred. Need more logs to debug.                              4003       WebRTC error                       Some webRTC error has occurred. Need more logs to debug.                              4004       WebRTC error                       Some webRTC error has occurred. Need more logs to debug.                              4005       ICE Connection Failed due to network issue        Mention user to check their network connection or try again after some time.                    5001       Trying to join a room which is already joined      Trying to join an already joined room.                                       5002       Trying to start Preview which is already started     Trying to start Preview which is already started.                                  6000       Client failed to connect                 Client failed to connect.                                              6002       webRTC Error: Error while renegotiating         Please try again.                                                  6004       Json parsing failed                   Need to debug further with logs.                                          6008       Unable to send message                  Cannot send message. Peer is null. The SDK must be disconnected from a room.                    6008       API not supported                    This API is not support on the current Android Version (Android-31).                        7001       Platform Not Supported                  The platform is not supported for plugin                                      7002       Plugin Init Failed                    Plugin initialization has failed                                          7003       Plugin Processing Failed                 Plugin processing failed                                              7004       Plugin Add Already Going on               Plugin add is already in progress                                          7005       Bluetooth Sco Connection Failed             Bluetooth headset is either not available or in a processing state.                         400       Error occurred                      This can usually happen due to token issues(Check logs for more description). Need more logs to debug.       401       Error occurred                      This can usually happen due to token issues(Check logs for more description). Need more logs to debug.       410       Peer is gone                       The peer is no more present in the room.                                      500       Error occurred                      This is a general server error(Check logs for more description). Need more logs to debug.          "
    },
    {
        "title": "HLS Streaming / Recording",
        "link": "/android/v2/features/hls",
        "platformName": "Android",
        "objectID": "/android/v2/features/hls",
        "keywords": [],
        "content": "   HLS Streaming (./../foundation/live-streaming) allows for scaling to millions of viewers in near real time. You can give a link of your web-app which will be converted to a HLS feed by our server and can be played across devices for consumption. Behind the scenes, this will be achieved by having a bot join your room and stream what it sees and hears. Once the feed is ready, the server will give a URL which can be played using any HLS Player. > Note that the media server serving the content in this case is owned by 100ms. If you're looking for a way to stream > on YouTube, Twitch etc., please have a look at our RTMP streaming docs  here (./rtmp-recording).   Starting HLS can be started in two ways depending on the level of customization you need. 1. Default View: The simplest view to just begin a stream with default UI and parameters. 2. Custom Views: To use your own UI for HLS streaming, you need to provide your own web-app URL for our bot to join and stream.    Default View Begins a stream with default parameters.  s id=\"hls-default-view\" items=  'Kotlin', 'Java'       id='hls-default-view-0'>   kotlin hmsSdkHost.startHLSStreaming(hmsActionResultListener = object : HMSActionResultListener     override fun onSuccess()       // hls was started successfully       override fun onError(error: HMSException)       // hls failed to start with error $error      )   </Tab>   id='hls-default-view-1'>   java hmsSdkHost.startHLSStreaming(null, new HMSActionResultListener()     @Override   public void onSuccess()       // hls was started successfully       @Override   public void onError(@NonNull HMSException error)       // hls failed to start with error $error      );    </Tab>    Custom View To use our own browser UI for HLS, you'll need to pass in a meeting URL. The 100ms bot will open this URL to join your room, so it must allow access without any user level interaction. In the future it'll be possible to start HLS for multiple such URLs for the same room. For this purpose the API supports taking in an array, although currently only the first element of the array will be used. To distinguish between multiple URLs an additional field metadata can be optionally passed. The meetingURL and metadata are clubbed together to form what we'll call a variant . You can call hmsSDK.startHLSStreaming with a HMSHLSConfig having an array of such variants. HMSHlsRecordingConfig is optional.  s id=\"hls\" items=  'Kotlin', 'Java'       id='hls-0'>   kotlin fun hlsStreaming(meetingUrl : String, hmssdk: HMSSDK)       val meetingUrlVariant1 = HMSHLSMeetingURLVariant(       meetingUrl = meetingUrl,       metadata = \"tag for reference\"     )     val hlsConfig = HMSHLSConfig(listOf(meetingUrlVariant1))     hmssdk.startHLSStreaming(hlsConfig, object : HMSActionResultListener         override fun onSuccess()                 override fun onError(error: HMSException)                )        </Tab>   id='hls-1'>   java public void hlsStreaming(String meetingUrl, HMSSDK hmssdk)     HMSHLSMeetingURLVariant meetingURLVariant1 = new HMSHLSMeetingURLVariant(       meetingUrl,       \"tag for reference\"   );   ArrayList<HMSHLSMeetingURLVariant> variantList = new ArrayList<>();   variantList.add(meetingURLVariant1);   HMSHLSConfig hlsConfig = new HMSHLSConfig(variantList, null);   hmssdk.startHLSStreaming(hlsConfig, new HMSActionResultListener()       @Override     public void onSuccess()             @Override     public void onError(@NonNull HMSException error)            );     </Tab>   Optional HLS Recording Optionally to record the HLS stream you may specify an HMSHlsRecordingConfig within the HMSHLSConfig . Here's what the HMSHlsRecordingConfig looks like   kotlin data class HMSHlsRecordingConfig(   val singleFilePerLayer : Boolean,   val videoOnDemand : Boolean )    1. singleFilePerLayer if the desired end result is a mp4 file per HLS layer, false by default. 2. videoOnDemand if the desired end result is a zip of m3u8 and all the chunks, false by default. Here's what it looks like to call for recording configs.  s id=\"hls-recording-config\" items=  'Kotlin', 'Java'       id='hls-recording-config-0'>   kotlin // Optional recording config val hlsRecordingConfig = HMSHlsRecordingConfig(false, false) val hlsConfig = HMSHLSConfig(listOf(meetingUrlVariant), hlsRecordingConfig)   </Tab>   id='hls-recording-config-1'>   java // Optional recording config HMSHlsRecordingConfig recordingConfig = new HMSHlsRecordingConfig(false, false); HMSHLSConfig hlsConfig = new HMSHLSConfig(variantList, recordingConfig);    </Tab>   Stopping HLS You can call hmsSDK.stopHLSStreaming to stop HLS Streaming which will stop all the variants.  s id=\"hls-stop\" items=  'Kotlin', 'Java'       id='hls-stop-0'>   kotlin hmssdk.stopHLSStreaming(null, object : HMSActionResultListener         override fun onSuccess()                 override fun onError(error: HMSException)                )   </Tab>   id='hls-stop-1'>   java hmssdk.stopHLSStreaming(null, new HMSActionResultListener()         @Override       public void onSuccess()                 @Override       public void onError(@NonNull HMSException error)                );   </Tab> > Want to see how this works in a live project?  Take a look (https://github.com/100mslive/100ms-android/blob/bba78d425c4e59e1344dc18f50b6494f5160a89f/app/src/main/java/live/hms/app2/ui/meeting/MeetingViewModel.kt L933) at our advanced sample app.   Current Room Status The current status for the room is always reflected in the HMSRoom object. Here are the relevant properties inside the HMSRoom object which you can read to get the current hls streaming status of the room namely: hlsStreamingState . The object contains a boolean running which lets you know if it's active on the room right now as well as list of active variants. 1.   hlsStreamingState   an instance of HMSHLSStreamingState , which looks like:   kotlin data class HMSHLSStreamingState(     val running : Boolean,     val variants : ArrayList<HMSHLSVariant>?, )   This represents a livestream to one or more HLS URLs in the container of HMSHLSVariant . Which looks like:   kotlin data class HMSHLSVariant(     val hlsStreamUrl: String?,     val meetingUrl: String?,     val metadata: String?,     val startedAt: Long? )   The room status should be checked in following two places  1. In the onJoin(room: HMSRoom) callback of HMSUpdateListener   The properties mentioned above will be on the HMSRoom object. 2. In the onRoomUpdate(type: HMSRoomUpdate, hmsRoom: HMSRoom) callback of HMSUpdateListener .  The HMSRoomUpdate type will be HMSRoomUpdate.HLS_STREAMING_STATE_UPDATED .   Tips   If you're using the dashboard web-app from 100ms, please make sure to use a role which doesn't have publish permissions for beam tile to not show up.   If using your own web-app, do put in place retries for API calls like tokens etc. just in case any call fails. As human users we're used to reloading the page in these scenarios which is difficult to achieve in the automated case.   Make sure to not disable the logs for the passed in meeting URL. This will allow for us to have more visibility into the room, refreshing the page if join doesn't happen within a time interval. "
    },
    {
        "title": "Integrating The SDK\r",
        "link": "/android/v2/features/integration",
        "platformName": "Android",
        "objectID": "/android/v2/features/integration",
        "keywords": [],
        "content": " -\r \r Three main sections here are:\r  Adding Dependencies ( adding-dependencies)  Dependencies for the 100ms Android SDK.\r  Adding the 100ms SDK ( adding-the-100ms-sdk)  To add the SDK to your project's dependencies.\r  Adding Permissions ( permissions)  Since a video calling app needs camera/record-audio permissions you'll need this for the apps to work.\r \r   Adding Dependencies\r \r \r     (https://img.shields.io/badge/dynamic/xml.svg?label=100ms&color=blue&query=%2F%2Fmetadata%2Fversioning%2Flatest&url=https%3A%2F%2Frepo1.maven.org%2Fmaven2%2Flive%2F100ms%2Fandroid-sdk%2Fmaven-metadata.xml) (https://jitpack.io/ 100mslive/android-sdk) \r \r  s id=\"sdk-imports\" items=  'Maven Central', 'Jitpack'    \r \r   id='sdk-imports-0'>\r ​\r \r  json 5-8 :settings.gradle section=AddSDKDependencies sectionIndex=1\r dependencies  \r // See the version in the badge above.\r // There are separate libraries for the sdk, virtual background and hls-player-stats.\r //   add just the ones you need.\r def hmsVersion = \"x.x.x\"\r implementation \"live.100ms:android-sdk:$hmsVersion\"  // Essential\r implementation \"live.100ms:virtual-background:$hmsVersion\" // Optional\r implementation \"live.100ms:hls-player-stats:$hmsVersion\"  // Optional\r  \r  \r \r </Tab>\r \r \r   id='sdk-imports-1'>\r \r There is one extra step to use the jitpack libraries.\r \r   Add the JitPack repository to your root settings.gradle at the end of the respositories closure:\r \r You can open it in Android Studio by double tapping shift and typing settings.gradle .\r \r  json 7 :settings.gradle section=AddSDKDependencies sectionIndex=1\r dependencyResolutionManagement  \r repositoriesMode.set(RepositoriesMode.FAIL_ON_PROJECT_REPOS)\r repositories  \r google()\r mavenCentral()\r jcenter() // Warning: this repository is going to shut down soon\r maven   url 'https://jitpack.io'  \r    \r  \r rootProject.name = \"MyVideoCallApp\"\r include ':app'\r \r  \r \r   Add the 100ms sdk dependency to your app-level build.gradle \r \r  json 2,4-6 :build.gradle section=AddSDKDependencies sectionIndex=2\r dependencies  \r def hmsVersion = \"x.x.x\"\r // See the version in the jitpack badge above.\r implementation \"com.github.100mslive.android-sdk:lib:$hmsVersion\"     // Essential\r implementation \"com.github.100mslive.android-sdk:virtualBackground:$hmsVersion\" // Optional\r implementation \"com.github.100mslive.android-sdk:stats:$hmsVersion\"      // Optional\r \r  \r  \r \r </Tab>\r \r \r   Permissions\r \r Camera, Recording Audio and Internet permissions are required. Add them to your manifest.\r \r  xml:AndroidManifest.xml\r   <uses-permission android:name=\"android.permission.CAMERA\"  \r   <uses-permission android:name=\"android.permission.RECORD_AUDIO\"  \r   <uses-permission android:name=\"android.permission.INTERNET\"  \r  \r \r You will also need to request Camera and Record Audio permissions at runtime before you join a call or display a preview. Please follow  Android Documentation (https://developer.android.com/training/permissions/requesting request-permission) for runtime permissions.\r "
    },
    {
        "title": "Interruption Handling\r",
        "link": "/android/v2/features/interruption-handling",
        "platformName": "Android",
        "objectID": "/android/v2/features/interruption-handling",
        "keywords": [],
        "content": " -\r \r You're chatting away on your video call and, uh-oh, a phone call comes in. No worries, the 100ms SDK abstracts away handling this situation for you.\r \r When a telephonic voice call comes in and the user accepts the following happens.\r \r   All videos including your own are turned off.\r   You are muted.\r \r When the call ends the SDK will restore all videos, taking care not to un-mute you if you had muted yourself before the call came in. Neither will it un-mute all the peers if you had muted them from the app.\r \r No code required, this happens automatically for all calls.\r "
    },
    {
        "title": "Join Room\r",
        "link": "/android/v2/features/join",
        "platformName": "Android",
        "objectID": "/android/v2/features/join",
        "keywords": [],
        "content": " -\r \r  Overview\r \r To join and interact with others in audio or video call, the user needs to join a room .\r \r When user indicates that they want to _join_ the room, your app should have -\r \r 1. User Name  the name which should be displayed to other peers in the room.\r 2.  Authentication Token (../guides/token)  the client side authentication token generated by the Token Service.\r \r You can also optionally pass these fields -\r \r 1.  Track settings (../advanced-features/track-settings) can used to set initial audio/video mute status.\r 2.  User metadata (../advanced-features/peer-metadata-update)  this can be used to pass any additional metadata associated with the user.\r \r  Join a Room\r \r We'll call the join method on the HMSSDK object with an optional config containing above fields to join the room.\r \r 1. First, create an instance of HMSSDK class. Store this instance as a property. Ensure that the SDK object is alive in memory so that you can receive event callbacks from SDK. The simplest way to do this is:\r \r  s id=\"create-hmssdk\" items=  'Kotlin', 'Java'    \r \r   id='create-hmssdk-0'>\r \r  kotlin\r  private val hmsSDK = HMSSDK\r   .Builder(application)\r     // Optional\r     .setTrackSettings(trackSettings)\r   .build()\r  \r \r </Tab>\r \r   id='create-hmssdk-1'>\r \r  java\r   HMSSDK hmssdk = new HMSSDK\r             .Builder(application)\r               // Optional\r               .setTrackSettings(trackSettings) \r             .build();\r \r  \r \r </Tab>\r \r > Note: If you are using Preview then you must already have an instance of HMSSDK before invoking Preview APIs.\r \r 2. Next, create an object of HMSConfig class using the available joining configurations.\r \r We will be passing in:\r   A user display name.\r   The auth token.\r   Metadata for the peer.\r \r  s id=\"createconfig\" items=  'Kotlin', 'Java'    \r \r   id=\"createconfig-0\">\r \r  kotlin\r val hmsConfig = HMSConfig( userName = \"user display name\",\r               authtoken = authToken,\r               metadata = \"\"\" city: 'Winterfell', knowledge: 'nothing' \"\"\")\r  \r \r </Tab>\r \r   id=\"createconfig-1\">\r \r  java\r HMSConfig config =\r new HMSConfig(\"user display name\",\r         authToken,\r         \" city: 'Winterfell', knowledge: 'nothing' \");\r  \r \r </Tab>\r \r 3. You'll want to handle the callbacks from joining the room by making your ViewModel, Presenter or whatever you're using to handle business logic implement the HMSUpdateListener interface.\r \r 4. Now, we are primed to join the room. All you have to do is pass the config object to hmsSDK \r \r  s id=\"join-room\" items=  'Kotlin', 'Java'    \r \r   id=\"join-room-0\">\r \r  kotlin\r fun joinRoom(config : HMSConfig, hmsUpdateListener : HMSUpdateListener) \r   hmsSdk.join(config, hmsUpdateListener)\r  \r  \r \r </Tab>\r \r   id='join-room-1'>\r \r  java\r private void joinRoom(HMSConfig config, HMSUpdateListener hmsUpdateListener )  \r   hmsSdk.join(config, hmsUpdateListener);\r  \r  \r \r </Tab>\r \r That's it. You have joined a room successfully 🥳. You should now be able to have an audio only call with this.\r \r Now, let's take a look at the signature of the Join API\r \r  kotlin\r fun join(config: HMSConfig, hmsUpdateListener: HMSUpdateListener)\r  \r \r As evident, join accepts 2 arguments -\r \r   config : an object of type HMSConfig class, the room configuration object which encapsulates user & token data.\r   hmsUpdateListener : a class conforming to HMSUpdateListener interface.\r \r The methods of HMSUpdateListener are invoked to notify updates happening in the room like a peer joins/leaves, a track got muted/unmutes, etc.\r \r After calling join your app will be provided an update from the 100ms SDK.\r \r ✅ If successful, the fun onJoin(room: HMSRoom) method of HMSUpdateListener will be invoked with information about the room encapsulated in the HMSRoom object.\r \r ❌ If failure, the fun onError(error: HMSException) method will be invoked with exact failure reason.\r \r  Getting Current Room State\r \r To navigate to another page once join has completed:\r \r Use the onJoin callback in HMSUpdateListener.\r \r  s id=\"join-room-callbacks\" items=  'Kotlin', 'Java'    \r \r   id=\"join-room-callbacks-0\">\r \r  kotlin\r fun joinRoom(config : HMSConfig) \r   hmsSDK.join(config, object : fun onJoin(room: HMSRoom)  \r     override fun onJoin(room: HMSRoom)  \r       // called when the user joins.\r      \r     ...\r    )\r  \r  \r \r </Tab>\r \r   id=\"join-room-callbacks-1\">\r \r  java\r private void joinRoom(HMSSDK hmsSdk, HMSConfig config)  \r   hmsSdk.join(config, new HMSUpdateListener()  \r     @Override\r     public void onJoin(@NonNull HMSRoom room)  \r       // called when the user joins\r      \r    );\r  \r  \r \r </Tab>\r "
    },
    {
        "title": "Leave Room\r",
        "link": "/android/v2/features/leave",
        "platformName": "Android",
        "objectID": "/android/v2/features/leave",
        "keywords": [],
        "content": " -\r \r Once you're done with a call and want to exit, call leave on the HMSSDK instance you created to join it.\r \r  s id=\"leave\" items=  'Kotlin', 'Java'    \r \r   id='leave-0'>\r \r  kotlin\r hmsSDK.leave()\r  \r </Tab>\r \r   id='leave-1'>\r \r  java\r hmsSDK.leave();\r  \r </Tab>\r "
    },
    {
        "title": "Mirrored Video",
        "link": "/android/v2/features/Mirror",
        "platformName": "Android",
        "objectID": "/android/v2/features/Mirror",
        "keywords": [],
        "content": "  Whenever the video is rendered on the SurfaceViewRenderer we can flip the video frame on the horizontally i.e creating a mirror frame.    Code ( code)    Visual Examples ( visual-examples)    Switching between front and back camera on the same view ( camera-flip-effects)   Code Invoke setMirror method with true , to flip it horizontally and false to restore it in it's original form. Since this is a property of the SurfaceViewRenderer it will persist across release and reinitializations.   s id=\"switch-device\" items=  'Kotlin', 'Java'       id='switch-device-0'>   kotlin surfaceViewRenderer.setMirror(true)   </Tab>   id='switch-device-1'>   java surfaceViewRenderer.setMirror(true);   </Tab>   Sample App Code Long press on a video tile in the sample app to get the Mirror option. There you can select between inverted or normal views for a given video tile.   Visual Examples    Front Camera The default view has no mirroring. If you were facing the front camera, here's what that would look like on a display on the phone. Actual position is how you sit, display position is how it will look in the SurfaceViewRenderer .      Actual Position            Display Position ┌─────────────────────────┐      ┌─────────────────────────┐ │             │      │             │ │             │      │             │ │  Phone Front Camera  │      │  Phone Front Camera  │ │             │      │             │ │             │      │             │ │             │      │             │ │          You  │      │  You          │ │             │      │             │ │             │      │             │ └─────────────────────────┘      └─────────────────────────┘    To change this, turn on mirroring with setMirror(true) .      Actual Position            Display Position ┌─────────────────────────┐      ┌─────────────────────────┐ │             │      │             │ │             │      │             │ │  Phone Front Camera  │      │  Phone Front Camera  │ │             │      │             │ │             │      │             │ │             │      │             │ │          You  │      │          You  │ │             │      │             │ │             │      │             │ └─────────────────────────┘      └─────────────────────────┘      Back Camera Since the left and right for the back camera vs the display are the same, a non mirrored view of the back camera is the same as a mirrored view for the front camera. The Actual and Display positions are reverse for the back camera.   Camera Flip Effects If you call setMirror on an instance of SurfaceViewRenderer and then flip the camera from front to back or vice versa, the value persists. A given surface once mirrored will remain mirrored. It should be noted however that left for the front facing the camera and left for the back facing camera are opposites. So you may want to change setMirror when flipping for that reason. "
    },
    {
        "title": "Mute / Unmute\r",
        "link": "/android/v2/features/mute",
        "platformName": "Android",
        "objectID": "/android/v2/features/mute",
        "keywords": [],
        "content": " -\r \r Mute is something that applies to both audio and video. When you mute audio, you can't be heard by other people. When you mute video, you will not be broadcasting your video to other people.\r \r It can be called on local (you) or remote (other people) peers' audio or video tracks.\r \r You get a reference to your own local peer from getLocalPeer on HMSSDK .\r \r You can do the same for remote peers and HMSRemoteVideoTrack and HMSRemoteAudioTrack . Get other people's peers by calling hmsSDK.getRemotePeers() \r \r > 💡 You may get null for any track if they were not allowed to publish that track according to their HMSRole .\r \r  s id=\"peer\" items=  'Kotlin', 'Java'    \r \r   id='peer-0'>\r \r  kotlin\r val myPeer = hmsSDK.getLocalPeer()\r myPeer?.videoTrack?.setMute(true)\r myPeer?.audioTrack?.setMute(true)\r  \r </Tab>\r \r   id='peer-1'>\r \r  java\r HMSLocalPeer myPeer = hmssdk.getLocalPeer();\r HMSLocalAudioTrack myAudioTrack = myPeer.getAudioTrack();\r if(myAudioTrack  = null)  \r   myAudioTrack.setMute(true);\r  \r \r HMSLocalVideoTrack myVideoTrack = myPeer.getVideoTrack();\r if(myVideoTrack  = null)  \r   myVideoTrack.setMute(true);\r  \r  \r </Tab>\r \r "
    },
    {
        "title": "Network Quality Reports\r",
        "link": "/android/v2/features/network-quality-reports",
        "platformName": "Android",
        "objectID": "/android/v2/features/network-quality-reports",
        "keywords": [],
        "content": " -\r \r Video/Audio conferencing is by nature a data intensive operation. The 100ms SDK attempts to stabilize connections especially if subscribe degradation is turned on in the template but it's possible for really bad connections that users will still have problems.\r \r It can be helpful to measure a user's connection speed before joining a room in order to set expectations or decide to have them join with video off etc.\r \r The 100ms SDK provides a way to measure the user's downlink speed in the preview screen.\r \r > ⚠️ The downlink speed is measured by having the user download a file (1mb as of this writing) after the WebSocket connection is established during a preview.\r \r The download will be continued for at most a fixed number of seconds (For example 10 seconds) and the speed during that interval is calculated. The entire file may not be downloaded if it exceeds the timeout.\r \r DNS time is not counted in this, only throughput is measured.\r \r  Requesting/Reading a Network Quality Update in Preview\r \r When creating an HMSConfig object to request a preview, set the captureNetworkQualityInPreview to true to measure the user's downlink network quality.\r \r When available, the information will be returned in onPeerUpdate of the HMSPreviewListener in the update type HMSPeerUpdate.NETWORK_QUALITY_UPDATED . It can be retrieved out of the HMSPeer object's networkQuality property.\r \r  s id=\"network-quality-preview\" items=  'Kotlin', 'Java'    \r \r   id='network-quality-preview-0'>\r \r  kotlin\r fun requestPreviewWithNetworkQuality(hmsSdk: HMSSDK,\r    authToken : String, metadata: String)  \r \r   val requestNetworkQuality = true\r \r   val hmsPreviewListener = object : HMSPreviewListener  \r     override fun onPeerUpdate(type: HMSPeerUpdate, peer: HMSPeer)  \r       if(type == HMSPeerUpdate.NETWORK_QUALITY_UPDATED)\r         println(\"NetworkQuality is $ peer.networkQuality?.downlinkQuality \")\r      \r \r     override fun onPreview(room: HMSRoom, localTracks: Array<HMSTrack>)   \r     override fun onRoomUpdate(type: HMSRoomUpdate, hmsRoom: HMSRoom)   \r     override fun onError(error: HMSException)   \r    \r \r   val config = HMSConfig(\r     \"Aniket\",\r     authToken,\r     metadata,\r     requestNetworkQuality\r     )\r \r \r   hmsSdk.preview(config, hmsPreviewListener)\r  \r  \r \r </Tab>\r \r   id='network-quality-preview-1'>\r \r  java\r \r void requestPreviewWithNetworkQuality(HMSSDK hmsSdk,\r                     String authToken,\r                     String metadata)  \r   Boolean requestNetworkQuality = true;\r \r   HMSPreviewListener hmsPreviewListener = new HMSPreviewListener()  \r     @Override\r     public void onPeerUpdate(@NonNull HMSPeerUpdate type, @NonNull HMSPeer hmsPeer)  \r       if (type == HMSPeerUpdate.NETWORK_QUALITY_UPDATED)\r         Log.d(TAG, \"NetworkQuality is $ peer.networkQuality?.downlinkQuality \");\r      \r \r     @Override public void onPreview(@NonNull HMSRoom hmsRoom, @NonNull HMSTrack   hmsTracks)   \r     @Override public void onRoomUpdate(@NonNull HMSRoomUpdate hmsRoomUpdate, @NonNull HMSRoom hmsRoom)   \r     @Override public void onError(@NonNull HMSException e)   \r \r    ;\r \r   HMSConfig config = new HMSConfig(\r       \"Aniket\",\r       authToken,\r       metadata,\r       requestNetworkQuality\r   );\r \r   hmsSdk.preview(config, hmsPreviewListener);\r  \r  \r \r </Tab>\r \r Here's the class definition of HMSNetworkQuality , which is a property on the HMSPeer .\r \r  kotlin\r data class HMSNetworkQuality(\r   val downlinkQuality: Int\r )\r  \r \r  Interpreting the Values\r \r peer.networkQuality?.downlinkQuality will be a value between -1 and 5.\r \r   -1 -> Test timeout.\r   0 -> Very bad network or network check failure.\r   1 -> Poor network.\r   2 -> Bad network.\r   3 -> Average.\r   4 -> Good.\r   5 -> Best.\r "
    },
    {
        "title": "Peer Count",
        "link": "/android/v2/features/PeerCount",
        "platformName": "Android",
        "objectID": "/android/v2/features/PeerCount",
        "keywords": [],
        "content": "  The HMSRoom object has a property called peerCount . This property will be updated in the background and can be read when the HMSUpdateListener 's onRoomUpdate is called specifically the type HMSRoomUpdate.ROOM_PEER_COUNT_UPDATED  The android SDK will update these values whenever a peer joins or leaves. To receive these values even if no peer joins the room, look into the webhooks for the peer join events.  s id=\"peer-count\" items=  'Kotlin', 'Java'       id='peer-count-0'>   kotlin hmsSDK.join(config, object : HMSUpdateListener       override fun onRoomUpdate(type: HMSRoomUpdate, hmsRoom: HMSRoom)       when (type)         HMSRoomUpdate.ROOM_PEER_COUNT_UPDATED ->           val count = hmsRoom.peerCount                        </Tab>   id='peer-count-1'>   java hmsSdk.join(config, new HMSUpdateListener()     @Override   public void onRoomUpdate(@NonNull HMSRoomUpdate type, @NonNull HMSRoom hmsRoom)       switch (type)        case ROOM_PEER_COUNT_UPDATED:         hmsRoom.getPeerCount();       break;                </Tab> "
    },
    {
        "title": "PiP Mode (Picture-in-picture)",
        "link": "/android/v2/features/Pip-Mode",
        "platformName": "Android",
        "objectID": "/android/v2/features/Pip-Mode",
        "keywords": [],
        "content": "  100ms Android SDK provides support for creating Picture in Picture mode experience for video calls.   Minimum Requirements   Minimum version required to support PiP is Android 8.0 (API level 26)   How to add PiP support 1. You need to update the activity tag in the AndroidManifest.xml   xml     <activity       ....       android:supportsPictureInPicture=\"true\"       android:configChanges=\"screenSize smallestScreenSize screenLayout orientation\"       ...      2. Pip mode resizes your whole activity to a small conatiner. Inorder to show elements in PiP you would like to hide the visibility of views you don't want to show.   kotlin   //To launch pip-mode you can call this   activity.enterPictureInPictureMode()   //Optionally you can pass in params to have it in a particular aspect ratio. more info  https://developer.android.com/develop/ui/views/picture-in-picture      3. Once you enter pip mode, you'll recieve a callback. Over there you can put your own custom logic, to show which custom video tracks should be visibile while in PiP. To hide videos while in PiP you can call videoTrack.removeSink(surfaceViewRender)   kotlin  override fun onPictureInPictureModeChanged(isInPictureInPictureMode: Boolean)     ...   //hiding views for pip/non-pip layout     if (isInPictureInPictureMode)      //add logic here to show video tracks which would be visible     supportActionBar?.hide()     else      //add logic here to restore video tracks here    supportActionBar?.show()           You can refer to this  PR (https://github.com/100mslive/100ms-android/pull/247/files) in our sample app to see the implementation of the Pip mode 👀 To see an example Android Picture in Picture implementation using 100ms SDK, checkout  our example project (https://github.com/100mslive/100ms-android). "
    },
    {
        "title": "Preview",
        "link": "/android/v2/features/preview",
        "platformName": "Android",
        "objectID": "/android/v2/features/preview",
        "keywords": [],
        "content": "  Preview screen is a frequently used UX element which allows users to check if their input devices are working properly and set the initial state (mute/unmute) of their audio and video tracks before joining. 100ms SDKs provide an easy-to-use API to back this feature. Additionally, the SDK will try to establish a connection to 100ms server to verify there are no network issues and that the auth credentials are valid so that if everything is in order the subsequent room join is instant. To invoke this API, call preview with HMSConfig and HMSPreviewListener as params.   kotlin hmsSDK.preview(config, hmsPreviewListener)   You would need the same config object that you would pass to  join API (join).  HMSPreviewListener has three callbacks:   kotlin fun onPreview(room: HMSRoom, localTracks: Array<HMSTrack>)   Which is called when SDK has passed all its pre-flight checks and established the connection to 100ms server. This will pass an array of local audio/video tracks that you can display to the user (see  Render Video (render-video) and  Mute (mute) sections for more details).   kotlin fun onRoomUpdate(type: HMSRoomUpdate, hmsRoom: HMSRoom)   This is called when room data is updated, such as room-name changed or if peer-count changes.  type  it contains the type of HMSRoomUpdate occurred   kotlin fun onPeerUpdate(type: HMSPeerUpdate, peer: HMSPeer)   This is called when Peer information is updated, such as PEER_JOINED or PEER_LEFT etc.  type  it contains the type of HMSPeerUpdate occurred   Network Quality Reports A network quality report can be requested on preview. Additional details  here (network-quality-reports)."
    },
    {
        "title": "Remote Mute\r",
        "link": "/android/v2/features/remote-mute",
        "platformName": "Android",
        "objectID": "/android/v2/features/remote-mute",
        "keywords": [],
        "content": " -\r \r You're running a video call room and decide that someone who's currently talking shouldn't be talking.\r \r You'd prefer they'd stay mute. Or perhaps you want their video turned off as well as their audio. You're looking for a remote mute.\r \r Muting can apply to both audio and video.\r \r    Unmuting\r \r You may also decide that you want to let someone else speak who was currently muted. Or may want to ask someone to turn on their video.\r \r You can request people to un-mute themselves as well.\r \r   Permissions\r \r Can't let just anyone mute others. First you need to create a  role (../foundation/templates-and-roles) with the permissions to mute others and also to ask them to un-mute.\r \r   Permissions  Remote mute/unmute (/guides/permissions.png)\r \r The permission to mute others is within PermissionsParams  mute and you should check for that within the HMSRole of the peer to see if they have it.\r \r Similarly the permission to un-mute other peers is within PermissionsParams  unmute .\r \r Here's how to check whether the local peer has the permission to mute or un-mute others:\r \r  s id=\"muteunmute\" items=  'Kotlin', 'Java 8', 'Java 7'    \r \r   id='muteunmute-0'>\r \r  kotlin\r \r fun isAllowedToMuteOthers(): Boolean  \r   return hmsSDK.getLocalPeer()  \r         .hmsRole.permission?\r         .mute == true\r  \r \r fun isAllowedToUnMuteOthers(): Boolean  \r   return hmsSDK.getLocalPeer()  \r         .hmsRole.permission?\r         .unmute == true\r  \r \r  \r \r </Tab>\r \r   id='muteunmute-1'>\r \r  java\r \r private boolean hasMutePermissions()  \r   return Optional.ofNullable(hmsSdk.getLocalPeer())\r       .map(HMSPeer::getHmsRole)\r       .map(HMSRole::getPermission)\r       .map(PermissionsParams::getMute)\r       .orElse(false);\r  \r \r private boolean hasUnMutePermissions()  \r   return Optional.ofNullable(hmsSdk.getLocalPeer())\r       .map(HMSPeer::getHmsRole)\r       .map(HMSRole::getPermission)\r       .map(PermissionsParams::getUnmute)\r       .orElse(false);\r  \r \r  \r \r </Tab>\r \r   id='muteunmute-2'>\r \r  java\r \r private boolean hasMutePermissions()  \r   HMSLocalPeer peer = hmsSdk.getLocalPeer();\r   if (peer  = null)  \r     HMSRole role = peer.getHmsRole();\r     if (role  = null)  \r       PermissionsParams permissions = role.getPermission();\r       if (permissions  = null)  \r         return permissions.getMute();\r        \r      \r    \r   return false;\r  \r \r private boolean hasUnMutePermissions()  \r   HMSLocalPeer peer = hmsSdk.getLocalPeer();\r   if (peer  = null)  \r     HMSRole role = peer.getHmsRole();\r     if (role  = null)  \r       PermissionsParams permissions = role.getPermission();\r       if (permissions  = null)  \r         return permissions.getUnmute();\r        \r      \r    \r   return false;\r  \r \r  \r \r </Tab>\r \r hmsSdk.getLocalPeer() will not return null as long as you're in a preview or in a meeting. Since you likely won't need to check for permissions if you're not in one it would be ok.\r \r   Muting/Unmuting\r \r There are two APIs for muting/unmuting. changeTrackState is the overloaded function which handles both.\r \r   One which only works on a single track (audio/video) for a single peer at a time. changeTrackState(forRemoteTrack: HMSTrack, mute: Boolean, hmsActionResultListener: HMSActionResultListener) \r   One which can mute multiple tracks for multiple peers. Either by their role, or track source or track type or any combination of the above. changeTrackState(mute: Boolean, type : HMSTrackType?, source : String?, roles : List<HMSRole>?, hmsActionResultListener: HMSActionResultListener) \r \r For single peer/track scroll to \"Single Peer or Track\".\r \r For multiple peer/track muting simultaneously scroll to Multiple Peers or Tracks.\r \r   Single Peer or Track\r \r Once you have checked that the caller has permissions to mute another peer's audio or video, call for it with hmsSdk.changeTrackState .\r \r To mute audio for a single peer or track:\r \r hmsSdk.changeTrackState takes three parameters:\r \r hmsTrack: The HMSTrack that should be muted or unmuted. Can be audio or video.\r \r mute: A Boolean where true indicates that it should be muted. false indicates that it should be unmuted.\r \r hmsActionResultListener: A HMSActionResultListener instance which will receive onSuccess if the server acknowledges the the request as valid. onError if there was an exception along with the HMSException for it.\r \r  s id=\"audiomuteunmutecall\" items=  'Kotlin', 'Java 8', 'Java 7'    \r \r   id='audiomuteunmutecall-0'>\r \r  kotlin\r \r fun changeAudioState(hmsRemotePeer: HMSRemotePeer, mute : Boolean)  \r   val track = hmsRemotePeer.audioTrack\r   if(track  = null)  \r     hmsSdk.changeTrackState(track, mute, object : HMSActionResultListener  \r       override fun onError(error: HMSException)  \r        \r \r       override fun onSuccess()  \r        \r \r      )\r    \r  \r  \r \r </Tab>\r \r   id='audiomuteunmutecall-1'>\r \r  java\r \r public void changeAudioState(HMSRemotePeer hmsRemotePeer, Boolean mute)  \r     HMSRemoteAudioTrack track = hmsRemotePeer.getAudioTrack();\r     if(track  = null)  \r       hmsSdk.changeTrackState(track, mute, new HMSActionResultListener()  \r         @Override\r         public void onSuccess()  \r \r          \r \r         @Override\r         public void onError(@NonNull HMSException e)  \r \r          \r        );\r      \r    \r  \r \r </Tab>\r \r   id='audiomuteunmutecall-2'>\r \r  java\r \r private boolean hasMutePermissions()  \r   HMSLocalPeer peer = hmsSdk.getLocalPeer();\r   if(peer  = null)  \r     HMSRole role = peer.getHmsRole();\r     if(role  = null)  \r       PermissionsParams permissions = role.getPermission();\r       if(permissions  = null)  \r         return permissions.getMute();\r        \r      \r    \r   return false;\r  \r \r private boolean hasUnMutePermissions()  \r   HMSLocalPeer peer = hmsSdk.getLocalPeer();\r   if(peer  = null)  \r     HMSRole role = peer.getHmsRole();\r     if(role  = null)  \r       PermissionsParams permissions = role.getPermission();\r       if(permissions  = null)  \r         return permissions.getUnmute();\r        \r      \r    \r   return false;\r  \r  \r \r </Tab>\r \r Similarly the video track can be muted or unmuted.\r \r  s id=\"videomuteunmutecall\" items=  'Kotlin', 'Java'    \r \r   id='videomuteunmutecall-0'>\r \r  kotlin\r \r fun changeVideoState(hmsRemotePeer: HMSRemotePeer, mute : Boolean)  \r   val track = hmsRemotePeer.videoTrack\r   if(track  = null)  \r     hmsSdk.changeTrackState(track, mute, object : HMSActionResultListener  \r       override fun onError(error: HMSException)  \r        \r \r       override fun onSuccess()  \r        \r \r      )\r    \r  \r  \r \r </Tab>\r \r   id='videomuteunmutecall-1'>\r \r  java\r \r public void changeVideoState(HMSRemotePeer hmsRemotePeer, Boolean mute)  \r     HMSRemoteVideoTrack track = hmsRemotePeer.getVideoTrack();\r     if(track  = null)  \r       hmsSdk.changeTrackState(track, mute, new HMSActionResultListener()  \r         @Override\r         public void onSuccess()  \r \r          \r \r         @Override\r         public void onError(@NonNull HMSException e)  \r \r          \r        );\r      \r    \r  \r \r </Tab>\r \r   Handling a mute callback\r \r Mute callbacks are automatically applied to the receiver. No action is required.\r \r However there will still be a callback to onChangeTrackStateRequest after the SDK mutes the user.\r \r If you're doing something when the user receives a request, such as showing them a dialog, a check of the type would help avoid showing it unnecessarily:\r \r  kotlin\r     override fun onChangeTrackStateRequest(details: HMSChangeTrackStateRequest)  \r       if (details.track.isMute  = details.mute)  \r        // Handle HMSChangeTrackStateRequest\r        \r      \r  \r \r Without which, on a mute request the track will be muted and you'll still get a request on the caller side to mute.\r \r   Handling an unmute callback\r \r Unmute callbacks are received in the target peer's HMSUpdateListener.onChangeTrackStateRequest .\r \r The target peer will receive an object of HMSChangeTrackStateRequest .\r \r Here's its structure.\r \r  kotlin\r data class HMSChangeTrackStateRequest(\r   val track : HMSTrack,\r   val requestedBy : HMSPeer,\r   val mute : Boolean)\r  \r \r This contains information on which track is requested for unmuting. Check the track type and inform the user as appropriate.\r \r  s id=\"trackcheck\" items=  'Kotlin', 'Java'    \r \r   id='trackcheck-0'>\r \r  kotlin\r fun checkTrack(track : HMSTrack)  \r   if(track.type == HMSTrackType.AUDIO)  \r \r     else if (track.type == HMSTrackType.VIDEO)  \r \r    \r  \r  \r \r </Tab>\r \r   id='trackcheck-1'>\r \r  java\r public void checkTrack(HMSTrack track)  \r   if( track.getType() == HMSTrackType.AUDIO)  \r \r     else if (track.getType() == HMSTrackType.VIDEO)  \r \r    \r  \r  \r \r </Tab>\r \r Hold onto the information here, show a dialog to the user to ask if they want to accept the change and then apply the settings locally. The same as in a regular user  Mute/Unmute (mute).\r \r   Multiple Peers or Tracks\r \r Once you have checked that the caller has permissions to mute another peer's audio or video, call for it with hmsSdk.changeTrackState .\r \r To mute audio for a multiple peers or tracks:\r \r hmsSdk.changeTrackState takes five parameters:\r \r     mute  : _Boolean_ true if the track needs to be muted, false otherwise\r     type    HMSTrackType   optional, the HMSTrackType which should be affected. If this and   source     are specified, it is considered an AND operation. If null, all track sources are affected.\r     source   _Source_ optional, the  HMSTrackSource  which should be affected. If this and  type  are specified, it is considered an AND operation. If null, all track sources are affected.\r     roles   _ List<HMSRole> _ optional, a list of roles, may have a single item in a list, whose tracks should be affected. If null, all roles are affected.\r     hmsActionResultListener   _HMSActionResultListener_ the callback that would be called by SDK in case of a success or failure.\r \r Here's an example of how you would check if the caller was allowed to mute peers and then call for a mute/unmute on all peers in the chat.\r \r  s id=\"multimute\" items=  'Kotlin', 'Java'    \r \r   id='multimute-0'>\r \r  kotlin\r \r  fun remoteMute(mute: Boolean, roles: List<String>?)  \r   if (isAllowedToMutePeers())  \r    val selectedRoles = if (roles == null) null else  \r     hmsSdk.getRoles().filter   roles.contains(it.name)  \r     \r    hmsSdk.changeTrackState(\r      mute,\r      null,\r      null,\r      selectedRoles,\r      object: HMSActionResultListener  \r \r     override fun onSuccess()  \r      // Remote mute suceeded\r      \r \r     override fun onError(error: HMSException)  \r      // Error\r      \r \r     )\r    \r   \r  \r \r </Tab>\r \r   id='multimute-1'>\r \r  java\r private void remoteMute(boolean mute, List<String> hmsRoleNames)  \r     List<HMSRole> selectedRoles = new ArrayList<>();\r     if(hmsRoleNames  = null)  \r       selectedRoles = new ArrayList<>();\r       for( HMSRole role : hmsSdk.getRoles())  \r         if(hmsRoleNames.contains(role.getName()))  \r           selectedRoles.add(role);\r          \r        \r      \r     hmsSdk.changeTrackState(\r         mute,\r         null,\r         null,\r         selectedRoles,\r         new HMSActionResultListener()  \r           @Override\r           public void onSuccess()  \r             // Change track state succeeded.\r            \r \r           @Override\r           public void onError(@NonNull HMSException e)  \r             // Change track state failed.\r            \r          \r     );\r    \r  \r \r </Tab>\r \r This can be further narrowed by specifying only those tracks of type Audio like so:\r \r Note that HMSTrackSource.REGULAR is the peer's own audio and video as opposed to one provided by screenshare or a plugin.\r \r  s id=\"multimuteoptions\" items=  'Kotlin', 'Java'    \r \r   id='multimuteoptions-0'>\r \r  kotlin\r hmsSdk.changeTrackState(\r       true,\r       HMSTrackType.AUDIO,\r       HMSTrackSource.REGULAR,\r       emptyList(),\r       object : HMSActionResultListener  \r         override fun onSuccess()  \r \r          \r \r         override fun onError(error: HMSException)  \r \r          \r        \r     )\r  \r \r </Tab>\r \r   id='multimuteoptions-1'>\r \r  java\r     hmsSdk.changeTrackState(\r         true,\r         HMSTrackType.AUDIO,\r         HMSTrackSource.REGULAR,\r         Collections.emptyList(),\r         new HMSActionResultListener()  \r           @Override\r           public void onSuccess()  \r \r            \r \r           @Override\r           public void onError(@NonNull HMSException e)  \r \r            \r          \r     );\r  \r \r </Tab>\r "
    },
    {
        "title": "Remove Peer\r",
        "link": "/android/v2/features/remove-peer",
        "platformName": "Android",
        "objectID": "/android/v2/features/remove-peer",
        "keywords": [],
        "content": " -\r \r Someone's overstayed their welcome and now you need to remove a peer from the video call room. Just call hmsSdk.removePeerRequest .\r \r   Permissions\r \r Can't let just anyone remove others from the video call room. First you need to create a  role (../foundation/templates-and-roles) with the permissions to remove others.\r \r In the SDK, the permission to remove others from the room is within PermissionsParams  removeOthers and you should check for that within the HMSRole of the peer to see if they have it.\r \r Here's how to check whether the local peer has the permission to end the room:\r \r  s id=\"removepeerperm\" items=  'Kotlin', 'Java 8', 'Java 7'    \r \r   id='removepeerperm-0'>\r \r  kotlin\r fun isAllowedToRemovePeer(): Boolean  \r   return hmsSDK.getLocalPeer()  \r         .hmsRole.permission?\r         .removeOthers == true\r  \r  \r \r </Tab>\r \r   id='removepeerperm-1'>\r \r  java\r private boolean isAllowedToRemovePeer()  \r   return Optional.ofNullable(hmsSdk.getLocalPeer())\r       .map(HMSPeer::getHmsRole)\r       .map(HMSRole::getPermission)\r       .map(PermissionsParams::getRemoveOthers)\r       .orElse(false);\r  \r  \r \r </Tab>\r \r   id='removepeerperm-2'>\r \r  java\r private boolean isAllowedToRemovePeer()  \r   HMSLocalPeer peer = hmsSdk.getLocalPeer();\r   if(peer  = null)  \r     HMSRole role = peer.getHmsRole();\r     if(role  = null)  \r       PermissionsParams permissions = role.getPermission();\r       if(permissions  = null)  \r         return permissions.getRemoveOthers();\r        \r      \r    \r   return false;\r  \r  \r \r </Tab>\r \r hmsSdk.getLocalPeer() will not return null as long as you're in a preview or in a meeting. Since you likely won't need to check for permissions if you're not in one it would be ok.\r \r   Removing a peer\r \r Once the permissions are checked to ensure the caller has the permission to remove a peer, remove them by calling hmsSdk.removePeerRequest .\r \r The parameters are:\r \r peer: is the HMSRemotePeer that you'd like to be removed from the video call room.\r \r reason: is the string that should be conveyed to them as the reason for being removed.\r \r hmsActionResultListener: is the listener that will convey the success or error of the server accepting the request.\r \r  s id=\"removepeer\" items=  'Kotlin', 'Java'    \r \r   id='removepeer-0'>\r \r  kotlin\r fun requestPeerLeave(peer: HMSRemotePeer, reason: String)  \r   hmsSDK.removePeerRequest(peer, reason,\r     object : HMSActionResultListener \r       override fun onError(error: HMSException)  \r        \r \r       override fun onSuccess()  \r        \r    )\r   \r \r  \r \r </Tab>\r \r   id='removepeer-1'>\r \r  java\r public void requestPeerLeave(HMSRemotePeer peer, String reason)  \r   hmsSdk.removePeerRequest(peer, reason, new HMSActionResultListener()  \r     @Override\r     public void onSuccess()  \r \r      \r \r     @Override\r     public void onError(@NonNull HMSException e)  \r \r      \r    );\r  \r  \r \r </Tab>\r \r   Handling the remove peer callback\r \r The target of the removePeerRequest will receive a call in HMSUpdateListener of onRemovedFromRoom(notification : HMSRemovedFromRoom) .\r \r The HMSRemovedFromRoom object which is passed into the callback has the structure:\r \r  kotlin\r data class HMSRemovedFromRoom(\r   val reason : String,\r   val peerWhoRemoved : HMSRemotePeer?,\r   val roomWasEnded : Boolean\r )\r  \r \r reason: Is the string that the caller of removePeerRequest sent as the reason they were being removed from the room.\r \r peerWhoRemoved: Is an HMSRemotePeer instance containing the details of the person who called removePeerRequest . This can be used to show the name of the person who removed them.\r \r roomWasEnded: This will be false if the peer was removed. If true, it indicates that the peer was not removed, but the entire room was ended. See  End Room (end-room) for details.\r \r When this callback is received, the UI should be cleaned up from the client side. The video call room would be ended from the SDK once this callback is sent.\r "
    },
    {
        "title": "Render Video - SurfaceView (Deprecated)\r",
        "link": "/android/v2/features/render-video-old",
        "platformName": "Android",
        "objectID": "/android/v2/features/render-video-old",
        "keywords": [],
        "content": " -\r \r > 💡 This is the legacy way of rendering videos, you might want to check out  VideoView (https://www.100ms.live/docs/android/v2/features/render-video).\r \r At the very least for rendering video you'll need an instance of org.WebRTC.SurfaceViewRenderer and a HMSVideoTrack instance from a peer.\r \r For the SurfaceViewRenderer , as of 2.0.9 of the 100ms Android SDK, only adding the SDK is enough to receive WebRTC objects including the SurfaceViewRenderer. In versions prior to this, add the dependency implementation 'org.WebRTC:google-WebRTC:1.0.32006' to receive SurfaceViewRenderer.\r \r In XML layouts it would look like\r \r  xml\r   <org.WebRTC.SurfaceViewRenderer\r     android:id=\"@+id/peerVideo\"\r     android:layout_width=\"match_parent\"\r     android:layout_height=\"wrap_content\"\r      \r  \r \r The peer's video is in hmsPeer.videoTrack and their ScreenShare video will be an instance of HMSVideoTrack in the list hmsPeer.auxiliaryTracks .\r \r You would get them like so:\r \r  kotlin\r val hmsVideoTrack : HMSVideoTrack? = hmsPeer.videoTrack\r  \r \r   Getting and Preparing a SurfaceView\r \r Get an instance of the SurfaceView.\r \r  kotlin\r val surfaceView : SurfaceViewRenderer = findViewById(R.id.surface_view)\r  \r \r Set the scaling to whatever you prefer. Aspect Fit is recommended for common use cases. See more scale types  here (https://chromium.googlesource.com/external/WebRTC/trunk/talk/+/cdd35e557b600d1f7dbe9c99531a42adf194f973/app/WebRTC/java/android/org/WebRTC/RendererCommon.java 37).\r \r  kotlin\r surfaceView.setEnableHardwareScaler(true)\r surfaceView.setScalingType(RendererCommon.ScalingType.SCALE_ASPECT_FIT)\r  \r \r   Adding a video to the SurfaceView\r \r Each time you want to add a video to the surfaceview, it needs to be initialized again.\r \r  kotlin\r surfaceView.init(SharedEglContext.context, null)\r \r val hmsVideoTrack : HMSVideotrack = hmsPeer.videoTrack\r hmsVideoTrack?.addSink(surfaceView)\r  \r \r It's also important to remove it when done.\r \r > 💡 Only the SharedEglContext.context can be used. Creating a new context will result in a black screen.\r \r   Removing a video from the SurfaceView\r \r It's very important to remove the video when you're done showing it, or before you show a different video on the same SurfaceViewRenderer.\r \r  kotlin\r hmsVideoTrack.removeSink(this)\r surfaceView.release()\r  \r \r There is a limit to the number of SharedEglContext 's that can be bound per mobile device. It's normally high enough to easily accomodate all the videos that will reasonably fit on a screen. If the video's aren't released however, it becomes very easy to exceed this limit and the app will crash.\r \r Until you call removeSink the video is being streamed over the device's data so it's also important remove sinks.\r \r   Special cases for videos in RecyclerViews\r \r If you're going to be using a RecyclerView to display multiple videos in a grid there are specific places where you should do these operations.\r \r Currently the best known way is in the ViewHolder to have a method for binding items, a method to start the surfaceview and a method to release the surfaceview along a boolean check.\r \r The Adapter calls bind item as usual but also stops the surfaceview when it binds. Two additional Adapter methods are overloaded. onViewAttachedToWindow and onViewDetachedFromWindow which will call SurfaceView binding and releasing respectively but while checking the sinkAdded property.\r \r sinkAdded keeps track of whether a track was rendered to a Peer's surface view or not. Since someone might have turned off their video but still be speaking and so a tile might be shown with no video.\r \r Complete source code is available  here (https://github.com/100mslive/hello-world-android).\r \r   PeerViewHolder\r \r init Scaler types can be set here and don't need to be reset per bind call.\r \r startSurfaceView will init the SurfaceViewRenderer and binding the track if one exists, this also sets the sinkAdded to true.\r \r stopSurfaceView will release the SurfaceViewRenderer if there was a video attached and also remove the video sink.\r \r  kotlin:PeerViewHolder.kt\r class PeerViewHolder(view: View, private val getItem: (Int) -> TrackPeerMap) :\r   RecyclerView.ViewHolder(view)  \r   private val TAG = PeerViewHolder::class.java.simpleName\r   private var sinkAdded = false\r \r   init  \r     itemView.findViewById<SurfaceViewRenderer>(R.id.videoSurfaceView).apply  \r       setEnableHardwareScaler(true)\r       setScalingType(RendererCommon.ScalingType.SCALE_ASPECT_FIT)\r      \r    \r \r   fun startSurfaceView()  \r     if ( sinkAdded)  \r       itemView.findViewById<SurfaceViewRenderer>(R.id.videoSurfaceView).apply  \r \r         getItem(adapterPosition).videoTrack?.let   hmsVideoTrack ->\r           init(SharedEglContext.context, null)\r           hmsVideoTrack.addSink(this)\r           sinkAdded = true\r          \r        \r      \r    \r \r   fun stopSurfaceView()  \r     itemView.findViewById<SurfaceViewRenderer>(R.id.videoSurfaceView).apply  \r \r       if (sinkAdded && adapterPosition  = -1)  \r         getItem(adapterPosition).videoTrack?.let  \r           it.removeSink(this)\r           release()\r           sinkAdded = false\r          \r        \r      \r    \r \r   fun bind(peer: TrackPeerMap)  \r \r     if ( sinkAdded)  \r       itemView.findViewById<SurfaceViewRenderer>(R.id.videoSurfaceView).apply  \r         setEnableHardwareScaler(true)\r         setScalingType(RendererCommon.ScalingType.SCALE_ASPECT_FIT)\r         sinkAdded = false\r        \r      \r     itemView.findViewById<TextView>(R.id.peerName).text = peer.peer.name\r    \r \r  \r  \r \r   PeerAdapter\r \r The adapter stops the surface every time it binds. Starts it when the view is attached to the window, and stops it when the view is detached from the window. Also it passes its own getItem method to the PeerViewHolder in onCreateViewHolder .\r \r  kotlin:PeerAdapter.kt\r \r   override fun onBindViewHolder(holder: PeerViewHolder, position: Int)  \r     getItem(position)?.let  \r       holder.stopSurfaceView()\r       holder.bind(it)\r      \r    \r \r   override fun onViewAttachedToWindow(holder: PeerViewHolder)  \r     super.onViewAttachedToWindow(holder)\r     holder.startSurfaceView()\r    \r \r   override fun onViewDetachedFromWindow(holder: PeerViewHolder)  \r     super.onViewDetachedFromWindow(holder)\r     holder.stopSurfaceView()\r    \r \r   override fun onCreateViewHolder(parent: ViewGroup, viewType: Int): PeerViewHolder  \r     val view =\r       LayoutInflater.from(parent.context).inflate(R.layout.layout_peer_item, parent, false)\r     return PeerViewHolder(view, ::getItem)\r    \r \r  "
    },
    {
        "title": "Render Video - VideoView\r",
        "link": "/android/v2/features/render-video",
        "platformName": "Android",
        "objectID": "/android/v2/features/render-video",
        "keywords": [],
        "content": " -\r \r At the very least for rendering video you'll need an instance of HMSVideoView and a HMSVideoTrack instance from a peer.\r \r   Adding HMSVideoView dependency\r \r The latest HMSVideoView SDK version is:\r \r     (https://img.shields.io/badge/dynamic/xml.svg?label=100ms&color=blue&query=%2F%2Fmetadata%2Fversioning%2Flatest&url=https%3A%2F%2Frepo1.maven.org%2Fmaven2%2Flive%2F100ms%2Fandroid-sdk%2Fmaven-metadata.xml) (https://jitpack.io/ 100mslive/android-sdk) \r \r \r  json  :build.gradle section=AddSDKDependencies sectionIndex=1\r dependencies  \r def hmsVersion = \"x.x.x\"\r implementation \"live.100ms:android-sdk:$hmsVersion\"  // Essential base sdk\r implementation \"live.100ms:video-view:$hmsVersion\" // HMSVideoView\r  \r  \r \r  Minimum Requirements\r \r   SDK version 2.5.3 or higher\r \r \r In XML layouts it would look like\r \r  xml\r   <live.hms.videoview.HMSVideoView\r     android:id=\"@+id/peer_video\"\r     android:layout_width=\"match_parent\"\r     android:layout_height=\"wrap_content\"\r      \r  \r \r The peer's video is in hmsPeer.videoTrack and their ScreenShare video will be an instance of HMSVideoTrack in the list hmsPeer.auxiliaryTracks .\r \r You would get them like so:\r \r  kotlin\r val hmsVideoTrack : HMSVideoTrack? = hmsPeer.videoTrack\r  \r \r   Getting HMSVideoView\r \r Get an instance of the HMSVideoView.\r \r  kotlin\r val hmsVideoView : HMSVideoView = findViewById(R.id.peer_video)\r  \r \r Set the scaling to whatever you prefer. Aspect Fit is recommended for common use cases. See more scale types  here (https://chromium.googlesource.com/external/WebRTC/trunk/talk/+/cdd35e557b600d1f7dbe9c99531a42adf194f973/app/WebRTC/java/android/org/WebRTC/RendererCommon.java 37).\r \r  kotlin\r hmsVideoView.setScalingType(RendererCommon.ScalingType.SCALE_ASPECT_FIT)\r  \r \r   Adding a video to the HMSVideoView\r \r Each time you want to add a video to the HMSVideoView, you'll just need to update addTrack() .\r \r  kotlin\r val hmsVideoTrack : HMSVideotrack = hmsPeer.videoTrack\r hmsVideoView.addTrack(hmsVideoTrack)\r  \r \r It's also important to remove it when done.\r \r   Removing a video from the HMSVideoView\r \r It's very important to remove the video when you're done showing it.\r \r  kotlin\r hmsVideoView.removeTrack()\r  \r \r Until you call removeTrack() the video frame is being streamed over the device's data.\r \r   Capture Video Frame\r You can get a bitmap object of the frame getting rendered on the HMSVideoView \r \r In case you are calling this API on a videoView which is rendering localVideoTrack , it will return a bitmap whose resolution would be equal to the resolution at which this peer is supposed to publish.\r \r \r  kotlin\r hmsVideoView.captureBitmap(onBitmap: (Bitmap?) -> Unit, scale: Float = 1.0f)\r  \r \r   Disabling Auto Simulcast\r \r HMSVideoView has an automatic simulcast layer selection capability which is enabled if adaptive bitrate is enabled. You can check more about it  here (https://www.100ms.live/docs/android/v2/foundation/adaptive-bitrate) \r \r It will select a layer that best matches the current view frame size and reacts to frame updates. In case manual layer selection is preferred set disableAutoSimulcastLayerSelect property to true . By default, the track layer is set to high .\r \r  kotlin\r hmsVideoView.disableAutoSimulcastLayerSelect(true)\r  \r \r \r   Special cases for videos in RecyclerViews\r \r If you're going to be using a RecyclerView to display multiple videos in a grid there are specific places where you should do these operations.\r \r Currently the best known way is in the ViewHolder to have a method for binding items, a method to start the HMSVideoView addTrack() and a method to removeTrack() the hmsVideoView.\r \r Two additional Adapter methods are overloaded. onViewAttachedToWindow and onViewDetachedFromWindow which will call HMSVideoView adding and removing video track respectively.\r \r Complete source code is available  here (https://github.com/100mslive/hello-world-android).\r \r   PeerViewHolder\r \r init Video-Scale type can be set here.\r \r startHMSVideoView will add the video track.\r \r stopHMSVideoView will remove the attched video track.\r \r  kotlin:PeerViewHolder.kt\r class PeerViewHolder(view: View, private val getItem: (Int) -> TrackPeerMap) :\r   RecyclerView.ViewHolder(view)  \r \r   init  \r     itemView.findViewById<HMSVideoView>(R.id.peer_video).apply  \r       setScalingType(RendererCommon.ScalingType.SCALE_ASPECT_FIT)\r      \r    \r \r   fun startHMSVideoView()  \r       itemView.findViewById<HMSVideoView>(R.id.peer_video).apply  \r \r         getItem(adapterPosition).videoTrack?.let   hmsVideoTrack ->\r           hmsVideoTrack.addTrack(this)\r          \r        \r    \r \r   fun stopHMSVideoView()  \r     itemView.findViewById<HMSVideoView>(R.id.peer_video).apply  \r \r       if (adapterPosition  = -1)  \r         getItem(adapterPosition).videoTrack?.let   hmsVideoTrack ->\r           hmsVideoTrack.removeTrack()\r          \r        \r      \r    \r \r   fun bind(peer: TrackPeerMap)  \r \r     itemView.findViewById<HMSVideoView>(R.id.peer_video).apply  \r       setScalingType(RendererCommon.ScalingType.SCALE_ASPECT_FIT)\r      \r     ...\r     itemView.findViewById<TextView>(R.id.peerName).text = peer.peer.name\r    \r \r  \r  \r \r   PeerAdapter\r \r \r  kotlin:PeerAdapter.kt\r \r   override fun onBindViewHolder(holder: PeerViewHolder, position: Int)  \r     getItem(position)?.let  \r       holder.stopHMSVideoView()\r       holder.bind(it)\r      \r    \r \r   override fun onViewAttachedToWindow(holder: PeerViewHolder)  \r     super.onViewAttachedToWindow(holder)\r     holder.startHMSVideoView()\r    \r \r   override fun onViewDetachedFromWindow(holder: PeerViewHolder)  \r     super.onViewDetachedFromWindow(holder)\r     holder.stopHMSVideoView()\r    \r \r   override fun onCreateViewHolder(parent: ViewGroup, viewType: Int): PeerViewHolder  \r     val view =\r       LayoutInflater.from(parent.context).inflate(R.layout.layout_peer_item, parent, false)\r     return PeerViewHolder(view, ::getItem)\r    \r \r  \r "
    },
    {
        "title": "RTMP Streaming / Recording",
        "link": "/android/v2/features/rtmp-recording",
        "platformName": "Android",
        "objectID": "/android/v2/features/rtmp-recording",
        "keywords": [],
        "content": "  Want to preserve your video call for posterity in a recording? Or live stream it out to millions of viewers on Twitch or YouTube or whatever gives you an RTMP ingest URL? Turn on RTMP Streaming or Recording  In 100ms, recording and streaming is usually achieved by having a bot join your room and stream what it sees and hears to a file (recording) or to an RTMP ingest URL (streaming). The topics covered in this doc are: 1.  How to start streaming / recording. ( starting-streaming-recording) 2.  How to stop streaming / recording. ( stopping-streaming-recording) 3.  How to check the current status for streaming / recording. ( current-room-status) 4.  When to check the current status ( when-to-check-for-room-status)   Starting Streaming / Recording To start recording, streaming or both, create an instance of HMSRecordingConfig .  HMSRecordingConfig takes the following: 1.   meetingUrl  : _String_. The URL the 100ms bot user will open to join your room. It must allow access without any user level interaction. 2.    rtmpUrls   : _List<String >_. If streaming is required, this has to be one or more RTMP ingest Urls with a max limit of 3 urls where the stream should go. If only recording, this can be an empty list.    Format: rtmp://server.com/app/STREAM_KEY    Example: rtmp://a.rtmp.youtube.com/live2/k0jv-329m-1y7f-ktth-ck48      \"rtmp://a.rtmp.youtube.com/live2/\"  RTMP stream URL.      \"k0jv-329m-1y7f-ktth-ck48\"  RTMP stream key.    Please refer to the platform specific documentation for details on how to obtain the stream URL and stream key. Here are some examples:      YouTube (https://support.google.com/youtube/answer/2907883?hl=en&ref_topic=9257892)      Facebook (https://www.facebook.com/help/587160588142067)      Instagram (https://about.instagram.com/blog/tips-and-tricks/instagram-live-producer)      Twitch (https://help.twitch.tv/s/article/twitch-stream-key-faq?language=en_US)      LinkedIn (https://www.linkedin.com/help/linkedin/answer/a564446/go-live-using-a-custom-stream-rtmp) 3.   record  : _Boolean_. If recording is required, set true. If recording is not required, set false. This value has no effect on streaming. 4.   resolution  : _HMSRtmpVideoResolution_. An optional value for the output resolution of the stream. For instance default is landscape at 1280x720 but this could be set for a portrait mode of 720x1280. Or smaller values like 480x80. The HMSRtmpVideoResolution takes Width and Height.   If both rtmpUrls and record = true are provided, both streaming and recording will begin.   If only rtmpUrls are provided, only streaming will begin.   If only record true is provided, only recording will begin. If either one is started, the other can't be started without first stopping whatever is running. Eg: Only streaming is started. Recording can't be started unless streaming is stopped first. If both are required, they have to be started together by providing both RTMP ingest Urls and recording = true. The result of the action is sent in the callback for HMSActionResultListener . An attempt to start streaming/recording which is successful then onSuccess will be called. On failure to start then an error will be sent in onError(error: HMSException) .  s id=\"streamingrecording\" items=  'Kotlin', 'Java'       id='streamingrecording-0'>   kotlin hmsSdk.startRtmpOrRecording(hmsRecordingConfig, object : HMSActionResultListener     override fun onSuccess()       // started successfully       override fun onError(error: HMSException)       // an error occurred       )   </Tab>   id='streamingrecording-1'>   java hmsSdk.startRtmpOrRecording(hmsRecordingConfig, new HMSActionResultListener()     @Override   public void onSuccess()       // started successfully       @Override   public void onError(@NonNull HMSException e)       // an error occurred      );   </Tab>   Stopping Streaming / Recording To stop streaming AND recording. It is not currently possible to stop just one, whatever is running will be stopped. Here's how to stop both: The stopRtmpAndRecording only takes a single parameter, a callback for HMSActionResultListener . An attempt to stop streaming and recording which is successful then onSuccess will be called. On failure to stop then an error will be sent in onError(error: HMSException) .  s id=\"stopstreamingrecording\" items=  'Kotlin', 'Java'       id='stopstreamingrecording-0'>   kotlin hmsSdk.stopRtmpAndRecording(object : HMSActionResultListener     override fun onSuccess()       // Stop succeeded.       override fun onError(error: HMSException)       // Error while stopping.      )    </Tab>   id='stopstreamingrecording-1'>   java hmsSdk.stopRtmpAndRecording(new HMSActionResultListener()     @Override   public void onSuccess()       // Stop succeeded.       @Override   public void onError(@NonNull HMSException e)       // Error while stopping.      );   </Tab>   Current Room Status The current status for the room is always reflected in the HMSRoom object that is returned from the HMSUpdateListener . Here are the relevant properties inside the HMSRoom object which you can read to get the current recording/streaming status of the room namely: rtmpHMSRtmpStreamingState , browserRecordingState , serverRecordingState and hlsStreamingState . Each of them (except HLS) are objects which contain a boolean running which lets you know if it's active on the room right now and error which lets you know the details of an error if one occurred, and startedAt which contains a  UNIX epoch (https://www.epochconverter.com/) timestamp of when the stream began. Apart from the RTMP stream and the browser recording, which are ones you can start and stop, there is also a serverRecording, which can be turned on for the room for archival purposes and which cannot currently be stopped if enabled for the room from the dashboard. HLS streaming can be started as well. 1.   rtmpHMSRtmpStreamingState   an instance of HMSRtmpStreamingState , which looks like:   kotlin data class HMSRtmpStreamingState(   val running : Boolean,   val error : HMSException?,   val startedAt: Long?,   val stoppedAt: Long? )   This represents a livestream to one or more RTMP urls.  startedAt is the unix epoch timestamp of when this recording was started.  stoppedAt is the unix epoch timestamp of when the recording was ended. Both may be present and this means the recording was started at some point and stopped at a later date. If the recording is then restarted, it's possible for the stoppedAt to be before the startedAt . 2.   browserRecordingState   an instance of HMSBrowserRecordingState , which looks like:   kotlin data class HMSBrowserRecordingState(   val running : Boolean,   val error : HMSException?,   val startedAt: Long?,   val stoppedAt: Long? )   This represents the recording that can be requested to start.  startedAt is the unix epoch timestamp of when this recording was started.  stoppedAt is the unix epoch timestamp of when the recording was ended. Both may be present and this means the recording was started at some point and stopped at a later date. If the recording is then restarted, it's possible for the stoppedAt to be before the startedAt . 3.   serverRecordingState   an instance of HMSServerRecordingState , which looks like:   kotlin data class HMSServerRecordingState(   val running : Boolean,   val error : HMSException?,   val startedAt: Long? )   This represents that the room was set to be recorded when it was created and all sessions within it will always be recorded for archival by the server.  startedAt is the unix epoch timestamp of when this recording was started. However server-side has a special case for recording. It is considered started when the current room's session has begun. A session is defined as the time from which the room goes from zero peers in it, to one, until the time at which the room contains no peers. (note: beam bots which join for recording will leave after a timeout if no other peers are present.) Because the server side recording always begins when there's someone in the room the room creation time and this recording type's start time will be the same. Also there can't be a stoppedAt since that would mean the room has ended.   When to check for room status As of SDK version 2.2.2, the room status can be checked when any of the three events of type HMSRoomUpdate in HMSUpdateListener appear:   HMSRoomUpdate.SERVER_RECORDING_STATE_UPDATED,   HMSRoomUpdate.RTMP_STREAMING_STATE_UPDATED   HMSRoomUpdate.BROWSER_RECORDING_STATE_UPDATED  The properties mentioned above will be on the HMSRoom object.   The values of the streaming and recording will be updated on the room object returned in onJoin.   So saving an instance of the room received at that time is recommended.   Code sample Refer to the code example within the  100ms advanced sample app (https://github.com/100mslive/100ms-android/blob/0ebfc6527bafbda7236d5245dedb5fefb262bfcc/app/src/main/java/live/hms/app2/ui/meeting/MeetingViewModel.kt L295) for further information. "
    },
    {
        "title": "Screen Share",
        "link": "/android/v2/features/screen-share",
        "platformName": "Android",
        "objectID": "/android/v2/features/screen-share",
        "keywords": [],
        "content": "  Android SDK provides support for sharing the entire screen of the device to the room. Please note that for a peer to share their screen, their role must have screenshare enabled in the dashboard. Also select the appropriate resolution that you desire the screenshare track should be of.   ScreenshareDashboard (/docs/v2/screenshare-dashboard.png)   How to start screenshare from the app (without a custom notification) To start screen share, app needs to call the startScreenshare method of HMSSDK , which takes in two parameters    The first one is HMSActionResultListener which is a callback object needed to inform about success     or failure of the action   The second one is a Intent which is the result data of MediaProjection permission activity;   the calling app must validate that result code is Activity.RESULT_OK before   calling this method. Following is the snippet on how to use this:  s id=\"startscreenshare\" items=  'Kotlin'       id='startscreenshare-0'>   kotlin // Define a result Launcher  var resultLauncher = registerForActivityResult(ActivityResultContracts.StartActivityForResult())   result ->   if (result.resultCode == Activity.RESULT_OK)      val mediaProjectionPermissionResultData: Intent? = result.data    // Pass this intent to hmsSDK    hmsSDK.startScreenshare(object : HMSActionResultListener       override fun onError(error: HMSException)        // an error occurred           override fun onSuccess()        // started successfully            ,mediaProjectionPermissionResultData)          // Get the MEDIA_PROJECTION_SERVICE and launch the result launcher   val mediaProjectionManager: MediaProjectionManager? = requireContext().getSystemService(         Context.MEDIA_PROJECTION_SERVICE        ) as MediaProjectionManager   resultLauncher.launch(mediaProjectionManager?.createScreenCaptureIntent())    It is important to note that screenshare will not work if the resultCode is NOT Activity.RESULT_OK . SDK will also start a foreground service with a notification when screenshare is started, which is mandated above Android 10.   How to add a custom notification To take a look at how this is implemented in our sample app, click  here (https://github.com/100mslive/100ms-android/blob/3590777bf0a3677cb9b5001034ea9ed7087f9d90/app/src/main/java/live/hms/app2/ui/meeting/MeetingViewModel.kt L860).  hmssdk.startScreenShare also takes an optional third parameter of type notification. See how to build a custom notification in the android docs  here (https://developer.android.com/training/notify-user/build-notification). It could look something like this: Since this is a standard Android notification the full range of customization is available.   kotlin  fun startScreenshare(mediaProjectionPermissionResultData: Intent?, actionListener: HMSActionResultListener)     // Without custom notification   // hmsSDK.startScreenshare(actionListener ,mediaProjectionPermissionResultData)   // With custom notification   val notification = NotificationCompat.Builder(getApplication(), \"ScreenCapture channel\")    .setContentText(\"Screenshare running for roomId: $ hmsRoom?.roomId \")    .setSmallIcon(R.drawable.arrow_up_float)    .build()   hmsSDK.startScreenshare(actionListener, mediaProjectionPermissionResultData, notification)       </Tab>   How to get Screen Share Status Application needs to call the isScreenShareActive method of HMSSDK . This method returns a Boolean which will be true inscase ScreenShare is currently active and being used, and False for inactive state.  s id=\"screensharestatus\" items=  \"Kotlin\"       id='screensharestatus-0'>   kotlin  hmsSDK.isScreenShareActive()    </Tab>   How to stop the screenshare and dissmiss the foreground notification from the notification. To stop the screenshare, create a custom notification and add the following action to it. The image could be anything, but the action has to be gotten from the HMSScreenCaptureService. This will create a \"STOP SCREENSHARE\" button on the intent. This pending intent could be passed anywhere you'd like, including on tap of the notification itself.  kotlin val notification = NotificationCompat.Builder(getApplication(), \"ScreenCapture channel\")    .addAction(R.drawable.ic_menu_close_clear_cancel, \"Stop Screenshare\", HMSScreenCaptureService.getStopScreenSharePendingIntent(getApplication()))    .build()     How to stop screenshare Application needs to call the stopScreenshare method of HMSSDK and pass an instance of HMSActionResultListener to listen to the callbacks. SDK also stops the foreground service on calling this method.  s id=\"stopscreenshare\" items=  'Kotlin'       id='stopscreenshare-0'>   kotlin    hmsSDK.stopScreenshare(object : HMSActionResultListener       override fun onError(error: HMSException)        // an error occurred           override fun onSuccess()        // stopped successfully            )    </Tab>  >  It is advisable to call leave API from the onDestroy() of the activity / fragment that   started the screenshare otherwise the foreground service will be running if even user kills the   app from the Recents tab. >  DONOT forget to add the permission for foreground service   kotlin   <uses-permission android:name=\"android.permission.FOREGROUND_SERVICE\"     "
    },
    {
        "title": "Adaptive Bitrate",
        "link": "/android/v2/foundation/adaptive-bitrate",
        "platformName": "Android",
        "objectID": "/android/v2/foundation/adaptive-bitrate",
        "keywords": [],
        "content": "  Adaptive bitrate (ABR) refers to features that enable dynamic adjustments to video quality—to optimise for end-user experience under diverse network conditions. ABR ensures that every participant is able to consume the highest possible quality video in conferencing or streaming use-cases, based on their bandwidth constraints.\r \r In addition to network, ABR can also optimise for the right video quality based on the size of the video element. For example, a video call running on active speaker layout has larger video tiles that require higher quality video track. These adjustments can be made dynamically with adaptive bitrate.\r \r Learn about how 100ms enables adaptive bitrate in:\r \r    Conferencing scenarios ( abr-in-conferencing)\r    Live streaming scenarios ( abr-in-live-streaming)\r \r  ABR in conferencing\r \r Peers in 100ms rooms can publish multiple video quality levels simultaneously. This is called “simulcast” in 100ms. Peers that consume these video tracks can upgrade or downgrade video quality.\r \r You can enable simulcast on the publishing role's template, and use manual or automatic layer changes on the subscriber's side.\r \r   Publisher-side configuration\r \r Simulcast configuration is opt-in and can be enabled on the role's configuration inside your template. The role's publish video quality determines video quality layers on simulcast. For example, a role configured to publish at 720p can simulcast 180p, 360p and 720p layers.\r \r  Video publish quality  Possible simulcast layers \r               \r  1080p          1080p, 540p, 270p     \r  720p          720p, 360p, 180p      \r  480p          480p, 240p         \r  360p          360p, 180p         \r  240p          240p            \r  180p          180p            \r \r   Enable via dashboard\r \r Enable \"can publish simulcast\" on the template page for a particular role. You can also specify how many video quality layers will be simultaneously published by peers of this role. The peer will publish these layers assuming network bandwidth permits.\r \r   Simulcast configuration (/docs/guides/simulcast-on-dashboard.png)\r \r   Enable via API\r \r Update role configuration using the  server-side API (/docs/server-side/v2/policy/create-update-role). The simulcast config payload can include 2 or 3 layers that scale down the selected publish layer.\r \r In the example below, the role is configured to publish 720p with 3 simulcast layers:\r \r   f for full with scale down factor of 1 (= 720p)\r   h for half with scale down factor of 2 (= 360p)\r   q for quarter with scale down factor of 4 (= 180p)   js     \"publishParams\":       ...     \"simulcast\":         \"video\":           \"layers\":                           \"rid\": \"f\",             \"scaleResolutionDownBy\": 1,             \"maxBitrate\": 700,             \"maxFramerate\": 30            ,                         \"rid\": \"h\",             \"scaleResolutionDownBy\": 2,             \"maxBitrate\": 250,             \"maxFramerate\": 30            ,                         \"rid\": \"q\",             \"scaleResolutionDownBy\": 4,             \"maxBitrate\": 100,             \"maxFramerate\": 30                              ,       \"screen\":                  \r   Subscribe-side behavior\r \r   Manual layer selection\r \r The 100ms client-side SDKs provide methods to set a preferred quality layer for a remote peer's video track. See docs for your preferred platform:\r \r    JavaScript (/docs/javascript/v2/advanced-features/simulcast)\r    iOS (/docs/ios/v2/advanced-features/simulcast)\r    Android (/docs/android/v2/advanced-features/simulcast)\r \r   Automatic layer selection\r \r <br \r <video loop=\"true\" controls=\"controls\" id=\"vid\" muted>\r   <source src=\"/docs/guides/simulcast-tile-size-1.mp4\" type=\"video/mp4\"  \r </video>\r <br \r \r  Based on video tile size: The SDK automatically ensures appropriate video layer is subscribed to, as demonstrated in the video above. For example, if the video element is 360 px in width, 360p or the closest layer will be selected.\r      JavaScript  : The useVideo hook in the 100ms React SDK auto-selects the appropriate video quality layer.\r      iOS  : HMSVideoView can auto-select video quality layer.\r      Android  : HMSVideoView ( see docs (/docs/android/v2/migrations/surfaceview-migration)) can auto-select video quality layer.\r  Based on network quality: ABR will work alongside subscribe degradation and auto-downgrade video quality for peers. This is _coming soon_.\r \r  ABR in live streaming\r \r 100ms uses the HTTP Live Streaming (HLS) protocol in live streaming scenarios. HLS supports adaptive bitrate out of the box, and HLS video players can automatically or manually pick appropriate video quality levels.\r \r Learn more on  how HLS works on our blog (https://www.100ms.live/blog/hls-101-beginners-guide).\r "
    },
    {
        "title": "Basics\r",
        "link": "/android/v2/foundation/basics",
        "platformName": "Android",
        "objectID": "/android/v2/foundation/basics",
        "keywords": [],
        "content": " -\r \r  Architecture 100ms is a cloud platform that allows developers to add video and audio conferencing to Web, Android and iOS applications. The platform provides REST APIs, SDKs, and a dashboard that makes it simple to capture, distribute, record, and render live interactive audio, video. Any application built using 100ms' SDK has 2 components.     Client:   Use 100ms android, iOS, Web SDKs to manage connections, room states, render audio/video.     Server:   Use 100ms' APIs or dashboard to create rooms, setup room templates, trigger recording or RTMP streaming, access events.   Architecture (/docs/docs/v2/arch.png)   Basic Concepts   Room A room is the basic object that 100ms SDKs return on successful connection. This contains references to peers, tracks and everything you need to render a live a/v or live streaming app.   Peer A peer is the object returned by 100ms SDKs that contains all information about a user  name, role, video track etc.   Session A session depicts activity in a room. A session is created when one or more peers join a room to communicate with each other. A single room can have multiple sessions; a unique ID will be assigned to each session. The maximum allowed duration for a session on the 100ms platform is 12 hours.   Track A track is a segment of media (audio/video) captured from the peer's camera and microphone. Peers in a session publish local tracks and subscribe to remote tracks from other peers.   Role A role defines who can a peer see/hear, the quality at which they publish their video, whether they have permissions to publish video/screenshare, mute someone, change someone's role.   Template A template is a collection of roles, room settings, recording and RTMP settings (if used), that are used by the SDK to decide which geography to connect to, which tracks to return to the client, whether to turn on recording when a room is created, etc. Each room is associated with a template.   Destinations Destinations is used to save audio/video calls for offline viewing. 100ms supports 2 kinds of recording   SFU recording (/docs/javascript/v2/foundation/recordings sfu-recording-advanced) and  Browser recording (/docs/javascript/v2/foundation/recordings browser-recording-recommended). Also, HLS enabled configuration will allow you to live stream your room over HLS.   RTMP RTMP streaming is used to live stream your video conferencing apps to platforms like YouTube, Twitch, Facebook, MUX, etc.   Webhooks Webhook is an HTTP(S) endpoint used for pushing the notifications to your application. It will be invoked by 100ms servers to notify events of your room.   Workspace A workspace is an isolated environment which contains account data like templates, rooms, room and session history, etc. You can use workspaces to represent environments like “Production” and “Development” and invite team members to a workspace.   What are the steps to build a live app with 100ms? 1. Sign up on 100ms using the   Try For Free   button in the top navbar.   Signup for 100ms account (/docs/docs/v2/signup.png) 2. Once you're logged in to the dashboard, click on Create Your First App    Signup for 100ms account (/docs/docs/v2/create-your-first-app.png) 3.   Hover   on one of the Starter Kits. Deploy one of them. (We will use the   Video Conference Starter Kit   for this example)   Dashboard _ 100ms.png (/docs/docs/v2/select-starter-kit.png) 4. Select your account type and fill in the details   Dashboard _ 100ms.png (/docs/docs/v2/personal-details.png) 5. Choose a deployment option. This could be 100ms or Vercel (based on the Starter Kit you are deploying)   Video Conferencing Starter Kit (/docs/docs/v2/choose-your-deployment.png) 6. Enter a subdomain of your choice. Please avoid entering https/http/www or dots while entering the subdomain. Select a region closest to you and hit Continue.   choose subdomain (/docs/docs/v2/choose-subdomain.png) 7. Join or Invite someone to your deployed app with one of the roles:    join or invite (/docs/docs/v2/demo-your-app.png)   Where should I start? \r \r   Quickstart\r \r If you want to see 100ms' SDKs in action in under 5 minutes, run one of our quickstart  apps (/android/v2/guides/quickstart)\r "
    },
    {
        "title": "Handling audio-video edge cases\r",
        "link": "/android/v2/foundation/handling-audio-video-edge-cases",
        "platformName": "Android",
        "objectID": "/android/v2/foundation/handling-audio-video-edge-cases",
        "keywords": [],
        "content": " -\r \r  Introduction 100ms handles a lot of standard audio/video issues internally without the developer needing to handle it explicitly. This page describes some common issues and how 100ms handles them. There are 3 major issues of issues that can occur in a audio/video conference  Device capture exceptions  Network disconnection/switching network exceptions  Network bandwidth limitation/large room exceptions   Device failure A common issue is a failure to capture mic/camera even though the user has all devices connected. Common causes include differences in OS/browser implementations of device capture APIs, permission not being granted by the user, or the device being in use by another program. The usual recourse in these exceptions is to prompt a user action  \"Grant permission\", \"Please close any other app using microphone\", \"Switch to Safari\" 100ms' SDKs come with a  preview method (../features/preview) that can be called before joining a room. This will test for device failures, network connectivity and throw errors with a recommended user action.   Network disconnection/Switching networks Another set of common issues are minor network blips. Common causes are when a user moves from one room to another, or switches from wifi to data. 100ms will send a notification within 10s of detecting a network disconnection and will automatically retry when connection is available upto 60s. After 60s, a terminal error is thrown to the client.   Network bandwidth limitation/large rooms A common occurrence in large rooms, or constrained networks is dropped frames. This results in robotic voices, frozen frames, pixelated screenshare or entire pieces of audio/video that are lost. 100ms will automatically prioritize connections if network limits are reached. This prioritization can be controlled by developers using the dashboard or 100ms APIs. eg. A developer can prioritize host's screenshare higher than guests' videos. In low bandwidth constraints, guests' videos will be turned off, while host's screenshare will remain. \r "
    },
    {
        "title": "Interactive Live Streaming",
        "link": "/android/v2/foundation/live-streaming",
        "platformName": "Android",
        "objectID": "/android/v2/foundation/live-streaming",
        "keywords": [],
        "content": "    Overview Live video interactions can span various levels of interactivity. For example, in a virtual event, some participants can be on stage talking to each other, whereas participants in the audience can be listening to them. At 100ms, we think of this as the   3 levels of interactivity  . 100ms enables you to build live video use-cases by mixing and matching these 3 levels to get to your ideal solution.     Level 1  : Full duplex audio/video in real-time   Level 1 participants publish their audio/video, and interact with others in sub-second latency. This is real-time video conferencing, similar to Zoom or Google Meet.     Level 2  : Webinar-style audience in real-time   Level 2 participants consume audio/video from level 1 participants with sub-second latency, without publishing their own audio/video. Level 2 participants can engage with level 1 through messaging (chat, emojis, custom events). This is similar to a Zoom webinar.   Levels 1 and 2 are enabled using WebRTC.     Level 3  : Live stream audience consuming in near real-time   Level 3 participants consume a composite live stream in near real-time (<10 secs of latency) without publishing their audio/video. They can interact with other participants via messaging. This is similar to viewers on Twitch or YouTube Live, and is enabled via   100ms Interactive Live Streaming  .   Live streaming uses  HLS (https://www.100ms.live/blog/hls-101-beginners-guide) to achieve near real-time latency at scale. The  roles primitive (/docs/javascript/v2/foundation/templates-and-roles) can be used to define capabilities of a participant and associate them to an interaction level. A participant can move between levels using a single API call to change roles.   Try Interactive Live Streaming Use our  Live Streaming Starter Kit (https://www.100ms.live/marketplace/live-streaming-starter-kit) to try out the experience before you write a line of code. <StepsToc  parentId=\"try-live-streaming\"  descriptions=  \"Use our Live streaming starter kit to try out the experience before you write a line of code.\", \"Understand the difference between a stream broadcaster and stream viewer.\", \"Use the demo app link to go live for the first time as a broadcaster and join the stream as viewer.\", \"Use the 100ms self-serve dashboard to update the layout, aspect ratio, etc of the stream.\"   > <StepsContainer id=\"try-live-streaming\">   Create a new app   Live streaming starter kit (/docs/docs/v2/live-streaming-starter-kit.png) 1. Make sure that you have  an account with 100ms (https://dashboard.100ms.live/register) and can access the  100ms dashboard (https://dashboard.100ms.live/) 1. On the dashboard, create a new app using the Live Streaming Starter Kit 1. Specify a subdomain and region to deploy the app   Understand roles   Live Streaming roles (/docs/docs/v2/live-streaming-roles.png) This starter kit configures your new app with  two roles (/docs/javascript/v2/foundation/templates-and-roles):   broadcaster : This role represents a streamer who publishes their audio/video. There can be multiple peers who join as broadcasters   hls-viewer : This role represents a circle 3 audience, who subscribes to the composite live stream and can interact using messaging   Go live   Go live (/docs/docs/v2/live-streaming-go-live.gif) 1. To go live for the first time, join the room as a broadcaster and start the live stream 2. Once the stream has started, join the room as an hls-viewer and you should be able to see the ongoing live stream 3. Use chat messages to interact between the viewer and the broadcaster   Customize the stream   Go live (/docs/docs/v2/live-streaming-customise.png) By default, the live stream is composed in landscape mode for desktop viewers (with an aspect ratio of 16:9). You can customise the live stream for viewers on mobile or to support multiple broadcaster tiles. 1. On the 100ms dashboard, click the gear icon on your app to open configuration settings 2. Go to \"destinations\" and scroll down to find live stream (HLS) configuration 3. Update the configuration based on your needs:    If your viewers are on mobile, change the video aspect ratio to 9:16    If you have multiple broadcasters joining in, choose grid or active speaker based on your needs    In case of grid layout, choose the tile size that fits your use-case. For example, a stream with 2 streamers looks better with 1:1 tiles. </StepsContainer>   Integrate in your app To integrate 100ms Interactive Live Streaming in your app, follow these steps: 1.  Enable live streaming destination ( enable-destination) 2.  Integrate the 100ms SDK in your app ( sdk-integration) 3.  Integrate live stream playback ( live-stream-playback) 4.  Video on demand (VOD) use cases ( video-on-demand-vod-use-cases)   Enable destination   Enable HLS (/docs/docs/v2/live-streaming-enable.gif) If your app is based on the Live Streaming Starter Kit (as shown above), the live streaming destination is enabled out-of-the-box. For custom apps, you can enable the live streaming destination manually: 1. Open configuration for your existing app using the 100ms dashboard 1. In the \"destinations\" tab, enable \"Live Streaming with HLS\" 1. Ensure that you have roles for the broadcaster (who can publish their audio/video) and the viewer (who cannot publish audio/video)   SDK integration Use the 100ms client-side SDKs to integrate streaming in your application. See code snippets for the client-side SDK  here (/docs/javascript/v2/features/hls).   Live stream playback Using our client-side SDKs, you can enable live stream playback and add interactive experiences like chat, raise hand and other functionalities to your app using  peer metadata (/docs/javascript/v2/advanced-features/peer-metadata). The process is so simple: 1. Once you  start ( step-3-go-live) live streaming, you will get an HLS URL (M3U8 URL) which you can use for playback. 2. You can use the  client-side SDK (/docs/javascript/v2/features/hls) to get the HLS URL by checking the  current state (/docs/javascript/v2/features/hls current-room-status) of the room and start playback. If you need to only enable HLS playback and don't need interactivity, you can follow one of the below approaches to get the HLS URL:     Webhook:   You can listen to hls.started.success  webhook event (/docs/server-side/v2/introduction/webhook hls-started-success) and get the HLS URL from the url field. Please check the  webhooks guide (/docs/server-side/v2/introduction/webhook) to learn more about webhooks.     Static URL:   This configuration will help you get a static URL for playback. You can enable the Static playback URLs in your template from the  dashboard (https://dashboard.100ms.live/dashboard). You can go to Destination > enable \"Live streaming with HLS\" > under \"Customise stream video output\" section > enable \"Static playback URLs.\"     Enable Static URL (/docs/docs/v2/enable-static-url.png)     _Format_: https://cdn.100ms.live/beam/<customer_id>/<room_id>/master.m3u8     customer_id : replace this placeholder with your customer_id from  developer section (https://dashboard.100ms.live/developer) on your dashboard.     room_id : replace this placeholder with the room_id of the respective room from which the stream will be broadcasted.   Video on Demand (VOD) use cases If you wish to replay your HLS stream for Video on demand (VOD) use case, 100ms provides the capability to record the HLS stream which will be posted to your webhook as a ZIP file of M3U8 format (same playback format as HLS) with all the chunks once the stream ends. You can start recording a live stream using the  client-side SDK (/docs/javascript/v2/features/hls) or using the  server API (/docs/server-side/v2/Destinations/rtmp-streaming-and-browser-recording start-streaming-recording). Once the HLS recording is completed, you will get the details of recording as a callback to the webhook configured in your account. Check hls.recording.success  webhook event (/docs/server-side/v2/introduction/webhook hls-recording-success) for more information. "
    },
    {
        "title": "Recordings",
        "link": "/android/v2/foundation/recordings",
        "platformName": "Android",
        "objectID": "/android/v2/foundation/recordings",
        "keywords": [],
        "content": "  Recordings are an important part of the live video stack as they convert live, ephemeral content into a long-term asset. But the use of this asset varies from business to business depending on their respective use case. For example, one of the common use cases for recording is for archival purposes versus, for some, its content to be publicized. Based on your end goal, you can choose one of the recording types and its implementation. You can understand some key differences using the comparison table below.   Recording types   Recording types ( recording-types)    Quick Comparison ( quick-comparison)    Browser Recording  Recommended   ( browser-recording-recommended)    SFU Recording  Advanced   ( sfu-recording-advanced)    Recordings for Live Streaming Use-cases ( recordings-for-live-streaming-use-cases)     Video-on-demand Recording ( video-on-demand-recording)     Multiresolution Recording ( multiresolution-recording)   Configure storage ( configure-storage)    How to configure recording storage? ( how-to-configure-recording-storage)   Quick Comparison  Recording Features           Browser Recording  Recommended   SFU Recording  Advanced                                      Resolution               Upto 1080p            Only 720p              Participant-level Audio/Video Tracks  Not Available           Available              Portrait/Landscape Mode        Available             Not Available            Start/Stop Recording          On-demand             Auto start/stop with the session   Custom Layout             Available             Not Available            Role-Specific Recording        Available             Not Available            Recording Output            MP4                MP4, WebM                Browser Recording  Recommended  Browser recording is built to give users a participant-first recording experience. When enabled, our browser-based bot Beam joins a room to record the viewport like any other participant. The output is an MP4 file that captures the room's published audio/video tracks together into one single file. This option removes the complexity of syncing various audio/video tracks and offers an intuitive, participant-first recording experience. An example use case is to record a sales meeting for later usage.   Resources     How to implement Browser Recording (https://www.100ms.live/docs/server-side/v2/Destinations/rtmp-streaming-and-browser-recording)   SFU Recording  Advanced  SFU recording is built for advanced use cases, which require individual audio and video tracks for each participant. This recording option allows you to isolate recording at a participant level. Track recording allows you to record audio and video streams separately, making it easier to edit, layer, or reuse each of them. An example use case is to record a live podcast and later edit it for publishing. You can get track recordings in two forms:   Individual: Media for each peer is provided as a separate mp4 file. This file will have both audio and video of the peer. These files can be used for offline review or in implementing custom composition.   Composite  currently in beta : Audio and video of all peers are composed as per their joining/leaving the meeting and provided as a single mp4. This file can be used for offline viewing of the meeting.   Resources      How to implement SFU Recording (https://www.100ms.live/docs/server-side/v2/Destinations/recording)   Recordings for Live Streaming Use-cases These are the types of live streaming recordings:    Video-on-demand Recording Video-on-demand recording is available for our Interactive Live Streaming capability. This recording will be a file with an M3U8 file (same playback format as HLS), which can be used for replaying your HLS stream. This option is more suitable for Video-on-Demand use cases. For the implementation of this type of recording, please  contact us (https://www.100ms.live/contact).    Multiresolution Recording A multi-resolution recording is available for Interactive Live Streaming capability. This type of recording will have a multi-file structure for all available resolutions of the stream. The output will be multiple MP4 files with these resolutions: 240p, 480p, 720p, and 1080p. For an implementation of this type of recording, please  contact us (https://www.100ms.live/contact).   Configure storage You can specify a cloud storage location for your recording files in your template. Our current offering allows you to store your recordings in Amazon S3 buckets. Once you configure the S3 config of your bucket in a template, all respective recordings of sessions created via those templates will be sent to your configured bucket. This holds true for all types of aforementioned recordings.   How to configure recording storage? 1. Generate your credentials; for this example, you can check out a  guide from AWS (https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html). You can skip this step if you already have credentials. Please note that if you are running a Browser recording, you need to give upload permission to your key, but if you are running an SFU recording, you need to give both upload and download permission. 2. Go to 100ms Dashboard and go to template   configuration by selecting the configure icon  .   Create your first app (/docs/docs/v2/recording-storage-settings-step2.png) 3. Head over to the   Destinations   tab.   Destinations (/docs/docs/v2/recording-storage-settings-step3.png) 1. Key in your credentials (using an example of an S3 bucket here):    Access Key: Access Key generated from AWS IAM Console    Secret Key: Secret Key generated from AWS IAM Console    Bucket: Name of the bucket in S3    Region: Name of the region, for example, ap-south1    Prefix for Upload Path: Define the directory name (optional)   Destinations (/docs/docs/v2/recording-storage-settings-step4.png) 5. Use the   Validate Config   button to test your storage setup.   Destinations (/docs/docs/v2/recording-storage-settings-step5.png) 6. You will see a message that the AWS   configuration was successfully validated  .   Destinations (/docs/docs/v2/recording-storage-settings-step6.png) The above message ensures that your configuration is successful now, and all your recordings will start collecting in your configured destination. "
    },
    {
        "title": "Authentication and Tokens\r",
        "link": "/android/v2/foundation/security-and-tokens",
        "platformName": "Android",
        "objectID": "/android/v2/foundation/security-and-tokens",
        "keywords": [],
        "content": " -\r \r  Introduction In 100ms, two types of tokens are used to authenticate requests coming from your Client apps and Backend application server into the 100ms platform.    App token ( app-token) : Used to authenticate and allow end-users (peers) to join 100ms rooms. An App Token controls Peer identity and Room permissions in your real-time or Interactive live-streaming video application.    Management token ( management-token) : Used to authenticate all the requests to 100ms REST API. You can set the expiry to both these tokens; if you follow the code samples from this guide, the expiry will be set as 24 hours. However, a best practice is to set the expiry as short as feasible for your application. You must host your server to generate these tokens while transitioning your app to production.   App Token 100ms _client-side SDKs_ use App Tokens to authenticate a peer (participant) while  joining a room (./../features/join). Generate this token on the server side and make it available for your client-side apps that use the 100ms SDKs. To create an App Token, you need to use app_access_key , app_secret , room_id , and user_id .   You can get the app_access_key and app_secret from the  developer section (https://dashboard.100ms.live/developer) in your 100ms dashboard. This key and secret differ across workspaces so please ensure you are in the intended workspace before copying these credentials.     room_id  : This is the unique identifier for your room. You can get it from the  rooms page (https://dashboard.100ms.live/rooms) in your dashboard or in the response payload of the  create room server-side API (/docs/server-side/v2/Rooms/create-via-api).     user_id  : This identifier can be used to map a 100ms peer to your own internal user object for business logic. Specify your internal user identifier as the peer's user_id. If not available, use any random string.   How to use? You can get App tokens using a couple of approaches based on your app's lifecycle stage. Please check the below sections for more information:    Set up your own authentication endpoint (./../foundation/security-and-tokens set-up-your-own-authentication-endpoint)    Get app tokens from the token endpoint (./../foundation/security-and-tokens get-app-tokens-from-the-token-endpoint)    Get app tokens from the dashboard (./../foundation/security-and-tokens get-app-tokens-from-the-dashboard)    Set up your own authentication endpoint When you have completed your integration with 100ms, and while transitioning your app to production, we recommend you create your backend service for app token generation; use the code snippet below and set up the token generation service as per your preferred programming language. \r \r   Code sample: Generate app token\r \r  s id=\"client-code-token\" items=  'Node.js', 'Python', 'Java', 'Ruby', 'PHP'    \r \r   id='client-code-token-0'>\r \r  javascript\r var jwt = require('jsonwebtoken');\r var uuid4 = require('uuid4');\r \r var app_access_key = '<app_access_key>';\r var app_secret = '<app_secret>';\r \r var payload =  \r   access_key: app_access_key,\r   room_id: '<room_id>',\r   user_id: '<user_id>',\r   role: '<role>',\r   type: 'app',\r   version: 2,\r   iat: Math.floor(Date.now() / 1000),\r   nbf: Math.floor(Date.now() / 1000)\r  ;\r \r jwt.sign(\r   payload,\r   app_secret,\r    \r     algorithm: 'HS256',\r     expiresIn: '24h',\r     jwtid: uuid4()\r    ,\r   function (err, token)  \r     console.log(token);\r    \r );\r  \r \r </Tab>\r \r   id='client-code-token-1'>\r \r  python\r  /usr/bin/env python3\r import jwt\r import uuid\r import datetime\r import sys\r \r app_access_key = \"<app_access_key>\"\r app_secret = \"<app_secret>\"\r \r def generate(room_id, user_id, role):\r   expires = expires or 24   3600\r   now = datetime.datetime.utcnow()\r   exp = now+ datetime.timedelta(seconds=expires)\r   return jwt.encode(payload= \r         \"access_key\": app_access_key,\r         \"type\":\"app\",\r         \"version\":2,\r         \"room_id\": room_id,\r         \"user_id\": user_id,\r         \"role\":role,\r         \"jti\": str(uuid.uuid4()),\r         \"exp\": exp,\r         \"iat\": now,\r         \"nbf\": now,\r          , key=app_secret)\r if __name__ == \"__main__\":\r   if len(sys.argv) == 3:\r     room_id = sys.argv 0 \r     user_id = sys.argv 1 \r     role = sys.argv 2 \r   print(generate(room_id=room_id, user_id=user_id, role=role))\r  \r \r </Tab>\r \r   id='client-code-token-2' >\r \r  java\r import java.time.Instant;\r import java.util.Date;\r import java.util.HashMap;\r import java.util.Map;\r import java.util.UUID;\r import io.jsonwebtoken.Jwts;\r import io.jsonwebtoken.SignatureAlgorithm;\r \r private void generateHmsClientToken()  \r   Map<String, Object> payload = new HashMap<>();\r   payload.put(\"access_key\", \"<app_access_key>\");\r   payload.put(\"room_id\", \"<room_id>\");\r   payload.put(\"user_id\", \"<user_id>\");\r   payload.put(\"role\", \"<role>\");\r   payload.put(\"type\", \"app\");\r   payload.put(\"version\", 2);\r   String token = Jwts.builder().setClaims(payload).setId(UUID.randomUUID().toString())\r     .setExpiration(new Date(System.currentTimeMillis() + 86400   1000))\r     .setIssuedAt(Date.from(Instant.ofEpochMilli(System.currentTimeMillis()  60000)))\r     .setNotBefore(new Date(System.currentTimeMillis()))\r     .signWith(SignatureAlgorithm.HS256, \"<app_secret>\".getBytes()).compact();\r   \r  \r \r </Tab>\r \r   id='client-code-token-3' >\r \r  ruby\r require 'jwt'\r require 'securerandom'\r \r \r \r $app_access_key = \"<app_access_key>\"\r $app_secret = \"app_secret\"\r \r def generateAppToken(room_id, user_id, role)\r   now = Time.now\r   exp = now + 86400\r   payload =  \r     access_key: $app_access_key,\r     room_id: room_id,\r     user_id: user_id,\r     role: role,\r     type: \"app\",\r     jti: SecureRandom.uuid,\r     version: 2,\r     iat: now.to_i,\r     nbf: now.to_i,\r     exp: exp.to_i\r    \r \r   token = JWT.encode(payload, $app_secret, 'HS256')\r end\r \r puts generateAppToken \"<room_id>\", \"<user_id>\", \"<role>\"\r \r  \r \r </Tab>\r \r   id='client-code-token-4'>\r \r  php\r <?php\r \r use Firebase JWT JWT;\r use Ramsey Uuid Uuid;\r \r $issuedAt = new DateTimeImmutable();\r $expire  = $issuedAt->modify('+24 hours')->getTimestamp();\r $accessKey = \"<app_access_key>\";\r $secret = \"<app_secret>\";\r $version  = 2;\r $type   = \"app\";\r $role   = \"<role>\";\r $roomId  = \"<room_id>\";\r $userId  = \"<user_id>\";\r \r $payload =  \r   'iat' => $issuedAt->getTimestamp(),\r   'nbf' => $issuedAt->getTimestamp(),\r   'exp' => $expire,\r   'access_key' => $accessKey,\r   'type' => \"app\",\r   'jti' => Uuid::uuid4()->toString()\r   'version' => 2,\r   'role' => $role,\r   'room_id' => $roomId,\r   'user_id' => $userId\r  ;\r \r $token = JWT::encode(\r   $payload,\r   $secret,\r   'HS256'\r );\r  \r \r </Tab>\r \r <Note type=\"warning\">\r   We strictly advise you not to post <i>app_access_key</i> and <i>app_secret</i> publicly; if you\r   need to store them in <strong>Git</strong>, please change the repository from public to private.\r   <br  \r   <br   You cannot use an <strong>App token</strong> to trigger server API requests.\r </Note>\r \r   Get app tokens from the token endpoint If you are building integration with 100ms, you can get app tokens from the 100ms token endpoint without hosting a token generation backend service. Refer to  this guide (./../guides/token-endpoint get-an-app-token-using-token-endpoint) for more information.    Get app tokens from the dashboard If you are building your first app by following one of our  quickstart guides (/docs/javascript/v2/guides/javascript-quickstart), you can get the app token directly from 100ms dashboard to join a room for the first time. Refer to  this guide (./../guides/token get-a-temporary-token-from-100ms-dashboard) for more information.   Management Token 100ms uses management tokens to authenticate REST APIs.  If you're evaluating 100ms  server APIs (/docs/server-side/v2/introduction/basics), you can use our public  Postman collection (/docs/server-side/v2/introduction/postman-guide fork-the-collection), which doesn't require you to create a management token as we've managed it using a  pre-request script (/docs/server-side/v2/introduction/postman-guide simplified-token-generation) within the collection. If you're transitioning your app to production, we recommend you create your backend service for management token generation. You must use the app_access_key and app_secret from the  developer section (https://dashboard.100ms.live/developer) in your 100ms dashboard to create the management token.\r \r   Code sample: Generate management token\r \r  s id=\"test-code\" items=  'Node.js', 'Python', 'Java', 'Ruby', 'PHP'    \r \r   id='test-code-0'>\r \r  js\r var jwt = require('jsonwebtoken');\r var uuid4 = require('uuid4');\r \r var app_access_key = '<app_access_key>';\r var app_secret = '<app_secret>';\r \r jwt.sign(\r    \r     access_key: app_access_key,\r     type: 'management',\r     version: 2,\r     iat: Math.floor(Date.now() / 1000),\r     nbf: Math.floor(Date.now() / 1000)\r    ,\r   app_secret,\r    \r     algorithm: 'HS256',\r     expiresIn: '24h',\r     jwtid: uuid4()\r    ,\r   function (err, token)  \r     console.log(token);\r    \r );\r  \r \r </Tab>\r \r   id='test-code-1'>\r \r  py\r  /usr/bin/env python3\r import jwt\r import uuid\r import datetime\r \r app_access_key = '<app_access_key>'\r app_secret = '<app_secret>'\r \r \r def generateManagementToken():\r   expires = 24   3600\r   now = datetime.datetime.utcnow()\r   exp = now + datetime.timedelta(seconds=expires)\r   return jwt.encode(payload= \r     'access_key': app_access_key,\r     'type': 'management',\r     'version': 2,\r     'jti': str(uuid.uuid4()),\r     'iat': now,\r     'exp': exp,\r     'nbf': now\r      , key=app_secret)\r \r if __name__ == '__main__':\r   print(generateManagementToken())\r  \r \r </Tab>\r \r   id=\"test-code-2\">\r \r  java\r import java.time.Instant;\r import java.util.Date;\r import java.util.HashMap;\r import java.util.Map;\r import java.util.UUID;\r import io.jsonwebtoken.Jwts;\r import io.jsonwebtoken.SignatureAlgorithm;\r \r private void generateManagementToken()  \r   Map<String, Object> payload = new HashMap<>();\r   payload.put(\"access_key\", \"<app_access_key>\");\r   payload.put(\"type\", \"management\");\r   payload.put(\"version\", 2);\r   String token = Jwts.builder().setClaims(payload).setId(UUID.randomUUID().toString())\r     .setExpiration(new Date(System.currentTimeMillis() + 86400   1000))\r     .setIssuedAt(Date.from(Instant.ofEpochMilli(System.currentTimeMillis()  60000)))\r     .setNotBefore(new Date(System.currentTimeMillis()))\r     .signWith(SignatureAlgorithm.HS256, \"<app_secret>\".getBytes()).compact();\r   \r  \r \r </Tab>\r \r   id=\"test-code-3\">\r \r  ruby\r require 'jwt'\r require 'securerandom'\r \r $app_access_key = \"<app_access_key>\"\r $app_secret = \"<app_secret>\"\r \r def generateManagementToken()\r   now = Time.now\r   exp = now + 86400\r   payload =  \r   access_key: $app_access_key,\r   type: \"management\",\r   version: 2,\r   jti: SecureRandom.uuid,\r   iat: now.to_i,\r   nbf: now.to_i,\r   exp: exp.to_i\r  \r token = JWT.encode(payload, $app_secret, 'HS256')\r return token\r end\r \r puts generateManagementToken\r  \r \r </Tab>\r \r   id=\"test-code-4\">\r \r  php\r <?php\r \r use Firebase JWT JWT;\r use Ramsey Uuid Uuid;\r \r $app_access_key = \"<app_access_key>\";\r $app_secret = \"<app_secret>\";\r \r $issuedAt  = new DateTimeImmutable();\r $expire   = $issuedAt->modify('+24 hours')->getTimestamp();\r \r $payload =  \r   'access_key' => $app_access_key,\r   'type' => 'management',\r   'version' => 2,\r   'jti' => Uuid::uuid4()->toString(),\r   'iat' => $issuedAt->getTimestamp(),\r   'nbf' => $issuedAt->getTimestamp(),\r   'exp' => $expire,\r  ;\r \r $token = JWT::encode($payload, $app_secret, 'HS256');\r ?>\r  \r \r </Tab>\r \r <Note type=\"warning\">\r   We strictly advise you not to post <i>app_access_key</i> and <i>app_secret</i> publicly; if you\r   need to store them in <strong>Git</strong>, please change the repository from public to private.\r   <br  \r   <br  \r   You cannot authenticate room join requests from your client-side apps with a <strong>\r     Management token\r   </strong>.\r </Note>\r "
    },
    {
        "title": "SDK Size Impact",
        "link": "/android/v2/foundation/size",
        "platformName": "Android",
        "objectID": "/android/v2/foundation/size",
        "keywords": [],
        "content": "    Increase APK size       (armeabi-v7)   5.8MB    (arm64-v8a)   (most common one)     6.4MB    (x86_64)   7.1MB    (x86)   7.2MB    "
    },
    {
        "title": "Templates and Roles",
        "link": "/android/v2/foundation/templates-and-roles",
        "platformName": "Android",
        "objectID": "/android/v2/foundation/templates-and-roles",
        "keywords": [],
        "content": "    Introduction Template is the blueprint of the room. It defines the settings of the room along with the behavior of users who are part of it. Room will inherit the properties from a template that you have specified while creating it. If you have not specified any template then it will pick the default template. Each template will be identified by its id or name. For example default_videoconf_7e450ffc-8ef1-4572-ab28-b32474107b89  Users can see or modify the templates by visiting  Templates on Dashboard (https://dashboard.100ms.live/templates) or via API (see below). After updating a template or some part of its like permissions, you need to rejoin or restart the session for the template updates to take place.   Template (/docs/docs/v2/template.png)   Roles Role is a collection of permissions that allows you to perform certain set of operations while being part of the room. It has the following attributes:   Name Every role has a name that should be unique inside a template. This name will be used while generating app tokens and referencing inside a template.   Priority Priority will determine the order in which the roles will be degraded. A lower number represents a higher priority.   Publish Strategies Publish strategies will be used to determine the tracks and their quality which can be published by this role.  Strategy       Description                                                                                                                                                                                                                            Can share audio    Whether the role is allowed to publish the audio track or not.                                                                                                                  Can share video    Whether the role is allowed to publish the video track or not                                                                                                                   Can share screen   Whether the role is allowed to do screen share or not                                                                                                                       Video quality     Quality of the video track which is going to be published by the role. Currently, 6 video qualities 1080p , 720p , 480p , 360p , 240p and 120p are predefined and the user can select one out of these values. This option will be visible only if the   Can share video   is enabled.   Screenshare quality  Quality of the screen which is going to be shared by the role. Currently, 2 video qualities 720p and 1080p are predefined and the user can select one out of these values. This option will be visible only if the   Can share screen   is enabled.                       Subscribe Strategies Subscribe strategies will be used to determine what all roles, this role can subscribe to.  Strategy        Description                                                                                                                                                                                                      Subscribe to      You can select all the roles of the template which this role will subscribe                                                                                             Subscribe Degradation  When this flag is turned on, one or more remote video tracks will be muted automatically when the network condition worsens. Such tracks will be marked as degraded . When the network condition improves, the degraded tracks will automatically be unmuted.    Permissions Permissions will contain a list of additional privileges that this role will have.  Permission               Description                                                                                                                                              Can change any participant's role   With this permission, user will be able to change the role of the other participant's who are present in the room                                  Can mute any participant        With this permission, user will be able to mute any participant's audio and/or video.                                                Can ask participant to unmute     With this permission, user will be able to ask any participant to unmute their audio and/or video.                                         Can remove participant from the room  With this permission, user will be able to remove any participant from the current session of the room.                                       Can end current session of the room  With this permission, user will be able to end the current session of the room.                                                   Can receive room state         With this permission, user will be able to receive room state like peer-count and peer-list on the preview screen.                                 Can start/stop RTMP livestream     With this permission, user will be able to publish live audio/video livestream externally to social media and custom platforms (e.g Youtube/Facebook/Twitter).           Can start/stop HLS livestream     With this permission, user will be able to publish audio/video livestream in the HLS format.                                            Can start/stop Beam Recording     With this permission, user will be able to record meeting/livestream via the browser recording approach where a bot will join the room and record the meeting/livestream as is.    Advanced Settings As the name suggests, Advanced Settings section contains more settings and controls for the advanced user.   Template (/docs/docs/v2/advanced-settings.png)   Preview room state Preview room state enables you to build a \"preview\" screen which shows the state of the room before joining. This room state includes a list of peers, which can be used to show who is in the room. Preview room state also includes recording and streaming state. Preview room state settings define strategy of sending state updates to client SDKs.  Setting                  Description                                                                                                                                                                                                 Room-state Message Interval (in seconds)  Room-state data will be sent over a regular interval of these many seconds. Consequently, the room state displayed on the preview screen will refresh accordingly. This value must be a multiple of 5, between 5 and 3600 seconds, both inclusive.   Send Peer List in Room-state        Enabling this will send peer-list info of the room. If disabled, only the peer count is sent.                                                                             Enable Room-State             If enabled, room-state data will be sent to the preview screen. If disabled, no such room-state data will be sent.                                                                   Roles with room-state permission      This is the list of all the roles which will get the room-state data. You can also individually toggle these settings in the Roles tab under the Permissions section.                                           API reference Apart from the dashboard, a programmatic way to interact with templates is via  API (/server-side/v2/policy/template-object). "
    },
    {
        "title": "Proguard configuration",
        "link": "/android/v2/guides/proguard-config",
        "platformName": "Android",
        "objectID": "/android/v2/guides/proguard-config",
        "keywords": [],
        "content": "    100ms Android SDK 2.1.2 or higher If using 100ms Android SDK version 2.1.2 and higher proguard configuration is bundled inside it. No additional code is required.   Prior to Android SDK version 2.1.2 Prior to Android SDK version 2.1.2, the following configuration was needed in release builds. Add it to your proguard-rules.pro   :proguard-rules.pro section=PriorToAndroidSDKVersion212 sectionIndex=1  100ms proguard rules -keep class org.webrtc.      ;   -keep class live.hms.video.      ;     "
    },
    {
        "title": "100ms SDK Android Quickstart Guide",
        "link": "/android/v2/guides/quickstart",
        "platformName": "Android",
        "objectID": "/android/v2/guides/quickstart",
        "keywords": [],
        "content": "    Overview This overview shows the major steps involved in creating a demo project with the 100ms SDK. Each section links to extra detail. Here are some sample apps demonstrating this.  Simplest implementaion. (https://github.com/100mslive/hello-world-android/)  Most full-featured implementation. (https://github.com/100mslive/100ms-android/) Jump to a section you're interested in or read top down to get the overview.    Getting the video call joining link ( getting-the-video-call-joining-link)    Add the sdk dependencies ( add-sdk-dependencies).    Create a login token to authenticate the user ( login).    Add permissions ( permissions)    Create an instance of the HMSSDK  ( instantiate-hmssdk).    Show an optional of the user's audio video with the 100ms hmssdk.preview .    Call hmssdk.join with the video call link to join the call ( join-a-video-call).    Know when people join or leave ( how-you-know-when-people-join-or-leave)    How to render audio/video ( how-to-render-audio-and-video)    Listening to updates effectively ( listening-to-updates-effectively)   <hr     Prerequisites Familiarity with Android Studio and the  fundamentals (https://developer.android.com/training/basics/firstapp) of android apps.   Supported Android API Levels 100ms' Android SDK supports Android API level 21 and higher. It is built for armeabi-v7a, arm64-v8a, x86, and x86_64 architectures.   Getting the video call joining link To join a video call you need an authentication token and a room id. Or a server that will translate the link into them. The  100ms Dashboard (https://dashboard.100ms.live/register) is one way to generate these auth tokens. In production your own server will generate these and manage user authentication. We encourage you to setup your own server to authenticate users rather than using the 100ms dashboard to do so in production. For the purposes of this quickstart you can rely on just the 100ms dashboard. Sign up for the 100ms Dashboard  here (https://dashboard.100ms.live/register). From either the dashboard, or your own server once implemented, you need to generate a video call link. Video call links generated by the 100ms Dashboard look like https://myname.app.100ms.live/meeting/correct-horse-battery . > ⚙️ For Production > > With your own server for authentication and link generation, the format of the link is up to you.   Add SDK dependencies The latest SDK version is:     (https://img.shields.io/badge/dynamic/xml.svg?label=100ms&color=blue&query=%2F%2Fmetadata%2Fversioning%2Flatest&url=https%3A%2F%2Frepo1.maven.org%2Fmaven2%2Flive%2F100ms%2Fandroid-sdk%2Fmaven-metadata.xml) (https://jitpack.io/ 100mslive/android-sdk)   s id=\"sdk-imports\" items=  'Maven Central', 'Jitpack'       id='sdk-imports-0'> ​   json 5-8 :settings.gradle section=AddSDKDependencies sectionIndex=1 dependencies   // See the version in the badge above. // There are separate libraries for the sdk, virtual background and hls-player-stats. //   add just the ones you need. def hmsVersion = \"x.x.x\" implementation \"live.100ms:android-sdk:$hmsVersion\"  // Essential implementation \"live.100ms:virtual-background:$hmsVersion\" // Optional implementation \"live.100ms:hls-player-stats:$hmsVersion\"  // Optional     </Tab>    id='sdk-imports-1'> There is one extra step to use the jitpack libraries.   Add the JitPack repository to your root settings.gradle at the end of the respositories closure: You can open it in Android Studio by double tapping shift and typing settings.gradle .   json 7 :settings.gradle section=AddSDKDependencies sectionIndex=1 dependencyResolutionManagement   repositoriesMode.set(RepositoriesMode.FAIL_ON_PROJECT_REPOS) repositories   google() mavenCentral() jcenter() // Warning: this repository is going to shut down soon maven   url 'https://jitpack.io'         rootProject.name = \"MyVideoCallApp\" include ':app'      Add the 100ms sdk dependency to your app-level build.gradle   json 2,4-6 :build.gradle section=AddSDKDependencies sectionIndex=2 dependencies   def hmsVersion = \"x.x.x\" // See the version in the jitpack badge above. implementation \"com.github.100mslive.android-sdk:lib:$hmsVersion\"     // Essential implementation \"com.github.100mslive.android-sdk:virtualBackground:$hmsVersion\" // Optional implementation \"com.github.100mslive.android-sdk:stats:$hmsVersion\"      // Optional     </Tab>   Login    Request Here's how to get an auth token with 100ms's demo authentication 1. Sign up to the dashboard. 2. Get your video call link. It should look like https://myname.app.100ms.live/meeting/correct-horse-battery 3. Send an http post request to https://prod-in.100ms.live/hmsapi/get-token 4. With the header \"subdomain\" set to myname.app.100ms.live if your link was https://myname.app.100ms.live/meeting/correct-horse-battery 5. For a link of the type https://myname.app.100ms.live/meeting/correct-horse-battery The body is json with the format  \"code\": \"correct-horse-battery\", \"user_id\":\"your-customer-id\"   the user_id can be any random string as well and you can create it with UUID.randomUUID().toString(), . > ⚙️ For Production > > Maybe you won't use links at all. You will need to generate tokens on the backend, and rooms for users. Look up the  Token Setup Guide (token) here.    Response The 100ms server will respond with an auth token like this  \"token\":\"some-token-string\"  .   Permissions Camera, Recording Audio and Internet permissions are required. Add them to your manifest.   xml:AndroidManifest.xml section=Permissions sectionIndex=1   <uses-permission android:name=\"android.permission.CAMERA\"     <uses-permission android:name=\"android.permission.RECORD_AUDIO\"     <uses-permission android:name=\"android.permission.INTERNET\"     You will also need to request Camera and Record Audio permissions at runtime before you join a call or display a preview. Please follow  Android Documentation (https://developer.android.com/training/permissions/requesting request-permission) for runtime permissions.   Instantiate HMSSDK Instantiate the HMSSDK like this:  s id=\"instanstiate-hmssdk\" items=  'Kotlin', 'Java'       id='instanstiate-hmssdk-0'>   kotlin section=InstantiateHMSSDK sectionIndex=1 tab=Kotlin   val hmsSdk = HMSSDK     .Builder(application)     .build()   </Tab>   id='instanstiate-hmssdk-1'>   java section=InstantiateHMSSDK sectionIndex=1 tab=Java HMSSDK hmssdk = new HMSSDK             .Builder(application)             .build();   </Tab>   Join a Video Call To join a video call, call the join method of hmssdk with the config and appropriate listeners. The main ones to know are:  onJoin  called when the join was successful and you have entered the room. > 💡 Audio will be automatically connected, video requires some work on your side.  onPeerUpdate  called when a person joins or leaves the call and when their audio/video mutes/unmutes.  onTrackUpdate  usually when a person joins the call, the listener will first call onPeerUpdate to notify about the join. Subsequently onTrackUpdate will be called with their actual video track. > 💡It's essential that this callback is listened to or you may have peers without video.  s id=\"join-call\" items=  'Kotlin', 'Java'       id='join-call-0'>   kotlin section=JoinVideoCall sectionIndex=1 tab=Kotlin val config = HMSConfig(\"user display name\", authToken) hmsSdk.join(config, MyHmsUpdateListener()) class MyHmsUpdateListener : HMSUpdateListener     override fun onJoin(room: HMSRoom)      override fun onTrackUpdate(type: HMSTrackUpdate, track: HMSTrack, peer: HMSPeer)      override fun onPeerUpdate(type: HMSPeerUpdate, peer: HMSPeer)      override fun onMessageReceived(message: HMSMessage)      override fun onRoleChangeRequest(request: HMSRoleChangeRequest)      override fun onRoomUpdate(type: HMSRoomUpdate, hmsRoom: HMSRoom)      override fun onError(error: HMSException)        </Tab>   id='join-call-1'>   java section=JoinVideoCall sectionIndex=1 tab=Java HMSConfig config = new HMSConfig(\"user display name\", authToken); hmsSdk.join(config, new MyHmsUpdateListener()); class MyHmsUpdateListener implements HMSUpdateListener     @Override public void onJoin(@NonNull HMSRoom hmsRoom)      @Override public void onMessageReceived(@NonNull HMSMessage hmsMessage)      @Override public void onPeerUpdate(@NonNull HMSPeerUpdate hmsPeerUpdate, @NonNull HMSPeer hmsPeer)      @Override public void onReconnected()      @Override public void onReconnecting(@NonNull HMSException e)      @Override public void onRoleChangeRequest(@NonNull HMSRoleChangeRequest hmsRoleChangeRequest)      @Override public void onRoomUpdate(@NonNull HMSRoomUpdate hmsRoomUpdate, @NonNull HMSRoom hmsRoom)      @Override public void onTrackUpdate(@NonNull HMSTrackUpdate hmsTrackUpdate, @NonNull HMSTrack hmsTrack, @NonNull HMSPeer hmsPeer)      @Override public void onError(@NonNull HMSException e)         </Tab>   How you know when people join or leave The join method takes an interface called HMSUpdateListener . It lets you know when peers join and leave the call, mute/unmute their audio and video and lots more. The HMSUpdateListener has a callback to notify about people joining or leaving. It is onPeerUpdate(type: HMSPeerUpdate, peer: HMSPeer) . > 💡 HMSPeer is object that represents a person in the call.   How to render audio and video The SDK plays the audio for every person who joins the call. Audio will begin playing when join succeeds. To see the person's video you need to create an instance of org.webrtc.SurfaceViewRenderer . The correct version for webrtc is exported via the 100ms android sdk already, so if you have version 2.0.9 and above no additional imports are required to use it.   Showing Videos A peer represents one person in the video call. A peer's video track is in hmsPeer.videoTrack . ScreenShares can be found in val screenShareVideoTrack = hmsPeer.auxiliaryTracks.find   it is HMSVideoTrack   . i.e the auxiliary tracks is a list of tracks, one of which can be a ScreenShare if they have chosen to share their screen. You would want a RecyclerView of participants in the video call. The adapter data should be a list of class that has both the peer and the track to display. Call it a TrackPeerPair. Your layout xml for a peer video would need to have an org.webrtc.SurfaceViewRenderer   xml section=ShowingVideos sectionIndex=1 <org.webrtc.SurfaceViewRenderer android:id=\"@+id/videoSurfaceView\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" app:layout_constraintStart_toStartOf=\"parent\" app:layout_constraintTop_toTopOf=\"parent\"     Initialize this when it's added to the window. Release it when it's removed. Call hmsPeer.videoTrack?.addSink(surfaceViewRenderer) to start showing videos.   Listening to Updates Effectively Each time there's a an onJoin, onPeerUpdate, or onTrackUpdate, you can add all the peers from hmsSdk.getPeers() into the adapter. You'd need to map them into TrackPeerPair's.   Where to go from here Checkout the  simple version (https://github.com/100mslive/hello-world-android/) of the project. Also a full featured  advanced version (https://github.com/100mslive/100ms-android/).   Glossary   Room : When you join a particular video call, all the peers said to be in a video call room '   Track : Media. Can be the audio track or the video track.   Peer : One participant in the video call. Local peers are you, remote peers are others.   Broadcast : Chat messages are broadcasts. "
    },
    {
        "title": "Auth Token Endpoint Guide",
        "link": "/android/v2/guides/token-endpoint",
        "platformName": "Android",
        "objectID": "/android/v2/guides/token-endpoint",
        "keywords": [],
        "content": "    Overview 100ms provides an option to get App Tokens without setting up a token generation backend service to simplify your integration journey while testing the  sample app (https://github.com/100mslive/100ms-web) or building integration with 100ms. You can find the token endpoint from the  developer page (https://dashboard.100ms.live/developer) in your 100ms dashboard.   Token endpoint (/guides/token-endpoint-dashboard.png) We recommend you move to your token generation service before you transition your app to production, as our token endpoint service will not scale in production. The \"Sample Apps\" built using 100ms client SDKs require an App Token to join a room to initiate a video conferencing or live streaming session. Please check the  Authentication and Tokens guide (./../foundation/security-and-tokens) Please note that you cannot use the token endpoint to create a Management Token for server APIs. Refer to the  Management Token section (./../foundation/security-and-tokens management-token) in Authentication and Tokens guide for more information.   Get an app token using token endpoint You can use the token endpoint from your 100ms dashboard while building integration with 100ms. This acts as a tool enabling front-end developers to complete the integration without depending on the backend developers to set up a token generation backend service.   URL format:   <YOUR_TOKEN_ENDPOINT>api/token  100ms token endpoint can generate an app token with the inputs passed, such as room_id, role, & user_id (optional  your internal user identifier as the peer's user_id). You can use  jwt.io (https://jwt.io/) to validate whether the app token contains the same input values. <PostRequest title=\"https://prod-in2.100ms.live/hmsapi/johndoe.app.100ms.live/api/token\"   <Request id=\"req-comp-0\">   bash curl location request POST 'https://prod-in2.100ms.live/hmsapi/johndoe.app.100ms.live/api/token'  header 'Content-Type: application/json'  data-raw '    \"room_id\":\"633fcdd84208780bf665346a\",   \"role\":\"host\",   \"user_id\":\"1234\"  '   </Request> <ResponseBox id=\"resp-0\" status=\"200 OK\">   json     \"token\": \"eyJ0eXAiOiJKV1QiLCJhbGciOi                           R3tT-Yk\",   \"msg\": \"token generated successfully\",   \"status\": 200,   \"success\": true,   \"api_version\": \"2.0.192\"     </ResponseBox>   Example client-side implementation You can directly add this to your client-side implementation, check our  sample app (https://github.com/100mslive/100ms-android/blob/26468836b68c6b2595c10a297187040111ababc7/app/src/main/java/live/hms/app2/model/TokenRequestWithRoomId.kt L5) for reference.   Disable 100ms token endpoint Due to some security concerns, if you don't wish to use the token endpoint to generate app tokens, then you can disable it on the  Developers page (https://dashboard.100ms.live/developer) on your dashboard by disabling the option \"Disable &lt;room_id&gt;/&lt;role&gt; link format.\"   Disable Token endpoint (/guides/disable-token-endpoint.png)    Error Response Once you're disabled it on the dashboard, the requests to create an app token using the 100ms token endpoint will throw the below error:   json     \"success\": false,   \"msg\": \"Generating token using the room_id and role is disabled.\",   \"api_version\": \"2.0.192\"     "
    },
    {
        "title": "Auth Token Quickstart Guide\r",
        "link": "/android/v2/guides/token",
        "platformName": "Android",
        "objectID": "/android/v2/guides/token",
        "keywords": [],
        "content": " -\r \r  Create a 100ms account  Create an account at  Dashboard of 100ms (https://dashboard.100ms.live/)   Create Account (/docs/guides/token/create-account.png)  After you have created your account you have to Confirm your Email , check the promotions tab in your Mail Box if you can't find it.  Login to the Dashboard and you will see an option to Create your first app. Click on it.   Create your first app (/docs/guides/token/starter-kit-initialize-first-step.png)  Then you would see this popup with multiple starter kits, hover over one of the starter kits and click Deploy . We will choose \"Video Conferencing\" for now.   Initialize Started Kit (/docs/guides/token/starter-kit-initialize.png)  In the Choose your deployment step, select 100ms and enter the subdomain you wish in the Subdomain field.   Domain details (/docs/guides/token/domain-details.png)  After you're App is set click on \"Go to Dashboard\" or  Go Here (https://dashboard.100ms.live/dashboard)   Dasboard (/docs/guides/token/go-to-dashboard.png)   Create a room  Go over to  Room in Dashboard (https://dashboard.100ms.live/rooms) and click on \"Create Room\" , While creating a room you can specify it's name, roles or enable recording.   Create Room (/docs/guides/token/create-room.png)  You will now see \"Room Details\" section and we have a room_id created, copy it somewhere.   Room Id (/docs/guides/token/room-id.png)   Get a temporary token from 100ms dashboard Any client connecting calling 100ms' service needs to authenticate using an auth token. In production you would have your own servers generating the tokens (see more  here (/docs/javascript/v2/foundation/security-and-tokens)), but for a quick start you can use the dashboard to create a token for you. The token will expire in 24 hours and should not be hard-coded into a production app.  To get a temporary token click on \"Join room\" button.   Join Room (/docs/guides/token/join-room.png)  In the popup that shows up click on icon with a key shape next to the role you want to join as.   Copy Token (/docs/guides/token/copy-token.png) The token will be copied to your clipboard. Use this along with the room_id to proceed with the quickstart guide.\r "
    },
    {
        "title": "Jitpack to Maven Central",
        "link": "/android/v2/migrations/jitpack-maven-central",
        "platformName": "Android",
        "objectID": "/android/v2/migrations/jitpack-maven-central",
        "keywords": [],
        "content": "description: Migrating from Jitpack to Maven Central  Sometimes Jitpack may be facing availability issues. In this case you can swap out the Jitpack imports with Maven Central ones. The same version numbers, whether on Maven or Jitpack, always refer to the same SDK. Here are Jitpack imports and the equivalent Maven Central ones.  Jitpack                            Maven Central                                              \"com.github.100mslive.android-sdk:lib:2.5.1\"   \"live.100ms:android-sdk:2.5.1\"    \"com.github.100mslive.android-sdk:virtualBackground:2.5.1\"   \"live.100ms:virtual-background:2.5.1\"    \"com.github.100mslive.android-sdk:stats:2.5.1\"   \"live.100ms:hls-player-stats:2.5.1\"   > 💡 Note that if you previously had: implementation 'com.github.100mslive.android-sdk:2.5.1 notice the lack of :lib: after android-sdk , then you were importing all modules and will need all the imports on the Maven Central side. "
    },
    {
        "title": "SurfaceViewRenderer to HMSVideoView",
        "link": "/android/v2/migrations/surfaceview-migration",
        "platformName": "Android",
        "objectID": "/android/v2/migrations/surfaceview-migration",
        "keywords": [],
        "content": "description: Migrating from SurfaceViewRenderer > HMSVideoView    Why should you migrate?  HMSVideoView provides a better abstraction to render live video and handles edge cases like managing release and init state. Can prevent accidentally subscribing to video track more than once.  Has in-built support for zoom and pan  Has in-built support to intelligently subscribe to video of the correct resolution. (Auto-Simulcast)   All the new features will be developed on HMSVideoView and SurfaceViewRenderer will be deprecated.   Minimum Requirements   SDK version 2.5.3 or higher To use HMSVideoView you'll need to migrate from SurfaceViewRenderer (if you are already using it to render live video), by downloading the dependency from Maven Central   Adding the HMSVideoView      (https://img.shields.io/badge/dynamic/xml.svg?label=100ms&color=blue&query=%2F%2Fmetadata%2Fversioning%2Flatest&url=https%3A%2F%2Frepo1.maven.org%2Fmaven2%2Flive%2F100ms%2Fandroid-sdk%2Fmaven-metadata.xml) (https://jitpack.io/ 100mslive/android-sdk)   json  :build.gradle dependencies     def hmsVersion = \"x.x.x\"   implementation \"live.100ms:video-view:$hmsVersion\" // HMSVideoView       Updating the layout   xml <o̶r̶g̶.̶W̶e̶b̶R̶T̶C̶.̶S̶u̶r̶f̶a̶c̶e̶V̶i̶e̶w̶R̶e̶n̶d̶e̶r̶e̶r̶ live.hms.videoview.HMSVideoView  android:id=\"@+id/peerVideo\"  android:layout_width=\"match_parent\"  android:layout_height=\"wrap_content\"      Initalising and adding video track   kotlin val hmsVideoTrack : HMSVideotrack = hmsPeer.videoTrack s̶u̶r̶f̶a̶c̶e̶V̶i̶e̶w̶.̶i̶n̶i̶t̶(̶S̶h̶a̶r̶e̶d̶E̶g̶l̶C̶o̶n̶t̶e̶x̶t̶.̶c̶o̶n̶t̶e̶x̶t̶,̶ ̶n̶u̶l̶l̶)̶ ̶h̶m̶s̶V̶i̶d̶e̶o̶T̶r̶a̶c̶k̶?̶.̶a̶d̶d̶S̶i̶n̶k̶(̶s̶u̶r̶f̶a̶c̶e̶V̶i̶e̶w̶)̶ hmsVideoView.addTrack(hmsVideoTrack)     Removing video track   kotlin h̶m̶s̶V̶i̶d̶e̶o̶T̶r̶a̶c̶k̶.̶r̶e̶m̶o̶v̶e̶S̶i̶n̶k̶(̶s̶u̶r̶f̶a̶c̶e̶V̶i̶e̶w̶)̶ ̶s̶u̶r̶f̶a̶c̶e̶V̶i̶e̶w̶.̶r̶e̶l̶e̶a̶s̶e̶(̶)̶ hmsVideoView.removeTrack()  "
    },
    {
        "title": "Virtual Background (Beta)",
        "link": "/android/v2/plugins/virtual-background",
        "platformName": "Android",
        "objectID": "/android/v2/plugins/virtual-background",
        "keywords": [],
        "content": "    Introduction Virtual Background plugin helps in customising one’s background i.e. replacing the background with a static image. This guide provides an overview of usage of the Virtual Background plugin of 100ms. <div style=  textAlign: 'center'  >   VirtualBackground (/docs/v2/android-vb.gif) </div>   Supported Devices/Versions/Resolutions   Minimum Android api level required to support Virtual Background plugin is 21, same as that required to use 100ms SDK   Minimum 100ms SDK version it can work with is 2.2.8   Virtual background plugin is built for armeabi-v7a, arm64-v8a, x86 and x86_64 architectures   Maximum supported resolution for this feature is 480p   We recommend that you use this feature on a high performance device for smooth experience   Size Increase Per Architecture   x86_64  9.2 MB   x86  9.5 MB   arm64-v8a  7.6 MB   armeabi-v7a  5 MB   Common Terms   plugin  We are calling virtual background feature as a plugin   image background  The image which the plugin should replace the background with in the user's video.   plugin load time  The time taken by the plugin to load the machine learning model for the first time. The machine learning model will identify that what part of the image the background is and is essential to virtual background working.   Add dependency   Adding the Virtual Background plugin and SDK dependency to your app-level build.gradle .     (https://img.shields.io/badge/dynamic/xml.svg?label=100ms&color=blue&query=%2F%2Fmetadata%2Fversioning%2Flatest&url=https%3A%2F%2Frepo1.maven.org%2Fmaven2%2Flive%2F100ms%2Fandroid-sdk%2Fmaven-metadata.xml) (https://jitpack.io/ 100mslive/android-sdk)   s id=\"sdk-imports\" items=  'Maven Central', 'Jitpack'       id='sdk-imports-0'>   json 5-8 :build.gradle section=AddSDKDependencies sectionIndex=1 dependencies   // See the version in the badge above. // There are separate libraries for the sdk, virtual background and hls-player-stats. //   add just the ones you need. def hmsVersion = \"x.x.x\" implementation \"live.100ms:android-sdk:$hmsVersion\"  // Essential implementation \"live.100ms:virtual-background:$hmsVersion\" // Optional     </Tab>    id='sdk-imports-1'>   json:build.gradle section=AddSDKDependencies sectionIndex=2 dependencies   def hmsVersion = \"x.x.x\" // See the version in the jitpack badge above. implementation \"com.github.100mslive.android-sdk:lib:$hmsVersion\" implementation \"com.github.100mslive.android-sdk:stats:$hmsVersion\"     </Tab>   How to Integrate Virtual Background   Instantiate VirtualBackgroundPlugin Instantiate the 100ms Virtual Background plugin like this:  s id=\"instanstiate-VirtualBackgroundPlugin\" items=  'Kotlin', 'Java'       id='instanstiate-VirtualBackgroundPlugin-0'>   kotlin val hmsSdk = HMSSDK    .Builder(application)    .build() val virtualBackgroundPlugin = HMSVirtualBackground(hmsSdk, imageBitmap)   </Tab>   id='instanstiate-VirtualBackgroundPlugin-1'>   java HMSSDK hmsSdk = new HMSSDK            .Builder(application)            .build(); HMSVirtualBackground virtualBackgroundPlugin = new HMSVirtualBackground(hmsSdk, imageBitmap);   </Tab> That's it. You have instantiated Virtual Background plugin successfully. 🥳 Now let's take a look at the method signature of HMSVirtualBackground.  HMSVirtualBackground accepts 2 argument    hmsSdk : the HMSSDK instance you have used to join the room   imageBitmap  Bitmap  The image that the background will be replaced with. We are supporting JPG/PNG/JPEG type images.   Check if plugin is supported  HMSVirtualBackground.isSupported can be used to check if the API level/Resolution of device is supported or not. If the input resolution is set more than 480p we are not supporting the feature It will return True/False based on these parameters.  s id=\"Check-plugin-support\" items=  'Kotlin', 'Java'       id='Check-plugin-support-0'>   kotlin if(virtualBackgroundPlugin.isSupported())    // Device/Resolution is supported  else    // Device/Resolution is not supported, check Supported Devices/Versions/Resolutions section     </Tab>   id='Check-plugin-support-1'>   java if(virtualBackgroundPlugin.isSupported())    // Device/Resolution is supported  else   // Device/Resolution is not supported, check Supported Devices/Versions/Resolutions section     </Tab>   Init(Optional) Init is used to load the machine learning model of Virtual Background for the first time. It takes on an average 50-200 ms. Calling init is handled internally by SDK if not done by user, in this case addPlugin call will take 50-200 ms for the first time and then less than 10ms in the subsequent calls. Check  this section ( add-plugin) for addPlugin API usage  s id=\"Init\" items=  'Kotlin', 'Java'       id='Init-0'>   kotlin try   virtualBackgroundPlugin.init()  catch(err)    // failed to init Plugin     </Tab>   id='Init-1'>   java try   virtualBackgroundPlugin.init();  catch(err)   // failed to init Plugin     </Tab>   Change Background Call setBackground on the HMSVirtualBackground instance to update the background again later if required. It accepts imageBitmap as a parameter   imageBitmap  Bitmap  The image that the background will be replaced with. Image will be scaled to fit to the video. Scaling will maintain image aspect ratio. If the aspect ratio of the background image is not the same as the video, the image will be cropped to fit in the background.  s id=\"Change-Background\" items=  'Kotlin', 'Java'       id='Change-Background-0'>   kotlin //Use this function to get imageBitmap from a JPG/PNG Image fun getBitmapFromAsset(context: Context, filename: String): Bitmap?    val assetManager = context.assets  val instr: InputStream  var bitmap: Bitmap? = null  try     instr = assetManager.open(filename)   bitmap = BitmapFactory.decodeStream(istr)    catch (e: IOException)     // error reading virtual background image     return bitmap       kotlin //BACKGROUND_FILE_PATH is the background image file location val imageBitmap = getBitmapFromAsset(context, BACKGROUND_FILE_PATH) //To set background as imageBitmap, Use virtualBackgroundPlugin.setBackground(imageBitmap)   </Tab>   id='Change-Background-1'>   java //Use this function to get imageBitmap from a JPG/PNG Image Bitmap getBitmapFromAsset(context: Context, filename: String)    AssetManager assetManager = context.assets;  InputStream instr;  Bitmap bitmap;  try     instr = assetManager.open(filename);   bitmap = BitmapFactory.decodeStream(istr);    catch (e: IOException)     // error reading virtual background image     return bitmap;       java //BACKGROUND_FILE_PATH is the background image file location Bitmap imageBitmap = getBitmapFromAsset(context, BACKGROUND_FILE_PATH); //To set background as imageBitmap, Use virtualBackgroundPlugin.setBackground(imageBitmap);   </Tab>   Tuning pluginFrameRate(Optional)  pluginFrameRate  pluginFrameRate helps in controlling the performance and experience of Virtual Background plugin. pluginFrameRate translates to the number of frames for which background is detected. Higher value will use more resources (CPU/memory/battery), while making the Virtual Background experience smooth. Lower value will be generous on resources, while lowering the Virtual Background smoothness. Recommended value is 15. Values higher than this will not significantly improve Virtual Background smoothness but will be heavy on resources. For lower end devices value can be in the range of 7-10.   Add Plugin To add Virtual background plugin app needs to call addPlugin method of HMSSDK which takes in 3 parameters  1. virtualBackgroundPlugin : An instance of the plugin. 2. HMSActionResultListener : A callback for success or failure of adding the virtual background. 3. pluginFrameRate : An optional parameter for how many frames per second the virtual background image replacement should be updated.   Default=15 if not set by user  s id=\"add-plugin\" items=  'Kotlin', 'Java'       id='add-plugin-0'>   kotlin // This will apply the Virtual Background effect to the local video // pluginFrameRate is an optional parameter hmsSdk.addPlugin(virtualBackgroundPlugin, object : HMSActionResultListener      override fun onError(error: HMSException)       // an error occurred         override fun onSuccess()       // added successfully          ,    pluginFrameRate)   </Tab>   id='add-plugin-1'>   Java // This will apply the Virtual Background effect to the local video // pluginFrameRate is an optional parameter hmsSdk.addPlugin(virtualBackgroundPlugin, new HMSActionResultListener      @Override    public void onError(error: HMSException)       // an error occurred         @Override    public void onSuccess()       // added successfully         ,   pluginFrameRate)   </Tab>   Remove Plugin To remove the virtual background plugin the app needs to call removePlugin method of HMSSDK which takes in 2 parameters. 1. virtualBackgroundPlugin : An instance of the plugin. 2. HMSActionResultListener : A callback for success or failure of adding the virtual background.  s id=\"remove-plugin\" items=  'Kotlin', 'Java'       id='remove-plugin-0'>   kotlin //This will remove the virtual background effect from the video hmsSdk.removePlugin(virtualBackgroundPlugin, object : HMSActionResultListener      override fun onError(error: HMSException)       // an error occurred         override fun onSuccess()       // added successfully          )   </Tab>   id='remove-plugin-1'>   Java //This will remove the virtual background effect from the video hmsSdk.removePlugin(virtualBackgroundPlugin, new HMSActionResultListener      @Override    public void onError(error: HMSException)       // an error occurred         @Override    public void onSuccess()       // added successfully            )   </Tab> "
    },
    {
        "title": "HMSAudioSettings",
        "link": "/api-reference/ios/v2/classes/HMSAudioSettings",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSAudioSettings",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSAudioSettings: Codable     Properties   bitRate   swift public let bitRate: Int     codec   swift public let codec: String   "
    },
    {
        "title": "HMSAudioTrack",
        "link": "/api-reference/ios/v2/classes/HMSAudioTrack",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSAudioTrack",
        "keywords": [],
        "content": "    CLASS     swift open class HMSAudioTrack : HMSTrack   "
    },
    {
        "title": "HMSAudioTrackSettings",
        "link": "/api-reference/ios/v2/classes/HMSAudioTrackSettings",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSAudioTrackSettings",
        "keywords": [],
        "content": "    CLASS     swift open class HMSAudioTrackSettings : NSObject, NSCopying     Properties   maxBitrate   swift open var maxBitrate: Int     trackDescription   swift open var trackDescription: String?     Methods   init   swift public init()     init   swift public init(maxBitrate: Int, trackDescription: String?)   "
    },
    {
        "title": "HMSBrowserRecordingState",
        "link": "/api-reference/ios/v2/classes/HMSBrowserRecordingState",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSBrowserRecordingState",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSBrowserRecordingState: NSObject     Properties   running   swift public let running: Bool     error   swift public let error: HMSError?     Methods   init()   swift public override init()   "
    },
    {
        "title": "HMSChangeTrackStateRequest",
        "link": "/api-reference/ios/v2/classes/HMSChangeTrackStateRequest",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSChangeTrackStateRequest",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSChangeTrackStateRequest: NSObject     Properties   track   swift public let track: HMSTrack     mute   swift public let mute: Bool     requestedBy   swift public let requestedBy: HMSPeer     Methods   init()   swift public override init()   "
    },
    {
        "title": "HMSCommonTrackSource",
        "link": "/api-reference/ios/v2/classes/HMSCommonTrackSource",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSCommonTrackSource",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSCommonTrackSource: NSObject   "
    },
    {
        "title": "HMSConfig",
        "link": "/api-reference/ios/v2/classes/HMSConfig",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSConfig",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSConfig: NSObject   To join a room created, clients need to create a HMSConfig instance and use that instance to call join method of HMSSDK   Properties   userName   swift public let userName: String   the name that the user wants to be displayed while in the room   userID   swift public let userID: String     roomID   swift public let roomID: String?     authToken   swift public let authToken: String   the auth token to be used   shouldSkipPIIEvents   swift public let shouldSkipPIIEvents: Bool     metaData   swift public let metaData: String?   any json string or metadata that can be passed while joining   endpoint   swift public let endpoint: String?   to override the default endpoint (advanced)   Methods   init(userName:userID:roomID:authToken:shouldSkipPIIEvents:metaData:endpoint:)   swift public init(userName: String = \"iOS User\",       userID: String = UUID().uuidString,       roomID: String? = nil,       authToken: String,       shouldSkipPIIEvents: Bool = false,       metaData: String? = nil,       endpoint: String? = nil)   "
    },
    {
        "title": "HMSDataSource",
        "link": "/api-reference/ios/v2/classes/HMSDataSource",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSDataSource",
        "keywords": [],
        "content": "    CLASS     swift public class HMSDataSource     Properties   hms   swift public var hms: HMSSDK?     sections   swift public var sections =  HMSSection ()     allModels   swift public var allModels:  HMSViewModel      delegate   swift public weak var delegate: HMSDataSourceDelegate?     sortComparator   swift public var sortComparator: ((HMSViewModel, HMSViewModel) -> Bool)?     filter   swift public var filter: ((HMSViewModel) -> Bool)?     sectionProducer   swift public var sectionProducer: ((HMSViewModel) -> HMSSection.SectionIdentifier)?     Methods   init()   swift public init()     reload()   swift public func reload()   "
    },
    {
        "title": "HMSDevice",
        "link": "/api-reference/ios/v2/classes/HMSDevice",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSDevice",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSDevice: NSObject   "
    },
    {
        "title": "HMSError",
        "link": "/api-reference/ios/v2/classes/HMSError",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSError",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSError: NSObject, Error     Properties   id   swift public let id: String     code   swift public let code: HMSErrorCode     message   swift public let message: String     info   swift public let info: String?     action   swift public let action: String?     params   swift public let params:  String: Any ?     description   swift public override var description: String     localizedDescription   swift public var localizedDescription: String     analyticsRepresentation   swift public var analyticsRepresentation:  String: Any      Methods   init(id:code:message:info:action:params:)   swift public init(id: String,       code: HMSErrorCode,       message: String,       info: String? = nil,       action: String? = nil,       params:  String: Any ? = nil)   "
    },
    {
        "title": "HMSLocalAudioTrack",
        "link": "/api-reference/ios/v2/classes/HMSLocalAudioTrack",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSLocalAudioTrack",
        "keywords": [],
        "content": "    CLASS     swift open class HMSLocalAudioTrack : HMSAudioTrack     Properties   settings   swift @NSCopying open var settings: HMSAudioTrackSettings     Methods   setMute   swift open func setMute(_ mute: Bool)   "
    },
    {
        "title": "HMSLocalPeer",
        "link": "/api-reference/ios/v2/classes/HMSLocalPeer",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSLocalPeer",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSLocalPeer: HMSPeer     Methods   localAudioTrack()   swift public func localAudioTrack() -> HMSLocalAudioTrack?     localVideoTrack()   swift public func localVideoTrack() -> HMSLocalVideoTrack?   "
    },
    {
        "title": "HMSLocalVideoTrack",
        "link": "/api-reference/ios/v2/classes/HMSLocalVideoTrack",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSLocalVideoTrack",
        "keywords": [],
        "content": "    CLASS     swift open class HMSLocalVideoTrack : HMSVideoTrack     Properties   settings   swift open var settings: HMSVideoTrackSettings     Methods   setMute   swift open func setMute(_ mute: Bool)     startCapturing   swift open func startCapturing()     stopCapturing   swift open func stopCapturing()     switchCamera   swift open func switchCamera()   "
    },
    {
        "title": "HMSMessage",
        "link": "/api-reference/ios/v2/classes/HMSMessage",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSMessage",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSMessage: NSObject     Properties   message   swift public let message: String     type   swift public let type: String     sender   swift public var sender: HMSPeer?     recipient   swift public var recipient: HMSMessageRecipient     time   swift public let time: Date     Methods   init(message:type:)   swift public init(message: String, type: String = \"chat\")     init(message:type:peerRecipient:)   swift public init(message: String, type: String = \"chat\", peerRecipient: HMSPeer)     init(message:type:rolesRecipient:)   swift public init(message: String, type: String = \"chat\", rolesRecipient:  HMSRole )   "
    },
    {
        "title": "HMSMessageRecipient",
        "link": "/api-reference/ios/v2/classes/HMSMessageRecipient",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSMessageRecipient",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSMessageRecipient: NSObject     Properties   type   swift public let type: HMSMessageRecipientType     peerRecipient   swift public let peerRecipient: HMSPeer?     rolesRecipient   swift public let rolesRecipient:  HMSRole ?     Methods   init()   swift public override init()   "
    },
    {
        "title": "HMSNetworkQuality",
        "link": "/api-reference/ios/v2/classes/HMSNetworkQuality",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSNetworkQuality",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSNetworkQuality: NSObject     Properties   peer   swift public let peer: HMSPeer     incomingAvailableBitrate   swift public var incomingAvailableBitrate: Int?     outgoingAvailableBitrate   swift public var outgoingAvailableBitrate: Int?   "
    },
    {
        "title": "HMSPeer",
        "link": "/api-reference/ios/v2/classes/HMSPeer",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSPeer",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSPeer: NSObject   A peer represents all participants connected to a room. Peers can be \"local\" or \"remote\"   Properties   peerID   swift public let peerID: String     customerUserID   swift public let customerUserID: String?     isLocal   swift public let isLocal: Bool     name   swift public internal(set) var name: String     role   swift public internal(set) var role: HMSRole?     customerDescription   swift public internal(set) var customerDescription: String?     audioTrack   swift public internal(set) var audioTrack: HMSAudioTrack?     videoTrack   swift public internal(set) var videoTrack: HMSVideoTrack?     auxiliaryTracks   swift public private(set) var auxiliaryTracks:  HMSTrack ?   "
    },
    {
        "title": "HMSPermissions",
        "link": "/api-reference/ios/v2/classes/HMSPermissions",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSPermissions",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSPermissions: Codable     Properties   endRoom   swift public let endRoom: Bool?     removeOthers   swift public let removeOthers: Bool?     stopPresentation   swift public let stopPresentation: Bool?     muteAll   swift public let muteAll: Bool?     unmute   swift public let unmute: Bool?     mute   swift public let mute: Bool?     changeRole   swift public let changeRole: Bool?   "
    },
    {
        "title": "HMSPublishSettings",
        "link": "/api-reference/ios/v2/classes/HMSPublishSettings",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSPublishSettings",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSPublishSettings: Codable     Properties   audio   swift public let audio: HMSAudioSettings     video   swift public let video: HMSVideoSettings     screen   swift public let screen: HMSVideoSettings     videoSimulcastLayers   swift public let videoSimulcastLayers: HMSSimulcastSettingsPolicy?     screenSimulcastLayers   swift public let screenSimulcastLayers: HMSSimulcastSettingsPolicy?     allowed   swift public let allowed:  String ?   "
    },
    {
        "title": "HMSRemoteAudioTrack",
        "link": "/api-reference/ios/v2/classes/HMSRemoteAudioTrack",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSRemoteAudioTrack",
        "keywords": [],
        "content": "    CLASS     swift open class HMSRemoteAudioTrack : HMSAudioTrack     Methods   isPlaybackAllowed   swift open func isPlaybackAllowed() -> Bool     setPlaybackAllowed   swift open func setPlaybackAllowed(_ playbackAllowed: Bool)   "
    },
    {
        "title": "HMSRemotePeer",
        "link": "/api-reference/ios/v2/classes/HMSRemotePeer",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSRemotePeer",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSRemotePeer: HMSPeer     Methods   remoteAudioTrack()   swift public func remoteAudioTrack() -> HMSRemoteAudioTrack?     remoteVideoTrack()   swift public func remoteVideoTrack() -> HMSRemoteVideoTrack?   "
    },
    {
        "title": "HMSRemoteVideoTrack",
        "link": "/api-reference/ios/v2/classes/HMSRemoteVideoTrack",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSRemoteVideoTrack",
        "keywords": [],
        "content": "    CLASS     swift open class HMSRemoteVideoTrack : HMSVideoTrack     Properties   layer   swift open var layer: HMSSimulcastLayer     layerDefinitions   swift open var layerDefinitions:  HMSSimulcastLayerDefinition ?     Methods   isPlaybackAllowed   swift open func isPlaybackAllowed() -> Bool     setPlaybackAllowed   swift open func setPlaybackAllowed(_ playbackAllowed: Bool)   "
    },
    {
        "title": "HMSRemovedFromRoomNotification",
        "link": "/api-reference/ios/v2/classes/HMSRemovedFromRoomNotification",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSRemovedFromRoomNotification",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSRemovedFromRoomNotification: NSObject     Properties   requestedBy   swift public let requestedBy: HMSPeer     reason   swift public let reason: String     roomEnded   swift public let roomEnded: Bool     Methods   init()   swift public override init()   "
    },
    {
        "title": "HMSRole",
        "link": "/api-reference/ios/v2/classes/HMSRole",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSRole",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSRole: NSObject, Codable     Properties   name   swift public let name: String     publishSettings   swift public let publishSettings: HMSPublishSettings     subscribeSettings   swift public let subscribeSettings: HMSSubscribeSettings     permissions   swift public let permissions: HMSPermissions     priority   swift public let priority: Int     generalPermissions   swift public let generalPermissions:  String: String ?     internalPlugins   swift public let internalPlugins:  String: String ?     externalPlugins   swift public let externalPlugins:  String: String ?   "
    },
    {
        "title": "HMSRoleChangeRequest",
        "link": "/api-reference/ios/v2/classes/HMSRoleChangeRequest",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSRoleChangeRequest",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSRoleChangeRequest: NSObject     Properties   suggestedRole   swift public let suggestedRole: HMSRole     requestedBy   swift public let requestedBy: HMSPeer     Methods   init()   swift public override init()   "
    },
    {
        "title": "HMSRoom",
        "link": "/api-reference/ios/v2/classes/HMSRoom",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSRoom",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSRoom: NSObject   A room represents a real-time audio, video session, the basic building block of the 100ms SDK   Properties   name   swift public internal(set) var name: String?     roomID   swift public internal(set) var roomID: String?     metaData   swift public internal(set) var metaData: String?     peers   swift public internal(set) var peers =  HMSPeer ()     browserRecordingState   swift public internal(set) var browserRecordingState: HMSBrowserRecordingState     serverRecordingState   swift public internal(set) var serverRecordingState: HMSServerRecordingState     rtmpStreamingState   swift public internal(set) var rtmpStreamingState: HMSRTMPStreamingState   "
    },
    {
        "title": "HMSRTMPConfig",
        "link": "/api-reference/ios/v2/classes/HMSRTMPConfig",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSRTMPConfig",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSRTMPConfig: NSObject   A configuration object for recording or RTMP stream   Methods   init()   swift public override init()     init(meetingURL:rtmpURLs:record:)   swift public init(meetingURL: URL?, rtmpURLs:  URL ?, record: Bool)   "
    },
    {
        "title": "HMSRTMPStreamingState",
        "link": "/api-reference/ios/v2/classes/HMSRTMPStreamingState",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSRTMPStreamingState",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSRTMPStreamingState: NSObject     Properties   running   swift public let running: Bool     error   swift public let error: HMSError?     Methods   init()   swift public override init()   "
    },
    {
        "title": "HMSSDK",
        "link": "/api-reference/ios/v2/classes/HMSSDK",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSSDK",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSSDK: NSObject   The public interface of 100ms SDK. Create an instance of HMSSDK to start using the SDK.   Key Concepts     Room    A room represents a real-time audio, video session, the basic building block of the 100mslive Video SDK   Track    A track represents either the audio or video that makes up a stream   Peer    A peer represents all participants connected to a room. Peers can be \"local\" or \"remote\"   Broadcast    A local peer can send any message/data to all remote peers in the room HMSSDK has other methods which the client app can use to get more info about the Room, Peer and Tracks   Properties   localPeer   swift public var localPeer: HMSLocalPeer?   Returns the local peer, which contains the local tracks.   remotePeers   swift public var remotePeers:  HMSRemotePeer ?   Returns all remote peers in the room.   roles   swift public var roles:  HMSRole    Returns all roles in the room.   room   swift public var room: HMSRoom?   Returns the room which was joined.   trackSettings   swift public var trackSettings: HMSTrackSettings = HMSTrackSettings()   Use to override track settings coming from role policy   analyticsLevel   swift public var analyticsLevel: HMSAnalyticsEventLevel = .error   Sets the verbosity of analytics events   logger   swift public weak var logger: HMSLogger?   Sets the logger instance to use for piping logs   Methods   build(block:)   swift public static func build(block: ((HMSSDK) -> Void)? = nil) -> HMSSDK   this will instantiate an HMSSDK object   Parameter block: pass a block with different settings as required   Returns: an instance of HMSSDK object    Parameters  Name  Description                                      block  pass a block with different settings as required    preview(config:delegate:)   swift public func preview(config: HMSConfig, delegate: HMSPreviewListener)   Begin a preview so that the local peer's audio and video can be displayed to them before they join the room.   Parameters:     config: The config object instance which contains joining information.     delegate: The update listener object which will receive all callbacks.    Parameters  Name    Description                                                  config   The config object instance which contains joining information.   delegate  The update listener object which will receive all callbacks.     preview(role:completion:)   swift public func preview(role: HMSRole, completion: (( HMSTrack ?, HMSError?) -> Void))   Call this preview API any time after joining in case you need to show a preview for a certain role. i.e before doing/accepting a role change request.   Parameters:     role: The role that would be used for checking which tracks would be required.     completion: The completion handler to be invoked when tracks are fetched or in case of an error.    Parameters  Name     Description                                                                   role     The role that would be used for checking which tracks would be required.         completion  The completion handler to be invoked when tracks are fetched or in case of an error.    cancelPreview()   swift public func cancelPreview()   Call this API in case the tracks obtained via preview(role:) API are no longer needed. There is no need to call this if role change happened. Call this if user has decided not to change role to release camera/mic.   join(config:delegate:)   swift public func join(config: HMSConfig, delegate: HMSUpdateListener)   Join the room.   Parameters:     config: The config object instance which contains joining information.     delegate: The update listener object which will receive all callbacks,    Parameters  Name    Description                                                  config   The config object instance which contains joining information.   delegate  The update listener object which will receive all callbacks,     leave( _:)   swift public func leave(_ completion: ((Bool, HMSError?) -> Void)? = nil)   Call the leave method on the HMSSDK instance to leave the current room.   Parameters:     completion: The completion handler to be invoked when the request succeeds or fails with an error.    Parameters  Name     Description                                                                    completion  The completion handler to be invoked when the request succeeds or fails with an error.    sendBroadcastMessage(type:message:completion:)   swift public func sendBroadcastMessage(type: String = \"chat\", message: String, completion: ((HMSMessage?, HMSError?) -> Void)? = nil)   Sends a message to everyone in the room.   Parameters:     type: The type of message     message: Content of the message.     completion: The completion handler to be invoked when message was sent, or when error happened during sending    Parameters  Name     Description                                                                             type     The type of message                                          message   Content of the message.                                        completion  The completion handler to be invoked when message was sent, or when error happened during sending    sendGroupMessage(type:message:roles:completion:)   swift public func sendGroupMessage(type: String = \"chat\", message: String, roles:  HMSRole , completion: ((HMSMessage?, HMSError?) -> Void)? = nil)   Sends a message to the specified roles defined. All peers that belong to the specified roles will receive the message.   Parameters:     type: The type of message     message: Content of the message.     roles: The list of roles to whom this message is directed.     completion: The completion handler to be invoked when message was sent, or when error happened during sending    Parameters  Name     Description                                                                             type     The type of message                                          message   Content of the message.                                        roles    The list of roles to whom this message is directed.                          completion  The completion handler to be invoked when message was sent, or when error happened during sending    sendDirectMessage(type:message:peer:completion:)   swift public func sendDirectMessage(type: String = \"chat\", message: String, peer: HMSPeer, completion: ((HMSMessage?, HMSError?) -> Void)? = nil)   Sends a direct message to the specified peer only.   Parameters:     type: The type of message.     message: Content of the message.     peer: The peer to whom this message is directed.     completion: The completion handler to be invoked when message was sent, or when error happened during sending    Parameters  Name     Description                                                                             type     The type of message.                                         message   Content of the message.                                        peer     The peer to whom this message is directed.                              completion  The completion handler to be invoked when message was sent, or when error happened during sending    changeRole(for:to:force:completion:)   swift public func changeRole(for peer: HMSPeer, to role: HMSRole, force: Bool = false, completion: ((Bool, HMSError?) -> Void)? = nil)   Requests a change of role for specified peer.   Parameters:     peer: The peer whose role should be changed.     role: The target role.     force: False if the peer should be prompted to accept the new role. true if their role should be changed without a prompt.     completion: The completion handler to be invoked when the request succeeds or fails with an error.    Parameters  Name     Description                                                                                          peer     The peer whose role should be changed.                                         role     The target role.                                                    force    False if the peer should be prompted to accept the new role. true if their role should be changed without a prompt.   completion  The completion handler to be invoked when the request succeeds or fails with an error.                   changeRolesOfAllPeers(to:limitToRoles:completion:)   swift public func changeRolesOfAllPeers(to role: HMSRole, limitToRoles: HMSRole ? = nil, completion: ((Bool, HMSError?) -> Void)? = nil)   Requests a change of role for all peers except for the local peer.   Parameters:     role: The target role.     limitToRoles: If specified, only peers currently belonging to roles in the array will be changed to the new role.     completion: The completion handler to be invoked when the request succeeds or fails with an error.    Parameters  Name      Description                                                                               role      The target role.                                            limitToRoles  If specified, only peers currently belonging to roles in the array will be changed to the new role.   completion   The completion handler to be invoked when the request succeeds or fails with an error.           accept(changeRole:completion:)   swift public func accept(changeRole request: HMSRoleChangeRequest, completion: ((Bool, HMSError?) -> Void)? = nil)   Call to accept the role change request sent to the current peer. Once this method is called, the peer's role will be changed to the requested one.   Parameters:     request The request that the SDK had sent to this peer (in HMSUpdateListener.onRoleChangeRequest).     completion: The completion handler to be invoked when the request succeeds or fails with an error.    Parameters  Name                                                 Description                                                                                          request The request that the SDK had sent to this peer (in HMSUpdateListener.onRoleChangeRequest).                                               completion                                              The completion handler to be invoked when the request succeeds or fails with an error.    changeTrackState(for:mute:completion:)   swift public func changeTrackState(for remoteTrack: HMSTrack, mute: Bool, completion: ((Bool, HMSError?) -> Void)? = nil)   To change the mute status of a remote HMSTrack.   Parameters:     remoteTrack: The HMSTrack whose mute status needs to be changed.     mute: True if the track needs to be muted, false otherwise.     completion: The completion handler to be invoked when the request succeeds or fails with an error.    Parameters  Name     Description                                                                     remoteTrack  The HMSTrack whose mute status needs to be changed.                    mute     True if the track needs to be muted, false otherwise.                   completion  The completion handler to be invoked when the request succeeds or fails with an error.    changeTrackState(mute:for:source:roles:completion:)   swift public func changeTrackState(mute: Bool, for trackKind: HMSTrackKind? = nil, source: String? = nil, roles:  HMSRole ? = nil, completion: ((Bool, HMSError?) -> Void)? = nil)   To change the mute status of multiple tracks of specific kinds for every peer or for peers with certain roles. Call changeTrackState(mute: true) to mute everybody's audio and video.   Parameters:     mute: True if the track needs to be muted, false otherwise.     trackKind: Pass an array of HMSTrackKind for change to apply to specific kind of tracks. Passing nil means all track kinds will be changed.     source: Pass a HMSTrackSource for change to apply to specific track source. Passing nil means all track sources will be changed.     roles: Pass an array of HMSRole for change to apply to specific roles. Passing nil means all roles will be changed.     completion: The completion handler to be invoked when the request succeeds or fails with an error.    Parameters  Name     Description                                                                                                    mute     True if the track needs to be muted, false otherwise.                                        trackKind  Pass an array of HMSTrackKind for change to apply to specific kind of tracks. Passing nil means all track kinds will be changed.   source    Pass a HMSTrackSource for change to apply to specific track source. Passing nil means all track sources will be changed.       roles    Pass an array of HMSRole for change to apply to specific roles. Passing nil means all roles will be changed.             completion  The completion handler to be invoked when the request succeeds or fails with an error.                         removePeer( _:reason:completion:)   swift public func removePeer(_ peer: HMSPeer, reason: String, completion: ((Bool, HMSError?) -> Void)? = nil)   Removes the specified peer from the current room.   Parameters:     peer: The peer to remove     reason: The reason for removing can be passed on to the peer.     completion: The completion handler to be invoked when the request succeeds or fails with an error.    Parameters  Name     Description                                                                    peer     The peer to remove                                     reason    The reason for removing can be passed on to the peer.                   completion  The completion handler to be invoked when the request succeeds or fails with an error.    endRoom(lock:reason:completion:)   swift public func endRoom(lock: Bool, reason: String, completion: ((Bool, HMSError?) -> Void)? = nil)   End the room and make all participants leave.   Parameters:     lock: Whether rejoining the room should be disabled till the room is unlocked.     reason: The reason for ending the room can be passed on to other peers.     completion: The completion handler to be invoked when the request succeeds or fails with an error.    Parameters  Name     Description                                                                    lock     Whether rejoining the room should be disabled till the room is unlocked.          reason    The reason for ending the room can be passed on to other peers.              completion  The completion handler to be invoked when the request succeeds or fails with an error.    startRTMPOrRecording(config:completion:)   swift public func startRTMPOrRecording(config: HMSRTMPConfig, completion: ((Bool, HMSError?) -> Void)? = nil)   Start RTMP stream and/or room recording.   Parameters:     config: RTMP/Recording parameters     completion: The completion handler to be invoked when the request succeeds or fails with an error.    Parameters  Name     Description                                                                    config    RTMP/Recording parameters                                 completion  The completion handler to be invoked when the request succeeds or fails with an error.    stopRTMPAndRecording(completion:)   swift public func stopRTMPAndRecording(completion: ((Bool, HMSError?) -> Void)? = nil)   Stop RTMP stream and room recording.   Parameters:     completion: The completion handler to be invoked when the request succeeds or fails with an error.    Parameters  Name     Description                                                                    completion  The completion handler to be invoked when the request succeeds or fails with an error.    add(delegate:)   swift public func add(delegate: HMSUpdateListener)   Adds another listener of SDK updates   Parameter delegate: the update listener object which will receive all callbacks    Parameters  Name    Description                                                delegate  the update listener object which will receive all callbacks    remove(delegate:)   swift public func remove(delegate: HMSUpdateListener)   Removes the listener of SDK updates   Parameter delegate: the update listener to remove    Parameters  Name    Description                         delegate  the update listener to remove    prepareForExternalAudioPlayback()   swift public func prepareForExternalAudioPlayback()     resumeAfterExternalAudioPlayback()   swift public func resumeAfterExternalAudioPlayback()    resumeAfterExternalAudioPlayback()   swift public func resumeAfterExternalAudioPlayback()   "
    },
    {
        "title": "HMSSection",
        "link": "/api-reference/ios/v2/classes/HMSSection",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSSection",
        "keywords": [],
        "content": "    CLASS     swift public class HMSSection     Properties   models   swift public var models:  HMSViewModel    "
    },
    {
        "title": "HMSServerRecordingState",
        "link": "/api-reference/ios/v2/classes/HMSServerRecordingState",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSServerRecordingState",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSServerRecordingState: NSObject     Properties   running   swift public let running: Bool     error   swift public let error: HMSError?     Methods   init()   swift public override init()   "
    },
    {
        "title": "HMSSimulcastLayerDefinition",
        "link": "/api-reference/ios/v2/classes/HMSSimulcastLayerDefinition",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSSimulcastLayerDefinition",
        "keywords": [],
        "content": "    CLASS     swift open class HMSSimulcastLayerDefinition : NSObject     Properties   layer   swift open var layer: HMSSimulcastLayer     resolution   swift open var resolution: CGSize     Methods   init   swift public init(layer: HMSSimulcastLayer, resolution: CGSize)   "
    },
    {
        "title": "HMSSimulcastLayerSettings",
        "link": "/api-reference/ios/v2/classes/HMSSimulcastLayerSettings",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSSimulcastLayerSettings",
        "keywords": [],
        "content": "    CLASS     swift open class HMSSimulcastLayerSettings : NSObject, NSCopying     Properties   maxBitrate   swift open var maxBitrate: Int     maxFrameRate   swift open var maxFrameRate: Int     scaleResolutionDownBy   swift open var scaleResolutionDownBy: Double     rid   swift open var rid: String     Methods   init   swift public init(witRID rid: String, maxBitrate: Int, maxFrameRate: Int, scaleResolutionDownBy: Double)   "
    },
    {
        "title": "HMSSimulcastLayerSettingsPolicy",
        "link": "/api-reference/ios/v2/classes/HMSSimulcastLayerSettingsPolicy",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSSimulcastLayerSettingsPolicy",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSSimulcastLayerSettingsPolicy: Codable     Properties   rid   swift public let rid: String     scaleResolutionDownBy   swift public let scaleResolutionDownBy: Double?     maxBitrate   swift public let maxBitrate, maxFramerate: Int?     maxFramerate   swift public let maxBitrate, maxFramerate: Int?   "
    },
    {
        "title": "HMSSimulcastSettingsPolicy",
        "link": "/api-reference/ios/v2/classes/HMSSimulcastSettingsPolicy",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSSimulcastSettingsPolicy",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSSimulcastSettingsPolicy: Codable     Properties   width   swift public let width, height: Int?     height   swift public let width, height: Int?     layers   swift public let layers:  HMSSimulcastLayerSettingsPolicy ?   "
    },
    {
        "title": "HMSSpeaker",
        "link": "/api-reference/ios/v2/classes/HMSSpeaker",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSSpeaker",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSSpeaker: NSObject     Properties   peer   swift public let peer: HMSPeer     track   swift public let track: HMSTrack     level   swift public let level: Int     Methods   init(peer:track:level:)   swift public init(peer: HMSPeer, track: HMSTrack, level: Int)   "
    },
    {
        "title": "HMSSubscribeDegradationPolicy",
        "link": "/api-reference/ios/v2/classes/HMSSubscribeDegradationPolicy",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSSubscribeDegradationPolicy",
        "keywords": [],
        "content": "    CLASS     swift public class HMSSubscribeDegradationPolicy: Codable     Properties   packetLossThreshold   swift public var packetLossThreshold, degradeGracePeriodSeconds, recoverGracePeriodSeconds: Int?     degradeGracePeriodSeconds   swift public var packetLossThreshold, degradeGracePeriodSeconds, recoverGracePeriodSeconds: Int?     recoverGracePeriodSeconds   swift public var packetLossThreshold, degradeGracePeriodSeconds, recoverGracePeriodSeconds: Int?   "
    },
    {
        "title": "HMSSubscribeSettings",
        "link": "/api-reference/ios/v2/classes/HMSSubscribeSettings",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSSubscribeSettings",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSSubscribeSettings: Codable     Properties   subscribeToRoles   swift public let subscribeToRoles:  String ?     maxSubsBitRate   swift public let maxSubsBitRate: Int     subscribeDegradation   swift public let subscribeDegradation: HMSSubscribeDegradationPolicy?   "
    },
    {
        "title": "HMSTrack",
        "link": "/api-reference/ios/v2/classes/HMSTrack",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSTrack",
        "keywords": [],
        "content": "    CLASS     swift open class HMSTrack : NSObject   A track represents either the audio or video that makes up a stream   Properties   trackId   swift open var trackId: String     kind   swift open var kind: HMSTrackKind     source   swift open var source: String     trackDescription   swift open var trackDescription: String     Methods   isMute   swift open func isMute() -> Bool   "
    },
    {
        "title": "HMSTrackSettings",
        "link": "/api-reference/ios/v2/classes/HMSTrackSettings",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSTrackSettings",
        "keywords": [],
        "content": "    CLASS     swift open class HMSTrackSettings : NSObject, NSCopying     Properties   video   swift open var video: HMSVideoTrackSettings?     audio   swift open var audio: HMSAudioTrackSettings?     Methods   init   swift public init()     init   swift public init(videoSettings: HMSVideoTrackSettings?, audioSettings: HMSAudioTrackSettings?)ckDescription: String?)   "
    },
    {
        "title": "HMSUtilities",
        "link": "/api-reference/ios/v2/classes/HMSUtilities",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSUtilities",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSUtilities: NSObject     Methods   getDevices()   swift public class func getDevices() ->  HMSDevice      getAllVideoTracks(in:)   swift public class func getAllVideoTracks(in room: HMSRoom) ->  HMSVideoTrack      getAllAudioTracks(in:)   swift public class func getAllAudioTracks(in room: HMSRoom) ->  HMSAudioTrack      getPeer(for:in:)   swift public class func getPeer(for id: String, in room: HMSRoom) -> HMSPeer?     getTrack(for:in:)   swift public class func getTrack(for id: String, in room: HMSRoom) -> HMSTrack?     getAudioTrack(for:in:)   swift public class func getAudioTrack(for id: String, in room: HMSRoom) -> HMSAudioTrack?     getVideoTrack(for:in:)   swift public class func getVideoTrack(for id: String, in room: HMSRoom) -> HMSVideoTrack?   "
    },
    {
        "title": "HMSUtility",
        "link": "/api-reference/ios/v2/classes/HMSUtility",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSUtility",
        "keywords": [],
        "content": "    CLASS     swift open class HMSUtility : NSObject     Methods   codec(from:)   swift open class func codec(from string: String) -> HMSCodec     codecString(from:)   swift open class func codecString(from codec: HMSCodec) -> String     source(from:)   swift open class func source(from string: String) -> HMSTrackSource     sourceString(from:)   swift open class func sourceString(from source: HMSTrackSource) -> String   "
    },
    {
        "title": "HMSVideoSettings",
        "link": "/api-reference/ios/v2/classes/HMSVideoSettings",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSVideoSettings",
        "keywords": [],
        "content": "    CLASS     swift @objcMembers public class HMSVideoSettings: Codable     Properties   bitRate   swift public let bitRate: Int?     codec   swift public let codec: String     frameRate   swift public let frameRate: Int     width   swift public let width: Int     height   swift public let height: Int   "
    },
    {
        "title": "HMSVideoTrack",
        "link": "/api-reference/ios/v2/classes/HMSVideoTrack",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSVideoTrack",
        "keywords": [],
        "content": "    CLASS     swift open class HMSVideoTrack : HMSTrack     Methods   addSink   swift open func addSink(_ sink: Any)     removeSink   swift open func removeSink(_ sink: Any)     isDegraded   swift open func isDegraded() -> Bool   "
    },
    {
        "title": "HMSVideoTrackSettings",
        "link": "/api-reference/ios/v2/classes/HMSVideoTrackSettings",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSVideoTrackSettings",
        "keywords": [],
        "content": "    CLASS     swift open class HMSVideoTrackSettings : NSObject, NSCopying     Properties   codec   swift open var codec: HMSCodec     resolution   swift open var resolution: HMSVideoResolution     maxBitrate   swift open var maxBitrate: Int     maxFrameRate   swift open var maxFrameRate: Int     cameraFacing   swift open var cameraFacing: HMSCameraFacing     trackDescription   swift open var trackDescription: String?     simulcastSettings   swift open var simulcastSettings:  HMSSimulcastLayerSettings ?     Methods   init   swift public init()     init   swift public init(codec: HMSCodec, resolution: HMSVideoResolution, maxBitrate: Int, maxFrameRate: Int, cameraFacing: HMSCameraFacing, trackDescription: String?)     init   swift public init(codec: HMSCodec, resolution: HMSVideoResolution, maxBitrate: Int, maxFrameRate: Int, cameraFacing: HMSCameraFacing, simulcastSettings:  HMSSimulcastLayerSettings ?, trackDescription: String?)   "
    },
    {
        "title": "HMSVideoView",
        "link": "/api-reference/ios/v2/classes/HMSVideoView",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSVideoView",
        "keywords": [],
        "content": "    CLASS     swift open class HMSVideoView : UIView     Properties   videoContentMode   swift open var videoContentMode: Int     disableAutoSimulcastLayerSelect   swift open var disableAutoSimulcastLayerSelect: Bool     mirror   swift open var mirror: Bool     Methods   setVideoTrack( _:)   swift open func setVideoTrack(_ track: HMSVideoTrack)     videoTrack()   swift open func videoTrack() -> HMSVideoTrack   "
    },
    {
        "title": "HMSViewModel",
        "link": "/api-reference/ios/v2/classes/HMSViewModel",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/classes/HMSViewModel",
        "keywords": [],
        "content": "    CLASS     swift public class HMSViewModel     Properties   identifier   swift public var identifier: String     peer   swift public let peer: HMSPeer     videoTrack   swift public var videoTrack: HMSVideoTrack?   "
    },
    {
        "title": "HMSAnalyticsEventLevel",
        "link": "/api-reference/ios/v2/enums/HMSAnalyticsEventLevel",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/enums/HMSAnalyticsEventLevel",
        "keywords": [],
        "content": "    ENUM     swift public enum HMSAnalyticsEventLevel : UInt     Cases   off   swift case off = 0     error   swift case error = 1     info   swift case info = 2     verbose   swift case verbose = 3   "
    },
    {
        "title": "HMSCameraFacing",
        "link": "/api-reference/ios/v2/enums/HMSCameraFacing",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/enums/HMSCameraFacing",
        "keywords": [],
        "content": "    ENUM     swift public enum HMSCameraFacing : UInt     Cases   front   swift case front = 0     back   swift case back = 1   "
    },
    {
        "title": "HMSCodec",
        "link": "/api-reference/ios/v2/enums/HMSCodec",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/enums/HMSCodec",
        "keywords": [],
        "content": "    ENUM     swift public enum HMSCodec : UInt     Cases   H264   swift case H264 = 0     VP8   swift case VP8 = 1   "
    },
    {
        "title": "HMSConnectionRole",
        "link": "/api-reference/ios/v2/enums/HMSConnectionRole",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/enums/HMSConnectionRole",
        "keywords": [],
        "content": "    ENUM     swift public enum HMSConnectionRole : UInt     Cases   pub   swift case pub = 0     sub   swift case sub = 1   "
    },
    {
        "title": "HMSErrorCode",
        "link": "/api-reference/ios/v2/enums/HMSErrorCode",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/enums/HMSErrorCode",
        "keywords": [],
        "content": "    ENUM     swift @objc public enum HMSErrorCode: Int     Cases   websocketGenericError   swift case websocketGenericError = 1000     websocketConnectionLost   swift case websocketConnectionLost = 1003     initServerError   swift case initServerError = 2000     initHTTPConnectionLost   swift case initHTTPConnectionLost = 2001     initHTTPErrorBadRequest   swift case initHTTPErrorBadRequest = 2400     initHTTPErrorInvalidEndpointURL   swift case initHTTPErrorInvalidEndpointURL = 2002     initHTTPErrorEndpointUnreachable   swift case initHTTPErrorEndpointUnreachable = 2003     tracksErrorGeneric   swift case tracksErrorGeneric = 3000     tracksErrorCantAccessCaptureDevice   swift case tracksErrorCantAccessCaptureDevice = 3001     tracksErrorDeviceNotAvailable   swift case tracksErrorDeviceNotAvailable = 3002     tracksErrorDeviceInUse   swift case tracksErrorDeviceInUse = 3003     tracksErrorPeerConnectionFactoryDisposed   swift case tracksErrorPeerConnectionFactoryDisposed = 3004     tracksErrorNothingToReturn   swift case tracksErrorNothingToReturn = 3005     tracksErrorInvalidVideoSettings   swift case tracksErrorInvalidVideoSettings = 3006     tracksErrorCodecChangeNotPermitted   swift case tracksErrorCodecChangeNotPermitted = 3007     tracksErrorAudioVideoSubsystemFailure   swift case tracksErrorAudioVideoSubsystemFailure = 3008     tracksErrorAudioShouldBeSilenced   swift case tracksErrorAudioShouldBeSilenced = 3009     tracksErrorMicCaptureFailed   swift case tracksErrorMicCaptureFailed = 3011     webrtcErrorCreateOfferFailed   swift case webrtcErrorCreateOfferFailed = 4001     webrtcErrorCreateAnswerFailed   swift case webrtcErrorCreateAnswerFailed = 4002     webrtcErrorSetLocalDescriptionFailed   swift case webrtcErrorSetLocalDescriptionFailed = 4003     webrtcErrorSetRemoteDescriptionFailed   swift case webrtcErrorSetRemoteDescriptionFailed = 4004     webrtcErrorICEFailure   swift case webrtcErrorICEFailure = 4005     joinErrorServer   swift case joinErrorServer = 5000     joinErrorAlreadyJoined   swift case joinErrorAlreadyJoined = 5001     genericErrorNotConnected   swift case genericErrorNotConnected = 6000     genericErrorSignalling   swift case genericErrorSignalling = 6001     genericErrorUnknown   swift case genericErrorUnknown = 6002     genericErrorNotReady   swift case genericErrorNotReady = 6003     genericErrorJsonParsingFailed   swift case genericErrorJsonParsingFailed = 6004     genericErrorTrackMetadataMissing   swift case genericErrorTrackMetadataMissing = 6005     genericErrorRTCTrackMissing   swift case genericErrorRTCTrackMissing = 6006     genericErrorPeerMetadataMissing   swift case genericErrorPeerMetadataMissing = 6007   rMetadataMissing = 6007      "
    },
    {
        "title": "HMSLogLevel",
        "link": "/api-reference/ios/v2/enums/HMSLogLevel",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/enums/HMSLogLevel",
        "keywords": [],
        "content": "    ENUM     swift @objc public enum HMSLogLevel: Int     Cases   off   swift case off = 0     error   swift case error     warning   swift case warning     verbose   swift case verbose   "
    },
    {
        "title": "HMSMessageRecipientType",
        "link": "/api-reference/ios/v2/enums/HMSMessageRecipientType",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/enums/HMSMessageRecipientType",
        "keywords": [],
        "content": "    ENUM     swift @objc public enum HMSMessageRecipientType: Int     Cases   broadcast   swift case broadcast = 0     peer   swift case peer     roles   swift case roles   "
    },
    {
        "title": "HMSPeerUpdate",
        "link": "/api-reference/ios/v2/enums/HMSPeerUpdate",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/enums/HMSPeerUpdate",
        "keywords": [],
        "content": "    ENUM     swift @objc public enum HMSPeerUpdate: Int, CustomStringConvertible     Cases   peerJoined   swift case peerJoined     peerLeft   swift case peerLeft     roleUpdated   swift case roleUpdated     defaultUpdate   swift case defaultUpdate     Properties   description   swift public var description: String   "
    },
    {
        "title": "HMSRoomUpdate",
        "link": "/api-reference/ios/v2/enums/HMSRoomUpdate",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/enums/HMSRoomUpdate",
        "keywords": [],
        "content": "    ENUM     swift @objc public enum HMSRoomUpdate: Int, CustomStringConvertible     Cases   roomTypeChanged   swift case roomTypeChanged     metaDataUpdated   swift case metaDataUpdated     browserRecordingStateUpdated   swift case browserRecordingStateUpdated     serverRecordingStateUpdated   swift case serverRecordingStateUpdated     rtmpStreamingStateUpdated   swift case rtmpStreamingStateUpdated     Properties   description   swift public var description: String   "
    },
    {
        "title": "HMSSimulcastLayer",
        "link": "/api-reference/ios/v2/enums/HMSSimulcastLayer",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/enums/HMSSimulcastLayer",
        "keywords": [],
        "content": "    ENUM     swift public enum HMSSimulcastLayer : UInt     Cases   high   swift case high = 0     mid   swift case mid = 1     low   swift case low = 2     none   swift case none = 3   "
    },
    {
        "title": "HMSTrackKind",
        "link": "/api-reference/ios/v2/enums/HMSTrackKind",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/enums/HMSTrackKind",
        "keywords": [],
        "content": "    ENUM     swift public enum HMSTrackKind : UInt     Cases   audio   swift case audio = 0     video   swift case video = 1   "
    },
    {
        "title": "HMSTrackSource",
        "link": "/api-reference/ios/v2/enums/HMSTrackSource",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/enums/HMSTrackSource",
        "keywords": [],
        "content": "    ENUM     swift public enum HMSTrackSource : UInt     Cases   regular   swift case regular = 0     screen   swift case screen = 1     plugin   swift case plugin = 2   "
    },
    {
        "title": "HMSTrackUpdate",
        "link": "/api-reference/ios/v2/enums/HMSTrackUpdate",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/enums/HMSTrackUpdate",
        "keywords": [],
        "content": "    ENUM     swift @objc public enum HMSTrackUpdate: Int, CustomStringConvertible     Cases   trackAdded   swift case trackAdded     trackRemoved   swift case trackRemoved     trackMuted   swift case trackMuted     trackUnmuted   swift case trackUnmuted     trackDescriptionChanged   swift case trackDescriptionChanged     trackDegraded   swift case trackDegraded     trackRestored   swift case trackRestored     Properties   description   swift public var description: String   "
    },
    {
        "title": "HMSVideoConnectionState",
        "link": "/api-reference/ios/v2/enums/HMSVideoConnectionState",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/enums/HMSVideoConnectionState",
        "keywords": [],
        "content": "    ENUM     swift public enum HMSVideoConnectionState : UInt     Cases   ready   swift case ready = 0     connecting   swift case connecting = 1     connected   swift case connected = 2     disconnected   swift case disconnected = 3     failed   swift case failed = 4   "
    },
    {
        "title": "HMSDataSource",
        "link": "/api-reference/ios/v2/extensions/HMSDataSource",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/extensions/HMSDataSource",
        "keywords": [],
        "content": "    EXTENSION     swift extension HMSDataSource: HMSUpdateListener     Methods   on(join:)   swift public func on(join room: HMSRoom)   initializes data source with current peers & tracks in the room   Parameter room: the room which the user joined    Parameters  Name  Description                         room  the room which the user joined    on(room:update:)   swift public func on(room: HMSRoom, update: HMSRoomUpdate)      Parameters  Name   Description                                                          room   the room which was joined                           update  the triggered update type. Should be used to perform different UI Actions    on(peer:update:)   swift public func on(peer: HMSPeer, update: HMSPeerUpdate)      Parameters  Name   Description                                                          peer   the peer who joined/left or was updated                    update  the triggered update type. Should be used to perform different UI Actions    on(track:update:for:)   swift public func on(track: HMSTrack, update: HMSTrackUpdate, for peer: HMSPeer)      Parameters  Name   Description                                           track  the track which was added, removed or updated       update  the triggered update type                 peer   the peer for which track was added, removed or updated    on(updated:)   swift public func on(updated speakers:  HMSSpeaker )      Parameters  Name    Description                  speakers  the list of speakers    on(error:)   swift public func on(error: HMSError)      Parameters  Name  Description                   error  the error that occurred    on(message:)   swift public func on(message: HMSMessage)      Parameters  Name   Description                          message  the received broadcast message    onReconnecting()   swift public func onReconnecting()     onReconnected()   swift public func onReconnected()   "
    },
    {
        "title": "HMSPeer",
        "link": "/api-reference/ios/v2/extensions/HMSPeer",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/extensions/HMSPeer",
        "keywords": [],
        "content": "    EXTENSION     swift extension HMSPeer     Methods   ==(_:_:)   swift public static func ==(lhs: HMSPeer, rhs: HMSPeer) -> Bool   "
    },
    {
        "title": "HMSSection",
        "link": "/api-reference/ios/v2/extensions/HMSSection",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/extensions/HMSSection",
        "keywords": [],
        "content": "    EXTENSION     swift extension HMSSection: Hashable     Methods   hash(into:)   swift public func hash(into hasher: inout Hasher)   Use identifier to find a unique section object   Parameter hasher: the hasher object    Parameters  Name   Description                hasher  the hasher object    ==(_:_:)   swift public static func == (lhs: HMSSection, rhs: HMSSection) -> Bool   resolves comparing of 2 section objects by differentiating based on unique identifier    Parameters:     lhs: the first section to compare     rhs: the second section to compare   Returns: a Boolean stating whether compared section objects are same or different    Parameters  Name  Description                        lhs  the first section to compare   rhs  the second section to compare  "
    },
    {
        "title": "HMSViewModel",
        "link": "/api-reference/ios/v2/extensions/HMSViewModel",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/extensions/HMSViewModel",
        "keywords": [],
        "content": "    EXTENSION     swift extension HMSViewModel: Hashable     Methods   hash(into:)   swift public func hash(into hasher: inout Hasher)   Use identifier to find a unique view model object   Parameter hasher: the hasher object    Parameters  Name   Description                hasher  the hasher object    ==(_:_:)   swift public static func == (lhs: HMSViewModel, rhs: HMSViewModel) -> Bool   resolves comparing of 2 view model objects by differentiating based on unique identifier    Parameters:     lhs: the first view model to compare     rhs: the second view model to compare   Returns: a Boolean stating whether compared view model objects are same or different    Parameters  Name  Description                          lhs  the first view model to compare   rhs  the second view model to compare  "
    },
    {
        "title": "iOS API reference",
        "link": "/api-reference/ios/v2/home/content",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/home/content",
        "keywords": [],
        "content": "    Protocols    HMSDataSourceDelegate (../protocols/HMSDataSourceDelegate)    HMSLogger (../protocols/HMSLogger)    HMSPreviewListener (../protocols/HMSPreviewListener)    HMSUpdateListener (../protocols/HMSUpdateListener)   Classes    HMSAudioSettings (../classes/HMSAudioSettings)    HMSAudioTrack (../classes/HMSAudioTrack)    HMSAudioTrackSettings (../classes/HMSAudioTrackSettings)    HMSBrowserRecordingState (../classes/HMSBrowserRecordingState)    HMSChangeTrackStateRequest (../classes/HMSChangeTrackStateRequest)    HMSCommonTrackSource (../classes/HMSCommonTrackSource)    HMSConfig (../classes/HMSConfig)    HMSDataSource (../classes/HMSDataSource)    HMSDevice (../classes/HMSDevice)    HMSError (../classes/HMSError)    HMSLocalAudioTrack (../classes/HMSLocalAudioTrack)    HMSLocalPeer (../classes/HMSLocalPeer)    HMSLocalVideoTrack (../classes/HMSLocalVideoTrack)    HMSMessage (../classes/HMSMessage)    HMSMessageRecipient (../classes/HMSMessageRecipient)    HMSNetworkQuality (../classes/HMSNetworkQuality)    HMSPeer (../classes/HMSPeer)    HMSPermissions (../classes/HMSPermissions)    HMSPublishSettings (../classes/HMSPublishSettings)    HMSRemoteAudioTrack (../classes/HMSRemoteAudioTrack)    HMSRemotePeer (../classes/HMSRemotePeer)    HMSRemoteVideoTrack (../classes/HMSRemoteVideoTrack)    HMSRemovedFromRoomNotification (../classes/HMSRemovedFromRoomNotification)    HMSRole (../classes/HMSRole)    HMSRoleChangeRequest (../classes/HMSRoleChangeRequest)    HMSRoom (../classes/HMSRoom)    HMSRTMPConfig (../classes/HMSRTMPConfig)    HMSRTMPStreamingState (../classes/HMSRTMPStreamingState)    HMSSDK (../classes/HMSSDK)    HMSSection (../classes/HMSSection)    HMSServerRecordingState (../classes/HMSServerRecordingState)    HMSSimulcastLayerSettingsPolicy (../classes/HMSSimulcastLayerSettingsPolicy)    HMSSimulcastSettingsPolicy (../classes/HMSSimulcastSettingsPolicy)    HMSSpeaker (../classes/HMSSpeaker)    HMSSubscribeDegradationPolicy (../classes/HMSSubscribeDegradationPolicy)    HMSSubscribeSettings (../classes/HMSSubscribeSettings)    HMSTrack (../classes/HMSTrack)    HMSTrackSettings (../classes/HMSTrackSettings)    HMSUtilities (../classes/HMSUtilities)    HMSUtility (../classes/HMSUtility)    HMSVideoSettings (../classes/HMSVideoSettings)    HMSVideoTrack (../classes/HMSVideoTrack)    HMSVideoTrackSettings (../classes/HMSVideoTrackSettings)    HMSVideoView (../classes/HMSVideoView)    HMSViewModel (../classes/HMSViewModel)   Enums    HMSAnalyticsEventLevel (../enums/HMSAnalyticsEventLevel)    HMSCameraFacing (../enums/HMSCameraFacing)    HMSCodec (../enums/HMSCodec)    HMSConnectionRole (../enums/HMSConnectionRole)    HMSErrorCode (../enums/HMSErrorCode)    HMSLogLevel (../enums/HMSLogLevel)    HMSMessageRecipientType (../enums/HMSMessageRecipientType)    HMSPeerUpdate (../enums/HMSPeerUpdate)    HMSRoomUpdate (../enums/HMSRoomUpdate)    HMSSimulcastLayer (../enums/HMSSimulcastLayer)    HMSTrackKind (../enums/HMSTrackKind)    HMSTrackSource (../enums/HMSTrackSource)    HMSTrackUpdate (../enums/HMSTrackUpdate)    HMSVideoConnectionState (../enums/HMSVideoConnectionState)   Extensions    HMSDataSource (../extensions/HMSDataSource)    HMSPeer (../extensions/HMSPeer)    HMSSection (../extensions/HMSSection)    HMSViewModel (../extensions/HMSViewModel)   Structs    HMSPerformanceStats (../structs/HMSPerformanceStats)   Typealiases    HMSSection.SectionIdentifier (../typealiases/HMSSection.SectionIdentifier)    HMSVideoResolution (../typealiases/HMSVideoResolution) "
    },
    {
        "title": "HMSDataSourceDelegate",
        "link": "/api-reference/ios/v2/protocols/HMSDataSourceDelegate",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/protocols/HMSDataSourceDelegate",
        "keywords": [],
        "content": "    PROTOCOL     swift public protocol HMSDataSourceDelegate: AnyObject     Methods   didUpdate( _:)   swift func didUpdate(_ model: HMSViewModel?)     didUpdate( _:)   swift func didUpdate(_ speakers:  HMSViewModel )   "
    },
    {
        "title": "HMSLogger",
        "link": "/api-reference/ios/v2/protocols/HMSLogger",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/protocols/HMSLogger",
        "keywords": [],
        "content": "    PROTOCOL     swift @objc public protocol HMSLogger: AnyObject     Methods   log(_:_:)   swift @objc(logMessage:level:) func log(_ message: String, _ level: HMSLogLevel)   "
    },
    {
        "title": "HMSPreviewListener",
        "link": "/api-reference/ios/v2/protocols/HMSPreviewListener",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/protocols/HMSPreviewListener",
        "keywords": [],
        "content": "    PROTOCOL     swift @objc public protocol HMSPreviewListener: AnyObject     Methods   onPreview(room:localTracks:)   swift @objc(onPreview:localTracks:) func onPreview(room: HMSRoom, localTracks:  HMSTrack )     on(error:)   swift @objc(onError:) func on(error: HMSError)   "
    },
    {
        "title": "HMSUpdateListener",
        "link": "/api-reference/ios/v2/protocols/HMSUpdateListener",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/protocols/HMSUpdateListener",
        "keywords": [],
        "content": "    PROTOCOL     swift @objc public protocol HMSUpdateListener: AnyObject   100ms SDK provides callbacks to the client app about any change or update happening in the room after a user has joined by implementing HMSUpdateListener. These updates can be used to render the video on screen or to display other info regarding the room.   Methods   on(join:)   swift @objc(onJoin:) func on(join room: HMSRoom)   This will be called on a successful JOIN of the room by the user This is the point where applications can stop showing its loading state   Parameter room: the room which was joined    Parameters  Name  Description                     room  the room which was joined    on(room:update:)   swift @objc(onRoom:update:) func on(room: HMSRoom, update: HMSRoomUpdate)   This is called when there is a change in any property of the Room   Parameters:     room: the room which was joined     update: the triggered update type. Should be used to perform different UI Actions    Parameters  Name   Description                                                          room   the room which was joined                           update  the triggered update type. Should be used to perform different UI Actions    on(peer:update:)   swift @objc(onPeer:update:) func on(peer: HMSPeer, update: HMSPeerUpdate)   This will be called whenever there is an update on an existing peer or a new peer got added/existing peer is removed. This callback can be used to keep a track of all the peers in the room   Parameters:     peer: the peer who joined/left or was updated     update: the triggered update type. Should be used to perform different UI Actions    Parameters  Name   Description                                                          peer   the peer who joined/left or was updated                    update  the triggered update type. Should be used to perform different UI Actions    on(track:update:for:)   swift @objc(onTrack:update:peer:) func on(track: HMSTrack, update: HMSTrackUpdate, for peer: HMSPeer)   This is called when there are updates on an existing track or a new track got added/existing track is removed This callback can be used to render the video on screen whenever a track gets added   Parameters:     track: the track which was added, removed or updated     update: the triggered update type     peer: the peer for which track was added, removed or updated    Parameters  Name   Description                                           track  the track which was added, removed or updated       update  the triggered update type                 peer   the peer for which track was added, removed or updated    on(error:)   swift @objc(onError:) func on(error: HMSError)   This will be called when there is an error in the system and SDK has already retried to fix the error   Parameter error: the error that occurred    Parameters  Name  Description                   error  the error that occurred    on(message:)   swift @objc(onMessage:) func on(message: HMSMessage)   This is called when there is a new broadcast message from any other peer in the room This can be used to implement chat is the room   Parameter message: the received broadcast message    Parameters  Name   Description                          message  the received broadcast message    on(roleChangeRequest:)   swift @objc(roleChangeRequest:) optional func on(roleChangeRequest: HMSRoleChangeRequest)   This is called when a role change request arrives   Parameter roleChangeRequest: the request for role change info    Parameters  Name        Description                             roleChangeRequest  the request for role change info    on(changeTrackStateRequest:)   swift @objc(changeTrackStateRequest:) optional func on(changeTrackStateRequest: HMSChangeTrackStateRequest)   This is called when a change track state request arrives   Parameter changeTrackStateRequest: the request for changing track state    Parameters  Name           Description                                  changeTrackStateRequest  the request for changing track state    on(removedFromRoom:)   swift @objc(removedFromRoom:) optional func on(removedFromRoom notification: HMSRemovedFromRoomNotification)   This is called when someone removes the local peer for the current room   Parameter notification: the notification containing reason for removing and the initiating peer    Parameters  Name      Description                                                          notification  the notification containing reason for removing and the initiating peer    on(updated:)   swift @objc(onUpdatedSpeakers:) func on(updated speakers:  HMSSpeaker )   This is called every 1 second with list of active speakers   A HMSSpeaker object contains    peer: the peer who is speaking   track: the track which is emitting audio   level: a number within range 1-100 indicating the audio volume   A peer who is not present in the list indicates that the peer is not speaking   This can be used to highlight currently speaking peers in the room   Parameter speakers: the list of speakers    Parameters  Name    Description                  speakers  the list of speakers    onReconnecting()   swift @objc func onReconnecting()   This is called when SDK detects a network issue and is trying to recover   onReconnected()   swift @objc func onReconnected()   This is called when SDK successfully recovered from a network issue a network issue "
    },
    {
        "title": "HMSPerformanceStats",
        "link": "/api-reference/ios/v2/structs/HMSPerformanceStats",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/structs/HMSPerformanceStats",
        "keywords": [],
        "content": "    STRUCT     swift public struct HMSPerformanceStats     Properties   memory   swift public var memory: Double     battery   swift public var battery: Double     Methods   init   swift public init()     init   swift public init(cpu: Double, memory: Double, battery: Double)   "
    },
    {
        "title": "HMSSection.SectionIdentifier",
        "link": "/api-reference/ios/v2/typealiases/HMSSection.SectionIdentifier",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/typealiases/HMSSection.SectionIdentifier",
        "keywords": [],
        "content": "    TYPEALIAS     swift public typealias SectionIdentifier = String   "
    },
    {
        "title": "HMSVideoResolution",
        "link": "/api-reference/ios/v2/typealiases/HMSVideoResolution",
        "platformName": "iOS",
        "objectID": "/api-reference/ios/v2/typealiases/HMSVideoResolution",
        "keywords": [],
        "content": "    TYPEALIAS     swift public typealias HMSVideoResolution = CGSize   "
    },
    {
        "title": "HMSReactiveStore",
        "link": "/api-reference/javascript/v2/classes/HMSReactiveStore",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/classes/HMSReactiveStore",
        "keywords": [],
        "content": "  Constructors   constructor •   new HMSReactiveStore  ( hmsStore? , hmsActions? , hmsNotifications? )    Parameters  Name         Type                                  :      :                   hmsStore?        IHMSStore  (/api-reference/javascript/v2/interfaces/IHMSStore)    hmsActions?       HMSActions  (/api-reference/javascript/v2/interfaces/HMSActions)   hmsNotifications?   HMSNotifications                            Methods   getActions ▸   getActions  ():   HMSActions  (/api-reference/javascript/v2/interfaces/HMSActions) Any action which may modify the store or may need to talk to the SDK will happen through the IHMSActions instance returned by this    Returns   HMSActions  (/api-reference/javascript/v2/interfaces/HMSActions)     getHMSActions ▸   getHMSActions  ():   HMSActions  (/api-reference/javascript/v2/interfaces/HMSActions) Any action which may modify the store or may need to talk to the SDK will happen through the IHMSActions instance returned by this    Deprecated    use getActions    Returns   HMSActions  (/api-reference/javascript/v2/interfaces/HMSActions)     getNotifications ▸   getNotifications  ():   HMSNotifications  (/api-reference/javascript/v2/interfaces/HMSNotifications) This return notification handler function to which you can pass your callback to receive notifications like peer joined, peer left, etc. to show in your UI or use for analytics    Returns   HMSNotifications  (/api-reference/javascript/v2/interfaces/HMSNotifications)     getStats ▸   getStats  ():   HMSStatsStoreWrapper  (/api-reference/javascript/v2/interfaces/HMSStatsStoreWrapper)    Returns   HMSStatsStoreWrapper  (/api-reference/javascript/v2/interfaces/HMSStatsStoreWrapper)     getStore ▸   getStore  ():   HMSStoreWrapper  (/api-reference/javascript/v2/interfaces/HMSStoreWrapper) A reactive store which has a subscribe method you can use in combination with selectors to subscribe to a subset of the store. The store serves as a single source of truth for all data related to the corresponding HMS Room.    Returns   HMSStoreWrapper  (/api-reference/javascript/v2/interfaces/HMSStoreWrapper)     triggerOnSubscribe ▸   triggerOnSubscribe  (): void  By default store.subscribe does not call the handler with the current state at time of subscription, this behaviour can be modified by calling this function. What it means is that instead of calling the handler only for changes which happen post subscription we'll also call it exactly once at the time of subscription with the current state. This behaviour is similar to that of BehaviourSubject in rxjs. This will be an irreversible change Note: you don't need this if you're using our react hooks, it takes care of this requirement.    Returns  void "
    },
    {
        "title": "HMSAudioPluginType",
        "link": "/api-reference/javascript/v2/enums/HMSAudioPluginType",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/enums/HMSAudioPluginType",
        "keywords": [],
        "content": " Specifies the type of the plugin a transforming plugin will get an output audio node to give the resulting transformation. While an analyzing plugin will only be passed the input node. For analyse plugins, you can return the source node passed to plugin.processTrack to not modify anything   Enumeration Members   ANALYZE •   ANALYZE   = \"ANALYZE\"     TRANSFORM •   TRANSFORM   = \"TRANSFORM\" "
    },
    {
        "title": "HMSLogLevel",
        "link": "/api-reference/javascript/v2/enums/HMSLogLevel",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/enums/HMSLogLevel",
        "keywords": [],
        "content": "  Enumeration Members   DEBUG •   DEBUG   = 1     ERROR •   ERROR   = 6     INFO •   INFO   = 2     NONE •   NONE   = 7     TIME •   TIME   = 4     TIMEEND •   TIMEEND   = 5     VERBOSE •   VERBOSE   = 0     WARN •   WARN   = 3 "
    },
    {
        "title": "HMSNotificationSeverity",
        "link": "/api-reference/javascript/v2/enums/HMSNotificationSeverity",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/enums/HMSNotificationSeverity",
        "keywords": [],
        "content": "  Enumeration Members   ERROR •   ERROR   = \"error\"     INFO •   INFO   = \"info\" "
    },
    {
        "title": "HMSNotificationTypes",
        "link": "/api-reference/javascript/v2/enums/HMSNotificationTypes",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/enums/HMSNotificationTypes",
        "keywords": [],
        "content": "  Enumeration Members   CHANGE_MULTI_TRACK_STATE_REQUEST •   CHANGE_MULTI_TRACK_STATE_REQUEST   = \"CHANGE_MULTI_TRACK_STATE_REQUEST\"     CHANGE_TRACK_STATE_REQUEST •   CHANGE_TRACK_STATE_REQUEST   = \"CHANGE_TRACK_STATE_REQUEST\"     DEVICE_CHANGE_UPDATE •   DEVICE_CHANGE_UPDATE   = \"DEVICE_CHANGE_UPDATE\"     ERROR •   ERROR   = \"ERROR\"     METADATA_UPDATED •   METADATA_UPDATED   = \"METADATA_UPDATED\"     NAME_UPDATED •   NAME_UPDATED   = \"NAME_UPDATED\"     NEW_MESSAGE •   NEW_MESSAGE   = \"NEW_MESSAGE\"     PEER_JOINED •   PEER_JOINED   = \"PEER_JOINED\"     PEER_LEFT •   PEER_LEFT   = \"PEER_LEFT\"     PEER_LIST •   PEER_LIST   = \"PEER_LIST\"     PLAYLIST_TRACK_ENDED •   PLAYLIST_TRACK_ENDED   = \"PLAYLIST_TRACK_ENDED\"     RECONNECTED •   RECONNECTED   = \"RECONNECTED\"     RECONNECTING •   RECONNECTING   = \"RECONNECTING\"     REMOVED_FROM_ROOM •   REMOVED_FROM_ROOM   = \"REMOVED_FROM_ROOM\"     ROLE_UPDATED •   ROLE_UPDATED   = \"ROLE_UPDATED\"     ROOM_ENDED •   ROOM_ENDED   = \"ROOM_ENDED\"     TRACK_ADDED •   TRACK_ADDED   = \"TRACK_ADDED\"     TRACK_DEGRADED •   TRACK_DEGRADED   = \"TRACK_DEGRADED\"     TRACK_DESCRIPTION_CHANGED •   TRACK_DESCRIPTION_CHANGED   = \"TRACK_DESCRIPTION_CHANGED\"     TRACK_MUTED •   TRACK_MUTED   = \"TRACK_MUTED\"     TRACK_REMOVED •   TRACK_REMOVED   = \"TRACK_REMOVED\"     TRACK_RESTORED •   TRACK_RESTORED   = \"TRACK_RESTORED\"     TRACK_UNMUTED •   TRACK_UNMUTED   = \"TRACK_UNMUTED\" "
    },
    {
        "title": "HMSPlaylistType",
        "link": "/api-reference/javascript/v2/enums/HMSPlaylistType",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/enums/HMSPlaylistType",
        "keywords": [],
        "content": "  Enumeration Members   audio •   audio   = \"audio\"     video •   video   = \"video\" "
    },
    {
        "title": "HMSRoomState",
        "link": "/api-reference/javascript/v2/enums/HMSRoomState",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/enums/HMSRoomState",
        "keywords": [],
        "content": " Check out internal-docs/RoomStateFlow.tldr for flow of room state View it by  Installing tldraw for VSCode(https://marketplace.visualstudio.com/items?itemName=tldraw-org.tldraw-vscode), or  Open the file in https://www.tldraw.com/   Enumeration Members   Connected •   Connected   = \"Connected\"     Connecting •   Connecting   = \"Connecting\"     Disconnected •   Disconnected   = \"Disconnected\"     Disconnecting •   Disconnecting   = \"Disconnecting\"     Failed •   Failed   = \"Failed\"     Preview •   Preview   = \"Preview\"     Reconnecting •   Reconnecting   = \"Reconnecting\" "
    },
    {
        "title": "HMSSimulcastLayer",
        "link": "/api-reference/javascript/v2/enums/HMSSimulcastLayer",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/enums/HMSSimulcastLayer",
        "keywords": [],
        "content": "  Enumeration Members   HIGH •   HIGH   = \"high\"     LOW •   LOW   = \"low\"     MEDIUM •   MEDIUM   = \"medium\"     NONE •   NONE   = \"none\" "
    },
    {
        "title": "HMSVideoPluginCanvasContextType",
        "link": "/api-reference/javascript/v2/enums/HMSVideoPluginCanvasContextType",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/enums/HMSVideoPluginCanvasContextType",
        "keywords": [],
        "content": "  Enumeration Members   2D •   2D   = \"2d\"     WEBGL •   WEBGL   = \"webgl\"     WEBGL2 •   WEBGL2   = \"webgl2\" "
    },
    {
        "title": "HMSVideoPluginType",
        "link": "/api-reference/javascript/v2/enums/HMSVideoPluginType",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/enums/HMSVideoPluginType",
        "keywords": [],
        "content": " Specifies the type of the plugin a transforming plugin will get an output canvas to give the resulting transformation. While an analyzing plugin will only be passed the input canvas.   Enumeration Members   ANALYZE •   ANALYZE   = \"ANALYZE\"     TRANSFORM •   TRANSFORM   = \"TRANSFORM\" "
    },
    {
        "title": "Web SDK API Reference",
        "link": "/api-reference/javascript/v2/home/content",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/home/content",
        "keywords": [],
        "content": "  Enumerations   HMSAudioPluginType (/api-reference/javascript/v2/enums/HMSAudioPluginType)   HMSLogLevel (/api-reference/javascript/v2/enums/HMSLogLevel)   HMSNotificationSeverity (/api-reference/javascript/v2/enums/HMSNotificationSeverity)   HMSNotificationTypes (/api-reference/javascript/v2/enums/HMSNotificationTypes)   HMSPlaylistType (/api-reference/javascript/v2/enums/HMSPlaylistType)   HMSRoomState (/api-reference/javascript/v2/enums/HMSRoomState)   HMSSimulcastLayer (/api-reference/javascript/v2/enums/HMSSimulcastLayer)   HMSVideoPluginCanvasContextType (/api-reference/javascript/v2/enums/HMSVideoPluginCanvasContextType)   HMSVideoPluginType (/api-reference/javascript/v2/enums/HMSVideoPluginType)   Classes   HMSReactiveStore (/api-reference/javascript/v2/classes/HMSReactiveStore)   Core Interfaces   HMSActions (/api-reference/javascript/v2/interfaces/HMSActions)   HMSNotifications (/api-reference/javascript/v2/interfaces/HMSNotifications)   HMSStoreWrapper (/api-reference/javascript/v2/interfaces/HMSStoreWrapper)   Other Interfaces   DeviceMap (/api-reference/javascript/v2/interfaces/DeviceMap)   HLSConfig (/api-reference/javascript/v2/interfaces/HLSConfig)   HLSMeetingURLVariant (/api-reference/javascript/v2/interfaces/HLSMeetingURLVariant)   HLSVariant (/api-reference/javascript/v2/interfaces/HLSVariant)   HMSAudioTrack (/api-reference/javascript/v2/interfaces/HMSAudioTrack)   HMSAudioTrackSettings (/api-reference/javascript/v2/interfaces/HMSAudioTrackSettings)   HMSChangeMultiTrackStateParams (/api-reference/javascript/v2/interfaces/HMSChangeMultiTrackStateParams)   HMSChangeMultiTrackStateRequest (/api-reference/javascript/v2/interfaces/HMSChangeMultiTrackStateRequest)   HMSChangeMultiTrackStateRequestNotification (/api-reference/javascript/v2/interfaces/HMSChangeMultiTrackStateRequestNotification)   HMSChangeTrackStateRequest (/api-reference/javascript/v2/interfaces/HMSChangeTrackStateRequest)   HMSChangeTrackStateRequestNotification (/api-reference/javascript/v2/interfaces/HMSChangeTrackStateRequestNotification)   HMSConfig (/api-reference/javascript/v2/interfaces/HMSConfig)   HMSConfigInitialSettings (/api-reference/javascript/v2/interfaces/HMSConfigInitialSettings)   HMSDeviceChangeEvent (/api-reference/javascript/v2/interfaces/HMSDeviceChangeEvent)   HMSDeviceChangeEventNotification (/api-reference/javascript/v2/interfaces/HMSDeviceChangeEventNotification)   HMSException (/api-reference/javascript/v2/interfaces/HMSException)   HMSExceptionNotification (/api-reference/javascript/v2/interfaces/HMSExceptionNotification)   HMSHLS (/api-reference/javascript/v2/interfaces/HMSHLS)   HMSLeaveRoomRequest (/api-reference/javascript/v2/interfaces/HMSLeaveRoomRequest)   HMSLeaveRoomRequestNotification (/api-reference/javascript/v2/interfaces/HMSLeaveRoomRequestNotification)   HMSLocalTrackStats (/api-reference/javascript/v2/interfaces/HMSLocalTrackStats)   HMSMediaSettings (/api-reference/javascript/v2/interfaces/HMSMediaSettings)   HMSMessage (/api-reference/javascript/v2/interfaces/HMSMessage)   HMSMessageNotification (/api-reference/javascript/v2/interfaces/HMSMessageNotification)   HMSPeer (/api-reference/javascript/v2/interfaces/HMSPeer)   HMSPeerListNotification (/api-reference/javascript/v2/interfaces/HMSPeerListNotification)   HMSPeerNotification (/api-reference/javascript/v2/interfaces/HMSPeerNotification)   HMSPeerStats (/api-reference/javascript/v2/interfaces/HMSPeerStats)   HMSPeerWithMuteStatus (/api-reference/javascript/v2/interfaces/HMSPeerWithMuteStatus)   HMSPlaylist (/api-reference/javascript/v2/interfaces/HMSPlaylist)   HMSPlaylistItem (/api-reference/javascript/v2/interfaces/HMSPlaylistItem)   HMSPlaylistItemNotification (/api-reference/javascript/v2/interfaces/HMSPlaylistItemNotification)   HMSPlaylistSelection (/api-reference/javascript/v2/interfaces/HMSPlaylistSelection)   HMSPlaylistSelector (/api-reference/javascript/v2/interfaces/HMSPlaylistSelector)   HMSPublishAllowed (/api-reference/javascript/v2/interfaces/HMSPublishAllowed)   HMSRTMP (/api-reference/javascript/v2/interfaces/HMSRTMP)   HMSReconnectionNotification (/api-reference/javascript/v2/interfaces/HMSReconnectionNotification)   HMSRecording (/api-reference/javascript/v2/interfaces/HMSRecording)   HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats)   HMSRole (/api-reference/javascript/v2/interfaces/HMSRole)   HMSRoleChangeRequest (/api-reference/javascript/v2/interfaces/HMSRoleChangeRequest)   HMSRoleChangeStoreRequest (/api-reference/javascript/v2/interfaces/HMSRoleChangeStoreRequest)   HMSRoom (/api-reference/javascript/v2/interfaces/HMSRoom)   HMSScreenAudioTrack (/api-reference/javascript/v2/interfaces/HMSScreenAudioTrack)   HMSScreenShareConfig (/api-reference/javascript/v2/interfaces/HMSScreenShareConfig)   HMSScreenVideoTrack (/api-reference/javascript/v2/interfaces/HMSScreenVideoTrack)   HMSSimulcastLayerDefinition (/api-reference/javascript/v2/interfaces/HMSSimulcastLayerDefinition)   HMSSpeaker (/api-reference/javascript/v2/interfaces/HMSSpeaker)   HMSStatsStore (/api-reference/javascript/v2/interfaces/HMSStatsStore)   HMSStatsStoreWrapper (/api-reference/javascript/v2/interfaces/HMSStatsStoreWrapper)   HMSStore (/api-reference/javascript/v2/interfaces/HMSStore)   HMSTrackNotification (/api-reference/javascript/v2/interfaces/HMSTrackNotification)   HMSTrackStats (/api-reference/javascript/v2/interfaces/HMSTrackStats)   HMSVideoTrack (/api-reference/javascript/v2/interfaces/HMSVideoTrack)   HMSVideoTrackSettings (/api-reference/javascript/v2/interfaces/HMSVideoTrackSettings)   IHMSPlaylistActions (/api-reference/javascript/v2/interfaces/IHMSPlaylistActions)   IHMSStatsStore (/api-reference/javascript/v2/interfaces/IHMSStatsStore)   IHMSStore (/api-reference/javascript/v2/interfaces/IHMSStore)   RTMPRecordingConfig (/api-reference/javascript/v2/interfaces/RTMPRecordingConfig)   ScreenCaptureHandle (/api-reference/javascript/v2/interfaces/ScreenCaptureHandle)   References   IHMSNotifications Renames and re-exports  HMSNotifications (/api-reference/javascript/v2/interfaces/HMSNotifications)   Type Aliases   HMSMessageID Ƭ   HMSMessageID  : string     HMSNotification Ƭ   HMSNotification  :   HMSPeerNotification  (/api-reference/javascript/v2/interfaces/HMSPeerNotification)    HMSPeerListNotification  (/api-reference/javascript/v2/interfaces/HMSPeerListNotification)    HMSTrackNotification  (/api-reference/javascript/v2/interfaces/HMSTrackNotification)    HMSMessageNotification  (/api-reference/javascript/v2/interfaces/HMSMessageNotification)    HMSExceptionNotification  (/api-reference/javascript/v2/interfaces/HMSExceptionNotification)    HMSChangeTrackStateRequestNotification  (/api-reference/javascript/v2/interfaces/HMSChangeTrackStateRequestNotification)    HMSChangeMultiTrackStateRequestNotification  (/api-reference/javascript/v2/interfaces/HMSChangeMultiTrackStateRequestNotification)    HMSLeaveRoomRequestNotification  (/api-reference/javascript/v2/interfaces/HMSLeaveRoomRequestNotification)    HMSDeviceChangeEventNotification  (/api-reference/javascript/v2/interfaces/HMSDeviceChangeEventNotification)    HMSReconnectionNotification  (/api-reference/javascript/v2/interfaces/HMSReconnectionNotification)    HMSPlaylistItemNotification  (/api-reference/javascript/v2/interfaces/HMSPlaylistItemNotification)< any >     HMSNotificationCallback Ƭ   HMSNotificationCallback  < T >: ( notification :   HMSNotificationInCallback  (/api-reference/javascript/v2/home/content hmsnotificationincallback)< T >) => void    Type parameters  Name  Type                                                  :   :                            T   extends   HMSNotificationTypeParam  (/api-reference/javascript/v2/modules hmsnotificationtypeparam)     Type declaration ▸ ( notification ): void     Parameters  Name       Type                                                  :     :                            notification    HMSNotificationInCallback  (/api-reference/javascript/v2/modules hmsnotificationincallback)< T >     Returns  void     HMSNotificationInCallback Ƭ   HMSNotificationInCallback  < T >: T extends   HMSNotificationTypes  (/api-reference/javascript/v2/enums/HMSNotificationTypes)   ? MappedNotifications < T >  number   : T extends   HMSNotificationTypes  (/api-reference/javascript/v2/enums/HMSNotificationTypes) ?   HMSNotificationMapping  (/api-reference/javascript/v2/modules hmsnotificationmapping)< T > :   HMSNotification  (/api-reference/javascript/v2/modules hmsnotification)    Type parameters  Name  Type                                                  :   :                            T   extends   HMSNotificationTypeParam  (/api-reference/javascript/v2/modules hmsnotificationtypeparam)      HMSNotificationMapping Ƭ   HMSNotificationMapping  < T , C >:   CHANGE_MULTI_TRACK_STATE_REQUEST :   HMSChangeMultiTrackStateRequestNotification  (/api-reference/javascript/v2/interfaces/HMSChangeMultiTrackStateRequestNotification) ; CHANGE_TRACK_STATE_REQUEST :   HMSChangeTrackStateRequestNotification  (/api-reference/javascript/v2/interfaces/HMSChangeTrackStateRequestNotification) ; DEVICE_CHANGE_UPDATE :   HMSDeviceChangeEventNotification  (/api-reference/javascript/v2/interfaces/HMSDeviceChangeEventNotification) ; ERROR :   HMSExceptionNotification  (/api-reference/javascript/v2/interfaces/HMSExceptionNotification) ; METADATA_UPDATED :   HMSPeerNotification  (/api-reference/javascript/v2/interfaces/HMSPeerNotification) ; NAME_UPDATED :   HMSPeerNotification  (/api-reference/javascript/v2/interfaces/HMSPeerNotification) ; NEW_MESSAGE :   HMSMessageNotification  (/api-reference/javascript/v2/interfaces/HMSMessageNotification) ; PEER_JOINED :   HMSPeerNotification  (/api-reference/javascript/v2/interfaces/HMSPeerNotification) ; PEER_LEFT :   HMSPeerNotification  (/api-reference/javascript/v2/interfaces/HMSPeerNotification) ; PEER_LIST :   HMSPeerListNotification  (/api-reference/javascript/v2/interfaces/HMSPeerListNotification) ; PLAYLIST_TRACK_ENDED :   HMSPlaylistItemNotification  (/api-reference/javascript/v2/interfaces/HMSPlaylistItemNotification)< C > ; RECONNECTED :   HMSReconnectionNotification  (/api-reference/javascript/v2/interfaces/HMSReconnectionNotification) ; RECONNECTING :   HMSReconnectionNotification  (/api-reference/javascript/v2/interfaces/HMSReconnectionNotification) ; REMOVED_FROM_ROOM :   HMSLeaveRoomRequestNotification  (/api-reference/javascript/v2/interfaces/HMSLeaveRoomRequestNotification) ; ROLE_UPDATED :   HMSPeerNotification  (/api-reference/javascript/v2/interfaces/HMSPeerNotification) ; ROOM_ENDED :   HMSLeaveRoomRequestNotification  (/api-reference/javascript/v2/interfaces/HMSLeaveRoomRequestNotification) ; TRACK_ADDED :   HMSTrackNotification  (/api-reference/javascript/v2/interfaces/HMSTrackNotification) ; TRACK_DEGRADED :   HMSTrackNotification  (/api-reference/javascript/v2/interfaces/HMSTrackNotification) ; TRACK_DESCRIPTION_CHANGED :   HMSTrackNotification  (/api-reference/javascript/v2/interfaces/HMSTrackNotification) ; TRACK_MUTED :   HMSTrackNotification  (/api-reference/javascript/v2/interfaces/HMSTrackNotification) ; TRACK_REMOVED :   HMSTrackNotification  (/api-reference/javascript/v2/interfaces/HMSTrackNotification) ; TRACK_RESTORED :   HMSTrackNotification  (/api-reference/javascript/v2/interfaces/HMSTrackNotification) ; TRACK_UNMUTED :   HMSTrackNotification  (/api-reference/javascript/v2/interfaces/HMSTrackNotification)    T      Type parameters  Name  Type                                             :   :                         T   extends   HMSNotificationTypes  (/api-reference/javascript/v2/enums/HMSNotificationTypes)   C   any                                                HMSNotificationTypeParam Ƭ   HMSNotificationTypeParam  :   HMSNotificationTypes  (/api-reference/javascript/v2/enums/HMSNotificationTypes)    HMSNotificationTypes  (/api-reference/javascript/v2/enums/HMSNotificationTypes)     undefined     HMSPeerID Ƭ   HMSPeerID  : string     HMSPreferredSimulcastLayer Ƭ   HMSPreferredSimulcastLayer  : Exclude <  HMSSimulcastLayer  (/api-reference/javascript/v2/enums/HMSSimulcastLayer),   NONE  (/api-reference/javascript/v2/enums/HMSSimulcastLayer none) >     HMSRoleName Ƭ   HMSRoleName  : string     HMSRoomID Ƭ   HMSRoomID  : string     HMSTrack Ƭ   HMSTrack  :   HMSVideoTrack  (/api-reference/javascript/v2/interfaces/HMSVideoTrack)    HMSAudioTrack  (/api-reference/javascript/v2/interfaces/HMSAudioTrack)    HMSScreenVideoTrack  (/api-reference/javascript/v2/interfaces/HMSScreenVideoTrack)    HMSScreenAudioTrack  (/api-reference/javascript/v2/interfaces/HMSScreenAudioTrack)     HMSTrackDisplaySurface Ƭ   HMSTrackDisplaySurface  : \"application\"   \"browser\"   \"selfBrowser\"   \"monitor\"   \"window\"  Use this to identify what is being screenshared, not all browsers will support everything. application  all windows of an application are shared window  a particular window is being shared monitor  full screen share of a monitor display browser  a browser tab is shared selfBrowser  the current browser tab is being shared     HMSTrackFacingMode Ƭ   HMSTrackFacingMode  : \"user\"   \"environment\"   \"left\"   \"right\"     HMSTrackID Ƭ   HMSTrackID  : string     HMSTrackSource Ƭ   HMSTrackSource  : \"regular\"   \"screen\"   \"plugin\"   \"audioplaylist\"   \"videoplaylist\"   string     HMSTrackType Ƭ   HMSTrackType  : \"audio\"   \"video\"   Variables   parsedUserAgent • Const   parsedUserAgent  : UAParserInstance     selectAudioPlaylist • Const   selectAudioPlaylist  :   HMSPlaylistSelector  (/api-reference/javascript/v2/interfaces/HMSPlaylistSelector)     selectHMSStats • Const   selectHMSStats  : Object    Type declaration  Name              Type                                                                                                                                                                                                                      :         :                                                                                                             availablePublishBitrate     OutputSelector <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore), undefined   number , ( res : undefined     HMSPeerStats  (/api-reference/javascript/v2/interfaces/HMSPeerStats)) => undefined   number >                                                                                                 availableSubscribeBitrate    OutputSelector <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore), undefined   number , ( res : undefined     HMSPeerStats  (/api-reference/javascript/v2/interfaces/HMSPeerStats)) => undefined   number >                                                                                                 jitter             OutputSelector <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore), undefined   number , ( res : undefined     HMSPeerStats  (/api-reference/javascript/v2/interfaces/HMSPeerStats)) => undefined   number >                                                                                                 localAudioTrackStats      OutputSelector <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore), undefined     HMSTrackStats  (/api-reference/javascript/v2/interfaces/HMSTrackStats), ( res1 : Record < string , undefined     HMSTrackStats  (/api-reference/javascript/v2/interfaces/HMSTrackStats)   >, res2 : undefined   string ) => undefined     HMSTrackStats  (/api-reference/javascript/v2/interfaces/HMSTrackStats) >   localAudioTrackStatsByID    ( id? : string ) => StoreSelector <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore), undefined     HMSTrackStats  (/api-reference/javascript/v2/interfaces/HMSTrackStats) >                                                                                                                    localPeerStats         OutputSelector <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore), undefined     HMSPeerStats  (/api-reference/javascript/v2/interfaces/HMSPeerStats), ( res1 : Record < string , undefined     HMSPeerStats  (/api-reference/javascript/v2/interfaces/HMSPeerStats) >, res2 : string ) => undefined     HMSPeerStats  (/api-reference/javascript/v2/interfaces/HMSPeerStats) >               localVideoTrackStats      OutputSelector <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore), undefined     HMSTrackStats  (/api-reference/javascript/v2/interfaces/HMSTrackStats), ( res1 : Record < string , undefined     HMSTrackStats  (/api-reference/javascript/v2/interfaces/HMSTrackStats)   >, res2 : undefined   string ) => undefined     HMSTrackStats  (/api-reference/javascript/v2/interfaces/HMSTrackStats) >   localVideoTrackStatsByID    ( id? : string ) => StoreSelector <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore), undefined     HMSTrackStats  (/api-reference/javascript/v2/interfaces/HMSTrackStats)   >                                                                                                                   localVideoTrackStatsByLayer  ( layer? :   HMSPreferredSimulcastLayer  (/api-reference/javascript/v2/modules hmspreferredsimulcastlayer)) => ( id? : string ) => StoreSelector <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore), undefined     HMSTrackStats  (/api-reference/javascript/v2/interfaces/HMSTrackStats) >                                                             packetsLost           OutputSelector <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore), undefined   number , ( res : undefined     HMSPeerStats  (/api-reference/javascript/v2/interfaces/HMSPeerStats)) => undefined   number >                                                                                                 peerStatsByID         ( id? : string ) => StoreSelector <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore), undefined     HMSPeerStats  (/api-reference/javascript/v2/interfaces/HMSPeerStats) >                                                                                                                     publishBitrate         OutputSelector <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore), undefined   number , ( res : undefined     HMSPeerStats  (/api-reference/javascript/v2/interfaces/HMSPeerStats)) => undefined   number >                                                                                                 subscribeBitrate        OutputSelector <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore), undefined   number , ( res : undefined     HMSPeerStats  (/api-reference/javascript/v2/interfaces/HMSPeerStats)) => undefined   number >                                                                                                 totalBytesReceived       OutputSelector <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore), undefined   number , ( res : undefined     HMSPeerStats  (/api-reference/javascript/v2/interfaces/HMSPeerStats)) => undefined   number >                                                                                                 totalBytesSent         OutputSelector <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore), undefined   number , ( res : undefined     HMSPeerStats  (/api-reference/javascript/v2/interfaces/HMSPeerStats)) => undefined   number >                                                                                                 trackStatsByID         ( id? : string ) => StoreSelector <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore), undefined     HMSTrackStats  (/api-reference/javascript/v2/interfaces/HMSTrackStats) >                                                                                                                       selectVideoPlaylist • Const   selectVideoPlaylist  :   HMSPlaylistSelector  (/api-reference/javascript/v2/interfaces/HMSPlaylistSelector)     simulcastMapping • Const   simulcastMapping  : Object    Type declaration  Name  Type                                      :   :                      f     HMSSimulcastLayer  (/api-reference/javascript/v2/enums/HMSSimulcastLayer)   h     HMSSimulcastLayer  (/api-reference/javascript/v2/enums/HMSSimulcastLayer)   q     HMSSimulcastLayer  (/api-reference/javascript/v2/enums/HMSSimulcastLayer)    Functions   createDefaultStatsStore ▸   createDefaultStatsStore  ():   HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore)    Returns   HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore)     selectAppData ▸   selectAppData  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), any > Select a particular key from ui app data by passed in key. if key is not passed, full data is returned.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), any >     selectAppDataByPath ▸   selectAppDataByPath  (... keys ): OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined   Record < string , any >, ( res : undefined   Record < string , any >) => undefined   Record < string , any > >    Parameters  Name    Type      :    :     ...keys   string        Returns  OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined   Record < string , any >, ( res : undefined   Record < string , any >) => undefined   Record < string , any > >     selectAudioPlaylistTrackByPeerID ▸   selectAudioPlaylistTrackByPeerID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined     HMSAudioTrack  (/api-reference/javascript/v2/interfaces/HMSAudioTrack) > takes in a normal selector which has store and id as input and curries it to make it easier to use. Before: store.getState((store) => normalSelector(store, peerID)) After: store.getState(curriedSelector(peerID))    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined     HMSAudioTrack  (/api-reference/javascript/v2/interfaces/HMSAudioTrack) >     selectAudioTrackByID ▸   selectAudioTrackByID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), null     HMSAudioTrack  (/api-reference/javascript/v2/interfaces/HMSAudioTrack) > Select the  HMSAudioTrack (/api-reference/javascript/v2/interfaces/HMSAudioTrack) object given a track ID.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), null     HMSAudioTrack  (/api-reference/javascript/v2/interfaces/HMSAudioTrack) >     selectAudioTrackByPeerID ▸   selectAudioTrackByPeerID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined     HMSAudioTrack  (/api-reference/javascript/v2/interfaces/HMSAudioTrack) > Select the primary audio track of a peer given a peer ID.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined     HMSAudioTrack  (/api-reference/javascript/v2/interfaces/HMSAudioTrack) >     selectAudioTrackVolume ▸   selectAudioTrackVolume  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined   number > Select the local audio volume of an audio track given a track ID. NOTE:   Volume   of a track is different from   Audio Level   of a track,  Audio Level measures the audio of a track and it comes from 100ms's servers.  Volume is how loud you hear the audio of a track, this is controlled by you at the client side.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined   number >     selectAudioVolumeByPeerID ▸   selectAudioVolumeByPeerID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined   number > Select the local audio volume of the primary audio track of a peer given a peer ID.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined   number >     selectAuxiliaryAudioByPeerID ▸   selectAuxiliaryAudioByPeerID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined     HMSAudioTrack  (/api-reference/javascript/v2/interfaces/HMSAudioTrack) > Select the first auxiliary audio track of a peer given a peer ID.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined     HMSAudioTrack  (/api-reference/javascript/v2/interfaces/HMSAudioTrack) >     selectAuxiliaryTracksByPeerID ▸   selectAuxiliaryTracksByPeerID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore),   HMSTrack  (/api-reference/javascript/v2/modules hmstrack)   > Select an array of auxiliary tracks of a peer given a peer ID.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore),   HMSTrack  (/api-reference/javascript/v2/modules hmstrack)   >     selectAvailableRoleNames ▸   selectAvailableRoleNames  ( state ): string    Select an array of names of available roles in the room.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  string        selectBroadcastMessages ▸   selectBroadcastMessages  ( state ):   HMSMessage  (/api-reference/javascript/v2/interfaces/HMSMessage)      Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns   HMSMessage  (/api-reference/javascript/v2/interfaces/HMSMessage)       selectBroadcastMessagesUnreadCount ▸   selectBroadcastMessagesUnreadCount  ( state ): number    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  number     selectCameraStreamByPeerID ▸   selectCameraStreamByPeerID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined     HMSVideoTrack  (/api-reference/javascript/v2/interfaces/HMSVideoTrack) > Select the camera stream of a peer given a peer ID. This is the primary video track of a peer.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined     HMSVideoTrack  (/api-reference/javascript/v2/interfaces/HMSVideoTrack) >     selectConnectionQualities ▸   selectConnectionQualities  ( store ): Record < string , HMSConnectionQuality >    Parameters  Name   Type                                :   :                  store    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  Record < string , HMSConnectionQuality >     selectConnectionQualityByPeerID ▸   selectConnectionQualityByPeerID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined   HMSConnectionQuality > takes in a normal selector which has store and id as input and curries it to make it easier to use. Before: store.getState((store) => normalSelector(store, peerID)) After: store.getState(curriedSelector(peerID))    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined   HMSConnectionQuality >     selectDegradedTracks ▸   selectDegradedTracks  ( state ):   HMSVideoTrack  (/api-reference/javascript/v2/interfaces/HMSVideoTrack)   Select an array of tracks that have been degraded(receiving lower video quality/no video) due to bad network locally.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns   HMSVideoTrack  (/api-reference/javascript/v2/interfaces/HMSVideoTrack)       selectDevices ▸   selectDevices  ( store ):   DeviceMap  (/api-reference/javascript/v2/interfaces/DeviceMap) Select the available audio input, audio output and video input devices on your machine.    Parameters  Name   Type                                :   :                  store    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns   DeviceMap  (/api-reference/javascript/v2/interfaces/DeviceMap) An object of array of available audio input, audio output and video input devices.   type DeviceMap =    audioInput: InputDeviceInfo  ;  audioOutput: MediaDeviceInfo  ;  videoInput: InputDeviceInfo  ;         selectDidIJoinWithin ▸   selectDidIJoinWithin  ( timeMs ): OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined   boolean , ( res :   HMSRoom  (/api-reference/javascript/v2/interfaces/HMSRoom)) => undefined   boolean > Returns a boolean to indicate if the local peer joined within the past timeMs milliseconds. Ex: to know if the local peer joined within the last one second   js const joinedWithinASecond = useHMSStore(selectDidIJoinWithin(1000));      Parameters  Name    Type     :    :     timeMs   number     Returns  OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined   boolean , ( res :   HMSRoom  (/api-reference/javascript/v2/interfaces/HMSRoom)) => undefined   boolean >     selectDominantSpeaker ▸   selectDominantSpeaker  ( state ): null     HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer) Select the peer who's speaking the loudest at the moment    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  null     HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)     selectHLSState ▸   selectHLSState  ( state ):   HMSHLS  (/api-reference/javascript/v2/interfaces/HMSHLS)    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns   HMSHLS  (/api-reference/javascript/v2/interfaces/HMSHLS)     selectHMSMessages ▸   selectHMSMessages  ( state ):   HMSMessage  (/api-reference/javascript/v2/interfaces/HMSMessage)   Select an array of messages in the room(sent and received).    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns   HMSMessage  (/api-reference/javascript/v2/interfaces/HMSMessage)       selectHMSMessagesCount ▸   selectHMSMessagesCount  ( state ): number  Select the number of messages(sent and received).    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  number     selectIsAllowedToPublish ▸   selectIsAllowedToPublish  ( state ):   HMSPublishAllowed  (/api-reference/javascript/v2/interfaces/HMSPublishAllowed) Select what streams is the local peer allowed to publish from video, audio and screenshare.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns   HMSPublishAllowed  (/api-reference/javascript/v2/interfaces/HMSPublishAllowed)     selectIsAllowedToSubscribe ▸   selectIsAllowedToSubscribe  ( state ): boolean  Select a boolean denoting whether if your local peer is allowed to subscribe to any other role.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  boolean     selectIsAudioLocallyMuted ▸   selectIsAudioLocallyMuted  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined   boolean > Select a boolean denoting whether you've muted an audio track locally(only for you) given a track ID.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined   boolean >     selectIsConnectedToRoom ▸   selectIsConnectedToRoom  ( state ): undefined   boolean  Select a boolean flag denoting whether you've joined a room. NOTE: Returns true only after join, returns false during preview.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  undefined   boolean     selectIsInPreview ▸   selectIsInPreview  ( state ): boolean  Select a boolean denoting whether the room is in Preview state.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  boolean     selectIsLocalAudioEnabled ▸   selectIsLocalAudioEnabled  ( store ): boolean  Select a boolean denoting whether your local audio is unmuted and the audio from your microphone is shared to remote peers    Parameters  Name   Type                                :   :                  store    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  boolean     selectIsLocalAudioPluginPresent ▸   selectIsLocalAudioPluginPresent  ( pluginName ): OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), boolean , ( res : string   ) => boolean >    Parameters  Name      Type     :     :     pluginName   string     Returns  OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), boolean , ( res : string   ) => boolean >     selectIsLocalScreenShared ▸   selectIsLocalScreenShared  ( state ): boolean  Select a boolean denoting whether your screen is shared to remote peers in the room.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  boolean     selectIsLocalVideoDisplayEnabled ▸   selectIsLocalVideoDisplayEnabled  ( store ): undefined   boolean  Select a boolean denoting whether you've chosen to unmute and share your local video. NOTE: Once you call hmsActions.setLocalVideoEnabled(true) to unmute your local video, it takes some time to fetch your video from your video source. This displayEnabled property gives immediate feedback for a more interactive UI, without waiting for the video source    Parameters  Name   Type                                :   :                  store    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  undefined   boolean     selectIsLocalVideoEnabled ▸   selectIsLocalVideoEnabled  ( store ): boolean  Select a boolean denoting whether your local video is unmuted and the video from your camera is shared to remote peers    Parameters  Name   Type                                :   :                  store    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  boolean     selectIsLocalVideoPluginPresent ▸   selectIsLocalVideoPluginPresent  ( pluginName ): OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), boolean , ( res : string   ) => boolean >    Parameters  Name      Type     :     :     pluginName   string     Returns  OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), boolean , ( res : string   ) => boolean >     selectIsLocallyMutedByPeerID ▸   selectIsLocallyMutedByPeerID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined   boolean > Select a boolean denoting whether you've muted the primary audio track of a peer locally(only for you) given a peer ID.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined   boolean >     selectIsPeerAudioEnabled ▸   selectIsPeerAudioEnabled  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), boolean > Select a boolean denoting whether a peer has unmuted audio and sharing it to other peers.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), boolean >     selectIsPeerVideoEnabled ▸   selectIsPeerVideoEnabled  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), boolean > Select a boolean denoting whether a peer has unmuted video and sharing it to other peers.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), boolean >     selectIsScreenShareLocallyMutedByPeerID ▸   selectIsScreenShareLocallyMutedByPeerID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined   boolean > Select a boolean denoting whether you've muted the screen share audio track of a peer locally(only for you) given a peer ID.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined   boolean >     selectIsSomeoneScreenSharing ▸   selectIsSomeoneScreenSharing  ( state ): boolean  Select a boolean denoting whether someone is sharing screen in the room.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  boolean     selectLocalAudioTrackID ▸   selectLocalAudioTrackID  ( state ): undefined   string  Select the track ID of your local peer's primary audio track    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  undefined   string     selectLocalMediaSettings ▸   selectLocalMediaSettings  ( store ):   HMSMediaSettings  (/api-reference/javascript/v2/interfaces/HMSMediaSettings) Select your media settings i.e., choosen audio input device, audio output device and video input device.    Parameters  Name   Type                                :   :                  store    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns   HMSMediaSettings  (/api-reference/javascript/v2/interfaces/HMSMediaSettings)     selectLocalPeer ▸   selectLocalPeer  ( state ):   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer) Select the local peer object object assigned to you.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)     selectLocalPeerID ▸   selectLocalPeerID  ( state ): string  Select the peer ID of your local peer.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  string     selectLocalPeerName ▸   selectLocalPeerName  ( state ): string  Select the peer name of your local peer.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  string     selectLocalPeerRole ▸   selectLocalPeerRole  ( state ): null     HMSRole  (/api-reference/javascript/v2/interfaces/HMSRole) Select the  HMSRole (/api-reference/javascript/v2/interfaces/HMSRole) object of your local peer.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  null     HMSRole  (/api-reference/javascript/v2/interfaces/HMSRole)     selectLocalPeerRoleName ▸   selectLocalPeerRoleName  ( state ): undefined   string  Select the role name of your local peer.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  undefined   string     selectLocalTrackIDs ▸   selectLocalTrackIDs  ( state ): string    Select an array of track IDs of all your local peer's tracks    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  string        selectLocalVideoTrackID ▸   selectLocalVideoTrackID  ( state ): undefined   string  Select the track ID of your local peer's primary video track    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  undefined   string     selectMessageIDsInOrder ▸   selectMessageIDsInOrder  ( store ): string    Select IDs of messages you've sent or received sorted chronologically.    Parameters  Name   Type                                :   :                  store    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  string        selectMessagesByPeerID ▸   selectMessagesByPeerID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined     HMSMessage  (/api-reference/javascript/v2/interfaces/HMSMessage)   > takes in a normal selector which has store and id as input and curries it to make it easier to use. Before: store.getState((store) => normalSelector(store, peerID)) After: store.getState(curriedSelector(peerID))    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined     HMSMessage  (/api-reference/javascript/v2/interfaces/HMSMessage)   >     selectMessagesByRole ▸   selectMessagesByRole  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined     HMSMessage  (/api-reference/javascript/v2/interfaces/HMSMessage)   > takes in a normal selector which has store and id as input and curries it to make it easier to use. Before: store.getState((store) => normalSelector(store, peerID)) After: store.getState(curriedSelector(peerID))    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined     HMSMessage  (/api-reference/javascript/v2/interfaces/HMSMessage)   >     selectMessagesUnreadCountByPeerID ▸   selectMessagesUnreadCountByPeerID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), number > takes in a normal selector which has store and id as input and curries it to make it easier to use. Before: store.getState((store) => normalSelector(store, peerID)) After: store.getState(curriedSelector(peerID))    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), number >     selectMessagesUnreadCountByRole ▸   selectMessagesUnreadCountByRole  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), number > takes in a normal selector which has store and id as input and curries it to make it easier to use. Before: store.getState((store) => normalSelector(store, peerID)) After: store.getState(curriedSelector(peerID))    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), number >     selectPeerAudioByID ▸   selectPeerAudioByID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), number > Select audio level of audioTrack of a peer given a peer IDß.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), number >     selectPeerByCondition ▸   selectPeerByCondition  ( predicate ): OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined     HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer), ( res :   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)  ) => undefined     HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer) > Selects the first peer passing the condition given by the argument predicate function Ex: to select a peer whose metadata has spotlight set to true(assuming peer.metadata is a valid json string), use   js const spotlightPeer = useHMSStore(selectPeerByCondition(peer => JSON.parse(peer.metadata).spotlight));      Parameters  Name     Type                                          :    :                        predicate  ( peer :   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)) => boolean     Returns  OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined     HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer), ( res :   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)  ) => undefined     HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer) >     selectPeerByID ▸   selectPeerByID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), null     HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer) > Select the  HMSPeer (/api-reference/javascript/v2/interfaces/HMSPeer) object given a peer ID.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), null     HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer) >     selectPeerCount ▸   selectPeerCount  ( state ): number  selectPeerCount gives the number of peers Inside the room. This doesn't count the local peer if they're still in preview and haven't yet joined the room. Note that this will not necessarily equal the number of peers received through selectPeers, it's possible to know total number of people in the room without having details of everyone depending on dashboard settings.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  number     selectPeerMetadata ▸   selectPeerMetadata  ( peerId ): OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), any , ( res : null     HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)) => any > Selects the peer metadata for the passed in peer and returns it as JSON. If metadata is not present or conversion to JSON gives an error, an empty object is returned. Please directly use peer.metadata in case the metadata is not JSON by design.    Parameters  Name    Type     :    :     peerId   string     Returns  OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), any , ( res : null     HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)) => any >     selectPeerName ▸   selectPeerName  ( peerId ): OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined   string , ( res : null     HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)) => undefined   string >    Parameters  Name    Type     :    :     peerId   string     Returns  OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined   string , ( res : null     HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)) => undefined   string >     selectPeerNameByID ▸   selectPeerNameByID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined   string > Select the name of a  HMSPeer (/api-reference/javascript/v2/interfaces/HMSPeer) given a peer ID.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined   string >     selectPeerScreenSharing ▸   selectPeerScreenSharing  ( state ): undefined     HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer) Select the first peer who is currently sharing their screen.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  undefined     HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)     selectPeerSharingAudio ▸   selectPeerSharingAudio  ( state ): undefined     HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer) Select the first peer who is currently sharing their audio only screen    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  undefined     HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)     selectPeerSharingAudioPlaylist ▸   selectPeerSharingAudioPlaylist  ( state ): undefined     HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  undefined     HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)     selectPeerSharingVideoPlaylist ▸   selectPeerSharingVideoPlaylist  ( state ): undefined     HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  undefined     HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)     selectPeers ▸   selectPeers  ( state ):   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)   Select an array of peers(remote peers and your local peer) present in the room.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)       selectPeersByCondition ▸   selectPeersByCondition  ( predicate ): OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore),   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)  , ( res :   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)  ) =>   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)   > Selects all peers passing the condition given by the argument predicate function Ex: to select peers with isHandRaised set to true in their metadata(assuming peer.metadata is a valid json string), use   js const handRaisedPeers = useHMSStore(selectPeersByCondition(peer => JSON.parse(peer.metadata).isHandRaised));      Parameters  Name     Type                                          :    :                        predicate  ( peer :   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)) => boolean     Returns  OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore),   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)  , ( res :   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)  ) =>   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)   >     selectPeersByRole ▸   selectPeersByRole  ( role ): OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore),   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)  , ( res :   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)  ) =>   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)   > Select an array of peers of a particular role    Parameters  Name   Type    Description   :   :    :      role   string  HMSRoleName     Returns  OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore),   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)  , ( res :   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)  ) =>   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)   > HMSPeer       selectPeersScreenSharing ▸   selectPeersScreenSharing  ( state ):   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)   Select an array of peers who are currently sharing their screen.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)       selectPeersWithAudioStatus ▸   selectPeersWithAudioStatus  ( state ):   HMSPeerWithMuteStatus  (/api-reference/javascript/v2/interfaces/HMSPeerWithMuteStatus)      Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns   HMSPeerWithMuteStatus  (/api-reference/javascript/v2/interfaces/HMSPeerWithMuteStatus)       selectPermissions ▸   selectPermissions  ( state ): undefined     browserRecording : boolean ; changeRole : boolean ; endRoom : boolean ; hlsStreaming : boolean ; mute : boolean ; removeOthers : boolean ; rtmpStreaming : boolean ; unmute : boolean   Select the permissions which determine what actions the local peer can do.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  undefined     browserRecording : boolean ; changeRole : boolean ; endRoom : boolean ; hlsStreaming : boolean ; mute : boolean ; removeOthers : boolean ; rtmpStreaming : boolean ; unmute : boolean       selectRTMPState ▸   selectRTMPState  ( state ):   HMSRTMP  (/api-reference/javascript/v2/interfaces/HMSRTMP)    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns   HMSRTMP  (/api-reference/javascript/v2/interfaces/HMSRTMP)     selectRecordingState ▸   selectRecordingState  ( state ):   HMSRecording  (/api-reference/javascript/v2/interfaces/HMSRecording)    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns   HMSRecording  (/api-reference/javascript/v2/interfaces/HMSRecording)     selectRemotePeers ▸   selectRemotePeers  ( state ):   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)   Select remote peers(other users you're connected with via the internet) present in the room.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)       selectRoleByRoleName ▸   selectRoleByRoleName  ( roleName ): OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore),   HMSRole  (/api-reference/javascript/v2/interfaces/HMSRole), ( res : Record < string ,   HMSRole  (/api-reference/javascript/v2/interfaces/HMSRole) >) =>   HMSRole  (/api-reference/javascript/v2/interfaces/HMSRole) >    Parameters  Name     Type     :    :     roleName   string     Returns  OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore),   HMSRole  (/api-reference/javascript/v2/interfaces/HMSRole), ( res : Record < string ,   HMSRole  (/api-reference/javascript/v2/interfaces/HMSRole) >) =>   HMSRole  (/api-reference/javascript/v2/interfaces/HMSRole) >     selectRoleChangeRequest ▸   selectRoleChangeRequest  ( state ): null     HMSRoleChangeRequest  (/api-reference/javascript/v2/interfaces/HMSRoleChangeRequest) Select the role change request received for your local peer.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  null     HMSRoleChangeRequest  (/api-reference/javascript/v2/interfaces/HMSRoleChangeRequest)     selectRolesMap ▸   selectRolesMap  ( store ): Record < string ,   HMSRole  (/api-reference/javascript/v2/interfaces/HMSRole) > Select available roles in the room as a map between the role name and  HMSRole (/api-reference/javascript/v2/interfaces/HMSRole) object.    Parameters  Name   Type                                :   :                  store    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  Record < string ,   HMSRole  (/api-reference/javascript/v2/interfaces/HMSRole) >     selectRoom ▸   selectRoom  ( store ):   HMSRoom  (/api-reference/javascript/v2/interfaces/HMSRoom) Select the current  HMSRoom (/api-reference/javascript/v2/interfaces/HMSRoom) object to which you are connected.    Parameters  Name   Type                                :   :                  store    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns   HMSRoom  (/api-reference/javascript/v2/interfaces/HMSRoom)     selectRoomID ▸   selectRoomID  ( state ): string  Select the ID of the current room to which you are connected.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  string     selectRoomStartTime ▸   selectRoomStartTime  ( state ): undefined   Date    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  undefined   Date     selectRoomStarted ▸   selectRoomStarted  ( state ): boolean    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  boolean     selectRoomState ▸   selectRoomState  ( state ):   HMSRoomState  (/api-reference/javascript/v2/enums/HMSRoomState) Select the current state of the room.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns   HMSRoomState  (/api-reference/javascript/v2/enums/HMSRoomState)     selectScreenAudioTrackByID ▸   selectScreenAudioTrackByID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), null     HMSAudioTrack  (/api-reference/javascript/v2/interfaces/HMSAudioTrack) > Select the  HMSScreenAudioTrack (/api-reference/javascript/v2/interfaces/HMSScreenAudioTrack) object given a track ID.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), null     HMSAudioTrack  (/api-reference/javascript/v2/interfaces/HMSAudioTrack) >     selectScreenShareAudioByPeerID ▸   selectScreenShareAudioByPeerID  ( id? ): OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore),   HMSScreenAudioTrack  (/api-reference/javascript/v2/interfaces/HMSScreenAudioTrack), ( res :   audio :   HMSScreenAudioTrack  (/api-reference/javascript/v2/interfaces/HMSScreenAudioTrack) ; video :   HMSScreenVideoTrack  (/api-reference/javascript/v2/interfaces/HMSScreenVideoTrack)  ) =>   HMSScreenAudioTrack  (/api-reference/javascript/v2/interfaces/HMSScreenAudioTrack) > Select the screen share audio track of a peer given a peer ID.    Parameters  Name  Type     :   :     id?   string     Returns  OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore),   HMSScreenAudioTrack  (/api-reference/javascript/v2/interfaces/HMSScreenAudioTrack), ( res :   audio :   HMSScreenAudioTrack  (/api-reference/javascript/v2/interfaces/HMSScreenAudioTrack) ; video :   HMSScreenVideoTrack  (/api-reference/javascript/v2/interfaces/HMSScreenVideoTrack)  ) =>   HMSScreenAudioTrack  (/api-reference/javascript/v2/interfaces/HMSScreenAudioTrack) >     selectScreenShareByPeerID ▸   selectScreenShareByPeerID  ( id? ): OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore),   HMSScreenVideoTrack  (/api-reference/javascript/v2/interfaces/HMSScreenVideoTrack), ( res :   audio :   HMSScreenAudioTrack  (/api-reference/javascript/v2/interfaces/HMSScreenAudioTrack) ; video :   HMSScreenVideoTrack  (/api-reference/javascript/v2/interfaces/HMSScreenVideoTrack)  ) =>   HMSScreenVideoTrack  (/api-reference/javascript/v2/interfaces/HMSScreenVideoTrack) > Select the screen share video track of a peer given a peer ID.    Parameters  Name  Type     :   :     id?   string     Returns  OutputSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore),   HMSScreenVideoTrack  (/api-reference/javascript/v2/interfaces/HMSScreenVideoTrack), ( res :   audio :   HMSScreenAudioTrack  (/api-reference/javascript/v2/interfaces/HMSScreenAudioTrack) ; video :   HMSScreenVideoTrack  (/api-reference/javascript/v2/interfaces/HMSScreenVideoTrack)  ) =>   HMSScreenVideoTrack  (/api-reference/javascript/v2/interfaces/HMSScreenVideoTrack) >     selectScreenSharesByPeerId ▸   selectScreenSharesByPeerId  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore),   audio :   HMSScreenAudioTrack  (/api-reference/javascript/v2/interfaces/HMSScreenAudioTrack) ; video :   HMSScreenVideoTrack  (/api-reference/javascript/v2/interfaces/HMSScreenVideoTrack)   > takes in a normal selector which has store and id as input and curries it to make it easier to use. Before: store.getState((store) => normalSelector(store, peerID)) After: store.getState(curriedSelector(peerID))    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore),   audio :   HMSScreenAudioTrack  (/api-reference/javascript/v2/interfaces/HMSScreenAudioTrack) ; video :   HMSScreenVideoTrack  (/api-reference/javascript/v2/interfaces/HMSScreenVideoTrack)   >     selectScreenVideoTrackByID ▸   selectScreenVideoTrackByID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), null     HMSScreenVideoTrack  (/api-reference/javascript/v2/interfaces/HMSScreenVideoTrack) > Select the  HMSScreenVideoTrack (/api-reference/javascript/v2/interfaces/HMSScreenVideoTrack) object given a track ID.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), null     HMSScreenVideoTrack  (/api-reference/javascript/v2/interfaces/HMSScreenVideoTrack) >     selectScreenshareAudioVolumeByPeerID ▸   selectScreenshareAudioVolumeByPeerID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined   number > Select the local audio volume of the screen share of a peer given a peer ID.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined   number >     selectSessionId ▸   selectSessionId  ( state ): string    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  string     selectSessionMetadata ▸   selectSessionMetadata  ( store ): any    Parameters  Name   Type                                :   :                  store    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  any     selectSimulcastLayerByTrack ▸   selectSimulcastLayerByTrack  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined     HMSSimulcastLayer  (/api-reference/javascript/v2/enums/HMSSimulcastLayer) > Select the current simulcast layer of a track given a track ID.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined     HMSSimulcastLayer  (/api-reference/javascript/v2/enums/HMSSimulcastLayer) >     selectSpeakers ▸   selectSpeakers  ( store ): Record < string ,   HMSSpeaker  (/api-reference/javascript/v2/interfaces/HMSSpeaker) >    Parameters  Name   Type                                :   :                  store    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  Record < string ,   HMSSpeaker  (/api-reference/javascript/v2/interfaces/HMSSpeaker) >     selectTrackAudioByID ▸   selectTrackAudioByID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), number > Select the audio level of a track given a track ID.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), number >     selectTrackByID ▸   selectTrackByID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), null     HMSAudioTrack  (/api-reference/javascript/v2/interfaces/HMSAudioTrack)    HMSVideoTrack  (/api-reference/javascript/v2/interfaces/HMSVideoTrack)    HMSScreenVideoTrack  (/api-reference/javascript/v2/interfaces/HMSScreenVideoTrack) > Select the  HMSTrack (/api-reference/javascript/v2/modules hmstrack) object given a track ID.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), null     HMSAudioTrack  (/api-reference/javascript/v2/interfaces/HMSAudioTrack)    HMSVideoTrack  (/api-reference/javascript/v2/interfaces/HMSVideoTrack)    HMSScreenVideoTrack  (/api-reference/javascript/v2/interfaces/HMSScreenVideoTrack) >     selectUnreadHMSMessagesCount ▸   selectUnreadHMSMessagesCount  ( state ): number  Select the number of unread messages.    Parameters  Name   Type                                :   :                  state    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  number     selectVideoPlaylistAudioTrackByPeerID ▸   selectVideoPlaylistAudioTrackByPeerID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined     HMSAudioTrack  (/api-reference/javascript/v2/interfaces/HMSAudioTrack) > takes in a normal selector which has store and id as input and curries it to make it easier to use. Before: store.getState((store) => normalSelector(store, peerID)) After: store.getState(curriedSelector(peerID))    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined     HMSAudioTrack  (/api-reference/javascript/v2/interfaces/HMSAudioTrack) >     selectVideoPlaylistVideoTrackByPeerID ▸   selectVideoPlaylistVideoTrackByPeerID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined     HMSVideoTrack  (/api-reference/javascript/v2/interfaces/HMSVideoTrack) > takes in a normal selector which has store and id as input and curries it to make it easier to use. Before: store.getState((store) => normalSelector(store, peerID)) After: store.getState(curriedSelector(peerID))    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined     HMSVideoTrack  (/api-reference/javascript/v2/interfaces/HMSVideoTrack) >     selectVideoTrackByID ▸   selectVideoTrackByID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), null     HMSVideoTrack  (/api-reference/javascript/v2/interfaces/HMSVideoTrack) > Select the  HMSVideoTrack (/api-reference/javascript/v2/interfaces/HMSVideoTrack) object given a track ID.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), null     HMSVideoTrack  (/api-reference/javascript/v2/interfaces/HMSVideoTrack) >     selectVideoTrackByPeerID ▸   selectVideoTrackByPeerID  ( id? ): StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined     HMSVideoTrack  (/api-reference/javascript/v2/interfaces/HMSVideoTrack) > Select the primary video track of a peer given a peer ID.    Parameters  Name  Type     :   :     id?   string     Returns  StoreSelector <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore), undefined     HMSVideoTrack  (/api-reference/javascript/v2/interfaces/HMSVideoTrack) > "
    },
    {
        "title": "DeviceMap",
        "link": "/api-reference/javascript/v2/interfaces/DeviceMap",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/DeviceMap",
        "keywords": [],
        "content": "  Properties   audioInput •   audioInput  : MediaDeviceInfo        audioOutput •   audioOutput  : MediaDeviceInfo        videoInput •   videoInput  : MediaDeviceInfo    "
    },
    {
        "title": "HLSConfig",
        "link": "/api-reference/javascript/v2/interfaces/HLSConfig",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HLSConfig",
        "keywords": [],
        "content": "  Properties   recording • Optional   recording  : Object  pass in this field if recording needs to be turned on as well    Type declaration  Name          Type    Description                                     :       :    :                       hlsVod?         boolean  if the desired end result is a zip of m3u8 and all the chunks, false by default   singleFilePerLayer?   boolean  if the desired end result is a mp4 file per HLS layer, false by default          variants • Optional   variants  :   HLSMeetingURLVariant  (/api-reference/javascript/v2/interfaces/HLSMeetingURLVariant)   A list of meeting url which needs to be streamed as HLS feed, only one url is currently supported, all entries except the first one will be ignored. "
    },
    {
        "title": "HLSMeetingURLVariant",
        "link": "/api-reference/javascript/v2/interfaces/HLSMeetingURLVariant",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HLSMeetingURLVariant",
        "keywords": [],
        "content": "  Properties   meetingURL •   meetingURL  : string  This meeting url is opened in a headless chrome instance for generating the HLS feed. Make sure this url leads the joiner straight to the room without any preview screen or requiring additional clicks.     metadata • Optional   metadata  : string  additional metadata for this url for e.g.  landscape/portrait, the field is not currently supported "
    },
    {
        "title": "HLSVariant",
        "link": "/api-reference/javascript/v2/interfaces/HLSVariant",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HLSVariant",
        "keywords": [],
        "content": "  Properties   meetingURL • Optional   meetingURL  : string     metadata • Optional   metadata  : string     startedAt • Optional   startedAt  : Date     url •   url  : string "
    },
    {
        "title": "HMSActions",
        "link": "/api-reference/javascript/v2/interfaces/HMSActions",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSActions",
        "keywords": [],
        "content": " The below interface defines our SDK API Surface for taking room related actions. It talks to our 100ms backend and handles error reconnections, state managements and lots of other things so you don't have to. You can use this gateway with any sort of UI to make connecting to our backend easier. In case you use react, we also provide a HMSProvider class with very powerful hooks and out of box components which you can use to setup your website in minutes. Our components have in built integration with this interface and you won't have to worry about passing props if you use them.    Remarks    There is a one to one mapping between an instance of this class and a 100ms room, in case you're creating multiple rooms please create new instance per room.   Properties   audioPlaylist •   audioPlaylist  :   IHMSPlaylistActions  (/api-reference/javascript/v2/interfaces/IHMSPlaylistActions) audio Playlist contains all actions that can be performed on the audio playlist This will be available after joining the room     endRoom •   endRoom  : ( lock : boolean , reason : string ) => Promise < void >    Type declaration ▸ ( lock , reason ): Promise < void > If you have the   endRoom   permission, you can end the room. That means everyone will be kicked out. If lock is passed as true, the room cannot be used further.    Parameters  Name    Type     :    :     lock    boolean    reason   string      Returns  Promise < void >     unblockAudio •   unblockAudio  : () => Promise < void >    Type declaration ▸ (): Promise < void > Method to be called with some UI interaction after autoplay error is received Most browsers have limitations where an audio can not be played if there was no user interaction. SDK throws an autoplay error in this case, this method can be called after an UI interaction to resolve the autoplay error    Returns  Promise < void >     videoPlaylist •   videoPlaylist  :   IHMSPlaylistActions  (/api-reference/javascript/v2/interfaces/IHMSPlaylistActions) video Playlist contains all actions that can be performed on the video playlist This will be available after joining the room   Methods   acceptChangeRole ▸   acceptChangeRole  ( request ): Promise < void > Accept the role change request received    Parameters  Name    Type                                           Description                :    :                       :            request    HMSRoleChangeRequest  (/api-reference/javascript/v2/interfaces/HMSRoleChangeRequest)  The original request that was received     Returns  Promise < void >     addPluginToAudioTrack ▸   addPluginToAudioTrack  ( plugin ): Promise < void > Add or remove a audio plugin from/to the local peer audio track. Eg. gain filter, noise suppression etc. Audio plugins can be added/removed at any time after the audio track is available    See    HMSAudioPlugin    Parameters  Name    Type        Description    :    :      :      plugin   HMSAudioPlugin  HMSAudioPlugin     Returns  Promise < void >     addPluginToVideoTrack ▸   addPluginToVideoTrack  ( plugin , pluginFrameRate? ): Promise < void > Add or remove a video plugin from/to the local peer video track. Eg. Virtual Background, Face Filters etc. Video plugins can be added/removed at any time after the video track is available. pluginFrameRate is the rate at which the output plugin will do processing    See    HMSVideoPlugin    Parameters  Name         Type        Description    :      :      :      plugin        HMSVideoPlugin  HMSVideoPlugin   pluginFrameRate?   number      number         Returns  Promise < void >     addTrack ▸   addTrack  ( track , type ): Promise < void > You can use the addTrack method to add an auxiliary track(canvas capture, electron screen-share, etc...) This method adds the track to the local peer's list of auxiliary tracks and publishes it to make it available to remote peers.    Parameters  Name   Type         Description                                          :   :      :                         track   MediaStreamTrack  MediaStreamTrack  Track to be added                              type   string       HMSTrackSource  'regular'  'screen'  'plugin'  Source of track  default: 'regular'     Returns  Promise < void >     attachVideo ▸   attachVideo  ( localTrackID , videoElement ): Promise < void > You can use the attach and detach video function to add/remove video from an element for a track ID. The benefit of using this instead of removing the video yourself is that it'll also auto unsubscribe to the stream coming from server saving significant bandwidth for the user.    Parameters  Name       Type         Description                       :     :      :                localTrackID   string       trackID as stored in the store for the peer       videoElement   HTMLVideoElement  HTML native element where the video has to be shown     Returns  Promise < void >     changeMetadata ▸   changeMetadata  ( metadata ): Promise < void > If you want to update the metadata of local peer. If an object is passed, it should be serializable using JSON.stringify.    Parameters  Name     Type   :    :    metadata   any     Returns  Promise < void >     changeName ▸   changeName  ( name ): Promise < void > If you want to update the name of peer.    Parameters  Name   Type     :   :     name   string     Returns  Promise < void >     changeRole ▸   changeRole  ( forPeerId , toRole , force? ): Promise < void > Request for a role change of a remote peer. Can be forced.    Parameters  Name     Type    Description                                    :    :    :                      forPeerId   string   The remote peer id whose role needs to be changed                  toRole    string   The name of the new role.                              force?    boolean  this being true would mean that user won't get a request to accept role change     Returns  Promise < void >     detachVideo ▸   detachVideo  ( localTrackID , videoElement ): Promise < void >    See    attachVideo    Parameters  Name       Type          :     :       localTrackID   string         videoElement   HTMLVideoElement     Returns  Promise < void >     ignoreMessageTypes ▸   ignoreMessageTypes  ( msgTypes , replace? ): void  ignore messages with this type for storing in store. You can use this to have a clear segregation between chat messages(you would want to persist for the duration of the call) and one off custom events(emoji reactions, stop screenshare, moderator messages, etc.). You can also use this to store messages on your own side if some additional processing is required(the default type is \"chat\"). Notifications for the ignored messages will still be sent, it'll only not be put in the store.    Parameters  Name     Type     Description                                                                                           :    :    :                                                  msgTypes   string     list of messages types to ignore for storing                                                                           replace?   boolean   (default is false) whether to replace the list of ignored messages. Types are appended to the existing list by default so you can call this method from different places and all will hold.     Returns  void     initAppData ▸   initAppData  ( data ): void    Parameters  Name   Type             Description                                                                                                                                                                           :   :        :                                                                                          data   Record < string , any >  full app data object. use this to initialise app data in store. App Data is a small space in the store for UI to keep a few non updating global state fields for easy reference across UI. Note that if the fields are updating at high frequency or there are too many of them, it's recommended to have another UI side store to avoid performance issues.     Returns  void     join ▸   join  ( config ): Promise < void > join function can be used to join the room, if the room join is successful, current details of participants and track details are populated in the store.    Remarks    If join is called while an earlier join is in progress for the room id, it is ignored    Parameters  Name    Type                                Description                         :    :                  :                 config    HMSConfig  (/api-reference/javascript/v2/interfaces/HMSConfig)  join config with room id, required for joining the room     Returns  Promise < void >     leave ▸   leave  (): Promise < void > This function can be used to leave the room, if the call is repeated it's ignored.    Returns  Promise < void >     populateSessionMetadata ▸   populateSessionMetadata  (): Promise < void > Fetch the current room metadata from the server and populate it in store  the API is not stable and might have breaking changes later    Returns  Promise < void >     preview ▸   preview  ( config ): Promise < void >    Parameters  Name    Type                                 :    :                   config    HMSConfig  (/api-reference/javascript/v2/interfaces/HMSConfig)     Returns  Promise < void >     refreshDevices ▸   refreshDevices  (): Promise < void >    Returns  Promise < void >     rejectChangeRole ▸   rejectChangeRole  ( request ): void  Reject pending role change request    Parameters  Name    Type                                           Description                :    :                       :            request    HMSRoleChangeRequest  (/api-reference/javascript/v2/interfaces/HMSRoleChangeRequest)  The original request that was received     Returns  void     removePeer ▸   removePeer  ( peerID , reason ): Promise < void > If you have   removeOthers   permission, you can remove a peer from the room.    Parameters  Name    Type    Description                                                                                             :    :    :                                                   peerID   string  peerID of the peer to be removed from the room                                                                            reason   string  a string explaining why the peer is removed from the room. This string could be used to notify the user before they're removed from the room using the REMOVED_FROM_ROOM type of notification     Returns  Promise < void >     removePluginFromAudioTrack ▸   removePluginFromAudioTrack  ( plugin ): Promise < void >    See    addPluginToAudioTrack    Parameters  Name    Type         :    :       plugin   HMSAudioPlugin     Returns  Promise < void >     removePluginFromVideoTrack ▸   removePluginFromVideoTrack  ( plugin ): Promise < void >    See    addPluginToVideoTrack    Parameters  Name    Type         :    :       plugin   HMSVideoPlugin     Returns  Promise < void >     removeTrack ▸   removeTrack  ( trackId ): Promise < void > You can use the removeTrack method to remove an auxiliary track. This method removes the track from the local peer's list of auxiliary tracks and unpublishes it.    Parameters  Name    Type    Description                :    :    :            trackId   string  string  ID of the track to be removed     Returns  Promise < void >     sendBroadcastMessage ▸   sendBroadcastMessage  ( message , type? ): Promise < void > Send a plain text message to all the other participants in the room.    Parameters  Name    Type    Description                              :    :    :                   message   string  string message to broadcast                      type?    string  type of message eg: image, video etc.  optional defaults to chat     Returns  Promise < void >     sendDirectMessage ▸   sendDirectMessage  ( message , peerID , type? ): Promise < void >    Parameters  Name    Type    Description                              :    :    :                   message   string                                      peerID   string  id of the peer to which message has to be sent             type?    string  type of message eg: image, video etc.  optional defaults to chat     Returns  Promise < void >     sendGroupMessage ▸   sendGroupMessage  ( message , roles , type? ): Promise < void >    Parameters  Name    Type     Description                              :    :    :                   message   string   string message to send                         roles    string     roles to which to send the message                   type?    string   type of message eg: image, video etc.  optional defaults to chat     Returns  Promise < void >     sendHLSTimedMetadata ▸   sendHLSTimedMetadata  ( metadataList ): Promise < void > Used to define date range metadata in a media playlist. This api adds EXT-X-DATERANGE tags to the media playlist. It is useful for defining timed metadata for interstitial regions such as advertisements, but can be used to define any timed metadata needed by your stream. usage (e.g) const metadataList =    payload: \"some string 1\", duration: 2  ,   payload: \"some string 2\", duration: 3    sendHLSTimedMetadata(metadataList);    Parameters  Name       Type           :     :        metadataList   HLSTimedMetadata        Returns  Promise < void >     sendMessage ▸   sendMessage  ( message ): void     Deprecated    The method should not be used    See    sendBroadcastMessage Send a plain text message to all the other participants in the room.    Parameters  Name    Type    Description           :    :    :          message   string  string message to broadcast     Returns  void     setAppData ▸   setAppData  ( key , value , merge? ): void  use it for updating a particular property in the appdata    Example      ts assume appdata is initially     mySettings:      setting1: 'val1',    setting2: 'val2',    ,   mySettings2: 43,   mySettings3: false,   ; after calling, setAppData(\"mySettings\",  setting1:'val1-edit', setting3:'val3' , true); it becomes     mySettings:      setting1: 'val1-edit',    setting2: 'val2',    setting3: 'val3',    ,   mySettings2: 43,   mySettings3: false,   ; Note: This is not suitable for keeping large data or data which updates at a high frequency, it is recommended to use app side store for those cases.      Parameters  Name    Type                   Description                                                                                                                                              :    :           :                                                                           key    string                 a string. Does not check for existence. If the key is already not a property of the appData, it is added.                                                                                                value   Record < string   number , any >  value to set for the key.                                                                                                                                        merge?   boolean                 set it to true if you want to merge the appdata.  Always replaces the value for a given key if this parameter is not explicitly set to true.  Always replaces if the value is anything other than a plain object (i.e) JSON.parse()able.  If set to true on non-plain objects, this is ignored.     Returns  void  ▸   setAppData  ( key , value ): void    Parameters  Name   Type     :   :     key    string    value   any       Returns  void     setAudioOutputDevice ▸   setAudioOutputDevice  ( deviceId ): Promise < void > Set the audio output(speaker) device    Parameters  Name     Type    Description                  :    :    :             deviceId   string  string deviceId of the audio output device     Returns  Promise < void >     setAudioSettings ▸   setAudioSettings  ( settings ): Promise < void > Change settings of the local peer's audio track    Parameters  Name     Type                                                  Description                                  :    :                           :                     settings   Partial <  HMSAudioTrackSettings  (/api-reference/javascript/v2/interfaces/HMSAudioTrackSettings) >  HMSAudioTrackSettings (  volume, codec, maxBitrate, deviceId, advanced  )     Returns  Promise < void >     setEnabledTrack ▸   setEnabledTrack  ( trackId , enabled ): Promise < void >    Parameters  Name    Type    Description                                       :    :    :                        trackId   string   string  ID of the track whose mute status needs to be set                enabled   boolean  boolean  true when we want to unmute the track and false when we want to unmute it     Returns  Promise < void >     setLocalAudioEnabled ▸   setLocalAudioEnabled  ( enabled ): Promise < void > This function can be used to enable/disable(unmute/mute) local audio track    Parameters  Name    Type    Description                 :    :    :             enabled   boolean  boolean  true to unmute, false to mute     Returns  Promise < void >     setLocalVideoEnabled ▸   setLocalVideoEnabled  ( enabled ): Promise < void > This function can be used to enable/disable(unmute/mute) local video track    Parameters  Name    Type    Description                 :    :    :             enabled   boolean  boolean  true to unmute, false to mute     Returns  Promise < void >     setLogLevel ▸   setLogLevel  ( level ): void  Set the type of logs from the SDK you want to be logged in the browser console. Note that HMSLogLevel is decremental meaning,  HMSLogLevel.VERBOSE(0)  will log every message from SDK.  HMSLogLevel.DEBUG(1)  will log messages that are helpful in debugging, important info, warnings and errors.  HMSLogLevel.INFO(2)  will log important info, warnings and errors.  HMSLogLevel.WARN(3)  will log warnings and errors.  HMSLogLevel.ERROR(4)  will log only errors.  HMSLogLevel.NONE(5)  won't log anything(Not recommended). Usage: hmsActions.setLogLevel(4) or hmsActions.setLogLevel(HMSlogLevel.ERROR) .    Parameters  Name   Type                                :   :                   level    HMSLogLevel  (/api-reference/javascript/v2/enums/HMSLogLevel)     Returns  void     setMessageRead ▸   setMessageRead  ( readStatus , messageId? ): void  If just readStatus argument is passed, the function will set read flag of every message as the readStatus argument passed. If both readStatus and messageId argument is passed, then just read flag of message with passed messageId will be set as readStatus argument. if message with passed messageId is not found in store, no change in store will take place.    Parameters  Name      Type    Description                                :     :    :                    readStatus   boolean  boolean value which you want to set as read flag for message/messages.   messageId?   string   message id whose read falg you want to set.                  Returns  void     setPreferredLayer ▸   setPreferredLayer  ( trackId , layer ): Promise < void > set the quality of the selected videoTrack for simulcast.    Parameters  Name    Type                                                   :    :                            trackId   string                                                  layer     HMSPreferredSimulcastLayer  (/api-reference/javascript/v2/home/content hmspreferredsimulcastlayer)     Returns  Promise < void >     setRemoteTrackEnabled ▸   setRemoteTrackEnabled  ( forRemoteTrackID , enabled ): Promise < void > Change track state a remote peer's track This can be used to mute/unmute a remote peer's track    Parameters  Name         Type           Description                                                                   :      :       :                                      forRemoteTrackID   string   string     The track ID or array of track IDs for which you want to change the state                                     enabled       boolean         true if you wish to enable(unmute permission is required) the track, false if you wish to disable(mute permission is required) the track     Returns  Promise < void >     setRemoteTracksEnabled ▸   setRemoteTracksEnabled  ( params ): Promise < void > Use this to mute/unmute multipe tracks by source, role or type    Parameters  Name    Type                                                      :    :                             params    HMSChangeMultiTrackStateParams  (/api-reference/javascript/v2/interfaces/HMSChangeMultiTrackStateParams)     Returns  Promise < void >     setScreenShareEnabled ▸   setScreenShareEnabled  ( enabled , config? ): Promise < void > If you want to enable screenshare for the local peer this class can be called. The store will be populated with the incoming track, and the subscriber(or react component if our hook is used) will be notified/rerendered    Parameters  Name    Type                                           Description                       :    :                       :                enabled   boolean                                         boolean                          config?    HMSScreenShareConfig  (/api-reference/javascript/v2/interfaces/HMSScreenShareConfig)  check the config object for details about the fields     Returns  Promise < void >     setSessionMetadata ▸   setSessionMetadata  ( metadata ): Promise < void > If you want to update the metadata of the session. If an object is passed, it should be serializable using JSON.stringify. Session metadata is available to every peer in the room and is persisted throughout a session till the last peer leaves a room  the API is not stable and might have breaking changes later    Parameters  Name     Type   :    :    metadata   any     Returns  Promise < void >     setVideoSettings ▸   setVideoSettings  ( settings ): Promise < void > Change settings of the local peer's video track    Parameters  Name     Type                                                  Description                                            :    :                           :                          settings   Partial <  HMSVideoTrackSettings  (/api-reference/javascript/v2/interfaces/HMSVideoTrackSettings) >  HMSVideoTrackSettings (  width, height, codec, maxFramerate, maxBitrate, deviceId, advanced  )     Returns  Promise < void >     setVolume ▸   setVolume  ( value , trackId? ): Promise < void > Set the output volume of audio tracks(overall/particular audio track)    Parameters  Name     Type    Description                                                              :    :    :                                   value    number  number between 0-100                                                          trackId?   string  string If undefined sets the overall volume(of every audio track in the room); If valid  set the volume of particular audio track     Returns  Promise < void >     startHLSStreaming ▸   startHLSStreaming  ( params? ): Promise < void > If you have configured HLS streaming from dashboard, no params are required. otherwise    Parameters  Name    Type                                 :    :                   params?    HLSConfig  (/api-reference/javascript/v2/interfaces/HLSConfig)     Returns  Promise < void >     startRTMPOrRecording ▸   startRTMPOrRecording  ( params ): Promise < void > If you want to start RTMP streaming or recording.    Parameters  Name    Type                                           :    :                        params    RTMPRecordingConfig  (/api-reference/javascript/v2/interfaces/RTMPRecordingConfig)     Returns  Promise < void >     stopHLSStreaming ▸   stopHLSStreaming  ( params? ): Promise < void > If you want to stop HLS streaming. The passed in arguments is not considered at the moment, and everything related to HLS is stopped.    Parameters  Name    Type                                 :    :                   params?    HLSConfig  (/api-reference/javascript/v2/interfaces/HLSConfig)     Returns  Promise < void >     stopRTMPAndRecording ▸   stopRTMPAndRecording  (): Promise < void > If you want to stop both RTMP streaming and recording.    Returns  Promise < void >     validateAudioPluginSupport ▸   validateAudioPluginSupport  ( plugin ): HMSPluginSupportResult  To check the support of the plugin, based on browser, os and audio devices    See    HMSPluginSupportResult    Parameters  Name    Type        Description    :    :      :      plugin   HMSAudioPlugin  HMSAudioPlugin     Returns  HMSPluginSupportResult     validateVideoPluginSupport ▸   validateVideoPluginSupport  ( plugin ): HMSPluginSupportResult  To check the support of the plugin, based on browser, os and audio devices    See    HMSPluginSupportResult    Parameters  Name    Type        Description    :    :      :      plugin   HMSVideoPlugin  HMSVideoPlugin     Returns  HMSPluginSupportResult "
    },
    {
        "title": "HMSAudioTrack",
        "link": "/api-reference/javascript/v2/interfaces/HMSAudioTrack",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSAudioTrack",
        "keywords": [],
        "content": "  Hierarchy  BaseTrack   ↳    HMSAudioTrack     ↳↳   HMSScreenAudioTrack  (/api-reference/javascript/v2/interfaces/HMSScreenAudioTrack)   Properties   deviceID • Optional   deviceID  : string    Inherited from BaseTrack.deviceID     displayEnabled • Optional   displayEnabled  : boolean    Inherited from BaseTrack.displayEnabled     enabled •   enabled  : boolean    Inherited from BaseTrack.enabled     id •   id  : string    Inherited from BaseTrack.id     peerId • Optional   peerId  : string    Inherited from BaseTrack.peerId     plugins • Optional   plugins  : string       Inherited from BaseTrack.plugins     source •   source  : string    Overrides BaseTrack.source     type •   type  : \"audio\"    Overrides BaseTrack.type     volume • Optional   volume  : number "
    },
    {
        "title": "HMSAudioTrackSettings",
        "link": "/api-reference/javascript/v2/interfaces/HMSAudioTrackSettings",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSAudioTrackSettings",
        "keywords": [],
        "content": "  Properties   advanced • Optional   advanced  : MediaTrackConstraintSet        codec • Optional   codec  : OPUS     deviceId • Optional   deviceId  : string     maxBitrate • Optional   maxBitrate  : number     volume • Optional   volume  : number "
    },
    {
        "title": "HMSChangeMultiTrackStateParams",
        "link": "/api-reference/javascript/v2/interfaces/HMSChangeMultiTrackStateParams",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSChangeMultiTrackStateParams",
        "keywords": [],
        "content": "  Properties   enabled •   enabled  : boolean     roles • Optional   roles  : string        source • Optional   source  : string     type • Optional   type  : \"audio\"   \"video\" "
    },
    {
        "title": "HMSChangeMultiTrackStateRequest",
        "link": "/api-reference/javascript/v2/interfaces/HMSChangeMultiTrackStateRequest",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSChangeMultiTrackStateRequest",
        "keywords": [],
        "content": "  Properties   enabled •   enabled  : boolean     requestedBy • Optional   requestedBy  :   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)     source • Optional   source  : string     tracks •   tracks  :   HMSTrack  (/api-reference/javascript/v2/home/content hmstrack)       type • Optional   type  : \"audio\"   \"video\" "
    },
    {
        "title": "HMSChangeMultiTrackStateRequestNotification",
        "link": "/api-reference/javascript/v2/interfaces/HMSChangeMultiTrackStateRequestNotification",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSChangeMultiTrackStateRequestNotification",
        "keywords": [],
        "content": "  Hierarchy  BaseNotification   ↳    HMSChangeMultiTrackStateRequestNotification      Properties   data •   data  :   HMSChangeMultiTrackStateRequest  (/api-reference/javascript/v2/interfaces/HMSChangeMultiTrackStateRequest)     id •   id  : number    Inherited from BaseNotification.id     message •   message  : string    Inherited from BaseNotification.message     severity • Optional   severity  :   HMSNotificationSeverity  (/api-reference/javascript/v2/enums/HMSNotificationSeverity)    Inherited from BaseNotification.severity     type •   type  :   CHANGE_MULTI_TRACK_STATE_REQUEST  (/api-reference/javascript/v2/enums/HMSNotificationTypes change_multi_track_state_request)    Overrides BaseNotification.type "
    },
    {
        "title": "HMSChangeTrackStateRequest",
        "link": "/api-reference/javascript/v2/interfaces/HMSChangeTrackStateRequest",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSChangeTrackStateRequest",
        "keywords": [],
        "content": "  Properties   enabled •   enabled  : boolean     requestedBy • Optional   requestedBy  :   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)     track •   track  :   HMSTrack  (/api-reference/javascript/v2/home/content hmstrack) "
    },
    {
        "title": "HMSChangeTrackStateRequestNotification",
        "link": "/api-reference/javascript/v2/interfaces/HMSChangeTrackStateRequestNotification",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSChangeTrackStateRequestNotification",
        "keywords": [],
        "content": "  Hierarchy  BaseNotification   ↳    HMSChangeTrackStateRequestNotification      Properties   data •   data  :   HMSChangeTrackStateRequest  (/api-reference/javascript/v2/interfaces/HMSChangeTrackStateRequest)     id •   id  : number    Inherited from BaseNotification.id     message •   message  : string    Inherited from BaseNotification.message     severity • Optional   severity  :   HMSNotificationSeverity  (/api-reference/javascript/v2/enums/HMSNotificationSeverity)    Inherited from BaseNotification.severity     type •   type  :   CHANGE_TRACK_STATE_REQUEST  (/api-reference/javascript/v2/enums/HMSNotificationTypes change_track_state_request)    Overrides BaseNotification.type "
    },
    {
        "title": "HMSConfig",
        "link": "/api-reference/javascript/v2/interfaces/HMSConfig",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSConfig",
        "keywords": [],
        "content": " the config object tells the sdk options you want to join with    Link    https://docs.100ms.live/javascript/v2/features/preview    Link    https://docs.100ms.live/javascript/v2/features/join   Properties   alwaysRequestPermissions • Optional   alwaysRequestPermissions  : boolean  Request Camera/Mic permissions irrespective of role to avoid delay in getting device list     audioSinkElementId • Optional   audioSinkElementId  : string     authToken •   authToken  : string  client token which encodes room id and role to join with    Link    https://docs.100ms.live/javascript/v2/foundation/security-and-tokens     autoVideoSubscribe • Optional   autoVideoSubscribe  : boolean     captureNetworkQualityInPreview • Optional   captureNetworkQualityInPreview  : boolean  Enable to get a network quality score while in preview. The score ranges from -1 to 5. -1 when we are not able to connect to 100ms servers within an expected time limit 0 when there is a timeout/failure when measuring the quality 1-5 ranges from poor to good quality.     initEndpoint • Optional   initEndpoint  : string     metaData • Optional   metaData  : string  optional metadata which can be attached with a peer. This can also be changed mid call.    Link    https://docs.100ms.live/javascript/v2/advanced-features/peer-metadata     rememberDeviceSelection • Optional   rememberDeviceSelection  : boolean  highly recommended to pass this as true, this will make sdk use the local storage to remember any manual device selection for future joins.     settings • Optional   settings  :   HMSConfigInitialSettings  (/api-reference/javascript/v2/interfaces/HMSConfigInitialSettings) initial settings for audio/video and device to be used. Please don't pass this field while joining if you're using preview, the state changes in preview will be remembered across to join.     userName •   userName  : string  the name of the peer, can be later accessed via peer.name and can also be changed mid call.    Link    https://docs.100ms.live/javascript/v2/features/peer-name "
    },
    {
        "title": "HMSConfigInitialSettings",
        "link": "/api-reference/javascript/v2/interfaces/HMSConfigInitialSettings",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSConfigInitialSettings",
        "keywords": [],
        "content": "  Properties   audioInputDeviceId • Optional   audioInputDeviceId  : string     audioOutputDeviceId • Optional   audioOutputDeviceId  : string     isAudioMuted • Optional   isAudioMuted  : boolean     isVideoMuted • Optional   isVideoMuted  : boolean     speakerAutoSelectionBlacklist • Optional   speakerAutoSelectionBlacklist  : string      \"all\"  When a peer joins the room for the first time or when a device change happens, after selecting the mic for audio input, we try to find the matching output device for selecting the speaker(on browsers where speaker selection is possible). For e.g. if the headset mic is selected, the headset speaker will also be selected, if the laptop default mix is selected, the corresponding laptop speaker will be selected. This is useful because if a non-matching pair is selected, it might lead to an echo in the room. This field can be used to override the above behavior, and always go for the default device selection as given by the browser. There are two ways to use this, you can pass in 'all' which will disable the above behaviour for all devices. Or you can pass in an array of labels which will be string matched to disable the behavior for specific devices. For e.g.  \"Yeti Stereo Microphone\" , as Yeti shows up often in audio output even when no device is plugged into its headphone jack.     videoDeviceId • Optional   videoDeviceId  : string "
    },
    {
        "title": "HMSDeviceChangeEvent",
        "link": "/api-reference/javascript/v2/interfaces/HMSDeviceChangeEvent",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSDeviceChangeEvent",
        "keywords": [],
        "content": " Test fails when adding InputDeviceInfo error TS2304: Cannot find name 'InputDeviceInfo'.  InputDeviceInfo extends MediaDeviceInfo. See https://w3c.github.io/mediacapture-main/ input-specific-device-info So, selection?: MediaDeviceInfo instead of selection?: InputDeviceInfo  MediaDeviceInfo is valid   Properties   devices •   devices  :   DeviceMap  (/api-reference/javascript/v2/interfaces/DeviceMap)     error • Optional   error  :   HMSException  (/api-reference/javascript/v2/interfaces/HMSException)     selection • Optional   selection  : MediaDeviceInfo     type •   type  : \"video\"   \"audioOutput\"   \"audioInput\" "
    },
    {
        "title": "HMSDeviceChangeEventNotification",
        "link": "/api-reference/javascript/v2/interfaces/HMSDeviceChangeEventNotification",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSDeviceChangeEventNotification",
        "keywords": [],
        "content": "  Hierarchy  BaseNotification   ↳    HMSDeviceChangeEventNotification      Properties   data • Optional   data  :   HMSDeviceChangeEvent  (/api-reference/javascript/v2/interfaces/HMSDeviceChangeEvent)     id •   id  : number    Inherited from BaseNotification.id     message •   message  : string    Inherited from BaseNotification.message     severity • Optional   severity  :   HMSNotificationSeverity  (/api-reference/javascript/v2/enums/HMSNotificationSeverity)    Inherited from BaseNotification.severity     type •   type  :   DEVICE_CHANGE_UPDATE  (/api-reference/javascript/v2/enums/HMSNotificationTypes device_change_update)    Overrides BaseNotification.type "
    },
    {
        "title": "HMSException",
        "link": "/api-reference/javascript/v2/interfaces/HMSException",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSException",
        "keywords": [],
        "content": " any mid call error notification will be in this format   Properties   action •   action  : string     code •   code  : number     description •   description  : string     isTerminal •   isTerminal  : boolean     message •   message  : string     name •   name  : string     nativeError • Optional   nativeError  : Error     timestamp •   timestamp  : Date "
    },
    {
        "title": "HMSExceptionNotification",
        "link": "/api-reference/javascript/v2/interfaces/HMSExceptionNotification",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSExceptionNotification",
        "keywords": [],
        "content": "  Hierarchy  BaseNotification   ↳    HMSExceptionNotification      Properties   data •   data  :   HMSException  (/api-reference/javascript/v2/interfaces/HMSException)     id •   id  : number    Inherited from BaseNotification.id     message •   message  : string    Inherited from BaseNotification.message     severity • Optional   severity  :   HMSNotificationSeverity  (/api-reference/javascript/v2/enums/HMSNotificationSeverity)    Inherited from BaseNotification.severity     type •   type  :   ERROR  (/api-reference/javascript/v2/enums/HMSNotificationTypes error)    Overrides BaseNotification.type "
    },
    {
        "title": "HMSHLS",
        "link": "/api-reference/javascript/v2/interfaces/HMSHLS",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSHLS",
        "keywords": [],
        "content": "  Properties   error • Optional   error  : HMSException     running •   running  : boolean     variants •   variants  :   HLSVariant  (/api-reference/javascript/v2/interfaces/HLSVariant)   "
    },
    {
        "title": "HMSLeaveRoomRequest",
        "link": "/api-reference/javascript/v2/interfaces/HMSLeaveRoomRequest",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSLeaveRoomRequest",
        "keywords": [],
        "content": "  Properties   reason •   reason  : string     requestedBy • Optional   requestedBy  :   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)     roomEnded •   roomEnded  : boolean "
    },
    {
        "title": "HMSLeaveRoomRequestNotification",
        "link": "/api-reference/javascript/v2/interfaces/HMSLeaveRoomRequestNotification",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSLeaveRoomRequestNotification",
        "keywords": [],
        "content": "  Hierarchy  BaseNotification   ↳    HMSLeaveRoomRequestNotification      Properties   data •   data  :   HMSLeaveRoomRequest  (/api-reference/javascript/v2/interfaces/HMSLeaveRoomRequest)     id •   id  : number    Inherited from BaseNotification.id     message •   message  : string    Inherited from BaseNotification.message     severity • Optional   severity  :   HMSNotificationSeverity  (/api-reference/javascript/v2/enums/HMSNotificationSeverity)    Inherited from BaseNotification.severity     type •   type  :   ROOM_ENDED  (/api-reference/javascript/v2/enums/HMSNotificationTypes room_ended)    REMOVED_FROM_ROOM  (/api-reference/javascript/v2/enums/HMSNotificationTypes removed_from_room)    Overrides BaseNotification.type "
    },
    {
        "title": "HMSLocalTrackStats",
        "link": "/api-reference/javascript/v2/interfaces/HMSLocalTrackStats",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSLocalTrackStats",
        "keywords": [],
        "content": " Extends RTCOutboundRtpStreamStats Ref: https://www.w3.org/TR/webrtc-stats/ dom-rtcoutboundrtpstreamstats   Hierarchy  BaseTrackStats   MissingOutboundStats   ↳    HMSLocalTrackStats     ↳↳   HMSTrackStats  (/api-reference/javascript/v2/interfaces/HMSTrackStats)   Properties   bitrate •   bitrate  : number    Inherited from BaseTrackStats.bitrate     bytesSent • Optional   bytesSent  : number    Inherited from MissingOutboundStats.bytesSent     codec • Optional   codec  : string    Inherited from BaseTrackStats.codec     codecId • Optional   codecId  : string    Inherited from BaseTrackStats.codecId     firCount • Optional   firCount  : number    Inherited from MissingOutboundStats.firCount     frameHeight • Optional   frameHeight  : number    Inherited from MissingOutboundStats.frameHeight     frameWidth • Optional   frameWidth  : number    Inherited from MissingOutboundStats.frameWidth     framesEncoded • Optional   framesEncoded  : number    Inherited from MissingOutboundStats.framesEncoded     framesPerSecond • Optional   framesPerSecond  : number    Inherited from MissingOutboundStats.framesPerSecond     id •   id  : string    Inherited from BaseTrackStats.id     kind •   kind  : string    Inherited from BaseTrackStats.kind     nackCount • Optional   nackCount  : number    Inherited from MissingOutboundStats.nackCount     packetsSent • Optional   packetsSent  : number    Inherited from MissingOutboundStats.packetsSent     peerID • Optional   peerID  : string    Inherited from BaseTrackStats.peerID     peerName • Optional   peerName  : string    Inherited from BaseTrackStats.peerName     pliCount • Optional   pliCount  : number    Inherited from MissingOutboundStats.pliCount     qpSum • Optional   qpSum  : number    Inherited from MissingOutboundStats.qpSum     qualityLimitationReason • Optional   qualityLimitationReason  : string    Inherited from MissingOutboundStats.qualityLimitationReason     remote • Optional   remote  : RTCRemoteInboundRtpStreamStats &   packetsLostRate? : number   Stats perceived by the server(SFU) while receiving the local track sent by the peer Ref: https://www.w3.org/TR/webrtc-stats/ dom-rtcstatstype-remote-inbound-rtp https://www.w3.org/TR/webrtc-stats/ dom-rtcremoteinboundrtpstreamstats     remoteId • Optional   remoteId  : string    Inherited from MissingOutboundStats.remoteId     rid • Optional   rid  : RID    Inherited from MissingOutboundStats.rid     roundTripTime • Optional   roundTripTime  : number    Inherited from MissingOutboundStats.roundTripTime     ssrc •   ssrc  : number    Inherited from BaseTrackStats.ssrc     timestamp •   timestamp  : number    Inherited from BaseTrackStats.timestamp     totalRoundTripTime • Optional   totalRoundTripTime  : number    Inherited from MissingOutboundStats.totalRoundTripTime     transportId • Optional   transportId  : string    Inherited from BaseTrackStats.transportId     type •   type  : RTCStatsType    Inherited from BaseTrackStats.type "
    },
    {
        "title": "HMSMediaSettings",
        "link": "/api-reference/javascript/v2/interfaces/HMSMediaSettings",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSMediaSettings",
        "keywords": [],
        "content": "  Properties   audioInputDeviceId •   audioInputDeviceId  : string     audioOutputDeviceId • Optional   audioOutputDeviceId  : string     videoInputDeviceId •   videoInputDeviceId  : string "
    },
    {
        "title": "HMSMessage",
        "link": "/api-reference/javascript/v2/interfaces/HMSMessage",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSMessage",
        "keywords": [],
        "content": "  Properties   id •   id  : string     ignored •   ignored  : boolean  true if message will not be put it in store because it has been ignored     message •   message  : any     read •   read  : boolean     recipientPeer • Optional   recipientPeer  : string     recipientRoles • Optional   recipientRoles  : string        sender • Optional   sender  : string     senderName • Optional   senderName  : string     senderRole • Optional   senderRole  : string     senderUserId • Optional   senderUserId  : string     time •   time  : Date     type •   type  : string "
    },
    {
        "title": "HMSMessageNotification",
        "link": "/api-reference/javascript/v2/interfaces/HMSMessageNotification",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSMessageNotification",
        "keywords": [],
        "content": "  Hierarchy  BaseNotification   ↳    HMSMessageNotification      Properties   data •   data  :   HMSMessage  (/api-reference/javascript/v2/interfaces/HMSMessage)     id •   id  : number    Inherited from BaseNotification.id     message •   message  : string    Inherited from BaseNotification.message     severity • Optional   severity  :   HMSNotificationSeverity  (/api-reference/javascript/v2/enums/HMSNotificationSeverity)    Inherited from BaseNotification.severity     type •   type  :   NEW_MESSAGE  (/api-reference/javascript/v2/enums/HMSNotificationTypes new_message)    Overrides BaseNotification.type "
    },
    {
        "title": "HMSNotifications",
        "link": "/api-reference/javascript/v2/interfaces/HMSNotifications",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSNotifications",
        "keywords": [],
        "content": "  Methods   onNotification ▸   onNotification  < T >( cb , types? ): () => void  you can subscribe to notifications for new message, peer add etc. using this function. note that this is not meant to maintain any state on your side, as the reactive store already does that. The intent of this function is mainly to display toast notifications or send analytics. We'll provide a display message which can be displayed as it is for common cases.    Type parameters  Name  Type                                                     :   :                             T   extends   HMSNotificationTypeParam  (/api-reference/javascript/v2/home/content hmsnotificationtypeparam)     Parameters  Name    Type                                                :    :                           cb      HMSNotificationCallback  (/api-reference/javascript/v2/modules hmsnotificationcallback)< T >   types?   T                                                   Returns  fn  ▸ (): void  you can subscribe to notifications for new message, peer add etc. using this function. note that this is not meant to maintain any state on your side, as the reactive store already does that. The intent of this function is mainly to display toast notifications or send analytics. We'll provide a display message which can be displayed as it is for common cases.    Returns  void "
    },
    {
        "title": "HMSPeer",
        "link": "/api-reference/javascript/v2/interfaces/HMSPeer",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSPeer",
        "keywords": [],
        "content": " HMSPeer stores the details of individual participants in the room   Properties   audioTrack • Optional   audioTrack  : string     auxiliaryTracks •   auxiliaryTracks  : string        customerUserId • Optional   customerUserId  : string     id •   id  : string     isLocal •   isLocal  : boolean     isStarred • Optional   isStarred  : boolean     joinedAt • Optional   joinedAt  : Date     metadata • Optional   metadata  : string     name •   name  : string     roleName • Optional   roleName  : string     videoTrack • Optional   videoTrack  : string "
    },
    {
        "title": "HMSPeerListNotification",
        "link": "/api-reference/javascript/v2/interfaces/HMSPeerListNotification",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSPeerListNotification",
        "keywords": [],
        "content": "  Hierarchy  BaseNotification   ↳    HMSPeerListNotification      Properties   data •   data  :   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)       id •   id  : number    Inherited from BaseNotification.id     message •   message  : string    Inherited from BaseNotification.message     severity • Optional   severity  :   HMSNotificationSeverity  (/api-reference/javascript/v2/enums/HMSNotificationSeverity)    Inherited from BaseNotification.severity     type •   type  :   PEER_LIST  (/api-reference/javascript/v2/enums/HMSNotificationTypes peer_list)    Overrides BaseNotification.type "
    },
    {
        "title": "HMSPeerNotification",
        "link": "/api-reference/javascript/v2/interfaces/HMSPeerNotification",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSPeerNotification",
        "keywords": [],
        "content": "  Hierarchy  BaseNotification   ↳    HMSPeerNotification      Properties   data •   data  :   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)     id •   id  : number    Inherited from BaseNotification.id     message •   message  : string    Inherited from BaseNotification.message     severity • Optional   severity  :   HMSNotificationSeverity  (/api-reference/javascript/v2/enums/HMSNotificationSeverity)    Inherited from BaseNotification.severity     type •   type  :   PEER_JOINED  (/api-reference/javascript/v2/enums/HMSNotificationTypes peer_joined)    PEER_LEFT  (/api-reference/javascript/v2/enums/HMSNotificationTypes peer_left)    ROLE_UPDATED  (/api-reference/javascript/v2/enums/HMSNotificationTypes role_updated)    NAME_UPDATED  (/api-reference/javascript/v2/enums/HMSNotificationTypes name_updated)    METADATA_UPDATED  (/api-reference/javascript/v2/enums/HMSNotificationTypes metadata_updated)    Overrides BaseNotification.type "
    },
    {
        "title": "HMSPeerStats",
        "link": "/api-reference/javascript/v2/interfaces/HMSPeerStats",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSPeerStats",
        "keywords": [],
        "content": "  Properties   publish • Optional   publish  : RTCIceCandidatePairStats &   bitrate : number       subscribe • Optional   subscribe  : RTCIceCandidatePairStats &   bitrate : number ; jitter : number ; packetsLost : number ; packetsLostRate : number   "
    },
    {
        "title": "HMSPeerWithMuteStatus",
        "link": "/api-reference/javascript/v2/interfaces/HMSPeerWithMuteStatus",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSPeerWithMuteStatus",
        "keywords": [],
        "content": "  Properties   isAudioEnabled • Optional   isAudioEnabled  : boolean     peer •   peer  :   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer) "
    },
    {
        "title": "HMSPlaylist<T\\>",
        "link": "/api-reference/javascript/v2/interfaces/HMSPlaylist",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSPlaylist",
        "keywords": [],
        "content": "  Type parameters  Name   :    T     Properties   audio •   audio  : Object    Type declaration  Name       Type                                                    :     :                             currentTime   number                                                   list       Record < string ,   HMSPlaylistItem  (/api-reference/javascript/v2/interfaces/HMSPlaylistItem)< T > >   playbackRate   number                                                   progress     number                                                   selection      HMSPlaylistSelection  (/api-reference/javascript/v2/interfaces/HMSPlaylistSelection)            volume      number                                                      video •   video  : Object    Type declaration  Name       Type                                                    :     :                             currentTime   number                                                   list       Record < string ,   HMSPlaylistItem  (/api-reference/javascript/v2/interfaces/HMSPlaylistItem)< T > >   playbackRate   number                                                   progress     number                                                   selection      HMSPlaylistSelection  (/api-reference/javascript/v2/interfaces/HMSPlaylistSelection)            volume      number                                                  "
    },
    {
        "title": "HMSPlaylistItem<T\\>",
        "link": "/api-reference/javascript/v2/interfaces/HMSPlaylistItem",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSPlaylistItem",
        "keywords": [],
        "content": "  Type parameters  Name   :    T     Properties   duration • Optional   duration  : number     id •   id  : string     metadata • Optional   metadata  : T     name •   name  : string     playing •   playing  : boolean     selected •   selected  : boolean     type •   type  :   HMSPlaylistType  (/api-reference/javascript/v2/enums/HMSPlaylistType)     url •   url  : string "
    },
    {
        "title": "HMSPlaylistItemNotification<T\\>",
        "link": "/api-reference/javascript/v2/interfaces/HMSPlaylistItemNotification",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSPlaylistItemNotification",
        "keywords": [],
        "content": "  Type parameters  Name   :    T     Hierarchy  BaseNotification   ↳    HMSPlaylistItemNotification      Properties   data •   data  :   HMSPlaylistItem  (/api-reference/javascript/v2/interfaces/HMSPlaylistItem)< T >     id •   id  : number    Inherited from BaseNotification.id     message •   message  : string    Inherited from BaseNotification.message     severity • Optional   severity  :   HMSNotificationSeverity  (/api-reference/javascript/v2/enums/HMSNotificationSeverity)    Inherited from BaseNotification.severity     type •   type  :   PLAYLIST_TRACK_ENDED  (/api-reference/javascript/v2/enums/HMSNotificationTypes playlist_track_ended)    Overrides BaseNotification.type "
    },
    {
        "title": "HMSPlaylistSelection",
        "link": "/api-reference/javascript/v2/interfaces/HMSPlaylistSelection",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSPlaylistSelection",
        "keywords": [],
        "content": "  Properties   hasNext •   hasNext  : boolean     hasPrevious •   hasPrevious  : boolean     id •   id  : string "
    },
    {
        "title": "HMSPlaylistSelector",
        "link": "/api-reference/javascript/v2/interfaces/HMSPlaylistSelector",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSPlaylistSelector",
        "keywords": [],
        "content": " Helpful selectors for audio and video playlist   Properties   currentTime •   currentTime  : ( store :   HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)) => number    Type declaration ▸ ( store ): number  returns the current time of the playlist in seconds    Parameters  Name   Type                                :   :                  store    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  number     list •   list  : <T >( store :   HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)) =>   HMSPlaylistItem  (/api-reference/javascript/v2/interfaces/HMSPlaylistItem)< T >      Type declaration ▸ < T >( store ):   HMSPlaylistItem  (/api-reference/javascript/v2/interfaces/HMSPlaylistItem)< T >   returns the playlist items list as set initially    Type parameters  Name   :    T      Parameters  Name   Type                                :   :                  store    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns   HMSPlaylistItem  (/api-reference/javascript/v2/interfaces/HMSPlaylistItem)< T >       playbackRate •   playbackRate  : ( store :   HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)) => number    Type declaration ▸ ( store ): number  returns the playback rate, a number between 0.25-2.0.    Parameters  Name   Type                                :   :                  store    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  number     progress •   progress  : ( store :   HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)) => number    Type declaration ▸ ( store ): number  returns the current progress percentage, a number between 0-100    Parameters  Name   Type                                :   :                  store    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  number     selectedItem •   selectedItem  : <T >( store :   HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)) =>   HMSPlaylistItem  (/api-reference/javascript/v2/interfaces/HMSPlaylistItem)< T >    Type declaration ▸ < T >( store ):   HMSPlaylistItem  (/api-reference/javascript/v2/interfaces/HMSPlaylistItem)< T > This returns playlist item for corresponding Id in selection    Type parameters  Name   :    T      Parameters  Name   Type                                :   :                  store    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns   HMSPlaylistItem  (/api-reference/javascript/v2/interfaces/HMSPlaylistItem)< T >     selection •   selection  : ( store :   HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)) =>   HMSPlaylistSelection  (/api-reference/javascript/v2/interfaces/HMSPlaylistSelection)    Type declaration ▸ ( store ):   HMSPlaylistSelection  (/api-reference/javascript/v2/interfaces/HMSPlaylistSelection) This returns playlist selection with  id, hasNext, hasPrev     Parameters  Name   Type                                :   :                  store    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns   HMSPlaylistSelection  (/api-reference/javascript/v2/interfaces/HMSPlaylistSelection)     volume •   volume  : ( store :   HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)) => number    Type declaration ▸ ( store ): number  returns the current volume the playlist is playing at, a number between 0-100    Parameters  Name   Type                                :   :                  store    HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore)     Returns  number "
    },
    {
        "title": "HMSPublishAllowed",
        "link": "/api-reference/javascript/v2/interfaces/HMSPublishAllowed",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSPublishAllowed",
        "keywords": [],
        "content": "  Properties   audio •   audio  : boolean     screen •   screen  : boolean     video •   video  : boolean "
    },
    {
        "title": "HMSReconnectionNotification",
        "link": "/api-reference/javascript/v2/interfaces/HMSReconnectionNotification",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSReconnectionNotification",
        "keywords": [],
        "content": "  Hierarchy  BaseNotification   ↳    HMSReconnectionNotification      Properties   data •   data  : null     id •   id  : number    Inherited from BaseNotification.id     message •   message  : string    Inherited from BaseNotification.message     severity • Optional   severity  :   HMSNotificationSeverity  (/api-reference/javascript/v2/enums/HMSNotificationSeverity)    Inherited from BaseNotification.severity     type •   type  :   RECONNECTING  (/api-reference/javascript/v2/enums/HMSNotificationTypes reconnecting)    RECONNECTED  (/api-reference/javascript/v2/enums/HMSNotificationTypes reconnected)    Overrides BaseNotification.type "
    },
    {
        "title": "HMSRecording",
        "link": "/api-reference/javascript/v2/interfaces/HMSRecording",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSRecording",
        "keywords": [],
        "content": "  Properties   browser •   browser  : Object    Type declaration  Name      Type        :     :      error?     HMSException    running    boolean      startedAt?   Date          hls •   hls  : HMSHLSRecording     server •   server  : Object    Type declaration  Name      Type        :     :      error?     HMSException    running    boolean      startedAt?   Date      "
    },
    {
        "title": "HMSRemoteTrackStats",
        "link": "/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats",
        "keywords": [],
        "content": " Extends RTCInboundRtpStreamStats Ref: https://www.w3.org/TR/webrtc-stats/ dom-rtcinboundrtpstreamstats   Hierarchy  BaseTrackStats   MissingInboundStats   ↳    HMSRemoteTrackStats     ↳↳   HMSTrackStats  (/api-reference/javascript/v2/interfaces/HMSTrackStats)   Properties   bitrate •   bitrate  : number    Inherited from BaseTrackStats.bitrate     bytesReceived • Optional   bytesReceived  : number    Inherited from MissingInboundStats.bytesReceived     codec • Optional   codec  : string    Inherited from BaseTrackStats.codec     codecId • Optional   codecId  : string    Inherited from BaseTrackStats.codecId     firCount • Optional   firCount  : number    Inherited from MissingInboundStats.firCount     frameHeight • Optional   frameHeight  : number    Inherited from MissingInboundStats.frameHeight     frameWidth • Optional   frameWidth  : number    Inherited from MissingInboundStats.frameWidth     framesDecoded • Optional   framesDecoded  : number    Inherited from MissingInboundStats.framesDecoded     framesDropped • Optional   framesDropped  : number    Inherited from MissingInboundStats.framesDropped     framesPerSecond • Optional   framesPerSecond  : number    Inherited from MissingInboundStats.framesPerSecond     id •   id  : string    Inherited from BaseTrackStats.id     jitter • Optional   jitter  : number    Inherited from MissingInboundStats.jitter     kind •   kind  : string    Inherited from BaseTrackStats.kind     nackCount • Optional   nackCount  : number    Inherited from MissingInboundStats.nackCount     packetsDiscarded • Optional   packetsDiscarded  : number    Inherited from MissingInboundStats.packetsDiscarded     packetsLost • Optional   packetsLost  : number    Inherited from MissingInboundStats.packetsLost     packetsLostRate • Optional   packetsLostRate  : number    Inherited from MissingInboundStats.packetsLostRate     packetsReceived • Optional   packetsReceived  : number    Inherited from MissingInboundStats.packetsReceived     peerID • Optional   peerID  : string    Inherited from BaseTrackStats.peerID     peerName • Optional   peerName  : string    Inherited from BaseTrackStats.peerName     pliCount • Optional   pliCount  : number    Inherited from MissingInboundStats.pliCount     qpSum • Optional   qpSum  : number    Inherited from MissingInboundStats.qpSum     remoteId • Optional   remoteId  : string    Inherited from MissingInboundStats.remoteId     ssrc •   ssrc  : number    Inherited from BaseTrackStats.ssrc     timestamp •   timestamp  : number    Inherited from BaseTrackStats.timestamp     transportId • Optional   transportId  : string    Inherited from BaseTrackStats.transportId     type •   type  : RTCStatsType    Inherited from BaseTrackStats.type "
    },
    {
        "title": "HMSRole",
        "link": "/api-reference/javascript/v2/interfaces/HMSRole",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSRole",
        "keywords": [],
        "content": "  Properties   name •   name  : string     permissions •   permissions  : Object    Type declaration  Name         Type     :      :     browserRecording   boolean    changeRole      boolean    endRoom       boolean    hlsStreaming     boolean    mute         boolean    removeOthers     boolean    rtmpStreaming    boolean    unmute        boolean      priority •   priority  : number     publishParams •   publishParams  : PublishParams     subscribeParams •   subscribeParams  : Object    Type declaration  Name           Type               :       :          maxSubsBitRate      number              subscribeDegradation?   SubscribeDegradationParams    subscribeToRoles     string              "
    },
    {
        "title": "HMSRoleChangeRequest",
        "link": "/api-reference/javascript/v2/interfaces/HMSRoleChangeRequest",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSRoleChangeRequest",
        "keywords": [],
        "content": "  Properties   requestedBy • Optional   requestedBy  :   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer)     role •   role  :   HMSRole  (/api-reference/javascript/v2/interfaces/HMSRole)     token •   token  : string "
    },
    {
        "title": "HMSRoleChangeStoreRequest",
        "link": "/api-reference/javascript/v2/interfaces/HMSRoleChangeStoreRequest",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSRoleChangeStoreRequest",
        "keywords": [],
        "content": "  Properties   requestedBy • Optional   requestedBy  : string     roleName •   roleName  : string     token •   token  : string "
    },
    {
        "title": "HMSRoom",
        "link": "/api-reference/javascript/v2/interfaces/HMSRoom",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSRoom",
        "keywords": [],
        "content": "  Properties   hls •   hls  :   HMSHLS  (/api-reference/javascript/v2/interfaces/HMSHLS)     id •   id  : string     isConnected • Optional   isConnected  : boolean     joinedAt • Optional   joinedAt  : Date     localPeer •   localPeer  : string     name •   name  : string     peerCount • Optional   peerCount  : number  if this number is available room.peers is not guaranteed to have all the peers.     peers •   peers  : string        recording •   recording  :   HMSRecording  (/api-reference/javascript/v2/interfaces/HMSRecording)     roomState •   roomState  :   HMSRoomState  (/api-reference/javascript/v2/enums/HMSRoomState)     rtmp •   rtmp  :   HMSRTMP  (/api-reference/javascript/v2/interfaces/HMSRTMP)     sessionId •   sessionId  : string     startedAt • Optional   startedAt  : Date "
    },
    {
        "title": "HMSRTMP",
        "link": "/api-reference/javascript/v2/interfaces/HMSRTMP",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSRTMP",
        "keywords": [],
        "content": "  Properties   error • Optional   error  : HMSException     running •   running  : boolean     startedAt • Optional   startedAt  : Date "
    },
    {
        "title": "HMSScreenAudioTrack",
        "link": "/api-reference/javascript/v2/interfaces/HMSScreenAudioTrack",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSScreenAudioTrack",
        "keywords": [],
        "content": "  Hierarchy    HMSAudioTrack  (/api-reference/javascript/v2/interfaces/HMSAudioTrack)  ↳    HMSScreenAudioTrack      Properties   deviceID • Optional   deviceID  : string    Inherited from  HMSAudioTrack (/api-reference/javascript/v2/interfaces/HMSAudioTrack). deviceID (/api-reference/javascript/v2/interfaces/HMSAudioTrack deviceid)     displayEnabled • Optional   displayEnabled  : boolean    Inherited from  HMSAudioTrack (/api-reference/javascript/v2/interfaces/HMSAudioTrack). displayEnabled (/api-reference/javascript/v2/interfaces/HMSAudioTrack displayenabled)     enabled •   enabled  : boolean    Inherited from  HMSAudioTrack (/api-reference/javascript/v2/interfaces/HMSAudioTrack). enabled (/api-reference/javascript/v2/interfaces/HMSAudioTrack enabled)     id •   id  : string    Inherited from  HMSAudioTrack (/api-reference/javascript/v2/interfaces/HMSAudioTrack). id (/api-reference/javascript/v2/interfaces/HMSAudioTrack id)     peerId • Optional   peerId  : string    Inherited from  HMSAudioTrack (/api-reference/javascript/v2/interfaces/HMSAudioTrack). peerId (/api-reference/javascript/v2/interfaces/HMSAudioTrack peerid)     plugins • Optional   plugins  : string       Inherited from  HMSAudioTrack (/api-reference/javascript/v2/interfaces/HMSAudioTrack). plugins (/api-reference/javascript/v2/interfaces/HMSAudioTrack plugins)     source •   source  : \"screen\"    Overrides  HMSAudioTrack (/api-reference/javascript/v2/interfaces/HMSAudioTrack). source (/api-reference/javascript/v2/interfaces/HMSAudioTrack source)     type •   type  : \"audio\"    Overrides  HMSAudioTrack (/api-reference/javascript/v2/interfaces/HMSAudioTrack). type (/api-reference/javascript/v2/interfaces/HMSAudioTrack type)     volume • Optional   volume  : number    Inherited from  HMSAudioTrack (/api-reference/javascript/v2/interfaces/HMSAudioTrack). volume (/api-reference/javascript/v2/interfaces/HMSAudioTrack volume) "
    },
    {
        "title": "HMSScreenShareConfig",
        "link": "/api-reference/javascript/v2/interfaces/HMSScreenShareConfig",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSScreenShareConfig",
        "keywords": [],
        "content": " Config to have control over screenshare being captured. Note that not all fields are supported on all browsers. Even when they're supported the fields acts as hints and the browser can override them.   Properties   audioOnly • Optional   audioOnly  : boolean  discard the video and only share audio track with others, useful for sharing music.    Default    false     cropElement • Optional   cropElement  : HTMLDivElement  used for region capture in screenshare, if the current tab is being screenshared the screenshare video track will be cropped to only this element. Will throw error if the element is not present in DOM.     cropTarget • Optional   cropTarget  : object  used for region capture in screenshare, the screenshare video track will be cropped to only the passed in cropTarget. This cropTarget must come from the tab which is being shared     displaySurface • Optional   displaySurface  : \"browser\"   \"monitor\"   \"window\"  preselect the relevant tab in screenshare menu browser  for preferring a browser tab window  for application window monitor  for full screen    Default    monitor     forceCurrentTab • Optional   forceCurrentTab  : boolean  show the current tab first in supported browser, throws error if user doesn't select current tab for sharing.    Default    false     preferCurrentTab • Optional   preferCurrentTab  : boolean  show the current tab first in supported browser, but don't throw error if user selects something else.    Default    false     selfBrowserSurface • Optional   selfBrowserSurface  : \"include\"   \"exclude\"  whether to show an option for sharing the current tab in the screen share prompt. Screen sharing current tab might lead to hall of mirrors effect. Default is exclude, if either of forceCurrentTab or preferCurrentTab are true, this is set to include.    Default    exclude     surfaceSwitching • Optional   surfaceSwitching  : \"include\"   \"exclude\"  whether to hint browser to show a \"share this tab instead\" option when tab is shared. Default is include, set to exclude if forceCurrentTab is true    Default    include     systemAudio • Optional   systemAudio  : \"include\"   \"exclude\"  whether to show option for sharing system level audio if full screen is being shared. Not applicable if isVideoOnly is true. Note that sharing system audio will cause echo if mic is on.    Default    exclude     videoOnly • Optional   videoOnly  : boolean  do not give an option to share audio while screen sharing.    Default    false "
    },
    {
        "title": "HMSScreenVideoTrack",
        "link": "/api-reference/javascript/v2/interfaces/HMSScreenVideoTrack",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSScreenVideoTrack",
        "keywords": [],
        "content": "  Hierarchy  Omit <  HMSVideoTrack  (/api-reference/javascript/v2/interfaces/HMSVideoTrack), \"facingMode\" >  ↳    HMSScreenVideoTrack      Properties   captureHandle • Optional   captureHandle  :   ScreenCaptureHandle  (/api-reference/javascript/v2/interfaces/ScreenCaptureHandle) this can be used to identify the shared tab, if the shared tab has set a captureHandle on its end as well as communicate with the tab for e.g. using broadcast channel.     degraded • Optional   degraded  : boolean    Inherited from Omit.degraded     deviceID • Optional   deviceID  : string    Inherited from Omit.deviceID     displayEnabled • Optional   displayEnabled  : boolean    Inherited from Omit.displayEnabled     displaySurface • Optional   displaySurface  :   HMSTrackDisplaySurface  (/api-reference/javascript/v2/home/content hmstrackdisplaysurface)     enabled •   enabled  : boolean    Inherited from Omit.enabled     height • Optional   height  : number    Inherited from Omit.height     id •   id  : string    Inherited from Omit.id     layer • Optional   layer  :   HMSSimulcastLayer  (/api-reference/javascript/v2/enums/HMSSimulcastLayer)    Inherited from Omit.layer     layerDefinitions • Optional   layerDefinitions  :   HMSSimulcastLayerDefinition  (/api-reference/javascript/v2/interfaces/HMSSimulcastLayerDefinition)      Inherited from Omit.layerDefinitions     peerId • Optional   peerId  : string    Inherited from Omit.peerId     plugins • Optional   plugins  : string       Inherited from Omit.plugins     preferredLayer • Optional   preferredLayer  :   HMSPreferredSimulcastLayer  (/api-reference/javascript/v2/modules hmspreferredsimulcastlayer)    Inherited from Omit.preferredLayer     source •   source  : \"screen\"    Overrides Omit.source     type •   type  : \"video\"    Inherited from Omit.type     width • Optional   width  : number    Inherited from Omit.width "
    },
    {
        "title": "HMSSimulcastLayerDefinition",
        "link": "/api-reference/javascript/v2/interfaces/HMSSimulcastLayerDefinition",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSSimulcastLayerDefinition",
        "keywords": [],
        "content": "  Properties   layer •   layer  :   HMSPreferredSimulcastLayer  (/api-reference/javascript/v2/home/content hmspreferredsimulcastlayer)     resolution •   resolution  : SimulcastResolution "
    },
    {
        "title": "HMSSpeaker",
        "link": "/api-reference/javascript/v2/interfaces/HMSSpeaker",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSSpeaker",
        "keywords": [],
        "content": " HMS Speaker stores the details of peers speaking at any point of time along with their audio levels. This can be used to current speakers or highlight videotiles.   Properties   audioLevel •   audioLevel  : number     peerID •   peerID  : string     trackID •   trackID  : string "
    },
    {
        "title": "HMSStatsStore",
        "link": "/api-reference/javascript/v2/interfaces/HMSStatsStore",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSStatsStore",
        "keywords": [],
        "content": "  Properties   localPeer •   localPeer  : Object    Type declaration  Name      Type     :     :     audioTrack?   string    id       string    videoTrack?   string      localTrackStats •   localTrackStats  : Record < string , undefined     HMSTrackStats  (/api-reference/javascript/v2/interfaces/HMSTrackStats)   >     peerStats •   peerStats  : Record < string , undefined     HMSPeerStats  (/api-reference/javascript/v2/interfaces/HMSPeerStats) >     remoteTrackStats •   remoteTrackStats  : Record < string , undefined     HMSTrackStats  (/api-reference/javascript/v2/interfaces/HMSTrackStats) > "
    },
    {
        "title": "HMSStatsStoreWrapper",
        "link": "/api-reference/javascript/v2/interfaces/HMSStatsStoreWrapper",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSStatsStoreWrapper",
        "keywords": [],
        "content": "  Hierarchy  IStoreReadOnly <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore) >  ↳    HMSStatsStoreWrapper      Properties   getState •   getState  : GetState <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore) > Get a part of store using a selector which is true at the current point of time. Usage: store.getState(selectDominantSpeaker);    Inherited from IStoreReadOnly.getState     subscribe •   subscribe  : Subscribe <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore) > Subscribe to a part of store using selectors, whenever the subscribed part changes, the callback is called with both the latest and previous value of the changed part. Usage:   const onSpeakerUpdate = (speaker, prevSpeaker) =>   console.log(\"speaker changed from  \", prevSpeaker, \", to  \", speaker);   store.subscribe(onSpeakerUpdate, selectDominantSpeaker);      Inherited from IStoreReadOnly.subscribe "
    },
    {
        "title": "HMSStore",
        "link": "/api-reference/javascript/v2/interfaces/HMSStore",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSStore",
        "keywords": [],
        "content": "  Properties   appData • Optional   appData  : Record < string , any >     connectionQualities •   connectionQualities  : Record < string , HMSConnectionQuality >     devices •   devices  :   DeviceMap  (/api-reference/javascript/v2/interfaces/DeviceMap)     errors •   errors  :   HMSException  (/api-reference/javascript/v2/interfaces/HMSException)       messages •   messages  : Object    Type declaration  Name    Type                                            :    :                         allIDs   string                                             byID    Record < string ,   HMSMessage  (/api-reference/javascript/v2/interfaces/HMSMessage) >      peers •   peers  : Record < string ,   HMSPeer  (/api-reference/javascript/v2/interfaces/HMSPeer) >     playlist •   playlist  :   HMSPlaylist  (/api-reference/javascript/v2/interfaces/HMSPlaylist)< any >     roleChangeRequests •   roleChangeRequests  :   HMSRoleChangeStoreRequest  (/api-reference/javascript/v2/interfaces/HMSRoleChangeStoreRequest)       roles •   roles  : Record < string ,   HMSRole  (/api-reference/javascript/v2/interfaces/HMSRole) >     room •   room  :   HMSRoom  (/api-reference/javascript/v2/interfaces/HMSRoom)     sessionMetadata • Optional   sessionMetadata  : any     settings •   settings  :   HMSMediaSettings  (/api-reference/javascript/v2/interfaces/HMSMediaSettings)     speakers •   speakers  : Record < string ,   HMSSpeaker  (/api-reference/javascript/v2/interfaces/HMSSpeaker) >     tracks •   tracks  : Record < string ,   HMSTrack  (/api-reference/javascript/v2/home/content hmstrack) > "
    },
    {
        "title": "HMSStoreWrapper",
        "link": "/api-reference/javascript/v2/interfaces/HMSStoreWrapper",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSStoreWrapper",
        "keywords": [],
        "content": " HMS store can be used to:  Get a part of the current store or state(getState)  Subscribe to different parts of the store using selectors and execute a callback when the value changes. Both getState and subscribe use selectors to query a part the store. Selectors are functions with HMSStore as an argument and returns a part of the store.   StoreSelector   is a type alias for this type of function.   Hierarchy  IStoreReadOnly <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore) >  ↳    HMSStoreWrapper      Properties   getState •   getState  : GetState <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore) > Get a part of store using a selector which is true at the current point of time. Usage: store.getState(selectDominantSpeaker);    Inherited from IStoreReadOnly.getState     subscribe •   subscribe  : Subscribe <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore) > Subscribe to a part of store using selectors, whenever the subscribed part changes, the callback is called with both the latest and previous value of the changed part. Usage:   const onSpeakerUpdate = (speaker, prevSpeaker) =>   console.log(\"speaker changed from  \", prevSpeaker, \", to  \", speaker);   store.subscribe(onSpeakerUpdate, selectDominantSpeaker);      Inherited from IStoreReadOnly.subscribe "
    },
    {
        "title": "HMSTrackNotification",
        "link": "/api-reference/javascript/v2/interfaces/HMSTrackNotification",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSTrackNotification",
        "keywords": [],
        "content": "  Hierarchy  BaseNotification   ↳    HMSTrackNotification      Properties   data •   data  :   HMSTrack  (/api-reference/javascript/v2/home/content hmstrack)     id •   id  : number    Inherited from BaseNotification.id     message •   message  : string    Inherited from BaseNotification.message     severity • Optional   severity  :   HMSNotificationSeverity  (/api-reference/javascript/v2/enums/HMSNotificationSeverity)    Inherited from BaseNotification.severity     type •   type  :   TRACK_ADDED  (/api-reference/javascript/v2/enums/HMSNotificationTypes track_added)    TRACK_REMOVED  (/api-reference/javascript/v2/enums/HMSNotificationTypes track_removed)    TRACK_MUTED  (/api-reference/javascript/v2/enums/HMSNotificationTypes track_muted)    TRACK_UNMUTED  (/api-reference/javascript/v2/enums/HMSNotificationTypes track_unmuted)    TRACK_DEGRADED  (/api-reference/javascript/v2/enums/HMSNotificationTypes track_degraded)    TRACK_RESTORED  (/api-reference/javascript/v2/enums/HMSNotificationTypes track_restored)    TRACK_DESCRIPTION_CHANGED  (/api-reference/javascript/v2/enums/HMSNotificationTypes track_description_changed)    Overrides BaseNotification.type "
    },
    {
        "title": "HMSTrackStats",
        "link": "/api-reference/javascript/v2/interfaces/HMSTrackStats",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSTrackStats",
        "keywords": [],
        "content": " Extends RTCOutboundRtpStreamStats Ref: https://www.w3.org/TR/webrtc-stats/ dom-rtcoutboundrtpstreamstats   Hierarchy    HMSLocalTrackStats  (/api-reference/javascript/v2/interfaces/HMSLocalTrackStats)    HMSRemoteTrackStats  (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats)  ↳    HMSTrackStats      Properties   bitrate •   bitrate  : number    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). bitrate (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats bitrate)     bytesReceived • Optional   bytesReceived  : number    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). bytesReceived (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats bytesreceived)     bytesSent • Optional   bytesSent  : number    Inherited from  HMSLocalTrackStats (/api-reference/javascript/v2/interfaces/HMSLocalTrackStats). bytesSent (/api-reference/javascript/v2/interfaces/HMSLocalTrackStats bytessent)     codec • Optional   codec  : string    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). codec (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats codec)     codecId • Optional   codecId  : string    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). codecId (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats codecid)     firCount • Optional   firCount  : number    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). firCount (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats fircount)     frameHeight • Optional   frameHeight  : number    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). frameHeight (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats frameheight)     frameWidth • Optional   frameWidth  : number    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). frameWidth (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats framewidth)     framesDecoded • Optional   framesDecoded  : number    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). framesDecoded (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats framesdecoded)     framesDropped • Optional   framesDropped  : number    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). framesDropped (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats framesdropped)     framesEncoded • Optional   framesEncoded  : number    Inherited from  HMSLocalTrackStats (/api-reference/javascript/v2/interfaces/HMSLocalTrackStats). framesEncoded (/api-reference/javascript/v2/interfaces/HMSLocalTrackStats framesencoded)     framesPerSecond • Optional   framesPerSecond  : number    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). framesPerSecond (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats framespersecond)     id •   id  : string    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). id (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats id)     jitter • Optional   jitter  : number    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). jitter (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats jitter)     kind •   kind  : string    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). kind (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats kind)     nackCount • Optional   nackCount  : number    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). nackCount (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats nackcount)     packetsDiscarded • Optional   packetsDiscarded  : number    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). packetsDiscarded (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats packetsdiscarded)     packetsLost • Optional   packetsLost  : number    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). packetsLost (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats packetslost)     packetsLostRate • Optional   packetsLostRate  : number    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). packetsLostRate (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats packetslostrate)     packetsReceived • Optional   packetsReceived  : number    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). packetsReceived (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats packetsreceived)     packetsSent • Optional   packetsSent  : number    Inherited from  HMSLocalTrackStats (/api-reference/javascript/v2/interfaces/HMSLocalTrackStats). packetsSent (/api-reference/javascript/v2/interfaces/HMSLocalTrackStats packetssent)     peerID • Optional   peerID  : string    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). peerID (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats peerid)     peerName • Optional   peerName  : string    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). peerName (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats peername)     pliCount • Optional   pliCount  : number    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). pliCount (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats plicount)     qpSum • Optional   qpSum  : number    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). qpSum (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats qpsum)     qualityLimitationReason • Optional   qualityLimitationReason  : string    Inherited from  HMSLocalTrackStats (/api-reference/javascript/v2/interfaces/HMSLocalTrackStats). qualityLimitationReason (/api-reference/javascript/v2/interfaces/HMSLocalTrackStats qualitylimitationreason)     remote • Optional   remote  : RTCRemoteInboundRtpStreamStats &   packetsLostRate? : number   Stats perceived by the server(SFU) while receiving the local track sent by the peer Ref: https://www.w3.org/TR/webrtc-stats/ dom-rtcstatstype-remote-inbound-rtp https://www.w3.org/TR/webrtc-stats/ dom-rtcremoteinboundrtpstreamstats    Inherited from  HMSLocalTrackStats (/api-reference/javascript/v2/interfaces/HMSLocalTrackStats). remote (/api-reference/javascript/v2/interfaces/HMSLocalTrackStats remote)     remoteId • Optional   remoteId  : string    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). remoteId (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats remoteid)     rid • Optional   rid  : RID    Inherited from  HMSLocalTrackStats (/api-reference/javascript/v2/interfaces/HMSLocalTrackStats). rid (/api-reference/javascript/v2/interfaces/HMSLocalTrackStats rid)     roundTripTime • Optional   roundTripTime  : number    Inherited from  HMSLocalTrackStats (/api-reference/javascript/v2/interfaces/HMSLocalTrackStats). roundTripTime (/api-reference/javascript/v2/interfaces/HMSLocalTrackStats roundtriptime)     ssrc •   ssrc  : number    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). ssrc (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats ssrc)     timestamp •   timestamp  : number    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). timestamp (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats timestamp)     totalRoundTripTime • Optional   totalRoundTripTime  : number    Inherited from  HMSLocalTrackStats (/api-reference/javascript/v2/interfaces/HMSLocalTrackStats). totalRoundTripTime (/api-reference/javascript/v2/interfaces/HMSLocalTrackStats totalroundtriptime)     transportId • Optional   transportId  : string    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). transportId (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats transportid)     type •   type  : RTCStatsType    Inherited from  HMSRemoteTrackStats (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats). type (/api-reference/javascript/v2/interfaces/HMSRemoteTrackStats type) "
    },
    {
        "title": "HMSVideoTrack",
        "link": "/api-reference/javascript/v2/interfaces/HMSVideoTrack",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSVideoTrack",
        "keywords": [],
        "content": "  Hierarchy  BaseTrack   ↳    HMSVideoTrack      Properties   degraded • Optional   degraded  : boolean     deviceID • Optional   deviceID  : string    Inherited from BaseTrack.deviceID     displayEnabled • Optional   displayEnabled  : boolean    Inherited from BaseTrack.displayEnabled     enabled •   enabled  : boolean    Inherited from BaseTrack.enabled     facingMode • Optional   facingMode  :   HMSTrackFacingMode  (/api-reference/javascript/v2/home/content hmstrackfacingmode)     height • Optional   height  : number     id •   id  : string    Inherited from BaseTrack.id     layer • Optional   layer  :   HMSSimulcastLayer  (/api-reference/javascript/v2/enums/HMSSimulcastLayer)     layerDefinitions • Optional   layerDefinitions  :   HMSSimulcastLayerDefinition  (/api-reference/javascript/v2/interfaces/HMSSimulcastLayerDefinition)       peerId • Optional   peerId  : string    Inherited from BaseTrack.peerId     plugins • Optional   plugins  : string       Inherited from BaseTrack.plugins     preferredLayer • Optional   preferredLayer  :   HMSPreferredSimulcastLayer  (/api-reference/javascript/v2/modules hmspreferredsimulcastlayer)     source •   source  : string    Overrides BaseTrack.source     type •   type  : \"video\"    Overrides BaseTrack.type     width • Optional   width  : number "
    },
    {
        "title": "HMSVideoTrackSettings",
        "link": "/api-reference/javascript/v2/interfaces/HMSVideoTrackSettings",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/HMSVideoTrackSettings",
        "keywords": [],
        "content": "  Properties   advanced • Optional   advanced  : MediaTrackConstraintSet        codec • Optional   codec  : HMSVideoCodec     deviceId • Optional   deviceId  : string     height • Optional   height  : number     maxBitrate • Optional   maxBitrate  : number     maxFramerate • Optional   maxFramerate  : number     width • Optional   width  : number "
    },
    {
        "title": "IHMSPlaylistActions",
        "link": "/api-reference/javascript/v2/interfaces/IHMSPlaylistActions",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/IHMSPlaylistActions",
        "keywords": [],
        "content": "  Methods   clearList ▸   clearList  (): Promise < void >    Returns  Promise < void >     pause ▸   pause  (): Promise < void > Pauses current playing item    Returns  Promise < void >     play ▸   play  ( id ): Promise < void > Pass the id of the item to be played    Parameters  Name  Type    Description       :   :    :        id   string  id of playlist item     Returns  Promise < void >     playNext ▸   playNext  (): Promise < void > PlayNext    Returns  Promise < void >     playPrevious ▸   playPrevious  (): Promise < void > PlayPrevious    Returns  Promise < void >     removeItem ▸   removeItem  ( id ): Promise < boolean >    Parameters  Name  Type     :   :     id   string     Returns  Promise < boolean >     seek ▸   seek  ( seekValue ): void  seek passing seekValue  this is relative to current position    Parameters  Name     Type    Description                                  :    :    :                     seekValue   number  number in seconds to move forwards(pass negative values to move backwards)     Returns  void     seekTo ▸   seekTo  ( seekValue ): void  seek passing seekValue  seekValue will be absolute    Parameters  Name     Type    Description                               :    :    :                    seekValue   number  value in seconds of absolute position in the playlist item duration     Returns  void     setIsAutoplayOn ▸   setIsAutoplayOn  ( autoplay ): void  set whether to autoplay next item in playlist after the current one ends    Parameters  Name     Type     :    :     autoplay   boolean     Returns  void     setList ▸   setList  < T >( list ): void  pass list to set playlist    Type parameters  Name   :    T      Parameters  Name   Type                                          Description      :   :                       :       list    HMSPlaylistItem  (/api-reference/javascript/v2/interfaces/HMSPlaylistItem)< T >    of playlist items     Returns  void     setPlaybackRate ▸   setPlaybackRate  ( playbackRate ): void  Control the playback speed  1.0 being normal, less than 1.0 will play it slowly and more than 1.0 will play it faster.    Parameters  Name       Type    Description         :     :    :         playbackRate   number  value from 0.25 and 2.0     Returns  void     setVolume ▸   setVolume  ( volume ): void  set volume passing volume    Parameters  Name    Type    Description       :    :    :        volume   number  number between 0-100     Returns  void     stop ▸   stop  (): Promise < void > Stop the current playback and remove the tracks    Returns  Promise < void > "
    },
    {
        "title": "IHMSStatsStore",
        "link": "/api-reference/javascript/v2/interfaces/IHMSStatsStore",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/IHMSStatsStore",
        "keywords": [],
        "content": "  Hierarchy  IStore <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore) >  ↳    IHMSStatsStore      Properties   destroy •   destroy  : Destroy    Inherited from IStore.destroy     getState •   getState  : GetState <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore) > Get a part of store using a selector which is true at the current point of time. Usage: store.getState(selectDominantSpeaker);    Inherited from IStore.getState     setState •   setState  : SetState <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore) >    Inherited from IStore.setState     subscribe •   subscribe  : Subscribe <  HMSStatsStore  (/api-reference/javascript/v2/interfaces/HMSStatsStore) > Subscribe to a part of store using selectors, whenever the subscribed part changes, the callback is called with both the latest and previous value of the changed part. Usage:   const onSpeakerUpdate = (speaker, prevSpeaker) =>   console.log(\"speaker changed from  \", prevSpeaker, \", to  \", speaker);   store.subscribe(onSpeakerUpdate, selectDominantSpeaker);      Inherited from IStore.subscribe "
    },
    {
        "title": "IHMSStore",
        "link": "/api-reference/javascript/v2/interfaces/IHMSStore",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/IHMSStore",
        "keywords": [],
        "content": " HMS Reactive store can be used to subscribe to different parts of the store using selectors and get a callback when the value changes.   Hierarchy  IStore <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore) >  ↳    IHMSStore      Properties   destroy •   destroy  : Destroy    Inherited from IStore.destroy     getState •   getState  : GetState <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore) > Get a part of store using a selector which is true at the current point of time. Usage: store.getState(selectDominantSpeaker);    Inherited from IStore.getState     setState •   setState  : SetState <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore) >    Inherited from IStore.setState     subscribe •   subscribe  : Subscribe <  HMSStore  (/api-reference/javascript/v2/interfaces/HMSStore) > Subscribe to a part of store using selectors, whenever the subscribed part changes, the callback is called with both the latest and previous value of the changed part. The subscribe also returns a reference to unsubscribe , which could be used to unsubscribe the subscription. Usage:   const onSpeakerUpdate = (speaker, prevSpeaker) =>   console.log(\"speaker changed from  \", prevSpeaker, \", to  \", speaker);   // The subscribe returns a reference to unsubscribe() which you can use to // ubsubscribe from the subscription in the future. const unsubscribe = store.subscribe(onSpeakerUpdate, selectDominantSpeaker); // unsubscribe from the listener. unsubscribe();      Inherited from IStore.subscribe "
    },
    {
        "title": "RTMPRecordingConfig",
        "link": "/api-reference/javascript/v2/interfaces/RTMPRecordingConfig",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/RTMPRecordingConfig",
        "keywords": [],
        "content": "  Properties   meetingURL •   meetingURL  : string     record •   record  : boolean     resolution • Optional   resolution  : RTMPRecordingResolution     rtmpURLs • Optional   rtmpURLs  : string    "
    },
    {
        "title": "ScreenCaptureHandle",
        "link": "/api-reference/javascript/v2/interfaces/ScreenCaptureHandle",
        "platformName": "JavaScript",
        "objectID": "/api-reference/javascript/v2/interfaces/ScreenCaptureHandle",
        "keywords": [],
        "content": "  Properties   exposeOrigin •   exposeOrigin  : boolean     handle •   handle  : string "
    },
    {
        "title": "Adaptive Bitrate (Simulcast)",
        "link": "/flutter/v2/advanced-features/adaptive-bitrate",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/advanced-features/adaptive-bitrate",
        "keywords": [],
        "content": "   Simulcast enables  Adaptive Bitrate (../foundation/adaptive-bitrate) (ABR) in video conferencing scenarios. This means 100ms SDKs can upgrade or downgrade video quality for remote video tracks based on user preferences or network conditions. To learn more about how Adaptive Bitrate enhances your applications refer the guide  here (../foundation/adaptive-bitrate). In this guide, we'll see how to customize Simulcast settings from your apps using the 100ms Flutter SDK.   Minimum Requirements   Flutter SDK version 1.2.0 or above   Simulcast enabled in Room Template   Simulcast APIs You can interact with the simulcast feature by using the setSimulcastLayer method of HMSRemoteVideoTrack . By default, on all Remote Video Tracks the layer is set to high i.e HMSSimulcastLayer.high   High layer implies that the 100ms SDK tries to fetch the highest quality video available for a Remote peer.  Let's look at the Remote Video Track interface showing the available APIs to customize Simulcast behaviour    dart ///  HMSRemoteVideoTrack  encapsulates the remote peer video track information class HMSRemoteVideoTrack    /// Invoke the function with the desired simulcast layer which can be of types   HMSSimulcastLayer.high ,  HMSSimulcastLayer.low ,  HMSSimulcastLayer.mid   Future<HMSException?> setSimulcastLayer(HMSSimulcastLayer layer)   /// Invoke the function to get the current Simulcast layer of the Remote Video Track  Future<HMSSimulcastLayer> getLayer()  \t/// Invoke the function to get the list of available Simulcast Layers for a Remote Video Track  Future<List<HMSSimulcastLayerDefinition>> getLayerDefinition()      The Simulcast layer definitions fetched from getLayerDefinition() method is as shown below    dart /// HMSSimulcastLayerDefinition contains available Simulcast layer and resolution class HMSSimulcastLayerDefinition(  /// HMSSimulcastLayer layer enum can be of types  high, mid, low  HMSSimulcastLayer hmsSimulcastLayer;  /// HMSResolution defines the width and height of the Video Track  HMSResolution hmsResolution; ) /// the available types of Simulcast layers which imply High, Medium or Low Video quality enum HMSSimulcastLayer   high, mid, low       setSimulcastLayer() : Use this method to set a custom Simulcast Layer for the Remote Video Track  getLayer() : This method is the currently selected Simulcast layer for the Video Track  getLayerDefinition() : This method returns a list of currently available layers for the Video Track  HMSSimulcastLayerDefinition : The Simulcast layer definition class you get its resolution and the layer name such as high/mid/low.   Disabling Auto Simulcast HMSVideoView has Automatic Simulcast Layer selection capability which is enabled if Adaptive Bitrate is enabled in the Room Template. ABR ensures that every participant is able to consume the highest possible quality video in conferencing or streaming use-cases, based on their bandwidth constraints. In addition to network, ABR can also optimise for the right video quality based on the size of the video element. For example, a video call running on active speaker layout has larger video tiles that require higher quality video track. These adjustments can be made dynamically with adaptive bitrate.  It will select a layer that best matches the current view frame size and reacts to frame updates. In case manual layer selection is preferred set disableAutoSimulcastLayerSelect property to true . By default, the disableAutoSimulcastLayerSelect property is false which means that an appropriate quality video track will be rendered for the HMSVideoView widget.   dart HMSVideoView(  track: remoteVideoTrack,  disableAutoSimulcastLayerSelect: true, // set value as true to Disable Automatic Simulcast Layer Selection of Video Track  key: Key(data.item1 .trackId), )    To learn more about Rendering Video refer the guide  here (../features/render-video).   Using Manual Simulcast APIs   Get Current Layer Here's how to get the current layer of remote video track:   dart  HMSRemoteVideoTrack remoteVideoTrack; // use the remote video track from the onTrackUpdate listener  HMSSimulcastLayer currentLayer = remoteVideoTrack.getLayer();     Get all Available Layers Here's how to get the all available layers with their resolution:   dart  HMSRemoteVideoTrack remoteVideoTrack; // use the remote video track from the onTrackUpdate listener  List<HMSSimulcastLayerDefinition> availableLayers = remoteVideoTrack.getLayerDefinition();     Change Current Layer If you want to can change layer manually then follow the follwing steps:   dart  HMSRemoteVideoTrack remoteVideoTrack;  List<HMSSimulcastLayerDefinition> availableLayers = remoteVideoTrack.getLayerDefinition();  // availableLayers will contains all the layers which remote peer is publishing.  // In this example we are taking first layer in availableLayers list.  HMSSimulcastLayer firstLayer = availableLayers 0 .hmsSimulcastLayer;  // set layer by passing it to setSimulcastLayer function.  remoteVideoTrack.setSimulcastLayer(firstLayer);   "
    },
    {
        "title": "Echo Cancellation\r",
        "link": "/flutter/v2/advanced-features/echo-cancellation",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/advanced-features/echo-cancellation",
        "keywords": [],
        "content": " -\r \r The 100ms Flutter SDK automatically applies the best-known settings to cancel echos from devices. However, some android devices have issues with their hardware echo cancellation and benefit from relying only on software for it.\r \r While we continually update the SDK with the latest known devices which have the issue, you may run into one we haven't tested with yet.\r \r If you find that a certain device echoes when it joins a meeting despite this, turning off its hardware echo cancellation may solve the problem. This is because if hardware echo cancellation is supported by the device, the SDK prefers hardware over software.\r \r Echo cancellation settings can only be applied before a meeting a joined. The 100ms Flutter SDK provides a way to implement this in the HMSSDK constructor before calling the build method.\r \r > 💡 Note this option must selectively be turned on for devices based on their  models (https://developer.android.com/reference/android/os/Build MODEL) disabling hardware echo cancellation for all devices will result in other devices echoing which didn't before.\r \r Let's see how you can enable echo cancellation:\r \r  dart\r HMSAudioTrackSetting audioTrackSetting = HMSAudioTrackSetting(useHardwareAcousticEchoCanceler:false);\r \r HMSTrackSetting hmsTrackSetting = HMSTrackSetting(audioTrackSetting: audioTrackSetting);\r \r HMSSDK hmsSDK = HMSSDK(hmsTrackSetting: hmsTrackSetting);\r  \r "
    },
    {
        "title": "Getter Methods",
        "link": "/flutter/v2/advanced-features/get-methods",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/advanced-features/get-methods",
        "keywords": [],
        "content": "  HMSSDK provides multiple Getter methods to read or get the data of the class fields which can be used to refresh data, update the stale state, re-render UI, etc Let's look at the methods one by one :   getRoom This is used to get info on the room that we are currently using. It returns HMSRoom? object as:   dart Future<HMSRoom?> getRoom() async     //returns null if we are not in a room   return await hmsSDK.getRoom();       getSessionMetadata This is used to get the currently set session metadata. This can be used to get some info attached to the session. This returns a String? as:   dart Future<String?> getSessionMetadata()     //returns null if no metadata is set   return hmsSDK.getSessionMetadata();      To set the session metadata setSessionMetadata method can be used.   getTrackSettings This is used to get the current track setting of the room. This returns an object of HMSTrackSetting as:   dart Future<HMSTrackSetting> getTrackSettings() async     return await hmsSDK.getTrackSettings();       getRoles This is used to get all the roles available in the current room. It returns a list of HMSRole objects as:   dart Future<List<HMSRole>> getRoles() async     return await hmsSDK.getRoles();       getLocalPeer This method helps in getting the local peer. It returns an object of HMSLocalPeer? object as:   dart Future<HMSLocalPeer?> getLocalPeer() async     //returns null if we haven't joined a room   return await hmsSDK.getLocalPeer();        getRemotePeers This method helps in getting the list of remote peers. It returns a list of HMSPeer objects as:   dart Future<List<HMSPeer>?> getRemotePeers() async     //returns null if we haven't joined a room   return await hmsSDK.getRemotePeers();        getPeers This method returns all the peers in the room.   dart Future<List<HMSPeer>?> getPeers() async     //returns null if we haven't joined a room   return await hmsSDK.getPeers();        getAudioDevicesList This is used to get a list of available audio devices. This returns a list of HMSAudioDevice as:   dart Future<List<HMSAudioDevice>> getAudioDevicesList() async     return await hmsSDK.getAudioDevicesList();        getCurrentAudioDevice This is used to get the currently selected audio device. This returns an object of HMSAudioDevice as:   dart Future<HMSAudioDevice> getCurrentAudioDevice() async     return await hmsSDK.getCurrentAudioDevice();     "
    },
    {
        "title": "Network Quality Reports\r",
        "link": "/flutter/v2/advanced-features/network-quality-reports",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/advanced-features/network-quality-reports",
        "keywords": [],
        "content": " -\r \r   Network Icons (https://www.100ms.live/docs/docs/v2/network-icons.png)\r \r Video/Audio conferencing is by nature a data-intensive operation. The 100ms SDK attempts to stabilize connections especially if subscribe degradation is turned on in the template but it's possible for really bad connections users will still have problems.\r \r It can be helpful to measure a user's connection speed before joining a room to set expectations or decide to have them join with video off etc.\r \r The 100ms SDK provides a way to measure the user's downlink speed in the preview screen.\r \r > ⚠️ The downlink speed is measured by having the user download a file (1MB as of this writing) after the WebSocket connection is established during a preview.\r \r The download will be continued for at most a fixed number of seconds (eg: 10 seconds) and the speed during that interval is calculated. The entire file may not be downloaded if it exceeds the timeout.\r \r DNS time is not counted in this, only throughput is measured.\r \r  Requesting/Reading a Network Quality Update in Preview\r When creating an HMSConfig object to request a preview, set the captureNetworkQualityInPreview to true to measure the user's downlink network quality.\r \r  dart\r class Preview implements HMSPreviewListener \r \r  ...\r  late HMSSDK hmsSDK;\r \r  Preview() \r    initHMSSDK();\r    \r \r   void initHMSSDK() async  \r    hmsSDK = HMSSDK();\r    await hmsSDK.build(); // ensure to await while invoking the build method\r    HMSConfig config = HMSConfig(\r     userName: \"John Doe\",\r     authToken: \"eyJH5c\",\r     //Setting this to true enables network quality reports in the preview\r     captureNetworkQualityInPreview: true,\r    );\r    hmsSDK.addPreviewListener(this);\r    hmsSDK.preview(config: config);\r    \r  \r  \r \r  Fetch Network quality reports \r \r When available, the information will be returned in onPeerUpdate of the HMSPreviewListener and HMSUpdateListener in the update type HMSPeerUpdate.networkQualityUpdated .\r It can be retrieved out of the HMSPeer object's networkQuality property.\r \r Here's the class definition of HMSNetworkQuality , which is a property on the HMSPeer .\r \r  dart\r class HMSNetworkQuality(\r   int quality;\r )\r  \r \r   In Preview\r \r We can find the updates through onPeerUpdate callback in HMSPreviewListener \r \r  dart\r class Preview implements HMSPreviewListener \r \r  ...\r  @override\r  void onPeerUpdate( required HMSPeer peer, required HMSPeerUpdate update )  \r    if (update == HMSPeerUpdate.networkQualityUpdated)  \r     print(\"Network Quality of $ peer.name  in Preview $ peer.networkQuality?.quality \");\r     \r    \r \r  \r  \r \r   In Room\r \r We can find the updates through onPeerUpdate callback in HMSUpdateListener \r \r  dart\r class Meeting implements HMSUpdateListener \r \r  ...\r \r  @override\r  void onPeerUpdate( required HMSPeer peer, required HMSPeerUpdate update )  \r    if (update == HMSPeerUpdate.networkQualityUpdated)  \r     print(\"Network Quality of $ peer.name  in Room $ peer.networkQuality?.quality \");\r     \r    \r \r  \r  \r \r  Interpreting the Values\r \r peer.networkQuality?.quality will be a value between -1 and 5.\r \r  -1 -> Network check failure\r  0 -> Very bad network or network check failure.\r  1 -> Poor network.\r  2 -> Bad network.\r  3 -> Average.\r  4 -> Good.\r  5 -> Best.\r "
    },
    {
        "title": "Persistent Participant States (Peer Metadata)\r",
        "link": "/flutter/v2/advanced-features/peer-metadata-update",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/advanced-features/peer-metadata-update",
        "keywords": [],
        "content": " -\r \r Looking for a persistent state that can be set on a peer and updated anytime, for everyone in the room? Peer metadata is it.\r \r Metadata can be set initially in the HMSConfig object that's passed into the join method. This can be used to display profile pictures as the user's avatar in meetings. You can imagine metadata as a persistent object attached to the peer which has more details about them.\r \r Peer Metadata can be changed after joining the Room as well. \r \r To set metadata before joining the room, pass the metadata property to HMSConfig used to Join the Room.\r \r  dart\r HMSConfig config = HMSConfig(userName: \"John Doe\",\r            authToken: \"eyJH5c\",\r            metadata: \" \"avatar\": \"location/on/amazon/storage\" \");\r  \r \r This section will show you how to:\r 1.  Get Peer Metadata ( get-peer-metadata).\r 2.  How to respond when a remote peer changes its metadata ( how-to-listen-to-updates-when-metadata-of-any-peer-is-updated).\r 3.  How to set a peer's metadata ( how-a-peer-can-set-its-metadata).\r \r \r  Get Peer metadata\r \r To get peer metadata, read the metadata value on any HMSPeer instance as:\r \r  dart\r void getPeerMetadata(HMSPeer peer) \r  //Setting String as nullable since metadata can be null as well\r  String? metadata = peer.metadata;\r  \r  \r \r  How to listen to updates when metadata of any peer is updated\r \r Whenever a remote peer's metadata is updated a callback will be received in onPeerUpdate( required HMSPeer peer, required HMSPeerUpdate update ) of HMSUpdateListener where the HMSPeerUpdate type will be HMSPeerUpdate.metadataChanged as:\r \r  dart \r class Meeting implements HMSUpdateListener, HMSActionResultListener \r \r  ...\r \r  @override\r  void onPeerUpdate(\r     required HMSPeer peer, required HMSPeerUpdate update ) async  \r \r     switch (update) \r      ...\r \r      case HMSPeerUpdate.metadataChanged:\r      //We can get the metadata from HMSPeer object\r      String? metadata = peer.metadata\r      break;\r \r      ...\r     \r      \r   \r  \r  \r \r When this callback is received the UI for that peer should be updated as well.\r \r  How a peer can set its metadata\r \r Here is how a peer can set their own metadata to a random string. In this case, the string is stringified JSON.\r \r Example String: \"  \"Zodiac Sign \": \"Virgo \"  \" \r \r  dart\r class Meeting implements HMSUpdateListener, HMSActionResultListener  \r \r   ... \r \r   void changeMetadata(\r       required String metadata,\r      required HMSActionResultListener hmsActionResultListener )  \r     /// metadata  is a stringified JSON as above\r     /// hmsActionResultListener : an instance of a class that implements HMSActionResultListener\r     //Here this is an instance of a class that implements HMSActionResultListener i.e. Meeting\r     hmsSDK.changeMetadata(\r       metadata: metadata, hmsActionResultListener: this);\r     \r \r   @override\r   void onSuccess(\r      HMSActionResultListenerMethod methodType =\r       HMSActionResultListenerMethod.unknown,\r     Map<String, dynamic>? arguments )  \r \r       switch (methodType)  \r \r       case HMSActionResultListenerMethod.changeMetadata:\r       //metadata updated successfully\r       break;\r       ...\r      \r    \r \r   @override\r   void onException(\r      HMSActionResultListenerMethod methodType =\r       HMSActionResultListenerMethod.unknown,\r     Map<String, dynamic>? arguments,\r     required HMSException hmsException )  \r \r       switch (methodType)  \r \r         case HMSActionResultListenerMethod.changeMetadata:\r         // Check the HMSException object for details about the error\r         break;\r         ...\r      \r    \r  \r  \r \r  Use cases \r \r   Implement hand raise in applications\r \r This is a very common requirement where the people attending a webinar or online class wish to ask a question they can let the\r speaker know by raising their hands in the application similar to what they would have done in an offline event \r \r Let's understand this with a diagram:\r \r   hand-raise (/docs/v2/flutter-raise-hand-flowchart.png)\r \r Let's check the implementation step-by-step:\r \r <div className=\"steps-container\">\r \r   PeerA calls changeMetadata\r \r Peer A calls changeMetadata with metadata as: \"  \"isHandRaised \":true \" since the metadata string is a stringified JSON. \r We can also use jsonEncode() method for this.\r \r  dart \r class Meeting implements HMSUpdateListener, HMSActionResultListener  \r \r   ...\r \r   String metadata = \"  \"isHandRaised \":true \";\r   hmsSDK.changeMetadata(\r     metadata: metadata, hmsActionResultListener: this);\r \r   @override\r   void onSuccess(\r      HMSActionResultListenerMethod methodType =\r       HMSActionResultListenerMethod.unknown,\r     Map<String, dynamic>? arguments )  \r \r       switch (methodType)  \r \r       case HMSActionResultListenerMethod.changeMetadata:\r       //Here we will get the update regarding changeMetadata method\r       break;\r       ...\r      \r    \r \r   @override\r   void onException(\r      HMSActionResultListenerMethod methodType =\r       HMSActionResultListenerMethod.unknown,\r     Map<String, dynamic>? arguments,\r     required HMSException hmsException )  \r \r       switch (methodType)  \r \r         case HMSActionResultListenerMethod.changeMetadata:\r         // Check the HMSException object for details about the error\r         break;\r         ...\r      \r    \r  \r  \r \r   PeerA receives onSuccess event\r \r This shows that the method call was successful.\r \r   All the peers receive onPeerUpdate\r \r As soon as we get onSuccess callback for changeMetadata method call, all the peers in the room will get onPeerUpdate callback\r with HMSPeerUpdate type as HMSPeerUpdate.metadataChanged .\r \r Let's see how we can update the UI using this:\r \r  dart \r class Meeting implements HMSUpdateListener, HMSActionResultListener \r \r  ...\r \r  @override\r  void onPeerUpdate(\r     required HMSPeer peer, required HMSPeerUpdate update ) async  \r \r     switch (update) \r      ...\r \r      case HMSPeerUpdate.metadataChanged:\r      //We can get the metadata from HMSPeer object\r      if(peer.metadata?.contains(\" \"isHandRaised \":true\")) \r       //This shows that peer has raised the hand\r       //UI can be updated by using the peer object\r       \r      break;\r \r      ...\r     \r      \r   \r  \r  \r \r </div>\r \r   Implement polls in room\r \r We can use peer's metadata property to implement polls similar to what we did above.\r \r There are many more use cases which can be implemented using peer metadata. Have any questions regarding the use cases please reach out to us  here (https://100ms.live/discord)\r "
    },
    {
        "title": "PIP Mode (Android Only)",
        "link": "/flutter/v2/advanced-features/pip-mode",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/advanced-features/pip-mode",
        "keywords": [],
        "content": "  100ms Flutter SDK provides support for creating Picture in Picture mode experience for video calls. PIP Mode lets the user watch the room video in a small window pinned to a corner of the screen while navigating between apps or browsing content on the main screen. Currently this functionality is only available on Android.   Minimum Requirements  Minimum version required to support PiP is Android 8.0 (API level 26)  Minimum hmssdk_flutter SDK version required is 1.0.0  To know more about PIP features minimum version requirements please check  Picture-in-picture (PiP) support (https://developer.android.com/develop/ui/views/picture-in-picture)   How to add PiP support <div className=\"steps-container\">    Update the manifest file You need to update the activity tag in the AndroidManifest.xml   xml <activity   ....   android:supportsPictureInPicture=\"true\"   android:configChanges=\"screenSize smallestScreenSize screenLayout orientation\"   ...       Check PIP availablility PIP mode only works above Android API Level 26(Android 8.0) but to check whether the current device supports PIP mode or not isPipAvailable method can be used:   dart hmsSDK.isPipAvailable();     Enter PIP mode To show PIP mode in the application:   dart // enterPipMode is a Future<bool> function where true indicates that the application has entered pip mode successfully. /// autoEnterPip  if true start pip mode will start automatically when app is minimized.For android 12 and above /// aspectRatio : List of double indicating ratio for PIP window as  width,height   bool isPipEnteredSuccesfully = await hmsSDK.enterPipMode(autoEnterPip: true/false, aspectRatio:  16, 9 );   </div>   How to check whether PIP is active To check whether PIP mode is currently active use isPipActive method as:   dart hmsSDK.isPipActive();   > To display UI according to PIP use the isPipActive to set the state for PIP mode and render the UI accordingly.   Checkout PIP in Action How PIP mode is implemented in our example app. Checkout here:  <video controls>  <source src=\"https://user-images.githubusercontent.com/93931528/205587304-772a5dd6-ed64-4d9e-8bb5-4fc3eed83bea.mp4 \" type=\"video/mp4\"    Your browser does not support the video tag. </video>  You can checkout the 100ms Flutter SDK Github repository which also contains the PIP implementation.  Example app implementation here (https://github.com/100mslive/100ms-flutter/tree/main/example).  "
    },
    {
        "title": "Set Track Settings (Video/Audio)",
        "link": "/flutter/v2/advanced-features/set-track-settings",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/advanced-features/set-track-settings",
        "keywords": [],
        "content": "nav: 4.01  Sometimes it is required to customize local peer's Audio & Video track settings while creating instances of 100ms SDK. These settings are optional parameters and are meant to be passed in the HMSSDK function as the hmsTrackSetting parameter which is an HMSTrackSetting object.   Set audio track settings For the audio track, we have following settings:     useHardwareAcousticEchoCanceler Property to enable Hardware echo cancellation. By default, it's set to true if the device supports it. Please note that on some devices the hardware wrongly reports the HW echo canceler to be present whereas it does not work In such cases, the application needs to set this to false , so that SW echo canceler is picked up This setting uses the phone's Acoustic echo Cancellation instead of relying on the SDK's software-based implementation.     audioSource Property to configure audio nodes for audio sharing.(iOS Only) More info about this can be found  here (../features/audio_sharing i-os-setup)     trackInitialState Property to set the initial state of the audio track i.e Mute/Unmute. More info about this can be found  here (../features/mute setting-video-and-mic-off-while-joining) We can create HMSAudioTrackSetting with these properties:  dart //To join with muted audio HMSAudioTrackSetting audioTrackSetting = HMSAudioTrackSetting(   useHardwareAcousticEchoCanceler: true,   trackInitialState: HMSTrackInitState.MUTED);     Set video track settings  For the video track, we can set the following properties:     cameraFacing Property specifies which camera to open while joining. It can be toggled later on. The default value is HMSCameraFacing.FRONT .   dart HMSCameraFacing.FRONT HMSCameraFacing.BACK       disableAutoResize The SDK intelligently downscales the resolution when publisher's bandwidth is flaky or is CPU bound. This results in a low resolution for the viewers. But if the viewers are persistent they want the highest resolution at all times, then this setting comes in handy. The default value is set as false     trackInitialState Property to set the initial state of the video track i.e Mute/Unmute. More info about this can be found  here (../features/mute setting-video-and-mic-off-while-joining)     forceSoftwareDecoder This can be used when a lot of videos are rendered at a single time. It is known that the hardware decoder on certain phones doesn't tend to work well with large grids. This may cause an adverse effect like the phone heating up, use this flag only when required. The default value is set as false .(Android Only) We can create HMSVideoTrackSetting with these properties:   dart //To join the room with muted video but with a back camera HMSVideoTrackSetting videoTrackSetting = HMSVideoTrackSetting(         cameraFacing: HMSCameraFacing.BACK,         trackInitialState:HMSTrackInitState.MUTED,         forceSoftwareDecoder: true);     Passing track settings in HMSSDK constructor Here's a sample implementation of adding track settings while initializing 100ms SDK    dart  HMSAudioTrackSetting audioTrackSetting = HMSAudioTrackSetting(   useHardwareAcousticEchoCanceler: false,   trackInitialState: HMSTrackInitState.UNMUTED  );   HMSVideoTrackSetting videoTrackSetting = HMSVideoTrackSetting(                   cameraFacing: HMSCameraFacing.FRONT,   trackInitialState: HMSTrackInitState.UNMUTED  );  HMSTrackSetting trackSetting = HMSTrackSetting(    audioTrackSetting: audioTrackSetting,    videoTrackSetting: videoTrackSetting   ); HMSSDK hmsSDK = HMSSDK(hmsTrackSetting: trackSetting);   We can fetch the track Setting using the method after the build method is called as follows    dart HMSTrackSetting trackSetting = hmsSDK.getTrackSettings();   "
    },
    {
        "title": "Set Volume",
        "link": "/flutter/v2/advanced-features/set-volume",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/advanced-features/set-volume",
        "keywords": [],
        "content": "  Sometime during calls we need to deafen or turn up a particular peer's volume,thus set volume comes to rescue. It allows you to vary the volume level of the remote peer's audio track. The change in volume level is only effective locally,remote peers remain unaffected. It can only be called on remote audio tracks. >Volume can vary from 0(min) to 10(max). The default value for volume would be 1.0. Let's see how you can call the setVolume on a remoteAudioTrack:   dart   HMSRemoteAudioTrack remoteAudioTrack;   remoteAudioTrack.setVolume(double volume);     Ways to set volume on audio tracks 1. Set volume while track is added   dart class Meeting implements HMSUpdateListener, HMSActionResultListener    ...      @override   void onTrackUpdate(     required HMSTrack track,    required HMSTrackUpdate trackUpdate,    required HMSPeer peer )       if(trackUpdate == HMSTrackUpdate.TRACK_ADDED && track.kind == HMSTrackKind.kHMSTrackKindAudio && peer.isLocal == false)        track.setVolume(1.0); //set the required volume               2. Set volume for particular peer > On UI this can be rendered as a slider to set a volume between 0-10 and passing it to the setVolume method on HMSRemoteAudioTrack .   dart void setVolume(HMSRemoteAudioTrack track,double volume)    track.setVolume(volume);     "
    },
    {
        "title": "Show Audio Levels\r",
        "link": "/flutter/v2/advanced-features/show-audio-level",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/advanced-features/show-audio-level",
        "keywords": [],
        "content": " -\r \r  Getting Audio Levels for all speaking peers\r \r We can find the currently speaking peers from onUpdateSpeakers method of HMSUpdateListener \r \r  dart\r class Meeting implements HMSUpdateListener, HMSActionResultListener \r \r  ...\r \r  @override\r  void onUpdateSpeakers( required List<HMSSpeaker> updateSpeakers ) \r  // This is called every 1 second with list of active speakers\r   \r  \r  \r \r Let's look at HMSSpeaker class\r \r  dart \r class HMSSpeaker  \r  final HMSPeer peer;\r  final HMSTrack track;\r  final int audioLevel;\r  \r  \r Let's understand the properties of HMSSpeaker class:\r \r 1.  audioLevel  : int -> The level of the audio. It may vary from 0-100. A higher value indicates a higher speaking volume.\r 2.  track  : HMSTrack -> The audio track corresponding to the speaker.\r 3.  peer  : HMSPeer -> The peer who is speaking.\r \r  Where can we use onUpdateSpeakers\r \r   Active Speaker Views\r \r To maintain an active speaker view, such as the default view in the  open source advanced sample app (https://github.com/100mslive/100ms-flutter/tree/main/example), you need to keep track of who the active\r speakers are over time. We'll be using the input from onUpdateSpeakers listener update as mentioned above and then building something that attempts to show all\r the active speakers while minimizing re-ordering/re-building the list.\r \r Checkout the active speaker mode  here (https://github.com/100mslive/100ms-flutter/tree/main/example application-settings-and-meeting-modes)\r \r   Dominant Speaker  the loudest speaker.\r \r The updateSpeakers list returned in onUpdateSpeakers is already in descending order based on the audioLevel.\r So, the first element of the list will be loudest speaker.\r \r "
    },
    {
        "title": "Strict Privacy Applications",
        "link": "/flutter/v2/advanced-features/strict-privacy-applications",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/advanced-features/strict-privacy-applications",
        "keywords": [],
        "content": "  Strict Privacy Applications are those applications which ask for permissions as & when required thus following something similar to principle of least privilege.   Steps to create a strict privacy application: <div className=\"steps-container\">   Join in a role without video permissions. Create three roles with permissions as:  Host    host (/docs/v2/flutter-host.png)  Guest   guest (/docs/v2/flutter-guest.png)  Viewer   viewer (/docs/v2/flutter-viewer.png) > These roles are just for examples. User can create their own roles based on use cases.   If role of user is changed to a role which has audio-video publishing permission then ask for permissions as:   dart class Meeting implements HMSUpdateListener, HMSActionResultListener       ...   @override   void onRoleChangeRequest( required HMSRoleChangeRequest roleChangeRequest )       if(roleChangeRequest.suggestedRole.publishSettings?.allowed.contains(\"video\")??false)        //if video permissions are not available       await Permission.camera.request();           if(roleChangeRequest.suggestedRole.publishSettings?.allowed.contains(\"audio\")??false)        //if audio permissions are not available       await Permission.microphone.request();       //if bluetooth permissions are not available       await Permission.bluetoothConnect.request();           //if the application have required permissions     hmsSDK.acceptChangeRole(roleChangeRequest);         </div> > This is applicable only when user asks for permission while changing role. In case of force role change > the application is required to have permissions before the role change. "
    },
    {
        "title": "Release Notes",
        "link": "/flutter/v2/changelog/release-notes",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/changelog/release-notes",
        "keywords": [],
        "content": "    1.2.0  2023-01-13   Breaking  Made await while building HMSSDK Required  Adding await to the build() method was optional earlier. It has been made   Required   now to eliminate edge cases where SDK initialization did not complete before invocation of join() method. Adding await ensures that an instance of HMSSDK is available before any further usages.    dart  hmsSDK = HMSSDK();  await hmsSDK.build(); // NEW: adding await while invoking build method on HMSSDK is now Required  hmsSDK.addUpdateListener(this);  HMSConfig config = HMSConfig(authToken: 'eyJH5c', // client-side token generated from your token service              userName: 'John Appleseed');  hmsSDK.join(config: config);      Added  Added support for Adaptive Bitrate (Simulcast)  Adaptive Bitrate refers to features that enable dynamic adjustments in video quality to optimize for end-user experience under diverse network conditions.  To learn more about how ABR works & how it enhances your app, refer the guide  here (https://www.100ms.live/docs/flutter/v2/foundation/adaptive-bitrate).   Using HMSVideoView on Android  HMSVideoView on Android provides a better abstraction to render live video and handles edge cases like managing Release/Init states. It has in-built support to subscribe to video of the correct resolution.  To learn more about Rendering Video, refer the guide  here (https://www.100ms.live/docs/flutter/v2/features/render-video).   Added Simulcast support for RTC Stats  RTC Call Stats are updated to show Simulcast layer data if available for Local Peer's Video Track. This can be used to diagnose user experience with metrics such as Audio/Video Bitrate, Round Trip Time, Packet loss, etc.  To learn more about using RTC Call Stats, refer the guide  here (https://www.100ms.live/docs/flutter/v2/features/call-stats).    Changed  Sending correct joined_at property on Android indicating the time when the peer joins the Room  Logging error messages when the App Group used for starting Screenshare from iOS is incorrect  On Android, the onJoin updates will now be triggered before onPeerUpdate when a user joins the room  Updated to Android SDK 2.5.6 & iOS SDK 0.5.5  Full Changelog:  1.1.0...1.2.0 (https://github.com/100mslive/100ms-flutter/compare/1.1.0...1.2.0)    1.1.0  2022-12-17   Added  Added support for Bulk Role Change   Bulk Role Change is used when you want to convert all Roles from a list of Roles, to another Role.  For example, if peers join a room with a _Waiting_ Role and now you want to change all these peers to _Viewer_ Role then use the changeRoleOfPeersWithRoles API.    dart  // fetch all available Roles in the room  List<HMSRole> roles = await hmsSDK.getRoles();  // get the Host Role object  HMSRole toHostRole = roles.firstWhere((element) => element.name == \"host\");  // get list of Roles to be updated  in this case \"Waiting\" and \"Guest\" Roles  roles.retainWhere((element) => ((element.name == \"waiting\")  (element.name == \"guest\")));  // now perform Role Change of all peers in \"Waiting\" and \"Guest\" Roles to the \"Host\" Role  hmsSDK.changeRoleOfPeersWithRoles(    toRole: toHostRole,    ofRoles: roles,    hmsActionResultListener: hmsActionResultListener);     For More Information, refer: https://www.100ms.live/docs/flutter/v2/features/change-role  Added Switch Audio Output APIs on iOS  Audio Output Routing is helpful when users want to switch output to a connected device other than the default one.     dart  hmsSDK.switchAudioOutput(audioDevice: HMSAudioDevice.SPEAKER_PHONE);     For More Information, refer: https://www.100ms.live/docs/flutter/v2/features/audio-output-routing    Deprecated  Deprecated changeRole API in favour of changeRoleOfPeer   No change in functionality or method signature.    Fixed  Microphone not getting captured on Role Change from a non-publishing to publishing Role on iOS  Corrected an issue where on iOS a default Audio Mixer was getting created if Track Settings was passed while building the HMSSDK instance  Updated to Native Android SDK 2.5.4 & Native iOS SDK 0.5.3 Full Changelog:  1.0.0...1.1.0 (https://github.com/100mslive/100ms-flutter/compare/1.0.0...1.1.0)   1.0.0  2022-12-09   Added  Added support for Picture in Picture mode on Android   PIP Mode lets the user watch the room video in a small window pinned to a corner of the screen while navigating between apps or browsing content on the main screen.   Example implementation for checking device support & enabling PIP mode:    dart   // to start PIP mode invoke the enterPipMode function, the parameters passed to it are optional   hmsSDK.enterPipMode(aspectRatio:  16, 9 , autoEnterPip: true);   // method to check whether PIP mode is available for current device   bool isPipAvailable = await hmsSDK.isPipAvailable();   // method to check whether PIP mode is active currently   bool isPipActive = await hmsSDK.isPipActive();      Added roomPeerCountUpdated type in HMSRoomUpdate   Added isPlaybackAllowed to Remote Audio & Video Tracks to check if the track is allowed to be played locally  Added convenience APIs to Mute / Unmute Audio or Video of the entire room locally    Fixed  Corrected parsing of HMSMessage objects sent Server-side APIs  Session Metadata can now be reset to a null value  Importing Native Android SDK dependency from Maven Central instead of Jitpack  HMSTrackSettings is now nullable while building the HMSSDK object  Corrected usage of Native Util functions to fetch Audio & Video tracks  Corrected default local audio track settings for iOS devices  Corrected sending of peer count in HMSRoom instance on iOS Updated to Native Android SDK 2.5.1 & Native iOS SDK 0.4.7 Full Changelog:  0.7.8...1.0.0 (https://github.com/100mslive/100ms-flutter/compare/0.7.8...1.0.0)    0.7.8  2022-10-31  Added support for Joining with Muted Audio & Video on iOS devices  Added Maven Central repository to look for Android dependencies  Added support for receiving Server-side HMSMessage  Added HMSLogSettings to configure Native Android SDK logs  Corrected setters for local audio/video track settings while building the HMSSDK object  Updated to Native Android SDK 2.5.1 & Native iOS SDK 0.4.6 Full Changelog:  0.7.7...0.7.8 (https://github.com/100mslive/100ms-flutter/compare/0.7.7...0.7.8)    0.7.7  2022-10-20  Added ability to set & get session metadata  Added ability to join with muted audio & video using Initial states (Muted / Unmuted) HMSVideoTrackSettings & HMSAudioTrackSettings in the builder of HMSSDK  Added better Telemetrics for analytics  Added option to use Software Decoder for Video rendering on Android devices  Added action result listener to switchCamera function on local video track  Fixed LetterBoxing (Black borders on top and bottom) observed when sharing the screen in landscape mode on Android  Fixed incorrect sending of Speaker Updates when peer has left the room  Removed unused setters for Local Audio & Video Track Settings  Updated to Native Android SDK 2.5.0 & Native iOS SDK 0.4.5 Full Changelog:  0.7.6...0.7.7 (https://github.com/100mslive/100ms-flutter/compare/0.7.6...0.7.7)    0.7.6  2022-09-23  Added audio output change listener callback while in Preview on Android  Added API to show Native Audio Output Picker on iOS  Corrected an issue where audio was always coming from the Earpiece instead of Built-In Speaker on iOS  Fixed an issue where audio gets distorted when headset is used while sharing audio playing on iOS  Updated HMSException class. Added canRetry attribute  Full Changelog:  0.7.6...0.7.5 (https://github.com/100mslive/100ms-flutter/compare/0.7.5...0.7.6)   0.7.5  2022-08-18  Added support on iOS for sharing audio from local files on your device & from other audio playing apps  Added ability to apply local peer track settings while initializing HMSSDK  Added APIs to fetch local peer track settings  Fixed an issue where exiting from Preview without joining room was not releasing camera access  Added destroy API to cleanup Native HMSSDK instance correctly  Disabled Hardware Scaler on Android to correct intermittent Video tile flickering  Updated to Native Android SDK 2.4.8 & Native iOS SDK 0.3.3   0.7.4  2022-07-29    Added  Added APIs to stream device audio in different modes  Added APIs to view and change the output speaker selected by the SDK to playout  setAudioMode API to change the Audio out mode manually between in-call volume and media volume    Fixed  Calling switchCamera API leads to triggering of onSuccess callback twice  onRoomUpdate with type HMSRoomUpdate.ROOM_PEER_COUNT_UPDATED not getting called when peer count changes in the room  Peer not able to publish tracks when updated to WebRTC from HLS if rejoins after a reconnection in WebRTC Mode    Changed  HMSHLSConfig is now an optional parameter while calling startHLSStreaming and stopHLSStreaming  The meetingUrl parameter is optional while creating the HMSHLSMeetingURLVariant instance for HMSHLSConfig. If nothing is provided HMS system will take the default meetingUrl for starting HLS stream  changeRoleForce permission in HMSRole is now removed and no longer used  recording permission in HMSRole is now broken into  browserRecording and rtmpStreaming  streaming permission in HMSRole is now hlsStreaming    0.7.3  2022-06-23  Added support for iOS Screen-share  Added HMSHLSRecordingConfig to perform recording while HLS Streaming  Updated error callback in HMSUpdateListener to onHMSError  Updated to Native Android SDK 2.4.2 & Native iOS SDK 0.3.1   0.7.2  2022-06-02  Segregated RTC Stats update notifications from HMSUpdateListener into HMSStatsListener  Removed room_peer_count_updated from HMSRoomUpdate enum  Added sessionId to the HMSRoom class  Updated to Native Android SDK 2.3.9 & Native iOS SDK 0.3.1   0.7.1  2022-05-20  Added RTC Stats Listener which provides info about local & remote peer's audio/video quality  Improved video rendering performance for Android devices  Correct RTMP Streaming & Recording configuration settings  Added support for Server-side Subscribe Degradation  Updated to Native Android SDK 2.3.9 & Native iOS SDK 0.2.13   0.7.0  2022-04-19    Added  Network Quality in preview. Network quality reports can now be requested at the preview screen. Use the returned value to determine if you should suggest people's internet is too slow to join with video etc.  Network Quality during calls. Peer Network Quality updates are now received during the call. Use this to show how strong any peer's internet is during the call.  Added HLS Recording to initial PeerList  onPeerUpdate and onRoomUpdate callbacks in 'HMSPreviewListener' to get info about the room at Preview screen  Added startedAt and stoppedAt field for Browser and SFU recording    Fixed  Error Analytics events not being sent  Leave not finishing if SDK is in reconnection state. Hence all join calls after that was getting queued up if called on the same HMSSDK instance  Improved subscribe degradation so that new add sinks are handled properly when SDK is already in degraded state  Crash fix on starting/stopping HLS where HlsStartRecording was null  HLS recording status wasn't always updated when stopped  Rare crash when cameras are unavailable and it seemed to the app like none exist  Updated to Native Android SDK 2.3.4 & Native iOS SDK 0.2.9   0.6.0  2022-01-25   Breaking Change  Updated Change Role APIs argument types  Changed Messaging APIs argument types  Updated argument types of changeTrackState , changeRole , acceptRoleChange , changeTrackStateForRoles APIs    Added  Added HLS Support. Now you can Start/Stop HLS Streaming from Flutter SDK  Added support to do ScreenShare from Android device    Changed  Updated callbacks for Permission based action APIs   0.5.0  2022-01-15   Breaking Change  Renamed SDK Public interface to HMSSDK class  Updated HMSConfig object which is used to join the room    Added  Added APIs to change remote track status  Added APIs to start/stop Recording  Added APIs to change metadata of local peer which can be used to implement raise hand functionality  Added API to change name of local peer  Added API to get current room status  Added API to get peer audio/video status  Added new Group & Direct Peer Messaging APIs  Added volume setter & getter APIs on audio tracks  Added Action Result Listeners to notify success or failure of API invocations    Changed  Updated HMSException object with isTerminal   Changed sendMessage API to sendBroadcastMessage to send a message to all peers in room  Changed HMSMessageResultListener to HMSActionResultListener in Messaging APIs  Video Rendering flow for Android & iOS video tracks  Preview API implementation    Fixed  Reconnection issues wherein even when network recovered, peer could not rejoin the room  Cleaning up config object whenever room is joined/left   0.4.1  2021-12-03  Added matchParent boolean on video view   0.4.0  2021-10-22  Updated Messaging APIs  Added audio level, peer & track object in HMSSpeaker  Updated track source type to string  Updated sample app   0.3.0  2021-10-14  Corrected crash on using getLocalPeer  Updated sample app   0.2.0  2021-10-07 This version of 100ms Flutter SDK comes loaded with bunch of features & improvements like   Improved low network performance  Added Active Speaker listener  Resolved build conflicts on iOS  Added APIs to Change Role of a Peer  Added APIs to Mute Audio/Video of a Remote Peer  Added APIs to End a Room  Updated Chat Messaging APIs to enable Broadcast, Group & Personal  Improved Reconnection after network switch/loss  Improved Interruption Handling when a Meeting is ongoing   0.1.0  2021-08-17 The first version of 100ms Flutter SDK comes power-packed with support for multiple features like   Join/Leave Rooms  Mute / Unmute Audio / Video  Switch Camera  Chat  Preview Screen  Change Role  Network Switch Support  Subscribe Degradation in bad network scenarios  Error Handling and much more. Take it for a spin  🥳 "
    },
    {
        "title": "Flutter version compatibility",
        "link": "/flutter/v2/debugging/faq#flutter-version-compatibility",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/debugging/faq#flutter-version-compatibility-0",
        "keywords": [],
        "content": "---\ntitle: Frequently Asked Questions\nnav: 5.1\n---\n\nThis page lists down frequently asked questions. If you want to add a new question or edit an older one, feel free to\n[send us a PR](https://github.com/100mslive/100ms-docs/blob/main/docs/flutter/v2/debugging/faq.mdx).\n\n## Flutter version compatibility\nHMSSDK works with flutter 3.3.x or above.\n\n## "
    },
    {
        "title": "Not getting event updates after hot reload/restart",
        "link": "/flutter/v2/debugging/faq#not-getting-event-updates-after-hot-reloadrestart",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/debugging/faq#not-getting-event-updates-after-hot-reloadrestart-1",
        "keywords": [],
        "content": "Not getting event updates after hot reload/restart\nThis is caused because the platform channel needs to be reinitialized again, hence this is intended behaviour.\nThe solution for this is to re-run the app.\n\nPractices for faster development :\n\n* Perform the intended changes.\n* Leave the room.\n* Perform hot reload/restart and rejoin the room.\n* Verify the changes.\n\nThe permanent solution for this is in pipeline, we will update once it's done.\n\n## "
    },
    {
        "title": "Issues while using `hmssdk_flutter` with flutter 3.0.x",
        "link": "/flutter/v2/debugging/faq#issues-while-using-hmssdk_flutter-with-flutter-30x",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/debugging/faq#issues-while-using-hmssdk_flutter-with-flutter-30x-2",
        "keywords": [],
        "content": "Issues while using `hmssdk_flutter` with flutter 3.0.x\nFlutter versions 3.0.0 to 3.0.5 had issues related to Platform View.\n\nRefer: [Android Platform View issue](https://github.com/flutter/flutter/issues/107313) & [Flutter Platform View bug](https://github.com/flutter/flutter/issues/103630)\n\nThese were resolved in Flutter versions 3.3.0 & above. Please update the Flutter version to 3.3.0 or above.\n\n\n## "
    },
    {
        "title": "Is there any limit to the number of HMSVideoView on-screen at a time ",
        "link": "/flutter/v2/debugging/faq#is-there-any-limit-to-the-number-of-hmsvideoview-on-screen-at-a-time",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/debugging/faq#is-there-any-limit-to-the-number-of-hmsvideoview-on-screen-at-a-time-3",
        "keywords": [],
        "content": "Is there any limit to the number of HMSVideoView on-screen at a time \n\n`HMSVideoView` internally uses `SurfaceView` in android and `UiKitView` in iOS. It is recommended to render at most 3 to 4 videos on a Single page/screen of the app and rest should be paginated for optimum performance.\n\n## "
    },
    {
        "title": "Do you have any implementation with popular State Management libraries -",
        "link": "/flutter/v2/debugging/faq#do-you-have-any-implementation-with-popular-state-management-libraries",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/debugging/faq#do-you-have-any-implementation-with-popular-state-management-libraries-4",
        "keywords": [],
        "content": "Do you have any implementation with popular State Management libraries -\nPlease find the implementations below:\n* [Provider](https://github.com/100mslive/100ms-flutter/tree/main/example)\n* [Bloc](https://github.com/100mslive/100ms-flutter/tree/main/sample%20apps/bloc)\n* [Getx](https://github.com/100mslive/100ms-flutter/tree/main/sample%20apps/getx)\n* [Mobx](https://github.com/100mslive/100ms-flutter/tree/main/sample%20apps/mobx)\n* [Riverpod](https://github.com/100mslive/100ms-flutter/tree/main/sample%20apps/riverpod)\n\n## "
    },
    {
        "title": "Not able to get room updates after joining the room",
        "link": "/flutter/v2/debugging/faq#not-able-to-get-room-updates-after-joining-the-room",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/debugging/faq#not-able-to-get-room-updates-after-joining-the-room-5",
        "keywords": [],
        "content": "Not able to get room updates after joining the room\nTo listen to the room updates please attach `HMSUpdateListener` as:\n```dart\nclass Meeting implements HMSUpdateListener {\n    Meeting(){\n        hmsSDK.addUpdateListener(updateListener);\n    }\n    ...\n}\n```\nYou can find more details about HMSUpdateListener [here](../features/update-listeners)\n\n## "
    },
    {
        "title": "Join room with muted audio/video ",
        "link": "/flutter/v2/debugging/faq#join-room-with-muted-audiovideo",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/debugging/faq#join-room-with-muted-audiovideo-6",
        "keywords": [],
        "content": "Join room with muted audio/video \nUser can join the room with muted audio/video by default. Please find the docs [here](../features/mute#setting-video-and-mic-off-while-joining)\n\n## "
    },
    {
        "title": "Get `onPeerUpdate` in preview",
        "link": "/flutter/v2/debugging/faq#get-onpeerupdate-in-preview",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/debugging/faq#get-onpeerupdate-in-preview-7",
        "keywords": [],
        "content": "Get `onPeerUpdate` in preview\nUser can get `onPeerUpdate` in preview the docs can be found [here](../features/preview#get-on-peer-update-and-room-state-in-preview)\n\n## "
    },
    {
        "title": "Getting updates multiple times in the room",
        "link": "/flutter/v2/debugging/faq#getting-updates-multiple-times-in-the-room",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/debugging/faq#getting-updates-multiple-times-in-the-room-8",
        "keywords": [],
        "content": "Getting updates multiple times in the room\nPlease ensure removing the `HMSUpdateListener` while leaving the room.\n```dart\nhmsSDK.removeUpdateListener(updateListener);\n```\nYou can find more details about HMSUpdateListener [here](../features/update-listeners)\n\n## "
    },
    {
        "title": "Can I create a room using API?",
        "link": "/flutter/v2/debugging/faq#can-i-create-a-room-using-api",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/debugging/faq#can-i-create-a-room-using-api-9",
        "keywords": [],
        "content": "Can I create a room using API?\nYes,please find the link [here](/server-side/v2/Rooms/create-via-api)\n\n## "
    },
    {
        "title": "Receiving too many logs from SDK ",
        "link": "/flutter/v2/debugging/faq#receiving-too-many-logs-from-sdk",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/debugging/faq#receiving-too-many-logs-from-sdk-10",
        "keywords": [],
        "content": "Receiving too many logs from SDK \nLogs can be turned OFF using the `hmsLogSettings` parameter of `HMSSDK`. More info about this can be found [here](../features/error-handling#setting-log-levels-in-sdk) \n\n## "
    },
    {
        "title": "Do I need to do anything to handle poor internet connection?",
        "link": "/flutter/v2/debugging/faq#do-i-need-to-do-anything-to-handle-poor-internet-connection",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/debugging/faq#do-i-need-to-do-anything-to-handle-poor-internet-connection-11",
        "keywords": [],
        "content": "Do I need to do anything to handle poor internet connection?\nNot much, just turn on a flag in dashboard, and show a proper UI when a video gets degraded/unsubscribed. More details [here](../features/auto-video-degrade-restore).\n\n## "
    },
    {
        "title": "How do I implement Raise Hand, polls in application ?",
        "link": "/flutter/v2/debugging/faq#how-do-i-implement-raise-hand-polls-in-application",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/debugging/faq#how-do-i-implement-raise-hand-polls-in-application-12",
        "keywords": [],
        "content": "How do I implement Raise Hand, polls in application ?\nYou can do using [peer metadata](../advanced-features/peer-metadata-update#use-cases).\n\n## "
    },
    {
        "title": "Why do I see videos getting stuck or frozen?",
        "link": "/flutter/v2/debugging/faq#why-do-i-see-videos-getting-stuck-or-frozen",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/debugging/faq#why-do-i-see-videos-getting-stuck-or-frozen-13",
        "keywords": [],
        "content": "Why do I see videos getting stuck or frozen?\nIf you have enabled subscribe degradation from the dashboard, the SDK might go in the degradation mode on poor internet connection turning off some videos to ensure good call quality. When this is done, a flag on the track will be turned on to let the UI know. The UI should treat it similar to the track turning off for purpose of displaying avatar etc. More details [here](../features/auto-video-degrade-restore).\n\n## "
    },
    {
        "title": "What is the maximum allowed duration for a session?",
        "link": "/flutter/v2/debugging/faq#what-is-the-maximum-allowed-duration-for-a-session",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/debugging/faq#what-is-the-maximum-allowed-duration-for-a-session-14",
        "keywords": [],
        "content": "What is the maximum allowed duration for a session?\nThe maximum allowed duration for a session on the 100ms platform is 12 hours.\n\n## "
    },
    {
        "title": "I want to suggest a new feature.",
        "link": "/flutter/v2/debugging/faq#i-want-to-suggest-a-new-feature",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/debugging/faq#i-want-to-suggest-a-new-feature-15",
        "keywords": [],
        "content": "I want to suggest a new feature.\nAwesome, we're always looking out for new ideas and features. Please reach out to us over [discord](https://100ms.live/discord)\n\n"
    },
    {
        "title": "Action Result Listeners",
        "link": "/flutter/v2/features/action-result-listeners",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/action-result-listeners",
        "keywords": [],
        "content": "  HMSActionResultListener is a listener to listen to success or failure callback for methods. Whenever an instance of HMSActionResultListener is passed with a method then it's status i.e whether it succeeded or failed can be listened using HMSActionResultListener's onSuccess & onException callbacks respectively.  > HMSActionResultListener should be implemented in the class where HMSActionResultListener methods are called. > We can implement HMSUpdateListener , HMSActionResultListener , HMSStatsListener etc. on a single class > checkout the example  here (https://github.com/100mslive/100ms-flutter/blob/1537a82a2213c8bbb1b0eb2dcc009c171e89bad1/example/lib/data_store/meeting_store.dart L33)   How to add an HMSActionResultListener This section contains info about how we can add HMSActionResultListener to our class: <div className=\"steps-container\">   Implement HMSActionResultListener Implement HMSActionResultListener in a class wherever you wish to call the HMSSDK methods and add override methods.   dart class Meeting implements HMSActionResultListener   @override  void onSuccess(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments )      @override  void onException(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments,    required HMSException hmsException )           Pass HMSActionResultListener instance in methods To get the onSuccess or onException callbacks we need to pass the HMSActionResultListener instance while calling methods as:   dart class Meeting implements HMSActionResultListener   ...  void leave() async     //this is the instance of class which implements HMSActionResultListener   await hmsSDK.leave(hmsActionResultListener: this);     void sendBroadcastMessage()    //this is the instance of class which implements HMSActionResultListener   hmsSDK.sendBroadcastMessage(   message: message,   type: type,   hmsActionResultListener: this);     @override  void onSuccess(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments )        switch (methodType)         case HMSActionResultListenerMethod.leave:       // Room leaved successfully       // Clear the local room state       break;       case HMSActionResultListenerMethod.sendBroadcastMessage:       //Do the needful actions here       break;       ...            @override  void onException(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments,    required HMSException hmsException )       switch (methodType)         case HMSActionResultListenerMethod.leave:       // Check the HMSException object for details about error       break;       case HMSActionResultListenerMethod.sendBroadcastMessage:       // Check the HMSException object for details about error       break;       ...            ...     </div> Other methods can be implemented in similar fashion.   Supplementary bytes This section explains HMSActionResultListener class methods. We implement this class to listen to method callbacks.   dart ///100ms HMSActionResultListener /// ///Whenever an instance of  HMSActionResultListener  is passed with a method then it's status i.e whether it succeeded or failed can be listened using HMSActionResultListener's onSuccess & onException callbacks respectively. abstract class HMSActionResultListener    void onSuccess(     HMSActionResultListenerMethod methodType,    Map<String, dynamic>? arguments );  void onException(     HMSActionResultListenerMethod methodType,    Map<String, dynamic>? arguments,    required HMSException hmsException );     > 💡 HMSActionResultListener : A class conforming to HMSActionResultListener interface. The methods of HMSActionResultListener are invoked to notify the status of the api's whether it failed or succedded.   HMSActionResultListenerMethod  HMSActionResultListenerMethod is an enum which can be used to segregate the HMSActionResultListener method type similar to what we did in above example.   dart enum HMSActionResultListenerMethod    leave,  changeTrackState,  changeMetadata,  endRoom,  removePeer,  acceptChangeRole,  changeRoleOfPeer,  changeTrackStateForRole,  startRtmpOrRecording,  stopRtmpAndRecording,  changeName,  sendBroadcastMessage,  sendGroupMessage,  sendDirectMessage,  hlsStreamingStarted,  hlsStreamingStopped,  startScreenShare,  stopScreenShare,  startAudioShare,  stopAudioShare,  setSessionMetadata,  switchCamera,  changeRoleOfPeersWithRoles,  //default case  unknown    "
    },
    {
        "title": "Audio Output Routing\r",
        "link": "/flutter/v2/features/audio-output-routing",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/audio-output-routing",
        "keywords": [],
        "content": " -\r \r Audio Output Routing is helpful when users want to switch the audio output to a connected device other than the default one.\r \r For example, you can implement an in-call speaker button that toggles audio output between the speaker, earpiece or any other connected earphones  wired or wireless, etc.\r \r Let's see how we can use hmsSDK to implement this:\r \r <div className=\"steps-container\">\r \r \r   Fetch Available Audio Devices\r \r Invoke the getAudioDevicesList method to fetch the available audio device list. The method returns a list of HMSAudioDevice enum.\r \r  dart\r List<HMSAudioDevice> availableDevices = await hmsSDK.getAudioDevicesList();\r \r \r enum HMSAudioDevice  \r  SPEAKER_PHONE,\r  EARPIECE,\r  WIRED_HEADSET, // Android only\r  BLUETOOTH, // Android only\r  AUTOMATIC, // Android only\r  UNKNOWN\r  \r  \r \r The getAudioDevicesList API returns different values based on the Android & iOS platforms. It can return the following HMSAudioDevice enum values :\r \r    For Android\r \r   SPEAKER_PHONE   route audio to the device speaker\r   EARPIECE  route audio to earpiece\r   WIRED_HEADSET    route audio to a connected wired device\r   BLUETOOTH  route audio to connected Bluetooth device\r   AUTOMATIC  automatic routing\r \r    For iOS\r \r   SPEAKER_PHONE   route audio to device speaker\r   EARPIECE  route audio earpiece\r \r \r   Switch Audio Focus to Another Device\r \r Invoke the switchAudioOutput method with the appropriate HMSAudioDevice value fetched from getAudioDevicesList to switch the audio to that device.\r \r \r The audioDevice is a required HMSAudioDevice enum type parameter which we discussed above.\r \r  dart\r /// audioDevice : is an enum of type HMSAudioDevice \r hmsSDK.switchAudioOutput(audioDevice: audioDevice);\r  \r \r For iOS calling, this method does the work \r \r For android, there are some extra steps to listen to the audio device updates in case a new device is connected or disconnected let's look at them below.\r \r   Receive the device change updates (Android Only)\r \r Implementing the onAudioDeviceChanged listener allows you to get event updates if a new audio device gets connected such as Bluetooth, Wired Headset etc. \r This method needs to be overridden in the class where HMSUpdateListener is implemented.\r \r Note that this event is available only on the Android platform.\r \r  dart \r class Meeting implements HMSUpdateListener, HMSActionResultListener \r \r  ...\r \r  @override\r  void onAudioDeviceChanged(\r   HMSAudioDevice? currentAudioDevice,\r  List<HMSAudioDevice>? availableAudioDevice )  \r  \r   // currentAudioDevice : audio device to which audio is curently being routed to\r   // availableAudioDevice : all other available audio devices \r   \r  \r  \r \r </div>\r \r  Get Current Focussed Device (Android Only)\r \r The getCurrentAudioDevice method allows you to get the device through which audio output is currently being routed to.\r \r  dart\r HMSAudioDevice currentAudioDevice = await hmsSDK.getCurrentAudioDevice();\r  \r \r > Note: onAudioDeviceChanged listener and getCurrentAudioDevice methods are available only on the Android platform.\r "
    },
    {
        "title": "Audio Share (Beta)",
        "link": "/flutter/v2/features/audio_sharing",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/audio_sharing",
        "keywords": [],
        "content": "  >  This feature is still in Beta. To know more or report any issues, feel free to reach out to us >  over  Discord (https://discord.com/invite/kGdmszyzq2).  This feature is the analog of screen capture, but for audio. There may be cases where the application needs to stream music which is either stored in the device locally or from some other app present on the device in the room where the peer is joined. Examples of such use cases can be an FM-like application where the host would want to stream music while also interacting with others in the room or a host in a gaming app who would want to stream music from their device in the room along with their regular audio track.   Android Setup  The Audio share option is currently available in Android 10 and above.   How does audio share work in android 100ms SDK uses the  MediaProjection (https://developer.android.com/guide/topics/media/av-capture capture_audio_playback) APIs of Android to capture the device audio and stream it along with the user's regular audio track. To achieve this, SDK starts a foreground service and starts capturing the device audio and mixes the bytes with the data collected from the mic, so that the stream contains both system music and mic data. This API gives apps the ability to copy the audio being played by other apps which have set their usage to USAGE_MEDIA, USAGE_GAME, or USAGE_UNKNOWN. (Audio from apps like YouTube etc can be captured) Let's see how you can implement this using HMSSDK : First, we will need to pass the intent from native android to HMSSDK. For this in your app's MainActivity add    kotlin import live.hms.hmssdk_flutter.HmssdkFlutterPlugin import android.app.Activity import android.content.Intent import live.hms.hmssdk_flutter.Constants override fun onActivityResult(requestCode: Int, resultCode: Int, data: Intent?)   super.onActivityResult(requestCode, resultCode, data)   if (requestCode == Constants.AUDIO_SHARE_INTENT_REQUEST_CODE && resultCode == Activity.RESULT_OK)      HmssdkFlutterPlugin.hmssdkFlutterPlugin?.requestAudioShare(data)         > DO NOT forget to add the permission for foreground service in AndroidManifest.xml   kotlin <uses-permission android:name=\"android.permission.FOREGROUND_SERVICE\"       How to start/stop device audio streaming from the application To start streaming device audio, the application needs to call the startAudioShare method of HMSSDK , which takes in two parameters  1. The first one is HMSActionResultListener which is a callback object needed to inform about the success or failure of the action 2. The second one is one of the modes of type HMSAudioMixingMode in which the user wants to stream.   This can be one out of the three available types     TALK_ONLY : Data captured by the mic will be streamed in the room    TALK_AND_MUSIC: Data captured by the mic as well as playback audio being captured from the device will be streamed in the room    MUSIC_ONLY: The playback audio being captured from the device will be streamed into the room Following is the snippet on how to start and stop audio share:   dart class Meeting implements HMSUpdateListener, HMSActionResultListener     ...   void startAudioShare()       /// audioMixingMode  HMSAudioMixingMode enum with values discussed above     /// hmsActionResultListener : an instance of a class that implements HMSActionResultListener     //Here this is an instance of a class that implements HMSActionResultListener i.e. Meeting     //For example if the mode needed is TALK_AND_MUSIC     hmsSDK.startAudioShare(audioMixingMode: HMSAudioMixingMode.TALK_AND_MUSIC,hmsActionResultListener: this);       void stopAudioShare()       /// hmsActionResultListener : an instance of a class that implements HMSActionResultListener     //Here this is an instance of a class that implements HMSActionResultListener i.e. Meeting     hmsSDK.stopAudioShare(hmsActionResultListener: this);       @override   void onSuccess(      HMSActionResultListenerMethod methodType =       HMSActionResultListenerMethod.unknown,     Map<String, dynamic>? arguments )         switch (methodType)         ...       case HMSActionResultListenerMethod.startAudioShare:       //Here we will receive the startAudioShare success callback       break;       case HMSActionResultListenerMethod.stopAudioShare:       //Here we will receive the stopAudioShare success callback       break;           ...             @override   void onException(      HMSActionResultListenerMethod methodType =       HMSActionResultListenerMethod.unknown,     Map<String, dynamic>? arguments,     required HMSException hmsException )         switch (methodType)           ...          case HMSActionResultListenerMethod.startAudioShare:         // Check the HMSException object for details about the error         break;         case HMSActionResultListenerMethod.stopAudioShare:         // Check the HMSException object for details about the error         break;         ...                 How to change the audio mixing mode To change the audio sharing mode, call the setAudioMixingMode API and pass one of the modes out of TALK_ONLY or TALK_AND_MUSIC or MUSIC_ONLY   dart //For setting mode as MUSIC_ONLY hmsSDK.setAudioMixingMode(audioMixingMode:HMSAudioMixingMode.MUSIC_ONLY);    Note that TALK_ONLY mode is equivalent to regular mode, i.e. without starting this API   iOS Setup  Minimum iOS version required to support Audio Share is iOS 13   How audio sharing works in iOS The audio that the device shares go to other peers through the mic channel. To be able to share audio we need to set up the SDK to use a custom audio source instead of the default mic. To do that you pass an instance of a custom audio source to HMSAudioTrackSettings on your HMSSDK instance.   How to use HMSSDK to share audio from a file 1. You create an instance of HMSAudioFilePlayerNode and an instance of HMSMicNode like below:   HMSAudioFilePlayerNode required a parameter type String which will be used at the control music player in the room.     dart HMSAudioFilePlayerNode audioFilePlayerNode = HMSAudioFilePlayerNode(\"nodeName\"); HMSMicNode micNode = HMSMicNode();   2. Next, you create an instance of HMSAudioMixerSource , passing an array of nodes that we created in the step above like below:   dart HMSAudioMixerSource audioMixerSource = HMSAudioMixerSource(nodes:  audioFilePlayerNode, micNode );    3. Next, you pass this custom audio source to the 'audioSource' parameter of HMSAudioTrackSetting that you set on HMSSDK instance like so:   dart HMSAudioTrackSetting audioSettings = HMSAudioTrackSetting(..., audioSource: audioMixerSource); HMSTrackSetting = HMSTrackSetting(..., audioTrackSetting: audioSettings); HMSSDK hmsSDK = HMSSDK(hmsTrackSetting: trackSetting); await hmsSDK.build(); // ensure to await while invoking the build method   That's all you need to set up the SDK to use your custom audio source. 4. You call the play function on audioFilePlayerNode to play a file on a local device in a meeting room with its file URL like below:   dart   HMSAudioFilePlayerNode audioFilePlayerNode = HMSAudioFilePlayerNode(\"nodeName\");   audioFilePlayerNode.play(fileUrl: ...);    Note the parameter value in HMSAudioFilePlayerNode must be the same as defined at the time of initializing HMSSDK .    How to change the volume of different nodes You can use the volume property on nodes to control the volume.   dart audioFilePlayerNode.setVolume(0.5); micNode.setVolume(0.9);   Note volume value range from 0.0 to 1.0   How to schedule multiple audio files for back-to-back playback You can set interrupts parameter to false to tell audioFilePlayerNode to not interrupt the current file playback, but schedule the file after the current file is finished. Like below:   dart audioFilePlayerNode.play(fileUrl: URL to file 1) audioFilePlayerNode.play(fileUrl: URL to file 2, interrupts: false) audioFilePlayerNode.play(fileUrl: URL to file 3, interrupts: false) ...     How to play multiple files concurrently You can pass multiple instances of audioFilePlayerNode and pass them as nodes when creating audioMixerSource like so:   dart HMSAudioFilePlayerNode backgroundMusicNode = HMSAudioFilePlayerNode(\"backgroundMusicNode\") backgroundMusicNode.setVolume(0.2) HMSAudioFilePlayerNode audioFilePlaybackNode = HMSAudioFilePlayerNode(\"audioFilePlaybackNode\") audioFilePlaybackNode.setVolume(0.5) HMSMicNode micNode = HMSMicNode() HMSAudioMixerSource audioMixerSource = HMSAudioMixerSource(nodes:  backgroundMusicNode, audioFilePlaybackNode, micNode )   Now, you can play looping background music at low volume and an audio file at the same time:   dart backgroundMusicNode.play(fileUrl: ..., loops: true) audioFilePlayerNode.play(fileUrl: ...)      How to pause, resume, stop playback and more You can use the following interfaces on HMSAudioFilePlayerNode to pause, resume or stop playback and more:   dart audioFilePlayerNode.pause() audioFilePlayerNode.resume() audioFilePlayerNode.stop() bool isPlaying = await audioFilePlayerNode.isPlaying() double currentPlaybackTime = audioFilePlayerNode.currentDuration() double totalPlaybackDuration = audioFilePlayerNode.duration()       How to share audio that's playing on your iPhone Note: iOS allows you to get access to audio playing on an iOS device (for example, from another app like Spotify) only while broadcasting your entire iPhone screen. So for this to work you should implement screen sharing in your app. You can follow along here to set it up  Screen Share (./../features/screen-share) Now once you have implemented the screen share feature from above. You can follow the below steps to enable system audio broadcasting while sharing your screen: 1. You get an instance of HMSScreenBroadcastAudioNode and add it to your mixer.   dart HMSScreenBroadcastAudioReceiverNode screenAudioNode = HMSScreenBroadcastAudioReceiverNode() HMSAudioMixerSource audioMixerSource = HMSAudioMixerSource(nodes:  audioFilePlaybackNode, micNode, screenAudioNode )     Note: you can pass only a single instance of HMSMicNode and HMSScreenBroadcastAudioNode to HMSAudioMixerSource, else you will receive an error. Now your mixer source is set to receive audio from your broadcast extension. 2. Next, you need to set up a broadcast extension to send audio to the main app. Broadcast extension receives audio that's playing on your iOS device in processSampleBuffer function in your RPBroadcastSampleHandler class. To send audio from the broadcast extension to the main app, you call the process(audioSampleBuffer) function on HMSScreenRenderer:   swift override func processSampleBuffer(_ sampleBuffer: CMSampleBuffer, with sampleBufferType: RPSampleBufferType)     ...     case RPSampleBufferType.audioApp:     _ = self.screenRenderer.process(audioSampleBuffer: sampleBuffer)     break   ...      Now your broadcast extension is set to send audio to the main app. And that's it. Now your custom mixer source in the main app can receive the audio from the broadcast extension as well. "
    },
    {
        "title": "Auto Video Degrade/Restore\r",
        "link": "/flutter/v2/features/auto-video-degrade-restore",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/auto-video-degrade-restore",
        "keywords": [],
        "content": " -\r \r Sometimes people have bad internet connections but everyone deserves a good meeting.\r \r When the network is too slow to support audio and video conversations together, the 100ms SDK can automatically turn off downloading other peer's videos, which may improve the audio quality and avoid disconnections.\r \r If the network quality improves, the videos will be restored automatically as well.\r \r To turn on subscribe degradation in your room, open the  templates (https://dashboard.100ms.live/templates) in the dashboard and enable it for roles there. Here's more information about  templates (/flutter/v2/foundation/templates-and-roles).\r \r   Dashboard Degradation Option (/docs/v2/degradation.png)\r \r  Responding in the app\r \r All HMSVideoTrack , within the HMSPeer 's have a variable called isDegraded .\r \r If isDegraded is true, treat it as if the video is turned off. If the UI is not changed, the video tile will appear black.\r \r  Video Track Update\r \r We can listen to the track update of trackDegraded and trackRestored in HMSTrackUpdate to update the UI whenever someone's track gets changed due to degradation.\r \r  dart \r class Meeting implements HMSUpdateListener, HMSActionResultListener \r \r  ...\r \r   @override\r   void onTrackUpdate(\r     required HMSTrack track,\r    required HMSTrackUpdate trackUpdate,\r    required HMSPeer peer )  \r \r     switch (update)  \r     \r       case HMSTrackUpdate.trackDegraded:\r       //Show as if video is turned off otherwise the tile becomes black\r       break;\r \r       case HMSTrackUpdate.trackRestored:\r       //Switch to normal video as track is restored now\r       break;\r     \r      \r \r    \r  \r  \r "
    },
    {
        "title": "Background Handling          ",
        "link": "/flutter/v2/features/background-handling",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/background-handling",
        "keywords": [],
        "content": "  While a user has joined a 100ms Room they can put the app in Background Mode & usually expect a certain subset of app functionality continues to work.  Background Mode implies that the application is not currently visible to the user, not responding to user input, and running in the Background.  Background state in Flutter apps is represented by AppLifecycleState.paused as described  here (https://api.flutter.dev/flutter/dart-ui/AppLifecycleState.html paused). Android & iOS have different mechanisms of handling app behaviour which includes limiting access to Camera & Microphone, time allowed to keep running before getting completely stopped, Aggressive Battery Optimisations on Android OEMs, etc. There are 2 prominent behaviours that are mainly affected when apps transition to Background State  Publishing Audio/Video, and Playing incoming Audio/Video. Let's breakdown how your apps can implement efficient handling of Background states on iOS & Android Platforms.   iOS Background Handling When your app goes into the background, by default it can no longer access camera or microphone and publish it to other peers in the room. Also, you cannot hear audio of other participants in the call if your app is in the background without enabling Background Modes. This is the default iOS platform behaviour whereby it limits access to Microphone & Camera for capturing Audio & Video. By default, iOS also stops playing audio of the Room when your app is in Background. By Enabling Background Modes you can ensure that your iOS app has access to Microphone & can play incoming Audio from the Room. Following steps show how to add Background Modes in iOS: <div className=\"steps-container\">   Open project in Xcode Open project in Xcode by right clicking on ios folder in project and select Open in Xcode as shown in image below.   OpenXcode (/docs/v2/flutter-background-service-1.png)   Add Capability Click on Runner in Navigator then select Runner under TARGETS and click on Signing & Capabilities and add Capability .   addCapability (/docs/v2/flutter-background-service-2.png)   Background Mode Search background mode and add it to project by clicking on it.   backgroundMode (/docs/v2/flutter-background-service-3.png)   Enable Background Mode Enable the checkbox under Background Modes named Audio, AirPlay, and Picture in Picture .   backgroundProcessing (/docs/v2/flutter-background-service-4.png) </div> Let's look at different scenarios on iOS with & without Enabling Background Modes.    Without Background Modes Enabled (Default iOS Behaviour) The table lists down iOS app behaviours when the app transitions to background when a 100ms Room is ongoing without enabling background processing. The \"  Scenario  \" on the left of the Table below implies the activity which is ongoing in the App when user has joined a 100ms Room. The \"  Behaviours  \" on the right of the Table shows what happens when the App transitions to Background.    Scenario       Behaviours     :     :                Mic is Unmuted  Mic will get   Muted     Camera is unmuted  Camera will get   Muted     Remote Peers are publishing audio  Incoming Audio from Room   Stops   Playing    Background Modes Enabled (Recommended) The table lists down iOS app behaviours when the app transitions to background when a 100ms Room is ongoing with Background Modes   Enabled  . The \"  Scenario  \" on the left of the Table below implies the activity which is ongoing in the App when user has joined a 100ms Room. The \"  Behaviours  \" on the right of the Table shows what happens when the App transitions to Background.    Scenario       Behaviours                     :     :                Mic is unmuted Mic will remain   Unmuted   and the user will able to publish audio without any restriction   Camera is unmuted  Camera will get   Muted     Remote Peers are publishing audio  Incoming Audio from Room   Continues   to Play    Android Background Handling On Android devices, by default Capturing Video & Audio from Camera & Microphone is   allowed   for sometime  usually 60 seconds. This time limit depends on different Android OEMs, Battery Optimisations Mode (Aggressive/Doze), etc. This leads to inconsistent behaviours of your apps in Background Mode on different Android devices. As per your use-case, apps can choose to implement an  Android Foreground Service (https://developer.android.com/guide/components/foreground-services) to ensure consistent behaviour. With a Foreground Service, you can continuously access Microphone & Camera & publish the captured Audio & Video to other peers in the Room.  Foreground Services show a status bar notification, so that users are actively aware that your app is performing a task and is consuming system resources.  > Note: Playing Audio continuously while app is in Background is allowed by default on Android devices.   Without Android Foreground Service (Default Android Behaviour) The table below lists down Android app behaviours when the app transitions to background when a 100ms Room is ongoing without using any Android Foreground Service. The \"  Scenario  \" on the left of the Table below implies the activity which is ongoing in the App when user has joined a 100ms Room. The \"  Behaviours  \" on the right of the Table shows what happens when the App transitions to Background.     Scenario       Behaviours                     :     :                Mic is unmuted  Mic will remain   Unmuted   and the user will able to publish audio for a limited period (time depending on OEMs)   Camera is Unmuted  Camera will remain   Unmuted   and the user will able to publish video for a limited period (time depending on OS)   Remote Peers are publishing audio  Able to play incoming audio from the Room    With Android Foreground Service When your app implements a Foreground Service it continues access to capturing Audio & Video even when the app is in Background Mode.    Scenario       Behaviours                     :     :                Mic is unmuted  Mic will remain unmuted and the user will able to publish audio without any restriction   Camera is unmuted  Camera will remain unmuted and the user will able to publish video without any restriction   Remote Peers are publishing audio  Able to play incoming audio from the Room   For implementing a Foreground Service we recommend using the  flutter_foreground_task (https://pub.dev/packages/flutter_foreground_task) package which allows the app to run in the background with a Persistent Status Bar Notification. Here is step by step guide how to integrate flutter_foreground_task in your apps   <div className=\"steps-container\">   Add plugin Add flutter_foreground_task as a dependency in your pubspec.yaml file.   dependencies:  flutter_foreground_task: \"3.10.0\"     Adding Service in AndroidManifest.xml Depending on your use-case, your app may want either access to microphone or camera or both when the app is in Background. Ensure that you set the correct foregroundServiceType inside the <activity> .  Add service inside <application> tag as follows:   <application>   ...   <activity>     ...   </activity>    // Set Service Type to capture Audio or Video as per your use-case. In most cases, only microphone access would be required. Then do not pass the camera    <service android:name=\"com.pravera.flutter_foreground_task.service.ForegroundService\"     android:foregroundServiceType=\"camera microphone\"   </application>     Start Foreground Service Wrap your _Meeting page_ with WillStartForegroundTask widget. The page which is shown when an 100ms Room is joined should be encapsulated as shown below:  dart @override Widget build(BuildContext context)    return MaterialApp(   // A widget to start the foreground service when the app is minimized or closed.   // This widget must be declared above the  Scaffold  widget.   home: WillStartForegroundTask(    onWillStart: () async       // Return whether to start the foreground service.     return true;     ,    androidNotificationOptions: AndroidNotificationOptions(     channelId: 'notification_channel_id',     channelName: 'Foreground Notification',     channelDescription: 'This notification appears when the foreground service is running.',     channelImportance: NotificationChannelImportance.LOW,     priority: NotificationPriority.LOW,     iconData: NotificationIconData(      resType: ResourceType.mipmap,      resPrefix: ResourcePrefix.ic,      name: 'launcher',     ),    ),    iosNotificationOptions: const IOSNotificationOptions(     showNotification: false,     playSound: false,    ),    foregroundTaskOptions: const ForegroundTaskOptions(     interval: 5000,     autoRunOnBoot: false,     allowWifiLock: false,    ),    notificationTitle: 'Foreground Service is running',    notificationText: 'Tap to return to the app',    callback: startCallback,    child: Scaffold(     appBar: AppBar(      title: const Text('Your App Title'),      centerTitle: true,     ),     body: MeetingApp(), // your app screen which is shown when 100ms Room is joined    ),   ),  );     </div> That's it. Now you'll have a Foreground Service up & running in your app. > Checkout the Background Handling implementations in 100ms Example app  here (https://github.com/100mslive/100ms-flutter/tree/main/example).  To know more about Android flutter_foreground_task package, visit  here (https://pub.dev/packages/flutter_foreground_task). "
    },
    {
        "title": "Call Stats\r",
        "link": "/flutter/v2/features/call-stats",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/call-stats",
        "keywords": [],
        "content": " -\r \r Sometimes you need a way to capture certain metrics related to a call. This may be helpful if you want to tailor the experience to your users or debug issues. Typical metrics of interest are audio/video bitrate, round trip time, total consumed bandwidth and packet loss.\r \r 100ms SDK provides this data via dedicated delegate callbacks found in HMSStatsListener . These will be called with a fixed interval of one second after a room has been joined. You can get stats on a per-track basis ( onRemoteAudioStats ) or as an overall summary ( onRTCStats ) \r \r  HMSStatsListener callbacks\r \r Here is the full list of callbacks:\r \r  dart\r /// This callback provides stats for a local audio track.\r void onLocalAudioStats( required HMSLocalAudioStats hmsLocalAudioStats, required HMSLocalAudioTrack track, required HMSPeer peer )\r \r /// This callback provides stats for a local video track with all the available layers.\r void onLocalVideoStats( required List<HMSLocalVideoStats> hmsLocalVideoStats, required HMSLocalVideoTrack track, required HMSPeer peer )\r \r /// This callback provides stats for a remote audio track.\r void onRemoteAudioStats( required HMSRemoteAudioStats hmsRemoteAudioStats, required HMSRemoteAudioTrack track, required HMSPeer peer )\r \r /// This callback provides stats for a remote video track.\r void onRemoteVideoStats( required HMSRemoteVideoStats hmsRemoteVideoStats, required HMSRemoteVideoTrack track, required HMSPeer peer )\r \r /// This callback provides combined stats for the session.\r void onRTCStats( required HMSRTCStatsReport hmsrtcStatsReport )\r  \r \r To listen to these callbacks we will need to attach HMSStatsListener and override the above methods.\r Let's see how we can listen to these callbacks: \r \r <div className=\"steps-container\">\r \r   Implement HMSStatsListener in a Class\r \r  dart \r class StatsReport implements HMSStatsListener  \r  \r \r   Call addStatsListener\r \r Call addStatsListener on the hmsSDK instance without which by default RTC Stats will not be sent to the app.\r \r  dart \r //hmsStatsListener is an instance of HMSStatsListener\r hmsSDK.addStatsListener(listener: hmsStatsListener);\r  \r \r   Implement HMSStatsListener Methods\r \r  dart\r class StatsReport implements HMSStatsListener \r \r  @override\r  void onLocalAudioStats(\r     required HMSLocalAudioStats hmsLocalAudioStats,\r    required HMSLocalAudioTrack track,\r    required HMSPeer peer )   \r \r  @override\r  void onLocalVideoStats(\r     required List<HMSLocalVideoStats> hmsLocalVideoStats,\r    required HMSLocalVideoTrack track,\r    required HMSPeer peer )   \r \r  @override\r  void onRemoteAudioStats(\r     required HMSRemoteAudioStats hmsRemoteAudioStats,\r    required HMSRemoteAudioTrack track,\r    required HMSPeer peer )   \r \r  @override\r  void onRemoteVideoStats(\r     required HMSRemoteVideoStats hmsRemoteVideoStats,\r    required HMSRemoteVideoTrack track,\r    required HMSPeer peer )   \r \r  @override\r  void onRTCStats( required HMSRTCStatsReport hmsrtcStatsReport )   \r  \r  \r \r   Remove the listener when not required\r \r To stop listening to updates or while leaving the room we will need to remove this listener as:\r \r  dart \r //hmsStatsListener is an instance of HMSStatsListener which we passed in addStatsListener\r hmsSDK.removeStatsListener(listener: hmsStatsListener);\r  \r </div>\r \r  Supplementary bytes\r \r   HMSRTCStatsReport\r \r This class will contain the combined stats for the room.\r \r  dart\r class HMSRTCStats  \r  // Total bytes sent in the current session.\r  int bytesSent;\r  // Total bytes received in the current session.\r  int bytesReceived;\r  // Total packets received in the current session.\r  int packetsReceived;\r  // Total packets lost in the current session.\r  int packetsLost;\r  // Total outgoing bitrate observed since the previous report.\r  double bitrateSent;\r  // Total incoming bitrate observed since the previous report in Kb/s.\r  double bitrateReceived;\r  // Average round trip time observed since the previous report in Kb/s.\r  double roundTripTime;\r  \r \r class HMSRTCStatsReport  \r  // Combined audio + video values\r  HMSRTCStats combined;\r  // Summary of all audio tracks\r  HMSRTCStats audio;\r  // Summary of all video tracks\r  HMSRTCStats video;\r  \r  \r \r   HMSLocalAudioStats\r \r This class contains stats related to the local audio track.\r \r  dart\r class HMSLocalAudioStats  \r  // Round trip time observed since the previous report\r  double roundTripTime;\r  // Total bytes sent by this track in the current session\r  int bytesSent;\r  // Outgoing bitrate of this track observed since the previous report in Kb/s\r  double bitrate;\r  \r  \r \r   HMSLocalVideoStats\r \r This class contains stats related to local video track.\r \r  dart\r class HMSLocalVideoStats  \r \t// Round trip time observed since the previous report.\r  double roundTripTime;\r  // Total bytes sent by this track in the current session.\r  int bytesSent;\r  // Outgoing bitrate of this track observed since the previous report in Kb/s.\r  double bitrate;\r  // Resolution of video frames being sent.\r  double frameRate;\r  // Frame rate of video frames being sent (FPS).\r  HMSResolution resolution;\r  ///Reason for quality limitations\r  HMSQualityLimitationReasons? hmsQualityLimitationReasons;\r  ///Simulcast Layer\r  HMSSimulcastLayer? hmsLayer;\r  \r  \r \r   HMSRemoteAudioStats\r \r This class contains stats related to the remote audio track.\r \r  dart\r class HMSRemoteAudioStats  \r  // Packet Jitter measured in seconds for this track. Calculated as defined in section 6.4.1. of RFC3550.\r  double jitter;\r  // Total bytes received by this track in the current session.\r  int bytesReceived;\r  // Incoming bitrate of this track observed since the previous report in Kb/s.\r  double bitrate;\r  // Total packets received by this track in the current session.\r  int packetsReceived;\r  // Total packets lost by this track in the current session.\r  int packetsLost;\r  \r  \r \r   HMSRemoteVideoStats\r \r This class contains stats related to the remote video track.\r \r  dart\r class HMSRemoteVideoStats  \r  // Packet Jitter measured in seconds for this track. Calculated as defined in section 6.4.1. of RFC3550.\r  double jitter;\r  // Total bytes received by this track in the current session.\r  int bytesReceived;\r  // Incoming bitrate of this track observed since the previous report in Kb/s.\r  double bitrate;\r  // Total packets received by this track in the current session.\r  int packetsReceived;\r  // Total packets lost by this track in the current session.\r  int packetsLost;\r  // Resolution of video frames being received.\r  HMSResolution resolution;\r  // Frame rate of video frames being received (FPS).\r  double frameRate;\r  \r  \r "
    },
    {
        "title": "Change Role",
        "link": "/flutter/v2/features/change-role",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/change-role",
        "keywords": [],
        "content": "   Role is a powerful concept that takes a lot of complexity away in handling permissions and supporting features like breakout rooms.  Each HMSPeer instance has a role property which returns an HMSRole instance. You can use this property to do the following: 1. Check what this Role is allowed to publish i.e. can it publish a video (and at what resolution)? Can it publish audio? Can it share a screen? Who can this role subscribe to?    For example, a Student Role can only see the Teacher's video. This is can be discovered by checking publishSettings and subscribeSettings properties.  2. Check what actions this Role can perform. i.e can it change someone else's current Role, End the Meeting, or remove someone from the room? This is can be discovered by checking the permissions property.  In certain scenarios, you may want to change someone's role. Imagine an audio room with 2 roles speaker and listener . Only someone with a speaker role can publish audio to the room while a listener can only subscribe. Now at some point, the speaker may decide to nominate some listener to become a speaker .  This is where  Change Role  capabilities come into play.  You may choose to do either: 1.  Single Peer Role Change ( single-peer-role-change): Change the role of a single peer to a specified one using the changeRoleOfPeer API  2.  Bulk Role Change ( bulk-role-change): Change the role of all peers with a certain role, to a specified one using the changeRoleOfPeersWithRoles API   Single Peer Role Change > Note: 🔑 changeRoleOfPeer is the same as changeRole  but we have deprecated changeRole and it will be removed in future releases. So, Please > use changeRoleOfPeer .   dart class Meeting implements HMSUpdateListener, HMSActionResultListener     ...   void changeRoleOfPeer( required HMSPeer peer,required HMSRole roleName,bool forceChange )       /// peer : peer whose role needs to be changed     /// toRole : the role which we want the peer to be in(destination role).     /// forceChange : it indicates whether to change the role of a peer forcefully     ///If set to true then the role will be changed without asking permission from the  peer      ///In case of false permissions will be asked and the role will be changed only after the request is accepted.     /// hmsActionResultListener : an instance of a class which implements HMSActionResultListener     //Here this is an instance of a class that implements HMSActionResultListener i.e. Meeting     hmsSDK.changeRoleOfPeer(     forPeer: peer,     toRole: roleName,     force: forceChange,     hmsActionResultListener: this);       @override   void onSuccess(      HMSActionResultListenerMethod methodType =       HMSActionResultListenerMethod.unknown,     Map<String, dynamic>? arguments )         switch (methodType)         ...       case HMSActionResultListenerMethod.changeRoleOfPeer:       //Here we will receive the success callback       break;       ...             @override   void onException(      HMSActionResultListenerMethod methodType =       HMSActionResultListenerMethod.unknown,     Map<String, dynamic>? arguments,     required HMSException hmsException )         switch (methodType)           ...          case HMSActionResultListenerMethod.changeRoleOfPeer:         // Check the HMSException object for details about the error         break;         ...                To invoke this method we will need 4 parameters.   forPeer : An instance of HMSPeer of the peer whose role you want to change.   toRole : The HMSRole instance for the target role.   force : Whether you want to change their role without asking them or give them a chance to accept/reject.   hmsActionResultListener : The HMSActionResultListener will get a success or failure callback depending on the result of the request. > How to handle it at receiver side if force is false.Check it out  here ( accept-role-change-request) > Note: success doesn't mean that the role was changed, just that the server accepted the request as valid. If the changeRole was succeeded you will get an update in the onPeerUpdate with roleUpdated as update type, With the same peer, you passed as forPeer and an HMSPeerUpdate.roleUpdated update type.   dart class Meeting implements HMSUpdateListener, HMSActionResultListener    ...  @override  void onPeerUpdate( required HMSPeer peer, required HMSPeerUpdate update ) async     switch (update)      ...    case HMSPeerUpdate.roleUpdated:    //Here we will get the update with peer being the same as the forPeer we sent in changeRole    break;    ...              Bulk Role Change Bulk Role Change is used when we want to change the role of peers with a specific role to another role. For example, if peers join a room with a waiting role and now you want to change all these peers to viewer role then use the changeRoleOfPeersWithRoles method.   dart class Meeting implements HMSUpdateListener, HMSActionResultListener     ...    void changeRoleOfPeersWithRoles(     required HMSRole toRole,    required List<HMSRole> ofRoles,    HMSActionResultListener? hmsActionResultListener )       /// toRole : the role you wish to move peers into(destination role)     /// ofRoles : List of roles whose role needs to be changed     /// hmsActionResultListener : an instance of a class which implements HMSActionResultListener     //Here this is an instance of a class that implements HMSActionResultListener i.e. Meeting     hmsSDK.changeRoleOfPeersWithRoles(     toRole: toRole,     ofRoles: ofRoles,     hmsActionResultListener: this);       @override   void onSuccess(      HMSActionResultListenerMethod methodType =       HMSActionResultListenerMethod.unknown,     Map<String, dynamic>? arguments )         switch (methodType)         case HMSActionResultListenerMethod.changeRoleOfPeer:       break;       case HMSActionResultListenerMethod.changeRoleOfPeersWithRoles:       //Here we will receive the success callback       break;       ...             @override   void onException(      HMSActionResultListenerMethod methodType =       HMSActionResultListenerMethod.unknown,     Map<String, dynamic>? arguments,     required HMSException hmsException )         switch (methodType)           case HMSActionResultListenerMethod.changeRoleOfPeer:         break;         case HMSActionResultListenerMethod.changeRoleOfPeersWithRoles:         // Check the HMSException object for details about the error         break;         ...                 Edge cases with bulk role change 1. Note that if an empty list is sent to ofRoles , no roles will be changed. This is to avoid accidentally changing roles you may not have intended such as the bots that provide recording and streaming with the roles beam . 2. Also, Bulk Role Changes are always forced i.e. no option will be given for the peer to accept it, they will just be changed immediately.   Let's have a look at an example of bulk role change Here's how the method could be called to change all waiting and guest roles to host :   dart // fetch all available Roles in the room List<HMSRole> roles = await hmsSDK.getRoles(); // get the Host Role object HMSRole toHostRole = roles.firstWhere((element) => element.name == \"host\"); // get a list of Roles to be updated  in this case, \"Waiting\" and \"Guest\" Roles roles.retainWhere((element) => ((element.name == \"waiting\")  (element.name == \"guest\"))); // now perform Role Change of all peers in \"Waiting\" and \"Guest\" Roles to the \"Host\" Role hmsSDK.changeRoleOfPeersWithRoles(   toRole: toHostRole,   ofRoles: roles,   hmsActionResultListener: hmsActionResultListener);     Bulk Role Change Errors You may get the following errors for bulk role change:  Message                 Meaning                                                      invalid role           A role in the list of roles to change does not exist in this room.        target role clash with requested roles  the 'toRole' is also listed as one to change to 'toRole'   role does not have required permission  Peer does not have role change permission.          peer left                The peer who's role was to be changed has left.       role invalid               The 'toRole' is invalid.                    Accept Role change Request When a peer wishes to change the role of another peer it calls changeRoleOfPeer with a parameter force . The force parameter in changeRoleOfPeer ,  when true role is directly changed and the peer receives an onPeerUpdate with method type as roleUpdated .  when false , is a polite request: \"Would you like to change your role from listener to speaker?\" Which can be ignored by the peer. The way it works is the other peer will first receive an onRoleChangeRequest callback in HMSUpdateListener : Let's understand this with a diagram:    accept-change-role (/docs/v2/flutter-accept-role-change.png) Now let's do it step-by-step: <div className=\"steps-container\">   PeerA calls changeRoleOfPeer with force as false   dart //Assuming that HMSActionResultListener is implemented in the class where this function is being called hmsSDK.changeRoleOfPeer(     forPeer: peerB,     toRole: new-role,     force: false,     hmsActionResultListener: this);     PeerB receives onRoleChangeRequest PeerB receives onRoleChangeRequest in the class wherever HMSUpdateListener is implemented.   dart class Meeting implements HMSUpdateListener, HMSActionResultListener    ...    @override  void onRoleChangeRequest( required HMSRoleChangeRequest roleChangeRequest )     //Show some popup asking for permissions        At this point, the app should show a prompt to the user asking for permission to accept or deny role change.    How to extract info from HMSRoleChangeRequest to show popups for permission  HMSRoleChangeRequest is a class that contains info about the role change request. It contains two attributes:   suggestedRole  -> This is an HMSRole object which contains info about the destination role   suggestedBy  -> This is an HMSPeer object which contains info about the peer who performed a role change  To get information like role name and peer who performed role change we can do it like :   dart class Meeting implements HMSUpdateListener, HMSActionResultListener      ...   @override  void onRoleChangeRequest( required HMSRoleChangeRequest roleChangeRequest )     String? peerWhoPerformedRoleChange = roleChangeRequest.suggestedBy?.name;   String? destinationRole = roleChangeRequest.suggestedRole.name          PeerB calls acceptChangeRole If the user wants to accept the request, the app should invoke acceptChangeRole on HMSSDK instance:   dart //roleChangeRequest: this is the same request which we received in onRoleChangeRequest hmsSDK.acceptChangeRole(hmsRoleChangeRequest: roleChangeRequest, hmsActionResultListener: hmsActionResultListener);     All the peers receive onPeerUpdate with type as roleUpdated Now, all peers in the room will receive an HMSPeerUpdate.roleUpdated callback so that they can do the necessary UI updates. PeerB will have all the permissions of new role. </div>    Case with changeRoleOfPeer with force as true Now let's imagine the newly nominated speaker is not behaving nicely and we want to move him back to the listener without a prompt. This is where the force parameter comes in. When it is set to true the other party will not receive a confirmation roleChangeRequest but instead will straight away receive a new set of updated permissions and stop publishing.  HMSPeerUpdate.roleUpdated callback will still be fired so that the app can update the user's UI state.  "
    },
    {
        "title": "Change User Name\r",
        "link": "/flutter/v2/features/change-user-name",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/change-user-name",
        "keywords": [],
        "content": " -\r \r Any peer can change their own name before or after joining a room. Before joining, the name would have to be specified in HMSConfig that is passed to the join method. This document shows how the name can be changed after joining.\r \r  Changing the name\r \r The peer who wants to change their name should call the following method on an HMSSDK instance.\r \r  dart\r class Meeting implements HMSUpdateListener, HMSActionResultListener \r \r   ...\r \r   void changeName(\r     required String name,\r    required HMSActionResultListener hmsActionResultListener )  \r     /// name  : the updated name\r     /// hmsActionResultListener : an instance of a class that implements HMSActionResultListener\r     //Here this is an instance of a class that implements HMSActionResultListener i.e. Meeting\r     hmsSDK.changeName(name: name, hmsActionResultListener: this);\r     \r \r   @override\r   void onSuccess(\r      HMSActionResultListenerMethod methodType =\r       HMSActionResultListenerMethod.unknown,\r     Map<String, dynamic>? arguments )  \r       switch (methodType)  \r       case HMSActionResultListenerMethod.changeName:\r       //Name successfully changed\r       break; \r          \r    \r \r \r   @override\r   void onException(\r      HMSActionResultListenerMethod methodType =\r       HMSActionResultListenerMethod.unknown,\r     Map<String, dynamic>? arguments,\r     required HMSException hmsException )  \r       switch (methodType)  \r \r       ...\r \r       case HMSActionResultListenerMethod.changeName:\r       // Check the HMSException object for details about the error\r       break;\r \r      \r    \r  \r  \r \r  Responding to name changes\r \r Once the name change is successful all the peers receive onPeerUpdate with the method type nameChanged which can be used to update peer info in the application.\r So, Whenever a remote peer's name is changed a callback will be received.\r \r  dart \r class Meeting implements HMSUpdateListener, HMSActionResultListener \r \r   ...\r \r   @override\r   void onPeerUpdate(\r     required HMSPeer peer, required HMSPeerUpdate update ) async  \r \r    switch (update)  \r \r     ...\r \r     case HMSPeerUpdate.nameChanged:\r     //update the HMSPeer object here in the application\r     break;\r \r     ...\r \r     \r \r    \r \r   void changeName(\r     required String name,\r    required HMSActionResultListener hmsActionResultListener )  \r     /// name  : the updated name\r     /// hmsActionResultListener : an instance of a class that implements HMSActionResultListener\r     //Here this is an instance of a class that implements HMSActionResultListener i.e. Meeting\r     hmsSDK.changeName(name: name, hmsActionResultListener: this);\r     \r \r   @override\r   void onSuccess(\r      HMSActionResultListenerMethod methodType =\r       HMSActionResultListenerMethod.unknown,\r     Map<String, dynamic>? arguments )  \r       switch (methodType)  \r       case HMSActionResultListenerMethod.changeName:\r       //Name successfully changed\r       break; \r          \r    \r \r \r   @override\r   void onException(\r      HMSActionResultListenerMethod methodType =\r       HMSActionResultListenerMethod.unknown,\r     Map<String, dynamic>? arguments,\r     required HMSException hmsException )  \r       switch (methodType)  \r \r       ...\r       \r       case HMSActionResultListenerMethod.changeName:\r       // Check the HMSException object for details about the error\r       break;\r \r      \r    \r  \r  \r "
    },
    {
        "title": "Chat",
        "link": "/flutter/v2/features/chat",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/chat",
        "keywords": [],
        "content": "  What's a video call without being able to send messages to each other too? 100ms supports chat for every video/audio room you create.   Message Types    Broadcast messages ( sending-broadcast-messages) are sent to Everyone in the chat hmsSDK.sendBroadcastMessage .    Direct messages ( sending-direct-messages) are sent to a specific person hmsSDK.sendDirectMessage .    Group messages ( sending-group-messages) are sent to everyone with a particular HMSRole . Such as all hosts or all teachers or all students  hmsSDK.sendGroupMessage .     You can learn more about  Templates and Roles here (../foundation/templates-and-roles).   Sending Chat Messages Let's look at a simple implementation for chat methods:   dart class Meeting implements HMSUpdateListener, HMSActionResultListener   ...  // onMessage is HMSUpdateListener method called when a new message is received  @override  void onMessage( required HMSMessage message )     //Here we will receive messages sent by other peers     void sendBroadcastMessage(String message,String type)       /// message : Message to be sent     /// type : Message type(More about this at the end)     /// hmsActionResultListener : instance of class implementing HMSActionResultListener     //Here this is an instance of class that implements HMSActionResultListener i.e. Meeting     hmsSDK.sendBroadcastMessage(      message: message,      type: type,      hmsActionResultListener: this);     void sendDirectMessage(String message, HMSPeer peerTo,String type) async       /// message : Message to be sent     /// peerTo : Peer to whom message needs to be sent     /// type : Message type(More about this at the end)     /// hmsActionResultListener : instance of class implementing HMSActionResultListener     //Here this is an instance of class that implements HMSActionResultListener i.e. Meeting     hmsSDK.sendDirectMessage(      message: message,      peerTo: peerTo,      type: type,      hmsActionResultListener: this);     void sendGroupMessage(String message, List<HMSRole> rolesToSendMessage,String type) async       /// message : Message to be sent     /// hmsRolesTo : Roles to which this message needs to be sent     /// type : Message type(More about this at the end)     /// hmsActionResultListener : instance of class implementing HMSActionResultListener     //Here this is an instance of class that implements HMSActionResultListener i.e. Meeting     hmsSDK.sendGroupMessage(       message: message,       hmsRolesTo: rolesToSendMessage,       type: type,       hmsActionResultListener: this);     @override  void onSuccess(    HMSActionResultListenerMethod methodType =     HMSActionResultListenerMethod.unknown,   Map<String, dynamic>? arguments )       switch (methodType)        case HMSActionResultListenerMethod.sendBroadcastMessage:      //Broadcast Message sent successfully      break;      case HMSActionResultListenerMethod.sendGroupMessage:      //Group Message sent successfully      break;      case HMSActionResultListenerMethod.sendDirectMessage:      //Direct Message sent successfully      break;      ...           @override  void onException(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments,    required HMSException hmsException )       switch (methodType)         case HMSActionResultListenerMethod.sendBroadcastMessage:       // Check the HMSException object for details about the error       break;       case HMSActionResultListenerMethod.sendGroupMessage:       // Check the HMSException object for details about the error       break;       case HMSActionResultListenerMethod.sendDirectMessage:       // Check the HMSException object for details about the error       break;       ...             ...      Now lets at each method one by one :   Sending Broadcast Messages Want to let everyone in the chat know something? Call sendBroadcastMessage on the instance of HMSSDK to send a broadcast. The parameters are: 1.  message : The text of the message. 2.  type  optional : The type of the message, default is chat . 3.  hmsActionResultListener : An instance of HMSActionResultListener . > 💡 Note that the callback only lets you know if the server has received your request for the message or if there was some error. > >It does not convey whether the message was delivered to or read by the recipient. > >Also it's important to make a new callback per message because it will only contain the results of that particular call for sending a message.   dart hmsSDK.sendBroadcastMessage(message: \"Hi\", hmsActionResultListener: hmsActionResultListener);     Sending Direct Messages Got secrets to share? Send a message directly to a single person with a direct message. Call sendDirectMessage on an instance of HMSSDK . The parameters are: 1.  message : The text of the message. 2.  peerTo : The HMSPeer instance that should receive the message. 3.  type  optional : The type of the message, default is chat . 4.  hmsActionResultListener : An instance of HMSActionResultListener .  > 💡 Note that the callback only lets you know if the server has received your request for the message or if there was some error. > >It does not convey whether the message was delivered to or read by the recipient. > >Also it's important to make a new callback per message because it will only contain the results of that particular call for sending a message.   dart hmsSDK.sendDirectMessage(     message: \"Hi\",     peerTo: peerTo,     hmsActionResultListener: hmsActionResultListener);      Sending Group Messages Need to call attention to all the hosts? All the teachers? All the developers? Call sendGroupMessage on an instance of HMSSDK . The parameters are: 1.  message : The text of the message. 2.  hmsRolesTo : The list of HMSRole i.e the roles to whom the message needs to be sent. 3.  type  optional : The type of the message, default is chat . 4.  hmsActionResultListener : An instance of HMSActionResultListener . > 💡 Note that the callback only lets you know if the server has received your request for the message or if there was some error. > >It does not convey whether the message was delivered to or read by the recipient. > >Also it's important to make a new callback per message because it will only contain the results of that particular call for sending a message.   dart hmsSDK.sendGroupMessage(message: \"Hi\",     hmsRolesTo: hmsRolesTo,     hmsActionResultListener: hmsActionResultListener);     Receiving Chat Messages When you called hmsSDK.join(config) to join a room, the HMSUpdateListener implementation that was passed in had the callback void onMessage( required HMSMessage message ); . This is where you'll receive new messages as HMSMessage during the call. Let's look at what the HMSMessage class looks like:   dart class HMSMessage(  final HMSPeer? sender;  final String message;  final String type;  final DateTime time;  HMSMessageRecipient? hmsMessageRecipient;  HMSMessage(    required this.sender,   required this.message,   required this.type,   required this.time,   this.hmsMessageRecipient   ); )      message : Content of the text message or the text description of the raw message.    type : Type of message sent. The default value is chat .    hmsMessageRecipient : The intended recipient(s) of this message as a HMSMessageRecipient .    time : DateTime of when the messaging server receives this message. This can be used for accurate ordering of your messages.    sender : The HMSPeer who is sending this message. Let's break down each parameter a bit more in detail : <div className=\"steps-container\">    Identifying Senders: The sender of a message is always contained in the sender field of HMSMessage. This lets you get the name and peer id of the sender.   Message Body: The body of the message is in message as a String.   Time: The time the message was sent is contained in time as a DateTime datatype.   Putting together a list of chat messages. The UI is completely up to you to decide  You'll also need to hold onto all the received messages if you want to display history.   Identifying who the message was for The HMSMessageRecipient contained in the hmsMessageRecipient field of HMSMessage lets you know who the message was for. The HMSMessageRecipient contains:   dart class HMSMessageRecipient    HMSPeer? recipientPeer;   List<HMSRole>? recipientRoles;   HMSMessageRecipientType hmsMessageRecipientType;      recipientPeer : Only contains a peer when a specific single peer is being direct messaged.  recipientRoles : Only contains values when a group message is being sent to many roles.  hmsMessageRecipientType : Can contain enum values BROADCAST , PEER and ROLES .  HMSMessageRecipientType.BROADCAST for a message being sent to everyone. If this is true, the other two fields will be null and empty respectively.  HMSMessageRecipientType.PEER will be set when it 's a direct message.  HMSMessageRecipientType.ROLES will be set when it 's a message to one or many roles.   Store Chat Messages You can store chat message by creating a list of HMSMessage and append new messages from the callback onMessage( required HMSMessage message ) as follow:   dart List<HMSMessage> messages =   ; @override void onMessage( required HMSMessage message )    messages.add(message);       Filter Chat Messages You can filter chat messages by creating a separate list of HMSMessage .   dart List<HMSMessage> broadcastList =   ; List<HMSMessage> rolesList =   ; List<HMSMessage> peerList =   ; List<HMSMessage> messageFilter(List<HMSMessage> messages, HMSMessageRecipientType type)    filterList =   ;  for(HMSMessage message in messages)     // Filter list based on Broadcast chat.   if(message.hmsMessageRecipient.hmsMessageRecipientType == HMSMessageRecipientType.BROADCAST)      broadcastList.add(message);       // Filter list based on Roles chat(group messages).   if(message.hmsMessageRecipient.hmsMessageRecipientType == HMSMessageRecipientType.ROLES)      rolesList.add(message);       // Filter list based on Peer chat(direct messages).   if(message.hmsMessageRecipient.hmsMessageRecipientType == HMSMessageRecipientType.PEER)      peerList.add(message);            </div>   Advanced Use-Cases Sometimes the app requires to show messages in different styles. Something similar to this:   Chat (/docs/v2/flutter-chat.gif) HMSSDK provides the type parameter of the HMSMessage object to take care of such use cases. You can send messages as:   dart hmsSDK.sendBroadcastMessage(message: \"😁\", type: \"emoji\", hmsActionResultListener: hmsActionResultListener);   Filter out the messages based on the type parameter and handle the UI accordingly.   dart //Example to show emoticons when the message type is set to emoji @override void onMessage( required HMSMessage message )    if(message.type==\"emoji\")     //Show as emoticons on UI     else    //Handle other cases        "
    },
    {
        "title": "End Room",
        "link": "/flutter/v2/features/end-room",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/end-room",
        "keywords": [],
        "content": "  Done with talking and it's time to end the video call room for everyone not just yourself? You may be looking to end the room.   How to end room  <div className=\"steps-container\">   Permissions Can't let just anyone end the video call room. First you need to create a  role (../foundation/templates-and-roles) with the permissions to end a room. The permission to end a room is called PermissionsParams.endRoom and you should check for that within the HMSRole of the peer to see if they have it. Here's how to check whether the local peer has the permission to end the room:   dart Future<bool> isAllowedToEndRoom() async    return (await hmsSDK.getLocalPeer()).role.permission?.endRoom;      hmsSdk.getLocalPeer() will not return null as long as you're in a preview or in a room. Since you likely won't need to check for permissions if you're not in one it would be safe to omit null check.   Ending the Room Once you're sure the peer has the permissions to end the room, we can end the room by calling:   dart class Meeting implements HMSActionResultListener    void endRoom(     required bool lock,    required String reason,    HMSActionResultListener? hmsActionResultListener ) async       //this is the instance of class which implements HMSActionResultListener     hmsSDK.endRoom(     lock: lock,     reason: \"Some reason to end room\",     hmsActionResultListener: this);        @override  void onSuccess(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments )        switch (methodType)         case HMSActionResultListenerMethod.endRoom:       //Room Ended successfully       break;       ...            @override  void onException(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments,    required HMSException hmsException )       switch (methodType)         case HMSActionResultListenerMethod.endRoom:       // Check the HMSException object for details about error       break;       ...                endRoom takes the following parameters  1.  lock : A Boolean for whether you want to prevent anyone from rejoining the room. If false, they will be allowed to enter the room again if the client called join . If this is true, they will NOT able to join this room again. 2  reason : reason String is the text that is passed describing why the room is being ended. 3.  hmsActionResultListener : It's the callback that would be called by SDK in case of a success or failure of endRoom operation > 💡 After calling endRoom the video calling UI should be disposed of as well since only the other peers will get the onPeerRemoved callback. The caller has to rely on the onSuccess callback for endRoom to decide when to terminate the meeting room UI locally.  </div>   How to handle an end room callback for receivers This section explains about how to handle the application if someone else ends the room. Once the peer with adequate permissions calls endRoom , all other peers in the room will receive a callback in HMSUpdateListener.onRemovedFromRoom .   dart  class Meeting implements HMSUpdateListener    @override   void onRemovedFromRoom( required HMSPeerRemovedFromPeer hmsPeerRemovedFromPeer )     //This callback is received if someone else ends the room   HMSPeer peerWhoEndedTheRoom = hmsPeerRemovedFromPeer.peerWhoRemoved   // roomWasEnded  will always be true in this case when room is ended.          The onRemovedFromRoom callback has a single parameter called HMSPeerRemovedFromPeer with the following structure.   dart class HMSPeerRemovedFromPeer    String reason;  bool roomWasEnded;  HMSPeer? peerWhoRemoved;     > 💡 This is the same callback that will be triggered if a peer is  removed from a room (/flutter/v2/features/remove-peer) as well. Except that roomEnded will be true when the entire room is ended.   _reason_: The string message detailing why the room was ended.   _peerWhoRemoved_: The details of the peer who called endRoom .   _roomWasEnded_: true if the entire room was ended. false if only the receiving peer(local peer) was removed. We can listen to this callback and show the appropriate UI. "
    },
    {
        "title": "Error Handling",
        "link": "/flutter/v2/features/error-handling",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/error-handling",
        "keywords": [],
        "content": "  When you make an API call to access an HMS SDK, the SDK may return error codes. Error codes are returned when a problem that cannot be recovered without app intervention has occurred. These are returned as HMSException in the onHMSError callback of the HMSUpdateListener . Following are the different error codes that are returned by the SDK. Before returning any error code, SDK retries the errors (whichever is possible ).    Error Code      Cause of the error                      Action to be taken                                               :     :               :                               1000       Generic Error                      Need to debug further with logs.                                          1003       Websocket disconnected  Happens due to network issues  Mention user to check their network connection or try again after some time.                    2002       Invalid Endpoint URL                   Check the endpoint provided while calling join on HMSSDK .                            2003       Endpoint is not reachable                Mention user to check their network connection or try again after some time.                    2004       Token is not in proper JWT format            The token passed while calling join is not in correct format. Retry getting a new token.             3000       Generic Error                      Need to debug further with logs.                                          3001       Cant Access Capture Device                Ask user to check permission granted to audio/video capture devices.                        3002       Capture Device is not Available             Ask user to check if the audio/video capture device is connected or not.                      3003       Capture device is in use by some other application    Show notification to user mentioning that the capturing device is used by some other application currently.     3005       There is no media to return               For building HMSTrackSettings either audio or video track has to be present.                    3006       Invalid Video Settings                  Simulcast cannot be started without providing video settings.                            3007       Codec cannot change mid call               Codec cannot be changed mid call.                                          3011       Mic Capture Failed                    Failed to capture mic access.                                            3012       Bluetooth Inaccessible                  BLUETOOTH_CONNECT permission missing .                                       3013       Bluetooth Inaccessible                  General Bluetooth Permission missing.                                            4001       WebRTC error                       Some webRTC error has occurred. Need more logs to debug.                              4002       WebRTC error                       Some webRTC error has occurred. Need more logs to debug.                              4003       WebRTC error                       Some webRTC error has occurred. Need more logs to debug.                              4004       WebRTC error                       Some webRTC error has occurred. Need more logs to debug.                              4005       ICE Connection Failed due to network issue        Mention user to check their network connection or try again after some time.                    5001       Trying to join a room which is already joined      Trying to join an already joined room.                                       5002       Trying to start Preview which is already started     Trying to start Preview which is already started.                                  6000       Client failed to connect                 Client failed to connect.                                              6002       webRTC Error: Error while renegotiating         Please try again.                                                  6004       Json parsing failed                   Need to debug further with logs.                                          6008       Unable to send message                  Cannot send message. Peer is null. The SDK must be disconnected from a room.                    6008       API not supported                    This API is not support on the current Android Version (Android-31).                        7001       Platform Not Supported                  The platform is not supported for plugin                                      7002       Plugin Init Failed                    Plugin initialization has failed                                          7003       Plugin Processing Failed                 Plugin processing failed                                              7004       Plugin Add Already Going on               Plugin add is already in progress                                          7005       Bluetooth Sco Connection Failed             Bluetooth headset is either not available or in a processing state.                         400       Error occurred                      This can usually happen due to token issues(Check logs for more description). Need more logs to debug.       401       Error occurred                      This can usually happen due to token issues(Check logs for more description). Need more logs to debug.       410       Peer is gone                       The peer is no more present in the room.                                      500       Error occurred                      This is a general server error(Check logs for more description). Need more logs to debug.            HMSException The SDK returns an error as an object of HMSException in onHMSError method.   dart @override void onHMSError( required HMSException error )    //Handle Error     Let's have a look at the HMSException object   dart class HMSException    final String? id;   //HMSException code  final HMSExceptionCode? code;  //Error message  final String message;  //Error description  String description;   //Action in which SDK failed  String action;    //Extra params sent with the error  Map? params;  //Whether the error is a terminal error or not  bool isTerminal;       Terminal Errors  Terminal errors are raised in cases when SDK has cleaned up the connection due to the error. isTerminal property of HMSException object can be used to check whether the error is terminal. Terminal error example:   Terminal Error (/docs/v2/flutter-errorCode-1003.png) The above error( errorCode:1003 ) occurs when the reconnection fails. The SDK returns this error after trying to reconnect for 60 seconds. In such cases, the isTerminal property can be used to handle the UI updates. > 🔑 More info on reconnection handling can be found  here (./reconnection-handling)   Setting log levels in SDK (Android Only) 100ms provides ability to save logs to disk on Android devices. These logs can be used to diagnose performance of room sessions. By default, logging is disabled i.e. set to HMSLogLevel.OFF . To enable logging, create the HMSLogSettings object & pass it while constructing the HMSSDK instance. This functionality of saving logs to Disk is not available on iOS.   dart // Create the Log Settings object HMSLogSettings hmsLogSettings = HMSLogSettings(     maxDirSizeInBytes: 1000000,     isLogStorageEnabled: true,     level: HMSLogLevel.VERBOSE); hmsSDK = HMSSDK(   hmsLogSettings: hmsLogSettings); // pass the Log Settings as a parameter while constructing the HMSSDK instance    HMSLogLevel is an enum with values:   dart enum HMSLogLevel    //To receive all the logs  VERBOSE,  //To receive warnings  WARN,  //To receive errors  ERROR,  //To turn OFF logs from SDK  OFF,  Unknown    "
    },
    {
        "title": "HLS Streaming",
        "link": "/flutter/v2/features/hls",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/hls",
        "keywords": [],
        "content": "   HLS Streaming (./../foundation/live-streaming) allows for scaling to millions of viewers in near real-time. We can give a link of our web app that will be converted to an HLS feed by our server and can be played across devices for consumption. Behind the scenes, this will be achieved by having a bot join our room and stream what it sees and hears. Once the feed is ready, the server will give a URL that can be played using any HLS Player. > Note that the media server serving the content in this case is owned by 100ms. If looking for a way to stream > on YouTube, Twitch etc., please have a look at our RTMP streaming docs  here (./recording).   Start/Stop HLS HLS can be started in two ways depending on the level of customization we need. 1. Default View: The simplest view to just begin a stream with default UI and parameters. 2. Custom Views: To use our own UI for HLS streaming, we need to provide our web-app URL for our bot to join and stream.          Also, we can record the stream.   Default View Begins a stream with default parameters. To start HLS with Default View, call hmsSDK.startHlsStreaming method.   dart class Meeting implements HMSUpdateListener, HMSActionResultListener   ...  void startHLSStreaming()    /// hmsActionResultListener : an instance of a class that implements HMSActionResultListener   //Here this is an instance of a class that implements HMSActionResultListener i.e. Meeting   hmsSDK.startHlsStreaming(hmsActionResultListener: this);     void stopHLSStreaming()    /// hmsActionResultListener : an instance of a class that implements HMSActionResultListener   //Here this is an instance of a class that implements HMSActionResultListener i.e. Meeting   hmsSDK.stopHlsStreaming(hmsActionResultListener: this);      @override  void onSuccess(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments )        switch (methodType)        ...      case HMSActionResultListenerMethod.hlsStreamingStarted:      //HLS Started successfully      break;       case HMSActionResultListenerMethod.hlsStreamingStopped:      //HLS Stopped successfully      break;              @override  void onException(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments,    required HMSException hmsException )        switch (methodType)        ...      case HMSActionResultListenerMethod.hlsStreamingStarted:      // Check the HMSException object for details about the error      break;      case HMSActionResultListenerMethod.hlsStreamingStopped:      // Check the HMSException object for details about the error      break;               Custom View To use our own browser UI for HLS, we'll need to pass in a meeting URL. The 100ms bot will open this URL to join our room, so it must allow access without any user-level interaction. We can call hmsSDK.startHLSStreaming with an HMSHLSConfig parameter. Let's look at its structure:   dart // this is the HLS config class class HMSHLSConfig    List<HMSHLSMeetingURLVariant>? meetingURLVariant;  HMSHLSRecordingConfig? hmsHLSRecordingConfig;     Let's look at its attributes one by one:    meetingURLVariant This is a list of HMSHLSMeetingURLVariant objects. In the future, it'll be possible to start HLS for multiple such URLs for the same room. So it's a list but currently, only the first element of the list is used. The HMSHLSMeetingURLVariant class looks like this:   dart class HMSHLSMeetingURLVariant    String? meetingUrl;  String metadata;       meetingUrl   URL of the meeting we wish to record   metadata   Metadata(Any extra info to identify the URL) we wish to pass with the URL. Let's look at how we can create an HMSHLSMeetingURLVariant and add it in meetingURLVariant :   dart  List<HMSHLSMeetingURLVariant>? meetingURLVariant =   ; HMSHLSMeetingURLVariant hlsUrlVariant = HMSHLSMeetingURLVariant(      meetingUrl: \"Meeting URL of the room\", metadata: \"HLS started from Flutter\"); meetingURLVariant.add(hlsUrlVariant);      hmsHLSRecordingConfig To record the HLS stream we may specify an HMSHLSRecordingConfig within the HMSHLSConfig . Here's what the HMSHlsRecordingConfig class looks like    dart class HMSHLSRecordingConfig    bool singleFilePerLayer;  bool videoOnDemand;       singleFilePerLayer : If the desired end result is a mp4 file per HLS layer, false by default.   enableVOD : If the desired result is a zip of m3u8 and all the chunks, false by default. Here's an example of how to create a recording config & start HLS Streaming with Recording    dart HMSHLSRecordingConfig recordingConfig = HMSHLSRecordingConfig(singleFilePerLayer: false, videoOnDemand: true);   This can be passed in HMSHLSConfig to start recording. Let's see the complete implementation of HLS streaming with recording turned ON:   dart class Meeting implements HMSUpdateListener, HMSActionResultListener   ...  void startHLSStreaming(String meetingUrl)    List<HMSHLSMeetingURLVariant>? meetingURLVariant =   ;   HMSHLSMeetingURLVariant hlsUrlVariant = HMSHLSMeetingURLVariant(        meetingUrl: meetingUrl, metadata: \"HLS started from Flutter\");   meetingURLVariant.add(hlsUrlVariant);   //This config enables recording with streaming   HMSHLSRecordingConfig recordingConfig = HMSHLSRecordingConfig(singleFilePerLayer: false, videoOnDemand: true);   HMSHLSConfig hlsConfig = HMSHLSConfig(     meetingURLVariant: meetingURLVariant,     hmsHLSRecordingConfig: hmshlsRecordingConfig);   /// hmshlsConfig : an instance of HMSHLSConfig which we created above   /// hmsActionResultListener : an instance of a class that implements HMSActionResultListener   //Here this is an instance of a class that implements HMSActionResultListener i.e. Meeting   hmsSDK.startHlsStreaming(hmshlsConfig: hlsConfig,hmsActionResultListener: this);     void stopHLSStreaming()    /// hmsActionResultListener : an instance of a class that implements HMSActionResultListener   //Here this is an instance of a class that implements HMSActionResultListener i.e. Meeting   hmsSDK.stopHlsStreaming(hmsActionResultListener: this);      @override  void onSuccess(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments )        switch (methodType)        ...      case HMSActionResultListenerMethod.hlsStreamingStarted:      //HLS Started successfully      break;       case HMSActionResultListenerMethod.hlsStreamingStopped:      //HLS Stopped successfully      break;              @override  void onException(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments,    required HMSException hmsException )        switch (methodType)        ...      case HMSActionResultListenerMethod.hlsStreamingStarted:      // Check the HMSException object for details about the error      break;      case HMSActionResultListenerMethod.hlsStreamingStopped:      // Check the HMSException object for details about the error      break;               How to display HLS stream and get HLS state in room The current status of the room is always reflected in the HMSRoom object. Here are the relevant properties inside the HMSRoom object which we can read to get the current HLS streaming status of the room namely: hlsStreamingState . The object contains a boolean running which lets us know if HLS is running in the room right now Also, it contains the m3u8 URL which we will use to display HLS on the screen. Just like this:   dart class Meeting implements HMSUpdateListener, HMSActionResultListener   ...   @override  void onRoomUpdate( required HMSRoom room, required HMSRoomUpdate update )        ...    //this contains info about whether HLS is running in the room or not   bool isHLSRunning = room.hmshlsStreamingState?.running??false;    //this contains the m3u8 URL if the HLS stream is running   //This URL can be passed to the video player to display HLS Stream.   String? hlsm3u8Url = hmshlsStreamingState?.variants 0 ?.hlsStreamUrl    ...         isHLSRunning can be used to show the live stream status on UI something similar to this:   hls-stream-state (/docs/v2/flutter-stream-state.jpg) > hlsm3u8Url string shown above contains the m3u8 URL which can be used to display HLS Stream.   When to check for room status The room status should be checked in the following places  1. In the onJoin(room: HMSRoom) callback of HMSUpdateListener   The properties mentioned above will be on the HMSRoom object. 2. In the onRoomUpdate(type: HMSRoomUpdate, hmsRoom: HMSRoom) callback of HMSUpdateListener .  The HMSRoomUpdate type will be HMSRoomUpdate.HLS_STREAMING_STATE_UPDATED .   Key Tips   If using the dashboard web app from 100ms, please make sure to use a role that doesn't have publish permissions for the beam tile to not show up.   If using a web app, do put in place retries for API calls like tokens etc. just in case any call fails. As human users, we're used to reloading the page in these scenarios which is difficult to achieve in the automated case.   Make sure to not disable the logs for the passed-in meeting URL. This will allow us to have more visibility into the room, refreshing the page if joining doesn't happen within a time interval.   Supplementary bytes   hlsStreamingState   an instance of HMSHLSStreamingState , which looks like:   dart /// running : bool value true indicates that RTMP streaming is running /// variants : a list of HMSHLSVariants class HMSHLSStreamingState(  bool running;  List<HMSHLSVariant?> variants; )     variants  -> This represents a live stream to one or more HLS URLs in the container of HMSHLSVariant . Which looks like this:   dart /// hlsStreamUrl : It contains m3u8 hlsStreamUrl which we will use to show the stream /// meetingUrl : URL of the room which is getting streamed /// metadata : Extra info about the room corresponding to meetingUrl which is passed in HMSHLSMeetingURLVariant while starting HLS /// startedAt : time at which RTMP streaming was started class HMSHLSVariant(  String? hlsStreamUrl;  String? meetingUrl;  String? metadata;  DateTime? startedAt; )   "
    },
    {
        "title": "HMSSDK Constructor",
        "link": "/flutter/v2/features/hmssdk",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/hmssdk",
        "keywords": [],
        "content": "    HMSSDK lifecycle    hmssdk-lifecycle (/docs/v2/flutter-hmssdk-lifecycle.png) Let's look this in detail:   Instantiate HMSSDK In majority of use-cases, constructing a simple HMSSDK object & calling build function are required.   dart HMSSDK hmsSDK = HMSSDK(); await hmsSDK.build(); // ensure to await while invoking the build method   For Advanced use-cases, 100ms provides following optional parameters while constructing the HMSSDK object   hmsTrackSetting : To customize local peer's Audio & Video track settings like Joining with Muted Audio or Video, changing default Camera (Front or Back), using Software Decoder for Video Rendering, etc. More details about Track Settings are available  here (https://www.100ms.live/docs/flutter/v2/advanced-features/set-track-settings).   appGroup & preferredExtension : (iOS Only) App Group & Preferred Extension are iOS only parameters required if you want to Start Screenshare from iOS devices. Passing correct App Group & Preferred Extension is required to allow users to perform Screenshare in a Room from iPhone or iPads. Refer iOS Screen share guide  here (https://www.100ms.live/docs/flutter/v2/features/screen-share i-os-setup).   hmsLogSettings : (Android Only) 100ms provides ability to save logs to disk on Android devices. These logs can be used to diagnose performance of room sessions. By default, logging is disabled i.e. set to HMSLogLevel.OFF . To enable logging, create the HMSLogSettings object & pass it while constructing the HMSSDK instance. This functionality of saving logs to Disk is not available on iOS. More details are available  here (https://www.100ms.live/docs/flutter/v2/features/error-handling setting-log-levels-in-sdk).   Initializing HMSSDK for Advanced Use-cases  You can initialize HMSSDK with either one or multiple of following optional parameters as per your use-cases. Say, for example, you want to Join with Muted Audio & Video and also allow users to start Screenshare from their iOS devices then pass the HMSTrackSetting , appGroup & preferredExtension parameters while constructing HMSSDK object. 1. Custom Track Settings using HMSTrackSetting  Following example shows creation of Track Settings object to join with Muted Audio & Video. By default, when you join a room, Camera & Microphone are ON. Using Track Settings you can allow users to choose to join with either Muted or Unmuted Audio & Video.   dart // create the Audio Track Settings object HMSAudioTrackSetting audioTrackSetting = HMSAudioTrackSetting(   trackInitialState: HMSTrackInitState.MUTED); // create the Video Track Settings object HMSVideoTrackSetting videoTrackSetting = HMSVideoTrackSetting(                   trackInitialState: HMSTrackInitState.MUTED); // use the above Audio & Video Track Settings object to create HMSTrackSettings HMSTrackSetting trackSetting = HMSTrackSetting(   audioTrackSetting: audioTrackSetting,   videoTrackSetting: videoTrackSetting); // Now, pass the Track Settings parameter while contructing the HMSSDK object HMSSDK hmsSDK = HMSSDK(hmsTrackSetting: trackSetting); await hmsSDK.build(); // ensure to await while invoking the build method   2. iOS Screenshare For adding appGroup and preferredExtension follow the iOS Screenshare guide  here (https://www.100ms.live/docs/flutter/v2/features/screen-share i-os-setup). After successfully following the iOS Screenshare guide you can find appGroup and preferredExtension name in Xcode under Signing and Capabilities section under target > yourExtensionName .  Once you have the correct App Group & Preferred Extension values created in Xcode & linked to your Apple Developer Account, you can now use them to start Screenshare from iOS devices.   dart // Pass the correct App Group & Preferred Extension parameters while contructing the HMSSDK object HMSSDK hmsSDK = HMSSDK(   appGroup: \"appGroup\", // App Group value linked to your Apple Developer Account   preferredExtension: \"preferredExtension\"); // Name of the Broadcast Upload Extension Target created in Xcode await hmsSDK.build(); // ensure to await while invoking the build method      iOS Screenshare Parameters (/docs/v2/flutter-dart-help.png)  3. Android Logging to Disk To diagnose 100ms room sessions you can enable Logging to Disk on Android devices by creating the HMSLogSettings object. The following example shows how to set debug level to VERBOSE .   dart // initialize Log Settings object HMSLogSettings hmsLogSettings = HMSLogSettings(   maxDirSizeInBytes: 1000000, // pass a value for setting the maximum log file size   isLogStorageEnabled: true,  // pass true to enable storage to Disk   level: HMSLogLevel.VERBOSE); // pass the appropriate log level  Verbose, Warn or Error // pass the Log Settings while contructing the HMSSDK object HMSSDK hmsSDK = HMSSDK(hmsLogSettings: hmsLogSettings); await hmsSDK.build(); // ensure to await while invoking the build method     What does Build Method do? The build method of HMSSDK creates an instance of 100ms SDK on Native iOS / Android platforms. It's recommended that you only create a single instance of HMSSDK . This allows for quick & simple integration & avoids State Management complexities. So the invocation of build method should only happen once in your app lifecycle. The build method should always be called after creating an instance of the HMSSDK .   dart await hmsSDK.build(); // ensure to await while invoking the build method     What does Destroy Method do? The destroy method performs the opposite action of build . It removes & clears 100ms SDK instances from Native iOS / Android platforms. destroy should be called when the user has a left a meeting room & you now want to perform cleanup of all references to 100ms. After calling destroy you should also set HMSSDK reference object to null.   dart HMSSDK hmsSDK = HMSSDK(); // construct HMSSDK object await hmsSDK.build(); // ensure to await while invoking the build method ... // perform Room actions like join room, chat, streaming, recording, screenshare, etc await hmsSDK.leave(hmsActionResultListener: this); // leave the room & attach a listener to check if leave was successful or not ... // when leave is successful, or you are going back to App Screen where 100ms Rooms are not required, invoke destroy method hmsSDK.destroy(); // destroy performs 100ms internal cleanup actions hmsSDK = null; // set the hmsSDK to null for cleanup   Now, if you want to join a room again, first initialize the HMSSDK , invoke build method & then call the  Join (https://www.100ms.live/docs/flutter/v2/features/join) method. "
    },
    {
        "title": "Integrating The SDK",
        "link": "/flutter/v2/features/integration",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/integration",
        "keywords": [],
        "content": "    Installing our libraries Add the hmssdk_flutter plugin in dependencies of pubspec.yaml   dart pubspec.yaml dependencies:  flutter:   sdk: flutter  hmssdk_flutter:   > HMSSDK is supported with flutter 3.3.x or above versions. > There are 2 ways of adding plugin in pubspec.yaml file with and without version locking. > Without version locking flutter will fetch the latest compatible package with your project. > Lock version will load the specific version written in pubspec.yaml file.   Setting up the library Run following in command in terminal.   bash flutter pub get    Update Version 2 ways of upgrade version 1. Find the latest version of hmssdk_flutter at  here (https://pub.dev/packages/hmssdk_flutter) and update pubspec.yaml file. 2. Using the following command in terminal:   bash flutter pub upgrade     Device Permissions   Android  Camera, Recording Audio and Internet permissions are required. Add them to your AndroidManifest.xml.   xml <uses-feature android:name=\"android.hardware.camera\"  <uses-feature android:name=\"android.hardware.camera.autofocus\"  <uses-permission android:name=\"android.permission.CAMERA\"  <uses-permission android:name=\"android.permission.CHANGE_NETWORK_STATE\"  <uses-permission android:name=\"android.permission.MODIFY_AUDIO_SETTINGS\"  <uses-permission android:name=\"android.permission.RECORD_AUDIO\"  <uses-permission android:name=\"android.permission.INTERNET\"  <uses-permission android:name=\"android.permission.ACCESS_NETWORK_STATE\"  <uses-permission android:name=\"android.permission.FOREGROUND_SERVICE\"   <uses-permission android:name=\"android.permission.BLUETOOTH\" android:maxSdkVersion=\"30\"   <uses-permission android:name=\"android.permission.BLUETOOTH_CONNECT\"       Update target Android version Update the Android SDK version to 21 or later by navigating to the directory and updating the android/app/build.gradle file   xml defaultConfig   minSdkVersion 21  …       iOS Add the entitlements for video, audio and network access to your Info.plist   xml <key>NSCameraUsageDescription</key> <string>Allow access to Camera to enable video calling.</string> <key>NSLocalNetworkUsageDescription</key> <string>Allow access to Camera to network to enable video calling.</string> <key>NSMicrophoneUsageDescription</key> <string>Allow access to Camera to mic to enable video calling.</string>     Update target iOS version Update the iOS version to 12 or later by navigating to the directory and updating the ios/Podfile file.   platform :ios, '12.0'     Flutter You will need to request Camera and Record Audio permissions at runtime before you join a call or display a preview as follows: 1. Add permission_handler plugin in pubspec.yaml file under dependencies.   yaml permission_handler:   2. Before navigating to the preview or meeting screen ask permission using the following code: > If android version is greater than android 12 then bluetoothConnect permission is also required.   dart Future<bool> getPermissions() async    // In iOS permission are handle by OS.  if (Platform.isIOS) return true;  await Permission.camera.request();  await Permission.microphone.request();  await Permission.bluetoothConnect.request();  while ((await Permission.camera.isDenied))     await Permission.camera.request();     while ((await Permission.microphone.isDenied))     await Permission.microphone.request();     while ((await Permission.bluetoothConnect.isDenied))     await Permission.bluetoothConnect.request();     return true;       Debugging Tools You can use  Android Studio (https://developer.android.com/studio),  VS Code (https://code.visualstudio.com/) &  Xcode (https://developer.apple.com/xcode/) to develop application with 100ms.    100ms Flutter app You can download & check out the 100ms Flutter app   🤖 The Flutter Android app from  Google Play Store here (https://play.google.com/store/apps/details?id=live.hms.flutter) 📱 Flutter iOS app from  Apple App Store here (https://apps.apple.com/app/100ms-live/id1576541989)    Github Repo You can checkout the 100ms Flutter SDK Github repo which also contains a fully fledged  Example app implementation here (https://github.com/100mslive/100ms-flutter/) "
    },
    {
        "title": "Join Room",
        "link": "/flutter/v2/features/join",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/join",
        "keywords": [],
        "content": "    Overview To join and interact with others in audio or video call, the user needs to join a room . When a user indicates that they want to _join_ the room, your app should have  1. User Name  The name which should be displayed to other peers in the room. 2.  Authentication Token (../guides/token)  The Client side Authentication Token generated by the Token Service. You can also optionally pass these fields  1. Track Settings  Such as joining a Room with Muted Audio or Video using the HMSTrackSetting object. More information available  here ( join-with-muted-audio-video) 2. User Metadata  This can be used to pass any additional metadata associated with the user using metadata of  HMSConfig object (https://pub.dev/documentation/hmssdk_flutter/latest/hmssdk_flutter/HMSConfig-class.html). For Eg: user-id mapping at the application side. More information is available  here (../advanced-features/peer-metadata-update).   Join a Room This section will take you through a sample code to join a room and receive room updates:   dart class Meeting implements HMSUpdateListener    late HMSSDK hmsSDK;   Meeting()       initHMSSDK();       void initHMSSDK() async       hmsSDK = HMSSDK();     await hmsSDK.build(); // ensure to await while invoking the build method     hmsSDK.addUpdateListener(this);     HMSConfig config = HMSConfig(authToken: 'eyJH5c', // client-side token generated from your token service                 userName: 'John Appleseed');     hmsSDK.join(config: config);        @override   void onJoin( required HMSRoom room ) async      // User Joined successfully.       @override   void onRoomUpdate( required HMSRoom room, required HMSRoomUpdate update )       //Room updates: https://www.100ms.live/docs/flutter/v2/features/update-listener-enums hms-room-update       @override   void onPeerUpdate(     required HMSPeer peer, required HMSPeerUpdate update ) async      //Peer updates: https://www.100ms.live/docs/flutter/v2/features/update-listener-enums hms-peer-update       @override   void onTrackUpdate(     required HMSTrack track,    required HMSTrackUpdate trackUpdate,    required HMSPeer peer )       //Track updates: https://www.100ms.live/docs/flutter/v2/features/update-listener-enums hms-track-update       @override   void onHMSError( required HMSException error )       //Error updates: https://www.100ms.live/docs/flutter/v2/features/error-handling hms-exception        @override   void onMessage( required HMSMessage message )       //Message updates: https://www.100ms.live/docs/flutter/v2/features/chat receiving-chat-messages       @override   void onRoleChangeRequest( required HMSRoleChangeRequest roleChangeRequest )       //role change request: https://www.100ms.live/docs/flutter/v2/features/change-role accept-role-change-request       @override   void onUpdateSpeakers( required List<HMSSpeaker> updateSpeakers )       //This is triggered every second with a list of speakers who are currently speaking.       @override   void onReconnecting()       //Reconnection callback: https://www.100ms.live/docs/flutter/v2/features/reconnection-handling reconnecting-and-reconnected-callbacks       @override   void onReconnected()       //Reconnection callback: https://www.100ms.live/docs/flutter/v2/features/reconnection-handling reconnecting-and-reconnected-callbacks       @override   void onChangeTrackStateRequest( required HMSTrackChangeRequest hmsTrackChangeRequest )       //Getting mute-unmute callback: https://www.100ms.live/docs/flutter/v2/features/remote-mute-unmute handling-an-unmute-callback       @override   void onRemovedFromRoom( required HMSPeerRemovedFromPeer hmsPeerRemovedFromPeer )       //Handle peer removal case: https://www.100ms.live/docs/flutter/v2/features/remove-peer handling-the-remove-peer-callback       @override   void onAudioDeviceChanged(     HMSAudioDevice? currentAudioDevice,    List<HMSAudioDevice>? availableAudioDevice )       //Get notified when the audio device is changed: https://www.100ms.live/docs/flutter/v2/features/audio-output-routing adding-audio-device-change-event-listener-android-only           Step-by-step guide This section will take you through the join journey step by step : <div className=\"steps-container\">   Create a class implementing HMSUpdateListener Let's create a class that implements HMSUpdateListener and acts as a data source for our UI   dart class Meeting implements HMSUpdateListener       Create HMSSDK instance Now, create an instance of HMSSDK using the build method. The build should always be called with await to ensure correct initialization & setup of the 100ms SDK.   dart HMSSDK hmsSDK = HMSSDK(); await hmsSDK.build(); // ensure to await while invoking the build method     Attach HMSUpdateListener The methods of HMSUpdateListener are invoked to notify updates happening in the room like as soon as join is successful we get onJoin callback. So, to get these updates we need to attach the listener as:   dart // this value corresponds to the instance implementing HMSUpdateListener hmsSDK.addUpdateListener(this);     Create HMSConfig object Next, create an object of  HMSConfig class (https://pub.dev/documentation/hmssdk_flutter/latest/hmssdk_flutter/HMSConfig-class.html) using the available join configurations.   dart HMSConfig config = HMSConfig(authToken: 'eyJH5c', // client-side token generated from your token service                userName: 'John Appleseed');     Invoke Join Now, we are primed to join the room. All you have to do is call join by passing the config object.   dart hmsSDK.join(config: config);   </div> The methods of HMSUpdateListener are invoked to notify updates happening in the room like a peer joins/leaves, a track mute/unmute, etc. After calling join your app will be provided an update from the 100ms SDK. ✅ If successful, the onJoin( required HMSRoom room ) method of HMSUpdateListener will be invoked with information about the room encapsulated in the HMSRoom object. ❌ If failure, the onHMSError( required HMSException error ) method will be invoked with exact failure reason. Once you get onJoin callback you have joined a room successfully 🥳   Join with Muted Audio / Video Joining with Muted Audio / Video is a customization that sets the microphone and camera state before joining the room. By default HMSSDK keeps the microphone and camera ON but by using this feature you can decide their state according to your use case. This can be achieved using the hmsVideoTrackInitState property for Video and hmsAudioTrackInitState for Audio in the hmsTrackSetting parameter of the HMSSDK constructor. Here trackInitialState property of HMSVideoTrackSetting and HMSAudioTrackSetting is an Enum of type HMSTrackInitState which has the possible values as:   dart enum HMSTrackInitState    //If the track needs to be kept mute while joining  MUTED,  //If the track needs to be kept unmute while joining  UNMUTED,     Let's see how this can be achieved in the following steps. 1. First, for joining a room with muted audio/video, these values need to be set in the hmsTrackSetting property as    dart HMSTrackSetting trackSettings = HMSTrackSetting(     // This is for joining with muted audio(mic off) and muted video(camera off)     audioTrackSetting: HMSAudioTrackSetting(trackInitialState: HMSTrackInitState.MUTED),     videoTrackSetting: HMSVideoTrackSetting(trackInitialState: HMSTrackInitState.MUTED));   2. Now, create the HMSSDK object by passing the HMSTrackSetting object created above :   dart class Meeting implements HMSUpdateListener    late HMSSDK hmsSDK;   Meeting()      // pass the trackSettings while constructing the HMSSDK instance     ̶h̶m̶s̶S̶D̶K̶ ̶=̶ ̶H̶M̶S̶S̶D̶K̶(̶)̶;̶     HMSSDK hmsSDK = HMSSDK(hmsTrackSetting: trackSettings);     await hmsSDK.build(); // ensure to await while invoking the build method     hmsSDK.addUpdateListener(this);     HMSConfig config = HMSConfig(authToken: 'eyJH5c', // client-side token generated from your token service                 userName: 'John Appleseed');     hmsSDK.join(config: config);         Rest all the steps are same as above. Now, the user joins the room with the microphone and the camera turned off. "
    },
    {
        "title": "Leave Room",
        "link": "/flutter/v2/features/leave",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/leave",
        "keywords": [],
        "content": "  Once you're done with the meeting and want to exit, call leave on the HMSSDK instance that you created to join the room. Before calling leave, remove the HMSUpdateListener instance as:  dart // updateListener is the instance of class in which HMSUpdateListener is implemented hmsSDK.removeUpdateListener(updateListener);   To leave the meeting, call the leave method of HMSSDK and pass the hmsActionResultListener parameter to get a success callback from SDK in the onSuccess override method as follow. > You will need to implement the HMSActionResultListener interface in a class to get onSuccess and onException callback. > To know about how to implement HMSActionResultListener  check the docs  here (../features/action-result-listeners)   dart class Meeting implements HMSActionResultListener  //this is the instance of class implementing HMSActionResultListener  await hmsSDK.leave(hmsActionResultListener: this); @override  void onSuccess(     HMSActionResultListenerMethod methodType = HMSActionResultListenerMethod.unknown, Map<String, dynamic>? arguments )       switch (methodType)        case HMSActionResultListenerMethod.leave:      // Room leaved successfully      // Clear the local room state      break;      ...                "
    },
    {
        "title": "Mute / Unmute Peer",
        "link": "/flutter/v2/features/mute",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/mute",
        "keywords": [],
        "content": "  Mute is something that applies to both audio and video. When you mute audio, you can't be heard by other people i.e your audio is OFF. When you mute a video, you will not be broadcasting your video to other people i.e your video is OFF.   Methods to mute/unmute This example shows implementation of three methods namely :    1. switchAudio : To switch local peer's audio on/off.   2. switchVideo : To switch local peer's video on/off.   3. switchCamera: Switch camera to the front or rear mode Let's look at their implementation:   dart class Meeting implements HMSUpdateListener,HMSActionResultListener  ... /// To switch local peer's audio on/off. /// Pass the bool value to  isOn  to change the current audio status ///  isOn  : Pass the current audio status i.e. true if unmuted and false if muted currently hmsSDK.switchAudio(isOn: true); //This mutes local peer(you) audio  /// To switch local peer's video on/off. /// Pass the bool value to  isOn  to change the current video status ///  isOn  : Pass the current video status i.e. true if unmuted and false if muted currently hmsSDK.switchVideo(isOn: true); //This mutes local peer(you) video /// Switch camera from front or rear mode hmsSDK.switchCamera(); ...       Mute Remote peers locally  To mute peers all the peers for yourself HMSSDK provides dedicated methods muteRoomAudioLocally for audio and muteRoomVideoLocally for video. Execution of this method mutes audio and video only locally others will still be able to hear audio and video. > These methods mute/unmute the room audio/video of all the peers for yourself.To mute/unmute certain peer check  this (../features/mute-unmute-remote-peer)   dart //To mute room audio locally hmsSDK.muteRoomAudioLocally(); //To mute room video locally hmsSDK.muteRoomVideoLocally();   Similarly their counterparts:   dart //To unmute room audio locally hmsSDK.unMuteRoomAudioLocally(); //To unmute room video locally hmsSDK.unMuteRoomVideoLocally();   "
    },
    {
        "title": "Preview",
        "link": "/flutter/v2/features/preview",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/preview",
        "keywords": [],
        "content": "  The preview screen is a frequently used UX element that allows users to check if their input devices are working properly and set the initial state (mute/unmute) of their audio and video tracks before joining.  100ms SDKs provide an easy-to-use API to back this feature. Additionally, the SDK will try to establish a connection to the 100ms server to verify there are no network issues and that the auth credentials are valid so that if everything is in order the subsequent room join will be much faster. Let's look at the flow of HMSSDK with a preview    preview-join-flow (/docs/v2/flutter-preview-join-flow.png) Let's follow the steps of joining a room with a preview: <div className=\"steps-container\">   Initialize HMSSDK   dart late HMSSDK hmsSDK; HMSSDK hmsSDK = HMSSDK();     Call build method on HMSSDK instance   dart await hmsSDK.build(); // ensure to await while invoking the build method     Attach preview listeners and call preview method  Let's see the complete implementation till now:   dart class Preview implements HMSPreviewListener    late HMSSDK hmsSDK;   Preview()     initHMSSDK();      void initHMSSDK() async      hmsSDK = HMSSDK();    await hmsSDK.build(); // ensure to await while invoking the build method    HMSConfig config = HMSConfig(     userName: \"John Doe\",     authToken: \"eyJH5c\", // client-side token generated from your token service    );    //Here this is an instance of a class that implements HMSPreviewListener i.e. Preview    hmsSDK.addPreviewListener(this);    hmsSDK.preview(config: config);      @override  void onPreview( required HMSRoom room, required List<HMSTrack> localTracks )      //We will get the callback here if the preview succeeds      @override  void onRoomUpdate( required HMSRoom room, required HMSRoomUpdate update )       //Room updates: https://www.100ms.live/docs/flutter/v2/features/update-listener-enums hms-room-update      @override  void onPeerUpdate(     required HMSPeer peer, required HMSPeerUpdate update ) async      //Peer updates: https://www.100ms.live/docs/flutter/v2/features/update-listener-enums hms-peer-update      @override  void onHMSError( required HMSException error )       //Error updates: https://www.100ms.live/docs/flutter/v2/features/error-handling hms-exception      @override  void onAudioDeviceChanged(     HMSAudioDevice? currentAudioDevice,    List<HMSAudioDevice>? availableAudioDevice )       //Get notified when the audio device is changed: https://www.100ms.live/docs/flutter/v2/features/audio-output-routing adding-audio-device-change-event-listener-android-only        After calling preview your app will be provided an update from the 100ms SDK. ✅ If successful, the onPreview( required HMSRoom room, required List<HMSTrack> localTracks ) method of HMSPreviewListener will be invoked with information about the room encapsulated in the HMSRoom object. ❌ If failure, the onHMSError( required HMSException error ) method will be invoked with exact failure reason.   Render Preview  In case of success onPreview provides an array of local tracks in the parameter localTracks that you can display to the user (see  Render Video (render-video) and  Mute (mute) sections for more details).  Fetching the track in onPreview callback:   dart class Preview implements HMSPreviewListener    ...  List<HMSVideoTrack> videoTracks =   ;   @override  void onPreview( required HMSRoom room, required List<HMSTrack> localTracks )      for (var track in localTracks)       if (track.kind == HMSTrackKind.kHMSTrackKindVideo)       isVideoOn =  (track.isMute);     videoTracks.add(track as HMSVideoTrack);           if (track.kind == HMSTrackKind.kHMSTrackKindAudio)       isAudioOn =  (track.isMute);                    Display the track using HMSVideoView :   dart //videoTracks is the List<HMSVideoTrack> which we have set above HMSVideoView(  scaleType:     ScaleType.SCALE_ASPECT_FILL,  track:videoTracks 0 ,//setting the first track from videoTracks list to render on screen  setMirror: true,  matchParent: false, )   Check how we have implemented the preview in our sample app  here (https://github.com/100mslive/100ms-flutter/tree/main/example/lib/preview)   Remove HMSPreviewListener before calling join   dart class Preview implements HMSPreviewListener    ...  void removePreviewListener()     //Here this is an instance of a class that implements HMSPreviewListener i.e. Preview    hmsSDK.removePreviewListener(this);        After calling removePreviewListener to remove HMSPreviewListener call join by passing the same HMSSDK instance and HMSConfig.   Call join method and attach HMSUpdateListener   dart class Meeting implements HMSUpdateListener    late HMSSDK hmsSDK;   Meeting( required HMSSDK hmsSDKFromPreview,HMSConfig configFromPreview )      hmsSDK = hmsSDKFromPreview;     hmsSDK.addUpdateListener(this);     hmsSDK.join(config: configFromPreview);      @override  void onJoin( required HMSRoom room ) async      // User Joined successfully.     ...     In this way, a room can be joined with Preview. The rest of the functions and their implementation is same as we do in the case of direct join. </div>   Get onPeerUpdate and onRoomUpdate while in Preview Mode To enable onPeerUpdate & onRoomUpdate in the Preview, we need to enable Room State from the  100ms Dashboard (https://dashboard.100ms.live/). This can be enabled by selecting a Template and then navigating to Advanced Settings. By default, Room State in Preview Mode is Disabled. So onPeerUpdate & onRoomUpdate events will not be received if these are not Enabled from Dashboard.   Advanced settings (/docs/v2/flutter-advanced-settings.png) These options are available in advanced settings:   Room State in preview (/docs/v2/flutter-room-state-in-preview.png)   Supplementary bytes Now, let's take a look at the listener class in detail.   dart /// HMSPreviewListener  listens to updates when you preview. /// ///Just implement it and get the preview updates. /// /// Check out the  Sample App  to know about how we are implementing it: https://github.com/100mslive/100ms-flutter. abstract class HMSPreviewListener    ///when an error is caught  onError  will be called  ///  ///  Parameters:  ///   error: error which you get.  void onHMSError( required HMSException error );  ///when you want to preview listen to this callback  ///  ///  Parameters:  ///   room: the room which was joined  ///   localTracks: local audio/video tracks list  void onPreview( required HMSRoom room, required List<HMSTrack> localTracks );  /// This is called when there is a change in any property of the Room  ///  ///  Parameters:  ///   room: the room which was joined  ///   update: the triggered update type. Should be used to perform different UI Actions  void onRoomUpdate( required HMSRoom room, required HMSRoomUpdate update );  /// This will be called whenever there is an update on an existing peer  /// or a new peer got added/existing peer is removed.  ///  /// This callback can be used to keep a track of all the peers in the room  ///  Parameters:  ///   peer: the peer who joined/left or was updated  ///   update: the triggered update type. Should be used to perform different UI Actions  void onPeerUpdate( required HMSPeer peer, required HMSPeerUpdate update );  ///whenever a new audio device is plugged in or the audio output is changed we  ///get the onAudioDeviceChanged update  ///This callback is only fired on Android devices. On iOS, this callback will not be triggered.  ///  Parameters:  ///   currentAudioDevice: Current audio output route  ///   availableAudioDevice: List of available audio output devices  void onAudioDeviceChanged(     HMSAudioDevice? currentAudioDevice,    List<HMSAudioDevice>? availableAudioDevice );     "
    },
    {
        "title": "Reconnection Handling",
        "link": "/flutter/v2/features/reconnection-handling",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/reconnection-handling",
        "keywords": [],
        "content": "  Real-world apps operate in varying network conditions and thus require handling for network bandwidth issues. The SDK provides callbacks when a user gets disconnected due to network issues or network switch and also when the user gets reconnected again. > 💡 Note: The SDK tries to reconnect automatically for 60 seconds after that the connection is terminated, the peer is removed from the room and the room has to be rejoined.   Reconnecting & Reconnected Callbacks  onReconnecting and onReconnected callbacks are provided in HMSUpdateListener .So, These method needs to be overridden in the class wherever HMSUpdateListener is implemented.   dart class Meeting implements HMSUpdateListener, HMSActionResultListener   ...  //OnReconnecting callback is triggered when the SDK detects a network issue or is trying to  // reconnect automatically on a network drop, switch etc.  @override  void onReconnecting()     // Reconnecting...    // Handle reconnection by showing loaders etc.     //OnReconnected callback is triggered when the SDK has successfully recovered from a network  // drop, switch or a network issue etc.  @override  void onReconnected()     // Reconnected...    // Handle by resetting the loaders and switching to normal UI.     ...     > 💡 Note: On getting reconnected the SDK will send all the updates(Track and Peer updates) again.   Case when a user never gets reconnected SDK tries to reconnect the user automatically for 60 seconds after that the connection gets terminated and the peer is removed from the room by SDK. If reconnection fails the SDK sends onHMSError callback.   dart class Meeting implements HMSUpdateListener, HMSActionResultListener   ...  @override   void onHMSError( required HMSException error )     //Handle the UI depending on the error object         The user will receive the following callbacks with error object having isTerminal:true and errorCode as :   Error Code: 1003, Cause: Websocket disconnected   Error Code:1003 (/docs/v2/flutter-errorCode-1003.png)   Error Code: 4005, Cause: ICE Connection Failed due to network issue PUBLISH    Error Code:1003 (/docs/v2/flutter-errorCode-4005-PUBLISH.png)   Error Code: 4005, Cause: ICE Connection Failed due to network issue SUBSCRIBE    Error Code:1003 (/docs/v2/flutter-errorCode-4005-SUBSCRIBE.png)   Best practices to handle reconnection The SDK will send onReconnecting callback first if the user disconnects, If the user reconnects the SDK sends onReconnected callback whereas if the user fails to connect then it sends onHMSError .These callbacks can be handled as follows:    Once the SDK sends onReconnecting callback, Do not allow the user to interact with UI and show loaders or reconnecting indicator.   If onReconnected is received then switch back to room UI allowing interaction as usual.   If the user fails to reconnect then SDK sends onHMSError callback, look for errorCode parameter and if it is something(generally 1003,4005 etc.) such that the user can never reconnect and then navigate to the HomeScreen, showing a toast or errorDialog to the user with a description. > 🗝️ You can find the errorCodes, descriptions and the suggested actions to be taken  here (./error-handling)"
    },
    {
        "title": "RTMP Streaming / Recording",
        "link": "/flutter/v2/features/recording",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/recording",
        "keywords": [],
        "content": "  Want to preserve your video call for posterity in a recording? Or live stream it out to millions of viewers on Twitch or YouTube or whatever gives you an RTMP ingest URL? Turn on RTMP Streaming or Recording  In 100ms, recording and streaming are usually achieved by having a bot join your room and stream what it sees and hears to a file (recording) or an RTMP ingest URL (streaming).   Types of recordings  Apart from the RTMP stream and the browser recording, there is also a serverRecording, which can be turned on for the room for.    Server Recording  This is used for archival purposes and cannot be stopped by method calls from SDK. This can only be enabled/disabled from the dashboard. This represents that the room was set to be recorded when it was created and all sessions within it will always be recorded for archival by the server.    Browser Recording  This is a normal recording that can be enabled/disabled using HMSSDK method startRtmpOrRecording with toRecord as true   RTMP Streaming & Recording The topics covered in this doc are: 1.  How to start stop streaming / recording. ( start-stop-streaming-recording) 2.  How to check the current status for streaming/recording. ( current-room-status) 3.  When to check the current status ( when-to-check-for-room-status)   Start/Stop Streaming / Recording Let's look at the implementation for starting/stopping RTMP streaming and browser recording:   dart class Meeting implements HMSUpdateListener, HMSActionResultListener   void startRtmpOrRecording(   required String meetingUrl,    required bool toRecord,List<String>? rtmpUrls,double height,double width ) async        HMSResolution streamResolution = HMSResolution(height: height, width: width);      HMSRecordingConfig hmsRecordingConfig = new HMSRecordingConfig(        meetingUrl: meetingUrl, toRecord: toRecord, rtmpUrls: rtmpUrls,resolution: streamResolution);      /// hmsRecordingConfig : the object of HMSRecordingConfig which we created above      /// hmsActionResultListener : an instance of a class that implements HMSActionResultListener      //Here this is an instance of a class that implements HMSActionResultListener i.e. Meeting      hmsSDK.startRtmpOrRecording(       hmsRecordingConfig: hmsRecordingConfig,       hmsActionResultListener: this);     void stopRtmpAndRecording()     /// hmsActionResultListener : an instance of a class that implements HMSActionResultListener   //Here this is an instance of a class that implements HMSActionResultListener i.e. Meeting   hmsSDK.stopRtmpAndRecording(     hmsActionResultListener: this);     @override  void onSuccess(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments )        switch (methodType)        ...      case HMSActionResultListenerMethod.startRtmpOrRecording:      //RTMP stream started successfully. We will get the update here      break;       case HMSActionResultListenerMethod.stopRtmpAndRecording:      //RTMP stream stopped successfully      break;              @override  void onException(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments,    required HMSException hmsException )        switch (methodType)        ...            case HMSActionResultListenerMethod.startRtmpOrRecording:      // Check the HMSException object for details about the error      break;      case HMSActionResultListenerMethod.stopRtmpAndRecording:      // Check the HMSException object for details about the error      break;             Let's break down the implementation To start recording, streaming or both, create an instance of HMSRecordingConfig .   dart   HMSRecordingConfig hmsRecordingConfig = new HMSRecordingConfig(     meetingUrl: \"meeting_url\", toRecord: \"true for enabling recording\", rtmpUrls: \"List of URLs\", resolution: \"HMSResolution object for stream resolution\");   Let's understand each parameter: 1.   meetingUrl  : _String_. The URL the 100ms bot user will open to join your room. It must allow access without any user-level interaction. 2.   rtmpUrls  : _List<String >_. If streaming is required, this has to be one or more RTMP Ingest Urls with a max limit of 3 URLs where the stream should go. If only recording, this can be an empty list.    Format: rtmp://server.com/app/STREAM_KEY       Example: rtmp://a.rtmp.youtube.com/live2/k0jv-329m-1y7f-ktth-ck48      \"rtmp://a.rtmp.youtube.com/live2/\"  RTMP stream URL.      \"k0jv-329m-1y7f-ktth-ck48\"  RTMP stream key.    Please refer to the platform specific documentation for details on how to obtain the stream URL and stream key. Here are some examples:      YouTube (https://support.google.com/youtube/answer/2907883?hl=en&ref_topic=9257892)      Facebook (https://www.facebook.com/help/587160588142067)      Instagram (https://about.instagram.com/blog/tips-and-tricks/instagram-live-producer)      Twitch (https://help.twitch.tv/s/article/twitch-stream-key-faq?language=en_US)      LinkedIn (https://www.linkedin.com/help/linkedin/answer/a564446/go-live-using-a-custom-stream-rtmp) 3.   toRecord  : _Boolean_. If the recording is required, set it true . If the recording is not required, set false. This value does not affect streaming. 4.   resolution  : _HMSResolution_. An optional value for the output resolution of the stream. For instance, the default is a landscape at 1280x720 but this could be set for a portrait mode to 720x1280 or smaller values like 480x80. The HMSResolution class takes Width and Height.   Key Notes   If both rtmpUrls and toRecord = true are provided, both streaming and recording will begin.   If only rtmpUrls are provided, only streaming will begin.   If only toRecord true is provided, only recording will begin. If either one is started, the other can't be started without stopping whatever is running. > Eg: If only streaming is started then recording can't be started unless streaming is stopped first. If both are required, they have to be started together by providing both rtmpUrls and toRecord = true .   Current Room Status The current status for the room is always reflected in the HMSRoom object that is returned from the HMSUpdateListener . This can be used to show the stream or recording status on UI something similar to this:   stream-state (/docs/v2/flutter-stream-state.jpg) Here are the relevant properties inside the HMSRoom object which we can read to get the current recording/streaming status of the room namely:   hmsRtmpStreamingState  Contains info about RTMP Streaming, running attribute if true indicates streaming is ON currently  hmsBrowserRecordingState  Contains info about Browser Recording, running attribute if true indicates browser recording is ON currently  hmsServerRecordingState  Contains info about Server Recording, running attribute if true indicates server recording is ON currently Each of them is an object which contains a boolean running which lets you know if it's active in the room right now and error which lets you know if there was an error. 1.   HMSRtmpStreamingState   an instance of HMSRtmpStreamingState , which looks like:   dart class HMSRtmpStreamingState    /// error : Gets populated if there is some error in starting the stream  /// running : bool value true indicates that RTMP streaming is running  /// startedAt : time at which RTMP streaming was started  final HMSException? error;  final bool running;  DateTime? startedAt;  HMSRtmpStreamingState( required this.error, required this.running, this.startedAt );     2.   browserRecordingState   an instance of HMSBrowserRecordingState , which looks like:   dart class HMSBrowserRecordingState    /// error : Gets populated if there is some error in starting the browser recording  /// running : bool value true indicates that browser recording is running  /// startedAt : time at which browser recording was started  final HMSException? error;  final bool running;  DateTime? startedAt;  HMSBrowserRecordingState( required this.error, required this.running, this.startedAt );     3.   serverRecordingState   an instance of HMSServerRecordingState , which looks like:   dart class HMSServerRecordingState    /// error : Gets populated if there is some error in starting the server recording  /// running : bool value true indicates that server recording is running  /// startedAt : time at which server recording was started  final HMSException? error;  final bool running;  DateTime? startedAt;  HMSServerRecordingState( required this.error,required this.running, this.startedAt );       When to check for room status The room status should be checked in three places: 1. onJoin callback of HMSUpdateListener 2. onRoomUpdate callback of HMSUpdateListener 3. When hmsSDK.startRtmpOrRecording() is called 4. When hmsSDK.stopRtmpAndRecording() is called.   In the onJoin(room: HMSRoom) & onRoomUpdate( required HMSRoom room, required HMSRoomUpdate update ) callback in HMSUpdateListener   The properties mentioned above will be on the HMSRoom object.   Whenever either of the start or stop recording functions is called, their success or error callbacks are called, the values of the streaming and recording will be updated on the room object returned in onRoomUpdate .   So update the instance of room in your application at that time. "
    },
    {
        "title": "Remote Mute/Unmute",
        "link": "/flutter/v2/features/remote-mute-unmute",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/remote-mute-unmute",
        "keywords": [],
        "content": "  You're running a video call room and decide that someone who's currently talking shouldn't be talking. You'd prefer they'd stay mute. Or perhaps you want their video turned off as well as their audio. You're looking for a remote mute. Muting can apply to both audio and video. Let's understand the steps:   Permissions Can't let just anyone mute others. First, you need to create a  role (../foundation/templates-and-roles) with the permissions to mute others and also to ask them to Unmute.   Permissions  Remote mute/unmute (/guides/permissions.png) The permission to mute others is within PermissionsParams  mute and you should check for that within the HMSRole of the peer to see if they have it. Similarly, the permission to Unmute other peers is within PermissionsParams  unmute . Here's how to check whether the local peer has permission to mute or Unmute others. You can do it like this:   dart Future<bool> isAllowedToMuteOthers() async    return (await hmsSDK.getLocalPeer()).role.permission .mute;   Future<bool> isAllowedToUnMuteOthers()    return (await hmsSDK.getLocalPeer()).role.permission .unMute;      hmsSDK.getLocalPeer() will not return null as long as you're in a preview or in a meeting room. Since you likely won't need to check for permissions if you're not in one So, it would be ok.   Mute/Unmute other peers  HMSSDK provides dedicated methods to mute/unmute:   Individual peer   Specific role These methods will only work if the peer has permission to mute/unmute another peer's audio/video. The permission can be checked as the steps mentioned above. Let's look at each of them:   Individual peer We can use the changeTrackState method to mute/unmute remote peer's audio/video   dart class Meeting implements HMSUpdateListener, HMSActionResultListener   ...  void changeTrackState(HMSTrack forRemoteTrack, bool mute,    HMSActionResultListener hmsActionResultListener)     ///  forRemoteTrack  : track whose state needs to be changed(mute -> unmute or unmute -> mute)   /// Set  mute  to true if the track needs to be muted, false otherwise.   /// hmsActionResultListener : an instance of a class that implements HMSActionResultListener   //Here this is an instance of a class that implements HMSActionResultListener i.e. Meeting   if(   //peer has permission to change track state   localPeer?.role.permissions.mute   )     hmsSDK.changeTrackState(     forRemoteTrack: forRemoteTrack,     mute: mute,     hmsActionResultListener: this);       else     //Peer doesn't have permission to mute/unmute other peers          @override   void onSuccess(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments )        switch (methodType)        ...      case HMSActionResultListenerMethod.changeTrackState:      //Track state successfully changed      break;                @override  void onException(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments,    required HMSException hmsException )        switch (methodType)        ...      case HMSActionResultListenerMethod.changeTrackState:      // Check the HMSException object for details about the error      break;              If the changeTrackState method is successful we will get the onSuccess callback on HMSActionResultListener attached and track update in onTrackUpdate .   Specific role We can use the changeTrackStateForRole method to mute/unmute peers under specific roles.   dart class Meeting implements HMSUpdateListener, HMSActionResultListener   ...  void changeTrackStateForRole(bool mute, HMSTrackKind? kind, String? source,    List<HMSRole>? roles, HMSActionResultListener? hmsActionResultListener)      if(//peer has permission to change track state     localPeer?.role.permissions.mute)             /// Set  mute  true if the track needs to be muted, false otherwise       ///  kind  is the HMSTrackType that should be affected. If this and the source are specified, it is considered an AND operation. If not specified, all track sources are affected.       ///  source  is the HMSTrackSource that should be affected. If this and type are specified, it is considered an AND operation. If not specified, all track types are affected.       ///  roles  is a list of roles, that may have a single item in a list, whose tracks should be affected.       //If not specified, all roles are affected.       /// hmsActionResultListener : an instance of a class that implements HMSActionResultListener       //Here this is an instance of a class that implements HMSActionResultListener i.e. Meeting        HMSSDK.changeTrackStateForRole(        mute: mute,        kind: kind,        source: source,        roles: roles,        hmsActionResultListener: this);          else     //Peer doesn't have permission to mute/unmute other peers              @override   void onSuccess(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments )        switch (methodType)        ...      case HMSActionResultListenerMethod.changeTrackState:      break;       case HMSActionResultListenerMethod.changeTrackStateForRole:      //Track state successfully changed      break;               @override  void onException(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments,    required HMSException hmsException )        switch (methodType)        ...      case HMSActionResultListenerMethod.changeTrackState:      break;      case HMSActionResultListenerMethod.changeTrackStateForRole:      // Check the HMSException object for details about the error      break;              If the changeTrackStateForRole method is successful we will get the onSuccess callback and track update in onTrackUpdate similar to changeTrackState . > If _roles_ is passed as an empty list then all the roles will get affected.   Handling a mute callback Mute callbacks are automatically applied to the receiver. No action is required.   Handling an unmute callback Let's turn the table now to what happens if a remote peer wishes to mute/unmute our audio/video.   In case when remote peer mutes our audio/video HMSSDK performs it automatically without asking permission In another case, we get the onChangeTrackStateRequest if   we accept the request we need to call switchVideo or switchAudio according to the request. Let's understand this with a diagram:   accept-change-track-state (/docs/v2/flutter-accept-track-change-request.png) Now let's do it step-by-step: <div className=\"steps-container\">   PeerA calls changeTrackState on PeerB's track   dart hmsSDK.changeTrackState(     forRemoteTrack: \"PeerB's track\",     mute: mute,     hmsActionResultListener: this);     PeerB receives onChangeTrackStateRequest   dart @override  void onChangeTrackStateRequest(     required HMSTrackChangeRequest hmsTrackChangeRequest )     How this is implemented in the example app can be found  here (https://github.com/100mslive/100ms-flutter/blob/1537a82a2213c8bbb1b0eb2dcc009c171e89bad1/example/lib/data_store/meeting_store.dart L736) Let's look at the HMSTrackChangeRequest structure :   dart class HMSTrackChangeRequest    bool mute;  HMSPeer requestBy;  HMSTrack track;     This contains information on which track is requested for unmuting. Check the track type and call the unmute methods accordingly.   dart public void checkTrack(HMSTrack track)     if(track.kind == HMSTrackType.kHMSTrackKindAudio)       //Requested track is an audio track     else if (track.kind == HMSTrackType.kHMSTrackKindVideo)       //Requested track is a video track         So final implementation looks like this:   dart class Meeting implements HMSUpdateListener, HMSActionResultListener   ...  public void checkTrack(HMSTrack track)     if(track.kind == HMSTrackType.kHMSTrackKindAudio)       hmsSDK.switchAudio(isOn: false);     else if (track.kind == HMSTrackType.kHMSTrackKindVideo)       hmsSDK.switchVideo(isOn: false);         @override  void onChangeTrackStateRequest(     required HMSTrackChangeRequest hmsTrackChangeRequest )       checkTrack(hmsTrackChangeRequest.track)          PeerB calls switchAudio/switchVideo depending on the track kind If the track is an audio track then PeerB calls switchAudio   dart /// isOn  as false since it's an unmute request hmsSDK.switchAudio(isOn: false);   If the track is a video track then PeerB calls switchVideo   dart /// isOn  as false since it's an unmute request hmsSDK.switchVideo(isOn: false);   Now all the peers will receive an onTrackUpdate with the type as trackUnmuted  </div>"
    },
    {
        "title": "Remove Peer",
        "link": "/flutter/v2/features/remove-peer",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/remove-peer",
        "keywords": [],
        "content": "  Someone's overstayed their welcome and now you need to remove a peer from the video call room.HMSSDK's removePeer comes to rescue. Let's look at this step-by-step <div className=\"steps-container\">   Permissions Can't let just anyone remove others from the video call room. First you need to create a  role (../foundation/templates-and-roles) with the permissions to remove others. In the SDK, the permission to remove others from the room is within PermissionsParams  removeOthers and you should check for that within the HMSRole of the peer to see if they have it. Here's how to check whether the local peer has the permission to end the room:   dart bool isAllowedToRemovePeer() async   return (await hmsSDK.getLocalPeer()).role.permission .removeOthers;      hmsSDK.getLocalPeer() will not return null as long as you're in a preview or in a meeting room. Since you likely won't need to check for permissions if you're not in one it would be ok.   Removing a peer Once the permissions are checked to ensure the caller has the permission to remove a peer, remove them by calling hmsSDK.removePeer .   dart class Meeting implements HMSUpdateListener, HMSActionResultListener   ...  void removePeer(     required HMSPeer peerToRemove,    required String reason )      /// peerToRemove : HMSPeer that needs to be removed from room    /// reason : Reason for removal    /// hmsActionResultListener : an instance of a class that implements HMSActionResultListener    //Here this is an instance of a class that implements HMSActionResultListener i.e. Meeting     hmsSDK.removePeer(      peer: peerToRemove,      reason: \"Reason to remove\",      hmsActionResultListener: this);        @override   void onSuccess(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments )        switch (methodType)        ...      case HMSActionResultListenerMethod.removePeer:      //Peer removed successfully      break;                @override  void onException(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments,    required HMSException hmsException )        switch (methodType)        ...      case HMSActionResultListenerMethod.removePeer:      // Check the HMSException object for details about the error      break;               Handling the remove peer callback The peer who is removed will receive a callback in HMSUpdateListener method onRemovedFromRoom( HMSPeerRemovedFromPeer hmsPeerRemovedFromPeer) . The HMSRemovedFromPeer object which is passed into the callback has the structure:   dart class HMSRemovedFromPeer    final HMSPeer peerWhoRemoved;  final String reason;  final bool roomWasEnded;      reason : the string that the caller of removePeerRequest sent as the reason they were being removed from the room.  peerWhoRemoved : HMSPeer instance containing the details of the person who called removePeerRequest . This can be used to show the name of the person who removed them.  roomWasEnded : This will be false if the peer was removed. If true, it indicates that the peer was not removed, but the entire room was ended. See  End Room (/flutter/v2/features/end-room) for details. > When this callback is received, the UI should be cleaned up from the client side. The video call room would be ended from the SDK once this callback is sent. </div>"
    },
    {
        "title": "Render Video",
        "link": "/flutter/v2/features/render-video",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/render-video",
        "keywords": [],
        "content": "  It all comes down to this. All the setup so far has been done so that we can show live-streaming videos in our beautiful app. 100ms Flutter Package provides the HMSVideoView widget that renders the video on the screen. We simply have to pass a Video Track object to the HMSVideoView to begin automatic rendering of Live Video Stream. We can also optionally pass props like key , scaleType , mirror or matchParent to customize the HMSVideoView widget.   dart HMSVideoView(  track: videoTrack,  key: Key(videoTrack.trackId), ),     How to get HMSVideoTrack  This section contains info about how to get video tracks so that we can display them on screen.  HMSVideoTrack is used to render video in HMSVideoView. Single HMSVideoView can be attached to only one HMSVideoTrack . > For resetting the track or updating any property in HMSVideoView it is required to use the new HMSVideoView widget. To get tracks we will be listening to HMSUpdateListener's onTrackUpdate .  onTrackUpdate provides updates for both Audio & Video tracks so to segregate them we can put a check like below:   dart /// check if the track is video or audio /// For Audio track.kind == HMSTrackKind.kHMSTrackKindAudio /// For Video track.kind == HMSTrackKind.kHMSTrackKindVideo   Let's see an example of how we can extract the video tracks from onTrackUpdate :   dart class Meeting implements HMSUpdateListener    late HMSSDK hmsSDK;  ...  void onTrackUpdate(     required HMSTrack track,    required HMSTrackUpdate trackUpdate,    required HMSPeer peer )       if (track.kind == HMSTrackKind.kHMSTrackKindVideo)        //We will get all the video tracks      if(track.source == \"REGULAR\")        //We will get only camera feed(Normal Video) tracks of peer here       if(peer.isLocal)         //We will get the local peer video track here               else         //We will get the remote peer video tracks here                     else        //To get screenshare or other video tracks from peers       //For screenshare       if(track.source == \"SCREEN\")         //We will get screen share track here                               Now, once we have the video tracks we can pass the video track to HMSVideoView to get live video on the screen. Screenshare as well as normal video tracks can be rendered in the same way which we will see below. > HMSSDK automatically handles audio tracks. So they are not required to be handled in the application. Although  mute/unmute (../features/mute) or  set audio volume (../advanced-features/set-volume) features are provided > by SDK.    Render Video This section contains information about how we can render the video once we have the track. To display video tracks HMSSDK provides the HMSVideoView widget. We just need to pass the track which is HMSVideoTrack instance to get our video running as:   dart //videoTrack is an instance of HMSVideoTrack HMSVideoView(track: videoTrack);     Configuring video view  HMSVideoView has several properties to configure the way video is rendered as:   1. matchParent  This is a bool type parameter, true indicates that HMSVideoView can take the size of parent widget.   2. scaleType The ScaleType property decides how much space the video will take from the available space. ScaleType is an enum with values:   dart enum ScaleType      //Video maintains the aspect ratio so it only occupies space based on the aspect ratio  SCALE_ASPECT_FIT,    //Video occupies all the available space and may get cropped  SCALE_ASPECT_FILL,   //Video aspect ratio is balanced similar to SCALE_ASPECT_FIT  SCALE_ASPECT_BALANCED     > 🔑 Note: SCALE_ASPECT_FIT is the default scaleType for HMSVideoView  SCALE_ASPECT_FILL might crop the video if the parent widget is small. So, for cases like screen share where cropping is not acceptable use the default type i.e SCALE_ASPECT_FIT .   3. setMirror This is a bool type parameter, true indicates that the video feed gets mirrored and false shows the track as it is. Generally true for local peer and false for a remote peers   4. disableAutoSimulcastLayerSelect HMSVideoView has an automatic simulcast layer selection capability which is enabled if adaptive bitrate is enabled. You can check more about it  here (../advanced-features/adaptive-bitrate). It will select a layer that best matches the current view frame size and reacts to frame updates. In case manual layer selection is preferred set disableAutoSimulcastLayerSelect property to true . By default, the track layer is set to high . > Always pass the Key parameter to HMSVideoView with the unique Video Track's trackId value so that it can be disposed of and reset correctly during rebuilds.   Best Practices    Dispose HMSVideoView to conserve bandwidth and cleanup elements Remove the HMSVideoView from UI and it will not render its video & conserve network bandwidth. It is always advised to stop rendering video when it is not required to save bandwidth consumption. This is done in the example app by setting the isOffscreen property of PeerTrackNode as true when the peer tile is off-screen. So that app does not download the video track when the tile is off-screen.   Limit the number of HMSVideoView on-screen at a time  HMSVideoView internally uses SurfaceView in android and UiKitView in iOS. It is recommended to render at most 4-6 videos at a time on-screen and rest should be paginated for better performance.  Check out our fully featured  Example app here (https://github.com/100mslive/100ms-flutter/tree/main/example).   Display Screen Share Tracks without getting cropped from edges A peer in Room can broadcast their Screen from any platform like Web, Android, iOS, etc. If a peer shares their Screen from Web & the viewer is on a Mobile platform, some content on the Screen can get cropped. It's necessary to configure the HMSVideoView correctly to ensure the complete Screen share content is visible without any clipping/cropping from edges. Always create the HMSVideoView with matchParent as false and ScaleType as ScaleType.SCALE_ASPECT_FIT for correctly showing Screenshare Tracks. Let's look at the implementation:   dart  HMSVideoView(  track: screenShareTrack, // pass the screen share track here  matchParent: false,  // set match parent as false to ensure Screenshare view is not cropped  scaleType: ScaleType.SCALE_ASPECT_FIT, // always set to Aspect Fit for Screenshare Tracks  key: Key(videoTrack.trackId),  // set a unique identifier using the trackId )     HMSVideoView Widget The 100ms Video View Widget named HMSVideoView is built over the Stateless Widget. This HMSVideoView widget and all of its available properties are listed below.   dart class HMSVideoView extends StatelessWidget    /// This will render video with trackId present in the track  ///  track   the video track to be displayed  final HMSVideoTrack track;  //  matchParent   to match the size of the parent widget  final bool matchParent;  //  scaleType   To set the video scaling  final ScaleType scaleType;  //  setMirror   To set mirroring of video  final bool setMirror;  //  disableAutoSimulcastLayerSelect   To disable auto simulcast (Adaptive Bitrate)  final bool disableAutoSimulcastLayerSelect;     "
    },
    {
        "title": "Screen Share",
        "link": "/flutter/v2/features/screen-share",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/screen-share",
        "keywords": [],
        "content": "  Flutter SDK provides support for sharing the entire screen of the device to the room. Please note that for a peer to share their screen, their role must have Screenshare enabled in the dashboard. Also select the appropriate resolution for the Screen share quality. 1080p is recommended for better text readability.   Prerequisites Let's first do some setup required for both the platforms    Android Setup You also need to pass the intent from android native side to HMS SDK in the following way : In your app's MainActivity add    kotlin import live.hms.hmssdk_flutter.HmssdkFlutterPlugin import android.app.Activity import android.content.Intent import live.hms.hmssdk_flutter.Constants override fun onActivityResult(requestCode: Int, resultCode: Int, data: Intent?)   super.onActivityResult(requestCode, resultCode, data)   if (requestCode == Constants.SCREEN_SHARE_INTENT_REQUEST_CODE && resultCode == Activity.RESULT_OK)      HmssdkFlutterPlugin.hmssdkFlutterPlugin?.requestScreenShare(data)         > DONOT forget to add the permission for foreground service in AndroidManifest.xml   kotlin <uses-permission android:name=\"android.permission.FOREGROUND_SERVICE\"       iOS Setup You need to create an iOS broadcast upload extension. It uses Apple's ReplayKit framework to record the device screen and delivers frame samples to your broadcast extension. You can share not only your own app but also the entire device sceeen including other apps on the device.   Step 1  Open project Open your iOS Xcode project, such as ios/Runner.xcworkspace for full-Flutter apps.   Step 2  Add Broadcast Upload Extension Click on your project in the Project Navigator to show the project settings. Press + at the bottom of the target list to add a new target.   AddExtension (/docs/v2/flutter-xcode.png) Select the Broadcast Upload Extension type for your new target.   SelectExtension (/docs/v2/flutter-select-extension.png) Enter your new target detail in the dialog. Uncheck Include UI Extension option.   DetailExtension (/docs/v2/flutter-extension-detail.png) In the following dialog, activate the new scheme for the new target.   ActivateExtension (/docs/v2/flutter-activate-extension.png)   Step 3  Add App Group Click + icon in Signing & Capabilities section. Select App Group from the list of Capabilities.   AddAppgroup (/docs/v2/flutter-add-appgroup.png) New section should be added under Signing & Capabilities named App Groups. Click + icon under that.   Appgroup (/docs/v2/flutter-appgroup.png) Enter App group name (create unique app group name ex: group.your.domain.name)   AppgroupDetail (/docs/v2/flutter-appgroup-detail.png)   Step 4  Edit Podfile In ios folder of your flutter project and open Podfile . Paste the following code and replace the extension name you just created:   target 'Your Extension Name here' do  use_modular_headers   pod 'HMSBroadcastExtensionSDK' end     Podfile (/docs/v2/flutter-podfile.png) In terminal change directory to ios and run pod install command.   Step 5  Edit SampleHandler Expand Runner > ExtensionName and open SampleHandler file.   SampleHandler (/docs/v2/flutter-sample-handler.png) Replace the code with the code below and pass app group name to the respected field:   swift import ReplayKit import HMSBroadcastExtensionSDK class SampleHandler: RPBroadcastSampleHandler     let screenRenderer = HMSScreenRenderer(appGroup: \"Enter App Group Name\")   override func broadcastStarted(withSetupInfo setupInfo:  String : NSObject ?)       // User has requested to start the broadcast. Setup info from the UI extension can be supplied but optional.       override func broadcastPaused()       // User has requested to pause the broadcast. Samples will stop being delivered.       override func broadcastResumed()       // User has requested to resume the broadcast. Samples delivery will resume.       override func broadcastFinished()       // User has requested to finish the broadcast.     screenRenderer.invalidate()       override func processSampleBuffer(_ sampleBuffer: CMSampleBuffer, with sampleBufferType: RPSampleBufferType)       switch sampleBufferType       case RPSampleBufferType.video:       // Handle video sample buffer       if let error = screenRenderer.process(sampleBuffer)           if error.code == .noActiveMeeting             finishBroadcastWithError(NSError(domain: \"ScreenShare\",                           code: error.code.rawValue,                           userInfo:  NSLocalizedFailureReasonErrorKey : \"You are not in a meeting.\" ))                         break     case RPSampleBufferType.audioApp:       // Handle audio sample buffer for app audio       break     case RPSampleBufferType.audioMic:       // Handle audio sample buffer for mic audio       break     @unknown default:       // Handle other sample buffer types       fatalError(\"Unknown type of sample buffer\")                  Key Notes  To start Screenshare from iOS devices you need to pass App Group and Preferred Extension name to HMSSDK constructor as shown below. You can find appGroup and preferredExtension name in Xcode under Signing and Capabilities section under Target > yourExtensionName. Once you have the correct App Group & Preferred Extension values created in Xcode & linked to your Apple Developer Account, you can now use them to start Screenshare from iOS devices (iPhone / iPad).   dart // Pass the correct App Group & Preferred Extension parameters while contructing the HMSSDK object HMSSDK hmsSDK = HMSSDK(   appGroup: \"appGroup\", // App Group value linked to your Apple Developer Account   preferredExtension: \"preferredExtension\"); // Name of the Broadcast Upload Extension Target created in Xcode await hmsSDK.build(); // ensure to await while invoking the build method      parameter (/docs/v2/flutter-dart-help.png)  After completing the setup let's see how we can Start the Screenshare from iOS.   How to Start/Stop Screenshare from the app To start screen share, app needs to call the startScreenshare method of HMSSDK and similar is it's counterpart stopScreenShare to stop screen share. Following is the snippet on how to use this:   dart class Meeting implements HMSUpdateListener, HMSActionResultListener    void startScreenShare()       /// hmsActionResultListener : an instance of a class that implements HMSActionResultListener     //Here this is an instance of a class that implements HMSActionResultListener i.e. Meeting     hmsSDK.startScreenShare(hmsActionResultListener: this);       void stopScreenShare()       /// hmsActionResultListener : an instance of a class that implements HMSActionResultListener     //Here this is an instance of a class that implements HMSActionResultListener i.e. Meeting     hmsSDK.stopScreenShare(hmsActionResultListener: this);       @override   void onSuccess(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments )        switch (methodType)        ...             case HMSActionResultListenerMethod.startScreenShare:       //Screen share started successfully       break;        case HMSActionResultListenerMethod.stopScreenShare:       //Screen share stopped successfully       break;               @override  void onException(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments,    required HMSException hmsException )        switch (methodType)        ...       case HMSActionResultListenerMethod.startScreenShare:       // Check the HMSException object for details about the error       break;       case HMSActionResultListenerMethod.stopScreenShare:       // Check the HMSException object for details about the error       break;               How to get Screen Share Status Application needs to call the isScreenShareActive method of HMSSDK . This method returns a Boolean which will be true incase ScreenShare is currently active and being used, and false for inactive state.   dart hmsSDK.isScreenShareActive();     How to display screenshare in apps  > Screen share track can be differentiated from normal video track using track's source property as track.source == \"SCREEN\"  A peer in Room can broadcast their Screen from any platform like Web, Android, iOS, etc. If a peer shares their Screen from Web & the viewer is on a Mobile platform, some content on the Screen can get cropped. It's necessary to configure the HMSVideoView correctly to ensure the complete Screen share content is visible without any clipping/cropping from edges. Always create the HMSVideoView with matchParent as false and ScaleType as ScaleType.SCALE_ASPECT_FIT for correctly showing Screenshare Tracks. Let's look at the implementation:   dart  HMSVideoView(  track: screenShareTrack, // pass the screen share track here  matchParent: false,  // set match parent as false to ensure Screenshare view is not cropped  scaleType: ScaleType.SCALE_ASPECT_FIT, // always set to Aspect Fit for Screenshare Tracks  key: Key(videoTrack.trackId),  // set a unique identifier using the trackId )   To learn more about Rendering any Video, refer the guide  here (https://www.100ms.live/docs/flutter/v2/features/render-video).   Troubleshooting Guide For starting Screenshare from iOS devices (iPhones or iPads) following are some common setup you should already have within your Apps    Bitcode Disabled Bitcode has been disabled by Apple from Xcode 14 & iOS 16 and above. So 100ms packages also have Disabled Bitcode to ensure compatibility. Ensure that in your Xcode project Bitcode is Disabled for all Targets.   Disable Bitcode in Xcode (/docs/v2/iOS-Xcode-Disable-Bitcode.png)    Podfile with Bitcode Disabled You can use the following Podfile which has post_install script to Disable Bitcode for all Pods . Ensure that you modify the target for your Main App and the Broadcast Upload Extension.  In this sample Podfile, the Target names are Runner and FlutterBroadcastUploadExtension . Change these to the actual Target names defined in your Xcode project.   ruby platform :ios, '13.0' project 'Runner',    'Debug' => :debug,  'Profile' => :release,  'Release' => :release,   def flutter_root  generated_xcode_build_settings_path = File.expand_path(File.join('..', 'Flutter', 'Generated.xcconfig'), __FILE__)  unless File.exist?(generated_xcode_build_settings_path)   raise \"  generated_xcode_build_settings_path  must exist. If you're running pod install manually, make sure flutter pub get is executed first\"  end  File.foreach(generated_xcode_build_settings_path) do line   matches = line.match(/FLUTTER_ROOT =(. )/)   return matches 1 .strip if matches  end  raise \"FLUTTER_ROOT not found in  generated_xcode_build_settings_path . Try deleting Generated.xcconfig, then run flutter pub get\" end require File.expand_path(File.join('packages', 'flutter_tools', 'bin', 'podhelper'), flutter_root) flutter_ios_podfile_setup  ENSURE TO SET THE CORRECT MAIN APP TARGET NAME BELOW target 'Runner' do  use_modular_headers   use_frameworks   flutter_install_all_ios_pods File.dirname(File.realpath(__FILE__)) end   ENSURE TO SET THE CORRECT SCREENSHARE EXTENSION TARGET NAME BELOW target 'FlutterBroadcastUploadExtension' do  use_modular_headers   use_frameworks   pod 'HMSBroadcastExtensionSDK' end  Post install script to Disable Bitcode from all Pods post_install do installer  installer.pods_project.targets.each do target   flutter_additional_ios_build_settings(target)   target.build_configurations.each do config    config.build_settings 'ENABLE_BITCODE'  = 'NO'   end  end end      Background Modes Enabled In majority use-cases, playing audio from Room would be required when app is in Background Mode. Mostly, if users are starting Screenshare from their iPhones/iPads they would want to continue listening to audio from the Room. So, ensure that you have Background Modes Enabled in your Xcode project.   Enable Background Modes (/docs/v2/enable-background-modes.png)    App Groups Enabled Ensure that you have enabled App Groups for both your Main App Target & the newly created Broadcast Extension Target. If the same App Group is not enabled on both Targets then the App & the Screenshare Extension won't be able to communicate & starting Screenshare from your iOS device will fail. Also, ensure that there's no typo / spelling mis-matches between the App Group enabled on Main App Target & the Screenshare Broadcast Extension Target.   Same App Group for both Targets (/docs/v2/same-app-group.png)    Permission Denied EXC_BAD_ACCESS error 100ms Example Apps already have configurations for starting Screenshare on iOS devices. The values for App Group & Preferred Extension used in 100ms Example Apps cannot be reused by any other apps as they won't be linked to your Apple Developer Account. If the 100ms values for App Group which is \"group.flutterhms\" & Preferred Extension which is \"FlutterBroadcastUploadExtension\" are resued by your apps then you will have an Xcode exception containing Permission Denied EXC_BAD_ACCESS error message similar to the one shown below    CFMessagePort: bootstrap_register(): failed 1100 (0x44c) 'Permission denied', port = 0xbc03, name = 'group.flutterhms.88C5E70E-F40F-4C36-A48C-E65736E85CAC.audio.mach.port' See /usr/include/servers/bootstrap_defs.h for the error codes.   thread 1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x8)   frame 0: 0x00000001c03ddc78 CoreFoundation_CFGetNonObjCTypeID + 92 ...   To resolve this issue ensure that you create a Unique App Group which is linked to your Apple Developer Account. If you do not have an Active Apple Developer Account or do not have permissions to create a New App Group, then you won't be able to this feature.    Running on Simulator Starting Screenshare from an iOS Simulator is not supported by Apple. You can start Screenshare only from an actual iOS device like an iPhone or iPad.    iOS Deployment Target Version 100ms Flutter Package is supported for iOS 12 and above versions. Ensure that the Minimum iOS Deployment Target Version is set to 12.0 or above in your Xcode project & the Podfile.    Flutter version 3.3.0+ 100ms Flutter Package is supported for Flutter versions 3.3.0 or above. You can check your current Flutter version by running the flutter doctor command.   Flutter Doctor (/docs/v2/flutter-doctor.png)    Role has Screenshare Permission Ensure that the  Role (../foundation/templates-and-roles) used to Join the Room has Screenshare permission Enabled from the  100ms Dashboard (https://dashboard.100ms.live/). If the Screenshare permission is not Enabled from the Dashboard, any users joining with this Role won't be able to start Screenshare. These users would still be able to see Screenshare performed by other Peers who have Screenshare permissions.   Screen Share Permission (/docs/v2/screenshare-permission.png) "
    },
    {
        "title": "Session Metadata",
        "link": "/flutter/v2/features/session-metadata",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/session-metadata",
        "keywords": [],
        "content": "  Session Metadata is an alpha feature that allows you to set and get metadata for a given session. > A session is defined as the period from when the first peer joins an empty room till the last peer leaves.  The same room can have multiple sessions. During one session the metadata will be preserved. Once a session ends the session metadata will also be cleared, that is, when the last peer leaves.   Limits Since session metadata is an alpha feature, it does not have the following: 1. Locks to ensure consistency of the data. If two peers update it at the same time, it will be a race condition for which one succeeds last, overwriting whatever was before. 2. SDKs are not made aware of session metadata updates on their own. This has to be done manually. One suggested way is listed  below ( updating-session-metadata-manually).   Set Session Metadata Any peer can set the session metadata by calling setSessionMetadata and passing String value to its metadata parameter. > No updates are sent to other peers after calling setSessionMetadata as it's an alpha feature. So, some extra code needs to be written > to send updates. The implementation can be found  here ( updating-session-metadata-manually)   dart class Meeting implements HMSUpdateListener, HMSActionResultListener   ...  void setSessionMetadata(    required String? metadata )      /// metadata : String data which you wish to set as session metadata    /// hmsActionResultListener : an instance of a class which implements HMSActionResultListener    //Here this is an instance of a class that implements HMSActionResultListener i.e. Meeting    hmsSDK.setSessionMetadata(     metadata: metadata, hmsActionResultListener: this);     @override  void onSuccess(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments )        switch (methodType)        ...      case HMSActionResultListenerMethod.setSessionMetadata:      //Session metadata set successfully      break;                @override  void onException(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments,    required HMSException hmsException )        switch (methodType)        ...      case HMSActionResultListenerMethod.setSessionMetadata:      // Check the HMSException object for details about the error      break;           > You will receive an update on onSuccess Callback after successfully setting metadata in HMSActionResultListenerMethod.setSessionMetadata .   Get Session Metadata Any peer can get the session metadata by calling getSessionMetadata .   dart String? metadata = await hmsSDK.getSessionMetadata();     Updating session metadata manually Since no updates are sent for session metadata as it is an alpha feature, here is one suggested way of getting peers to receive it once set. One way to notify other apps of a change in session metadata is to send a custom broadcast message when a set succeeds. The type can be set to something like \"metadata\" or whatever you choose and this should then be handled in the onMessage of all apps. To getSessionMetadata at that time instead of showing a message for that type. Let's understand it from a diagram:   session-metadata (/docs/v2/flutter-session-metadata.png) Let's look at them step-by-step: <div className=\"steps-container\">   PeerA calls setSessionMetadata   dart //Assuming this function is called in class where HMSActionResultListener is implemented as the above example hmsSDK.setSessionMetadata(     metadata: metadata, hmsActionResultListener: this);     PeerA gets onSuccess callback from HMSSDK and sends a broadcast message with type metadata  onSuccess callback sends a broadcast message as follows:   dart @override  void onSuccess(     HMSActionResultListenerMethod methodType =      HMSActionResultListenerMethod.unknown,    Map<String, dynamic>? arguments )       switch (methodType)        case HMSActionResultListenerMethod.setSessionMetadata:         hmssdk.sendBroadcastMessage(message: \"refresh\", type: \"metadata\", hmsActionResultListener:hmsActionResultListener);         break;              > 🔑 Note: We can set the type as any string just make sure you check the same type string on the onMessage callback. > We have taken metadata as an example   Other peers receive onMessage callback In onMessage callback check for message type and update session Metadata value using getSessionMetadata .   dart class Meeting implements HMSUpdateListener, HMSActionResultListener   ...   void getSessionMetadata() async    await hmsSDK.getSessionMetadata();     @override  void onMessage( required HMSMessage message )     if(message.type==\"metadata\")       getSessionMetadata();     return;            </div> "
    },
    {
        "title": "Set Playback Allowed",
        "link": "/flutter/v2/features/set-playback-allowed",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/set-playback-allowed",
        "keywords": [],
        "content": "  Mute/Unmute is something that applies to both audio and video and it's possible to mute remote peers only for yourself. When you mute audio or video for a user using this API, you won't be able to hear or see the remote person but it will be audible and visible to others   Set isPlaybackAllowed You can set playback for a certain remote peer's audio or video. If you set it to false it will turn off the audio or video only for you. You can revert it by setting it to true.   dart //remoteAudioTrack is the remote peer's audio track remoteAudioTrack.setPlaybackAllowed(false);//setting playback to false //remoteVideoTrack is the remote peer's video track remoteVideoTrack.setPlaybackAllowed(true);//setting playback to true   > For muting/unmuting entire room(all peers) for yourself check out the docs  here (../features/mute mute-remote-peers-locally)   Fetch isPlaybackAllowed You can check if playback is allowed or not for a certain remote peer's audio or video.   dart //remoteAudioTrack is the remote peer's audio track bool isPlaybackAllowedForAudioTrack = remoteAudioTrack.isPlaybackAllowed //remoteVideoTrack is the remote peer's video track bool isPlaybackAllowedForVideoTrack = remoteVideoTrack.isPlaybackAllowed   "
    },
    {
        "title": "Update Listeners Enums",
        "link": "/flutter/v2/features/update-listener-enums",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/update-listener-enums",
        "keywords": [],
        "content": "    HMSPeerUpdate Whenever there is an update related to a peer the HMSSDK sends it as HMSPeerUpdate enum type   dart ///PeerUpdates in a room. enum HMSPeerUpdate    ///When new peer joins the room.  peerJoined,  ///When peer left the room.  peerLeft,  ///when peer's role is changed  roleUpdated,  ///when meta data is changed(eg:Hand Raise)  metadataChanged,  ///When Peer name is changed  nameChanged,  ///Peer's network quality updates  networkQualityUpdated   ///Unknown Update  defaultUpdate       HMSTrackUpdate Whenever there is an update related to track the HMSSDK sends it as HMSTrackUpdate enum type   dart ///Track updates you will get when there is any change in the track enum HMSTrackUpdate    //when a new track is added.  trackAdded,  ///when track is removed.  trackRemoved,  ///when track is muted could be audio,video or both.  trackMuted,  ///when track is unmuted could be audio,video or both.  trackUnMuted,  ///When track is degraded  trackDegraded,  ///When track is restored  trackRestored,  ///when track description is changed  trackDescriptionChanged,  ///default update  defaultUpdate       HMSRoomUpdate Whenever there is an update related to room the HMSSDK sends it as HMSRoomUpdate enum type   dart enum HMSRoomUpdate    ///When room is muted  roomMuted,  ///When room is unmuted  roomUnmuted,  ///When server recording state is updated  serverRecordingStateUpdated,  ///When RTMP is started or stopped  rtmpStreamingStateUpdated,  ///When HLS is started or stopped  hlsStreamingStateUpdated,  ///When hls recording state is updated  hlsRecordingStateUpdated,  ///When browser recording state is changed  browserRecordingStateUpdated,  ///When peer Count is changed  roomPeerCountUpdated,  ///Default Update  defaultUpdate     "
    },
    {
        "title": "Update Listeners",
        "link": "/flutter/v2/features/update-listeners",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/features/update-listeners",
        "keywords": [],
        "content": "  Now, let's take a look at what the listener class looks like. > HMSUpdateListener : A class conforming to the HMSUpdateListener interface. > We can implement HMSUpdateListener , HMSActionResultListener , HMSStatsListener etc. on a single class > checkout the example  here (https://github.com/100mslive/100ms-flutter/blob/1537a82a2213c8bbb1b0eb2dcc009c171e89bad1/example/lib/data_store/meeting_store.dart L33) The methods of HMSUpdateListener are invoked to notify updates happening in the room like a peer join/leave, track mute/unmute etc. Let's see the flow for listener lifecycle:   update-listener (/docs/v2/flutter-update-listener.png)   How to add an HMSUpdateListener This section contains info about how can we attach HMSUpdateListener :   dart class Meeting implements HMSUpdateListener     Meeting()      initHMSSDK();       void initHMSSDK() async       HMSSDK hmsSDK = HMSSDK();     await hmsSDK.build(); // ensure to await while invoking the build method     // this corresponds to the instance implementing HMSUpdateListener     hmsSDK.addUpdateListener(this);       ...      addUpdateListener is the method responsible for attaching HMSUpdateListener .   Remove update listener If we don't want the room updates anymore or while leaving the room, we must remove the updateListener instance as:   dart hmsSDK.removeUpdateListener(this);     Supplementary bytes This section explains HMSUpdateListener class methods. We implement this class to listen to room updates.   dart /// 100ms SDK provides callbacks to the client app about any change or update happening in the room after a user has joined by implementing HMSUpdateListener. /// Implement this listener in a class where you want to perform UI Actions, update App State, etc. These updates can be used to render the video on the screen or to display other info regarding the room. /// Depending on your use case, you'll need to implement specific methods of the Update Listener. The most common ones are onJoin, onPeerUpdate, onTrackUpdate & onHMSError. abstract class HMSUpdateListener    void onJoin( required HMSRoom room );  /// This will be called whenever there is an update on an existing peer  /// or a new peer got added/existing peer is removed.  ///  /// This callback can be used to keep a track of all the peers in the room  ///  Parameters:  ///   peer: the peer who joined/left or was updated  ///   update: the triggered update type. Should be used to perform different UI Actions  void onPeerUpdate( required HMSPeer peer, required HMSPeerUpdate update );  /// This is called when there are updates on an existing track  /// or a new track got added/existing track is removed  ///  /// This callback can be used to render the video on screen whenever a track gets added  ///  Parameters:  ///   track: the track which was added, removed or updated  ///   trackUpdate: the triggered update type  ///   peer: the peer for which the track was added, removed or updated  void onTrackUpdate(     required HMSTrack track,    required HMSTrackUpdate trackUpdate,    required HMSPeer peer );  /// This will be called when there is an error in the system  ///  /// and SDK have already retried to fix the error  ///  Parameter error: the error that occurred  void onHMSError( required HMSException error );  /// This is called when there is a change in any property of the Room  ///  ///  Parameters:  ///   room: the room which was joined  ///   update: the triggered update type. Should be used to perform different UI Actions  void onRoomUpdate( required HMSRoom room, required HMSRoomUpdate update );  /// This is called when there is a new broadcast message from any other peer in the room  ///  /// This can be used to implement chat in the room  ///  Parameter message: the received broadcast message  void onMessage( required HMSMessage message );  /// This is called every 1 second with a list of active speakers  ///  ///  A HMSSpeaker object contains   ///   peerId: the peer identifier of HMSPeer who is speaking  ///   trackId: the track identifier of HMSTrack which is emitting audio  ///   audioLevel: a number within range 1-100 indicating the audio volume  ///  /// A peer who is not present in the list indicates that the peer is not speaking  ///  /// This can be used to highlight currently speaking peers in the room  ///  Parameter speakers: the list of speakers  void onUpdateSpeakers( required List<HMSSpeaker> updateSpeakers );  /// When the network connection is lost & the SDK is trying to reconnect to the room  void onReconnecting();  /// When you are back in the room after the network connection was lost  void onReconnected();  /// This is called when someone asks for a change of role  ///  /// for eg. the admin can ask a peer to become a host from a guest.  /// this triggers this call on the peer's app  void onRoleChangeRequest( required HMSRoleChangeRequest roleChangeRequest );  /// When someone requests for track change of your Audio, Video or an Auxiliary track like Screenshare, this event will be triggered  ///  Parameter hmsTrackChangeRequest: request instance consisting of all the required info about track change  void onChangeTrackStateRequest(     required HMSTrackChangeRequest hmsTrackChangeRequest );  /// When someone kicks you out or when someone ends the room at that time it is triggered  ///  Parameter hmsPeerRemovedFromPeer  it consists of info about who removed you and why.  void onRemovedFromRoom(     required HMSPeerRemovedFromPeer hmsPeerRemovedFromPeer );  /// Whenever a new audio device is plugged in or audio output is changed we get the onAudioDeviceChanged update  /// This callback is only fired on Android devices. On iOS, this callback will not be triggered.  ///  Parameters:  ///   currentAudioDevice: Current audio output route  ///   availableAudioDevice: List of available audio output devices  void onAudioDeviceChanged(     HMSAudioDevice? currentAudioDevice,    List<HMSAudioDevice>? availableAudioDevice );     "
    },
    {
        "title": "Adaptive Bitrate",
        "link": "/flutter/v2/foundation/adaptive-bitrate",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/foundation/adaptive-bitrate",
        "keywords": [],
        "content": "  Adaptive bitrate (ABR) refers to features that enable dynamic adjustments to video quality—to optimise for end-user experience under diverse network conditions. ABR ensures that every participant is able to consume the highest possible quality video in conferencing or streaming use-cases, based on their bandwidth constraints.\r \r In addition to network, ABR can also optimise for the right video quality based on the size of the video element. For example, a video call running on active speaker layout has larger video tiles that require higher quality video track. These adjustments can be made dynamically with adaptive bitrate.\r \r Learn about how 100ms enables adaptive bitrate in:\r \r    Conferencing scenarios ( abr-in-conferencing)\r    Live streaming scenarios ( abr-in-live-streaming)\r \r  ABR in conferencing\r \r Peers in 100ms rooms can publish multiple video quality levels simultaneously. This is called “simulcast” in 100ms. Peers that consume these video tracks can upgrade or downgrade video quality.\r \r You can enable simulcast on the publishing role's template, and use manual or automatic layer changes on the subscriber's side.\r \r   Publisher-side configuration\r \r Simulcast configuration is opt-in and can be enabled on the role's configuration inside your template. The role's publish video quality determines video quality layers on simulcast. For example, a role configured to publish at 720p can simulcast 180p, 360p and 720p layers.\r \r  Video publish quality  Possible simulcast layers \r               \r  1080p          1080p, 540p, 270p     \r  720p          720p, 360p, 180p      \r  480p          480p, 240p         \r  360p          360p, 180p         \r  240p          240p            \r  180p          180p            \r \r   Enable via dashboard\r \r Enable \"can publish simulcast\" on the template page for a particular role. You can also specify how many video quality layers will be simultaneously published by peers of this role. The peer will publish these layers assuming network bandwidth permits.\r \r   Simulcast configuration (/docs/guides/simulcast-on-dashboard.png)\r \r   Enable via API\r \r Update role configuration using the  server-side API (/docs/server-side/v2/policy/create-update-role). The simulcast config payload can include 2 or 3 layers that scale down the selected publish layer.\r \r In the example below, the role is configured to publish 720p with 3 simulcast layers:\r \r   f for full with scale down factor of 1 (= 720p)\r   h for half with scale down factor of 2 (= 360p)\r   q for quarter with scale down factor of 4 (= 180p)   js     \"publishParams\":       ...     \"simulcast\":         \"video\":           \"layers\":                           \"rid\": \"f\",             \"scaleResolutionDownBy\": 1,             \"maxBitrate\": 700,             \"maxFramerate\": 30            ,                         \"rid\": \"h\",             \"scaleResolutionDownBy\": 2,             \"maxBitrate\": 250,             \"maxFramerate\": 30            ,                         \"rid\": \"q\",             \"scaleResolutionDownBy\": 4,             \"maxBitrate\": 100,             \"maxFramerate\": 30                              ,       \"screen\":                  \r   Subscribe-side behavior\r \r   Manual layer selection\r \r The 100ms client-side SDKs provide methods to set a preferred quality layer for a remote peer's video track. See docs for your preferred platform:\r \r    JavaScript (/docs/javascript/v2/advanced-features/simulcast)\r    iOS (/docs/ios/v2/advanced-features/simulcast)\r    Android (/docs/android/v2/advanced-features/simulcast)\r \r   Automatic layer selection\r \r <br \r <video loop=\"true\" controls=\"controls\" id=\"vid\" muted>\r   <source src=\"/docs/guides/simulcast-tile-size-1.mp4\" type=\"video/mp4\"  \r </video>\r <br \r \r  Based on video tile size: The SDK automatically ensures appropriate video layer is subscribed to, as demonstrated in the video above. For example, if the video element is 360 px in width, 360p or the closest layer will be selected.\r      JavaScript  : The useVideo hook in the 100ms React SDK auto-selects the appropriate video quality layer.\r      iOS  : HMSVideoView can auto-select video quality layer.\r      Android  : HMSVideoView ( see docs (/docs/android/v2/migrations/surfaceview-migration)) can auto-select video quality layer.\r  Based on network quality: ABR will work alongside subscribe degradation and auto-downgrade video quality for peers. This is _coming soon_.\r \r  ABR in live streaming\r \r 100ms uses the HTTP Live Streaming (HLS) protocol in live streaming scenarios. HLS supports adaptive bitrate out of the box, and HLS video players can automatically or manually pick appropriate video quality levels.\r \r Learn more on  how HLS works on our blog (https://www.100ms.live/blog/hls-101-beginners-guide).\r "
    },
    {
        "title": "Basic Concepts\r",
        "link": "/flutter/v2/foundation/basics",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/foundation/basics",
        "keywords": [],
        "content": " -\r \r  Architecture 100ms is a cloud platform that allows developers to add video and audio conferencing to Web, Android and iOS applications. The platform provides REST APIs, SDKs, and a dashboard that makes it simple to capture, distribute, record, and render live interactive audio, video. Any application built using 100ms' SDK has 2 components.     Client:   Use 100ms android, iOS, Web SDKs to manage connections, room states, render audio/video.     Server:   Use 100ms' APIs or dashboard to create rooms, setup room templates, trigger recording or RTMP streaming, access events.   Architecture (/docs/docs/v2/arch.png)   Basic Concepts   Room A room is the basic object that 100ms SDKs return on successful connection. This contains references to peers, tracks and everything you need to render a live a/v or live streaming app.   Peer A peer is the object returned by 100ms SDKs that contains all information about a user  name, role, video track etc.   Session A session depicts activity in a room. A session is created when one or more peers join a room to communicate with each other. A single room can have multiple sessions; a unique ID will be assigned to each session. The maximum allowed duration for a session on the 100ms platform is 12 hours.   Track A track is a segment of media (audio/video) captured from the peer's camera and microphone. Peers in a session publish local tracks and subscribe to remote tracks from other peers.   Role A role defines who can a peer see/hear, the quality at which they publish their video, whether they have permissions to publish video/screenshare, mute someone, change someone's role.   Template A template is a collection of roles, room settings, recording and RTMP settings (if used), that are used by the SDK to decide which geography to connect to, which tracks to return to the client, whether to turn on recording when a room is created, etc. Each room is associated with a template.   Destinations Destinations is used to save audio/video calls for offline viewing. 100ms supports 2 kinds of recording   SFU recording (/docs/javascript/v2/foundation/recordings sfu-recording-advanced) and  Browser recording (/docs/javascript/v2/foundation/recordings browser-recording-recommended). Also, HLS enabled configuration will allow you to live stream your room over HLS.   RTMP RTMP streaming is used to live stream your video conferencing apps to platforms like YouTube, Twitch, Facebook, MUX, etc.   Webhooks Webhook is an HTTP(S) endpoint used for pushing the notifications to your application. It will be invoked by 100ms servers to notify events of your room.   Workspace A workspace is an isolated environment which contains account data like templates, rooms, room and session history, etc. You can use workspaces to represent environments like “Production” and “Development” and invite team members to a workspace.   What are the steps to build a live app with 100ms? 1. Sign up on 100ms using the   Try For Free   button in the top navbar.   Signup for 100ms account (/docs/docs/v2/signup.png) 2. Once you're logged in to the dashboard, click on Create Your First App    Signup for 100ms account (/docs/docs/v2/create-your-first-app.png) 3.   Hover   on one of the Starter Kits. Deploy one of them. (We will use the   Video Conference Starter Kit   for this example)   Dashboard _ 100ms.png (/docs/docs/v2/select-starter-kit.png) 4. Select your account type and fill in the details   Dashboard _ 100ms.png (/docs/docs/v2/personal-details.png) 5. Choose a deployment option. This could be 100ms or Vercel (based on the Starter Kit you are deploying)   Video Conferencing Starter Kit (/docs/docs/v2/choose-your-deployment.png) 6. Enter a subdomain of your choice. Please avoid entering https/http/www or dots while entering the subdomain. Select a region closest to you and hit Continue.   choose subdomain (/docs/docs/v2/choose-subdomain.png) 7. Join or Invite someone to your deployed app with one of the roles:    join or invite (/docs/docs/v2/demo-your-app.png)   Where should I start? \r \r   Quickstart\r \r If you want to see 100ms' SDKs in action in under 5 minutes, run one of our quickstart  apps (../guides/quickstart)\r \r   100ms Flutter app\r \r You can download & check out the 100ms Flutter app  \r \r 🤖 The Flutter Android app from  Google Play Store here (https://play.google.com/store/apps/details?id=live.hms.flutter)\r \r 📱 Flutter iOS app from  Apple App Store here (https://apps.apple.com/app/100ms-live/id1576541989)\r \r \r   Github Repo \r You can checkout the 100ms Flutter SDK Github repo which also contains a fully fledged  Example app implementation here (https://github.com/100mslive/100ms-flutter/) \r "
    },
    {
        "title": "Handling audio-video edge cases",
        "link": "/flutter/v2/foundation/handling-audio-video-edge-cases",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/foundation/handling-audio-video-edge-cases",
        "keywords": [],
        "content": "    Introduction 100ms handles a lot of standard audio/video issues internally without the developer needing to handle it explicitly. This page describes some common issues and how 100ms handles them. There are 3 major issues of issues that can occur in a audio/video conference  Device capture exceptions  Network disconnection/switching network exceptions  Network bandwidth limitation/large room exceptions   Device failure A common issue is a failure to capture mic/camera even though the user has all devices connected. Common causes include differences in OS/browser implementations of device capture APIs, permission not being granted by the user, or the device being in use by another program. The usual recourse in these exceptions is to prompt a user action  \"Grant permission\", \"Please close any other app using microphone\", \"Switch to Safari\" 100ms' SDKs come with a  preview method (../features/preview) that can be called before joining a room. This will test for device failures, network connectivity and throw errors with a recommended user action.   Network disconnection/Switching networks Another set of common issues are minor network blips. Common causes are when a user moves from one room to another, or switches from wifi to data. 100ms will send a notification within 10s of detecting a network disconnection and will automatically retry when connection is available upto 60s. After 60s, a terminal error is thrown to the client.   Network bandwidth limitation/large rooms A common occurrence in large rooms, or constrained networks is dropped frames. This results in robotic voices, frozen frames, pixelated screenshare or entire pieces of audio/video that are lost. 100ms will automatically prioritize connections if network limits are reached. This prioritization can be controlled by developers using the dashboard or 100ms APIs. eg. A developer can prioritize host's screenshare higher than guests' videos. In low bandwidth constraints, guests' videos will be turned off, while host's screenshare will remain.  "
    },
    {
        "title": "Interactive Live Streaming",
        "link": "/flutter/v2/foundation/live-streaming",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/foundation/live-streaming",
        "keywords": [],
        "content": "    Overview Live video interactions can span various levels of interactivity. For example, in a virtual event, some participants can be on stage talking to each other, whereas participants in the audience can be listening to them. At 100ms, we think of this as the   3 levels of interactivity  . 100ms enables you to build live video use-cases by mixing and matching these 3 levels to get to your ideal solution.     Level 1  : Full duplex audio/video in real-time   Level 1 participants publish their audio/video, and interact with others in sub-second latency. This is real-time video conferencing, similar to Zoom or Google Meet.     Level 2  : Webinar-style audience in real-time   Level 2 participants consume audio/video from level 1 participants with sub-second latency, without publishing their own audio/video. Level 2 participants can engage with level 1 through messaging (chat, emojis, custom events). This is similar to a Zoom webinar.   Levels 1 and 2 are enabled using WebRTC.     Level 3  : Live stream audience consuming in near real-time   Level 3 participants consume a composite live stream in near real-time (<10 secs of latency) without publishing their audio/video. They can interact with other participants via messaging. This is similar to viewers on Twitch or YouTube Live, and is enabled via   100ms Interactive Live Streaming  .   Live streaming uses  HLS (https://www.100ms.live/blog/hls-101-beginners-guide) to achieve near real-time latency at scale. The  roles primitive (/docs/javascript/v2/foundation/templates-and-roles) can be used to define capabilities of a participant and associate them to an interaction level. A participant can move between levels using a single API call to change roles.   Try Interactive Live Streaming Use our  Live Streaming Starter Kit (https://www.100ms.live/marketplace/live-streaming-starter-kit) to try out the experience before you write a line of code. <StepsToc  parentId=\"try-live-streaming\"  descriptions=  \"Use our Live streaming starter kit to try out the experience before you write a line of code.\", \"Understand the difference between a stream broadcaster and stream viewer.\", \"Use the demo app link to go live for the first time as a broadcaster and join the stream as viewer.\", \"Use the 100ms self-serve dashboard to update the layout, aspect ratio, etc of the stream.\"   > <StepsContainer id=\"try-live-streaming\">   Create a new app   Live streaming starter kit (/docs/docs/v2/live-streaming-starter-kit.png) 1. Make sure that you have  an account with 100ms (https://dashboard.100ms.live/register) and can access the  100ms dashboard (https://dashboard.100ms.live/) 1. On the dashboard, create a new app using the Live Streaming Starter Kit 1. Specify a subdomain and region to deploy the app   Understand roles   Live Streaming roles (/docs/docs/v2/live-streaming-roles.png) This starter kit configures your new app with  two roles (/docs/javascript/v2/foundation/templates-and-roles):   broadcaster : This role represents a streamer who publishes their audio/video. There can be multiple peers who join as broadcasters   hls-viewer : This role represents a circle 3 audience, who subscribes to the composite live stream and can interact using messaging   Go live   Go live (/docs/docs/v2/live-streaming-go-live.gif) 1. To go live for the first time, join the room as a broadcaster and start the live stream 2. Once the stream has started, join the room as an hls-viewer and you should be able to see the ongoing live stream 3. Use chat messages to interact between the viewer and the broadcaster   Customize the stream   Go live (/docs/docs/v2/live-streaming-customise.png) By default, the live stream is composed in landscape mode for desktop viewers (with an aspect ratio of 16:9). You can customise the live stream for viewers on mobile or to support multiple broadcaster tiles. 1. On the 100ms dashboard, click the gear icon on your app to open configuration settings 2. Go to \"destinations\" and scroll down to find live stream (HLS) configuration 3. Update the configuration based on your needs:    If your viewers are on mobile, change the video aspect ratio to 9:16    If you have multiple broadcasters joining in, choose grid or active speaker based on your needs    In case of grid layout, choose the tile size that fits your use-case. For example, a stream with 2 streamers looks better with 1:1 tiles. </StepsContainer>   Integrate in your app To integrate 100ms Interactive Live Streaming in your app, follow these steps: 1.  Enable live streaming destination ( enable-destination) 2.  Integrate the 100ms SDK in your app ( sdk-integration) 3.  Integrate live stream playback ( live-stream-playback) 4.  Video on demand (VOD) use cases ( video-on-demand-vod-use-cases)   Enable destination   Enable HLS (/docs/docs/v2/live-streaming-enable.gif) If your app is based on the Live Streaming Starter Kit (as shown above), the live streaming destination is enabled out-of-the-box. For custom apps, you can enable the live streaming destination manually: 1. Open configuration for your existing app using the 100ms dashboard 1. In the \"destinations\" tab, enable \"Live Streaming with HLS\" 1. Ensure that you have roles for the broadcaster (who can publish their audio/video) and the viewer (who cannot publish audio/video)   SDK integration Use the 100ms client-side SDKs to integrate streaming in your application. See code snippets for the client-side SDK  here (/docs/javascript/v2/features/hls).   Live stream playback Using our client-side SDKs, you can enable live stream playback and add interactive experiences like chat, raise hand and other functionalities to your app using  peer metadata (/docs/javascript/v2/advanced-features/peer-metadata). The process is so simple: 1. Once you  start ( step-3-go-live) live streaming, you will get an HLS URL (M3U8 URL) which you can use for playback. 2. You can use the  client-side SDK (/docs/javascript/v2/features/hls) to get the HLS URL by checking the  current state (/docs/javascript/v2/features/hls current-room-status) of the room and start playback. If you need to only enable HLS playback and don't need interactivity, you can follow one of the below approaches to get the HLS URL:     Webhook:   You can listen to hls.started.success  webhook event (/docs/server-side/v2/introduction/webhook hls-started-success) and get the HLS URL from the url field. Please check the  webhooks guide (/docs/server-side/v2/introduction/webhook) to learn more about webhooks.     Static URL:   This configuration will help you get a static URL for playback. You can enable the Static playback URLs in your template from the  dashboard (https://dashboard.100ms.live/dashboard). You can go to Destination > enable \"Live streaming with HLS\" > under \"Customise stream video output\" section > enable \"Static playback URLs.\"     Enable Static URL (/docs/docs/v2/enable-static-url.png)     _Format_: https://cdn.100ms.live/beam/<customer_id>/<room_id>/master.m3u8     customer_id : replace this placeholder with your customer_id from  developer section (https://dashboard.100ms.live/developer) on your dashboard.     room_id : replace this placeholder with the room_id of the respective room from which the stream will be broadcasted.   Video on Demand (VOD) use cases If you wish to replay your HLS stream for Video on demand (VOD) use case, 100ms provides the capability to record the HLS stream which will be posted to your webhook as a ZIP file of M3U8 format (same playback format as HLS) with all the chunks once the stream ends. You can start recording a live stream using the  client-side SDK (/docs/javascript/v2/features/hls) or using the  server API (/docs/server-side/v2/Destinations/rtmp-streaming-and-browser-recording start-streaming-recording). Once the HLS recording is completed, you will get the details of recording as a callback to the webhook configured in your account. Check hls.recording.success  webhook event (/docs/server-side/v2/introduction/webhook hls-recording-success) for more information. "
    },
    {
        "title": "Recordings",
        "link": "/flutter/v2/foundation/recordings",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/foundation/recordings",
        "keywords": [],
        "content": "  Recordings are an important part of the live video stack as they convert live, ephemeral content into a long-term asset. But the use of this asset varies from business to business depending on their respective use case. For example, one of the common use cases for recording is for archival purposes versus, for some, its content to be publicized. Based on your end goal, you can choose one of the recording types and its implementation. You can understand some key differences using the comparison table below.   Recording types   Recording types ( recording-types)    Quick Comparison ( quick-comparison)    Browser Recording  Recommended   ( browser-recording-recommended)    SFU Recording  Advanced   ( sfu-recording-advanced)    Recordings for Live Streaming Use-cases ( recordings-for-live-streaming-use-cases)     Video-on-demand Recording ( video-on-demand-recording)     Multiresolution Recording ( multiresolution-recording)   Configure storage ( configure-storage)    How to configure recording storage? ( how-to-configure-recording-storage)   Quick Comparison  Recording Features           Browser Recording  Recommended   SFU Recording  Advanced                                      Resolution               Upto 1080p            Only 720p              Participant-level Audio/Video Tracks  Not Available           Available              Portrait/Landscape Mode        Available             Not Available            Start/Stop Recording          On-demand             Auto start/stop with the session   Custom Layout             Available             Not Available            Role-Specific Recording        Available             Not Available            Recording Output            MP4                MP4, WebM                Browser Recording  Recommended  Browser recording is built to give users a participant-first recording experience. When enabled, our browser-based bot Beam joins a room to record the viewport like any other participant. The output is an MP4 file that captures the room's published audio/video tracks together into one single file. This option removes the complexity of syncing various audio/video tracks and offers an intuitive, participant-first recording experience. An example use case is to record a sales meeting for later usage.   Resources     How to implement Browser Recording (https://www.100ms.live/docs/server-side/v2/Destinations/rtmp-streaming-and-browser-recording)   SFU Recording  Advanced  SFU recording is built for advanced use cases, which require individual audio and video tracks for each participant. This recording option allows you to isolate recording at a participant level. Track recording allows you to record audio and video streams separately, making it easier to edit, layer, or reuse each of them. An example use case is to record a live podcast and later edit it for publishing. You can get track recordings in two forms:   Individual: Media for each peer is provided as a separate mp4 file. This file will have both audio and video of the peer. These files can be used for offline review or in implementing custom composition.   Composite  currently in beta : Audio and video of all peers are composed as per their joining/leaving the meeting and provided as a single mp4. This file can be used for offline viewing of the meeting.   Resources      How to implement SFU Recording (https://www.100ms.live/docs/server-side/v2/Destinations/recording)   Recordings for Live Streaming Use-cases These are the types of live streaming recordings:    Video-on-demand Recording Video-on-demand recording is available for our Interactive Live Streaming capability. This recording will be a file with an M3U8 file (same playback format as HLS), which can be used for replaying your HLS stream. This option is more suitable for Video-on-Demand use cases. For the implementation of this type of recording, please  contact us (https://www.100ms.live/contact).    Multiresolution Recording A multi-resolution recording is available for Interactive Live Streaming capability. This type of recording will have a multi-file structure for all available resolutions of the stream. The output will be multiple MP4 files with these resolutions: 240p, 480p, 720p, and 1080p. For an implementation of this type of recording, please  contact us (https://www.100ms.live/contact).   Configure storage You can specify a cloud storage location for your recording files in your template. Our current offering allows you to store your recordings in Amazon S3 buckets. Once you configure the S3 config of your bucket in a template, all respective recordings of sessions created via those templates will be sent to your configured bucket. This holds true for all types of aforementioned recordings.   How to configure recording storage? 1. Generate your credentials; for this example, you can check out a  guide from AWS (https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html). You can skip this step if you already have credentials. Please note that if you are running a Browser recording, you need to give upload permission to your key, but if you are running an SFU recording, you need to give both upload and download permission. 2. Go to 100ms Dashboard and go to template   configuration by selecting the configure icon  .   Create your first app (/docs/docs/v2/recording-storage-settings-step2.png) 3. Head over to the   Destinations   tab.   Destinations (/docs/docs/v2/recording-storage-settings-step3.png) 1. Key in your credentials (using an example of an S3 bucket here):    Access Key: Access Key generated from AWS IAM Console    Secret Key: Secret Key generated from AWS IAM Console    Bucket: Name of the bucket in S3    Region: Name of the region, for example, ap-south1    Prefix for Upload Path: Define the directory name (optional)   Destinations (/docs/docs/v2/recording-storage-settings-step4.png) 5. Use the   Validate Config   button to test your storage setup.   Destinations (/docs/docs/v2/recording-storage-settings-step5.png) 6. You will see a message that the AWS   configuration was successfully validated  .   Destinations (/docs/docs/v2/recording-storage-settings-step6.png) The above message ensures that your configuration is successful now, and all your recordings will start collecting in your configured destination. "
    },
    {
        "title": "Authentication and Tokens\r",
        "link": "/flutter/v2/foundation/security-and-tokens",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/foundation/security-and-tokens",
        "keywords": [],
        "content": " -\r \r  Introduction In 100ms, two types of tokens are used to authenticate requests coming from your Client apps and Backend application server into the 100ms platform.    App token ( app-token) : Used to authenticate and allow end-users (peers) to join 100ms rooms. An App Token controls Peer identity and Room permissions in your real-time or Interactive live-streaming video application.    Management token ( management-token) : Used to authenticate all the requests to 100ms REST API. You can set the expiry to both these tokens; if you follow the code samples from this guide, the expiry will be set as 24 hours. However, a best practice is to set the expiry as short as feasible for your application. You must host your server to generate these tokens while transitioning your app to production.   App Token 100ms _client-side SDKs_ use App Tokens to authenticate a peer (participant) while  joining a room (./../features/join). Generate this token on the server side and make it available for your client-side apps that use the 100ms SDKs. To create an App Token, you need to use app_access_key , app_secret , room_id , and user_id .   You can get the app_access_key and app_secret from the  developer section (https://dashboard.100ms.live/developer) in your 100ms dashboard. This key and secret differ across workspaces so please ensure you are in the intended workspace before copying these credentials.     room_id  : This is the unique identifier for your room. You can get it from the  rooms page (https://dashboard.100ms.live/rooms) in your dashboard or in the response payload of the  create room server-side API (/docs/server-side/v2/Rooms/create-via-api).     user_id  : This identifier can be used to map a 100ms peer to your own internal user object for business logic. Specify your internal user identifier as the peer's user_id. If not available, use any random string.   How to use? You can get App tokens using a couple of approaches based on your app's lifecycle stage. Please check the below sections for more information:    Set up your own authentication endpoint (./../foundation/security-and-tokens set-up-your-own-authentication-endpoint)    Get app tokens from the token endpoint (./../foundation/security-and-tokens get-app-tokens-from-the-token-endpoint)    Get app tokens from the dashboard (./../foundation/security-and-tokens get-app-tokens-from-the-dashboard)    Set up your own authentication endpoint When you have completed your integration with 100ms, and while transitioning your app to production, we recommend you create your backend service for app token generation; use the code snippet below and set up the token generation service as per your preferred programming language. \r \r   Code sample: Generate app token\r \r  s id=\"client-code-token\" items=  'Node.js', 'Python', 'Java', 'Ruby', 'PHP'    \r \r   id='client-code-token-0'>\r \r  javascript\r var jwt = require('jsonwebtoken');\r var uuid4 = require('uuid4');\r \r var app_access_key = '<app_access_key>';\r var app_secret = '<app_secret>';\r \r var payload =  \r   access_key: app_access_key,\r   room_id: '<room_id>',\r   user_id: '<user_id>',\r   role: '<role>',\r   type: 'app',\r   version: 2,\r   iat: Math.floor(Date.now() / 1000),\r   nbf: Math.floor(Date.now() / 1000)\r  ;\r \r jwt.sign(\r   payload,\r   app_secret,\r    \r     algorithm: 'HS256',\r     expiresIn: '24h',\r     jwtid: uuid4()\r    ,\r   function (err, token)  \r     console.log(token);\r    \r );\r  \r \r </Tab>\r \r   id='client-code-token-1'>\r \r  python\r  /usr/bin/env python3\r import jwt\r import uuid\r import datetime\r import sys\r \r app_access_key = \"<app_access_key>\"\r app_secret = \"<app_secret>\"\r \r def generate(room_id, user_id, role):\r   expires = expires or 24   3600\r   now = datetime.datetime.utcnow()\r   exp = now+ datetime.timedelta(seconds=expires)\r   return jwt.encode(payload= \r         \"access_key\": app_access_key,\r         \"type\":\"app\",\r         \"version\":2,\r         \"room_id\": room_id,\r         \"user_id\": user_id,\r         \"role\":role,\r         \"jti\": str(uuid.uuid4()),\r         \"exp\": exp,\r         \"iat\": now,\r         \"nbf\": now,\r          , key=app_secret)\r if __name__ == \"__main__\":\r   if len(sys.argv) == 3:\r     room_id = sys.argv 0 \r     user_id = sys.argv 1 \r     role = sys.argv 2 \r   print(generate(room_id=room_id, user_id=user_id, role=role))\r  \r \r </Tab>\r \r   id='client-code-token-2' >\r \r  java\r import java.time.Instant;\r import java.util.Date;\r import java.util.HashMap;\r import java.util.Map;\r import java.util.UUID;\r import io.jsonwebtoken.Jwts;\r import io.jsonwebtoken.SignatureAlgorithm;\r \r private void generateHmsClientToken()  \r   Map<String, Object> payload = new HashMap<>();\r   payload.put(\"access_key\", \"<app_access_key>\");\r   payload.put(\"room_id\", \"<room_id>\");\r   payload.put(\"user_id\", \"<user_id>\");\r   payload.put(\"role\", \"<role>\");\r   payload.put(\"type\", \"app\");\r   payload.put(\"version\", 2);\r   String token = Jwts.builder().setClaims(payload).setId(UUID.randomUUID().toString())\r     .setExpiration(new Date(System.currentTimeMillis() + 86400   1000))\r     .setIssuedAt(Date.from(Instant.ofEpochMilli(System.currentTimeMillis()  60000)))\r     .setNotBefore(new Date(System.currentTimeMillis()))\r     .signWith(SignatureAlgorithm.HS256, \"<app_secret>\".getBytes()).compact();\r   \r  \r \r </Tab>\r \r   id='client-code-token-3' >\r \r  ruby\r require 'jwt'\r require 'securerandom'\r \r \r \r $app_access_key = \"<app_access_key>\"\r $app_secret = \"app_secret\"\r \r def generateAppToken(room_id, user_id, role)\r   now = Time.now\r   exp = now + 86400\r   payload =  \r     access_key: $app_access_key,\r     room_id: room_id,\r     user_id: user_id,\r     role: role,\r     type: \"app\",\r     jti: SecureRandom.uuid,\r     version: 2,\r     iat: now.to_i,\r     nbf: now.to_i,\r     exp: exp.to_i\r    \r \r   token = JWT.encode(payload, $app_secret, 'HS256')\r end\r \r puts generateAppToken \"<room_id>\", \"<user_id>\", \"<role>\"\r \r  \r \r </Tab>\r \r   id='client-code-token-4'>\r \r  php\r <?php\r \r use Firebase JWT JWT;\r use Ramsey Uuid Uuid;\r \r $issuedAt = new DateTimeImmutable();\r $expire  = $issuedAt->modify('+24 hours')->getTimestamp();\r $accessKey = \"<app_access_key>\";\r $secret = \"<app_secret>\";\r $version  = 2;\r $type   = \"app\";\r $role   = \"<role>\";\r $roomId  = \"<room_id>\";\r $userId  = \"<user_id>\";\r \r $payload =  \r   'iat' => $issuedAt->getTimestamp(),\r   'nbf' => $issuedAt->getTimestamp(),\r   'exp' => $expire,\r   'access_key' => $accessKey,\r   'type' => \"app\",\r   'jti' => Uuid::uuid4()->toString()\r   'version' => 2,\r   'role' => $role,\r   'room_id' => $roomId,\r   'user_id' => $userId\r  ;\r \r $token = JWT::encode(\r   $payload,\r   $secret,\r   'HS256'\r );\r  \r \r </Tab>\r \r <Note type=\"warning\">\r   We strictly advise you not to post <i>app_access_key</i> and <i>app_secret</i> publicly; if you\r   need to store them in <strong>Git</strong>, please change the repository from public to private.\r   <br  \r   <br   You cannot use an <strong>App token</strong> to trigger server API requests.\r </Note>\r \r   Get app tokens from the token endpoint If you are building integration with 100ms, you can get app tokens from the 100ms token endpoint without hosting a token generation backend service. Refer to  this guide (./../guides/token-endpoint get-an-app-token-using-token-endpoint) for more information.    Get app tokens from the dashboard If you are building your first app by following one of our  quickstart guides (/docs/javascript/v2/guides/javascript-quickstart), you can get the app token directly from 100ms dashboard to join a room for the first time. Refer to  this guide (./../guides/token get-a-temporary-token-from-100ms-dashboard) for more information.   Management Token 100ms uses management tokens to authenticate REST APIs.  If you're evaluating 100ms  server APIs (/docs/server-side/v2/introduction/basics), you can use our public  Postman collection (/docs/server-side/v2/introduction/postman-guide fork-the-collection), which doesn't require you to create a management token as we've managed it using a  pre-request script (/docs/server-side/v2/introduction/postman-guide simplified-token-generation) within the collection. If you're transitioning your app to production, we recommend you create your backend service for management token generation. You must use the app_access_key and app_secret from the  developer section (https://dashboard.100ms.live/developer) in your 100ms dashboard to create the management token.\r \r   Code sample: Generate management token\r \r  s id=\"test-code\" items=  'Node.js', 'Python', 'Java', 'Ruby', 'PHP'    \r \r   id='test-code-0'>\r \r  js\r var jwt = require('jsonwebtoken');\r var uuid4 = require('uuid4');\r \r var app_access_key = '<app_access_key>';\r var app_secret = '<app_secret>';\r \r jwt.sign(\r    \r     access_key: app_access_key,\r     type: 'management',\r     version: 2,\r     iat: Math.floor(Date.now() / 1000),\r     nbf: Math.floor(Date.now() / 1000)\r    ,\r   app_secret,\r    \r     algorithm: 'HS256',\r     expiresIn: '24h',\r     jwtid: uuid4()\r    ,\r   function (err, token)  \r     console.log(token);\r    \r );\r  \r \r </Tab>\r \r   id='test-code-1'>\r \r  py\r  /usr/bin/env python3\r import jwt\r import uuid\r import datetime\r \r app_access_key = '<app_access_key>'\r app_secret = '<app_secret>'\r \r \r def generateManagementToken():\r   expires = 24   3600\r   now = datetime.datetime.utcnow()\r   exp = now + datetime.timedelta(seconds=expires)\r   return jwt.encode(payload= \r     'access_key': app_access_key,\r     'type': 'management',\r     'version': 2,\r     'jti': str(uuid.uuid4()),\r     'iat': now,\r     'exp': exp,\r     'nbf': now\r      , key=app_secret)\r \r if __name__ == '__main__':\r   print(generateManagementToken())\r  \r \r </Tab>\r \r   id=\"test-code-2\">\r \r  java\r import java.time.Instant;\r import java.util.Date;\r import java.util.HashMap;\r import java.util.Map;\r import java.util.UUID;\r import io.jsonwebtoken.Jwts;\r import io.jsonwebtoken.SignatureAlgorithm;\r \r private void generateManagementToken()  \r   Map<String, Object> payload = new HashMap<>();\r   payload.put(\"access_key\", \"<app_access_key>\");\r   payload.put(\"type\", \"management\");\r   payload.put(\"version\", 2);\r   String token = Jwts.builder().setClaims(payload).setId(UUID.randomUUID().toString())\r     .setExpiration(new Date(System.currentTimeMillis() + 86400   1000))\r     .setIssuedAt(Date.from(Instant.ofEpochMilli(System.currentTimeMillis()  60000)))\r     .setNotBefore(new Date(System.currentTimeMillis()))\r     .signWith(SignatureAlgorithm.HS256, \"<app_secret>\".getBytes()).compact();\r   \r  \r \r </Tab>\r \r   id=\"test-code-3\">\r \r  ruby\r require 'jwt'\r require 'securerandom'\r \r $app_access_key = \"<app_access_key>\"\r $app_secret = \"<app_secret>\"\r \r def generateManagementToken()\r   now = Time.now\r   exp = now + 86400\r   payload =  \r   access_key: $app_access_key,\r   type: \"management\",\r   version: 2,\r   jti: SecureRandom.uuid,\r   iat: now.to_i,\r   nbf: now.to_i,\r   exp: exp.to_i\r  \r token = JWT.encode(payload, $app_secret, 'HS256')\r return token\r end\r \r puts generateManagementToken\r  \r \r </Tab>\r \r   id=\"test-code-4\">\r \r  php\r <?php\r \r use Firebase JWT JWT;\r use Ramsey Uuid Uuid;\r \r $app_access_key = \"<app_access_key>\";\r $app_secret = \"<app_secret>\";\r \r $issuedAt  = new DateTimeImmutable();\r $expire   = $issuedAt->modify('+24 hours')->getTimestamp();\r \r $payload =  \r   'access_key' => $app_access_key,\r   'type' => 'management',\r   'version' => 2,\r   'jti' => Uuid::uuid4()->toString(),\r   'iat' => $issuedAt->getTimestamp(),\r   'nbf' => $issuedAt->getTimestamp(),\r   'exp' => $expire,\r  ;\r \r $token = JWT::encode($payload, $app_secret, 'HS256');\r ?>\r  \r \r </Tab>\r \r <Note type=\"warning\">\r   We strictly advise you not to post <i>app_access_key</i> and <i>app_secret</i> publicly; if you\r   need to store them in <strong>Git</strong>, please change the repository from public to private.\r   <br  \r   <br  \r   You cannot authenticate room join requests from your client-side apps with a <strong>\r     Management token\r   </strong>.\r </Note>\r "
    },
    {
        "title": "Templates and Roles",
        "link": "/flutter/v2/foundation/templates-and-roles",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/foundation/templates-and-roles",
        "keywords": [],
        "content": "    Introduction Template is the blueprint of the room. It defines the settings of the room along with the behavior of users who are part of it. Room will inherit the properties from a template that you have specified while creating it. If you have not specified any template then it will pick the default template. Each template will be identified by its id or name. For example default_videoconf_7e450ffc-8ef1-4572-ab28-b32474107b89  Users can see or modify the templates by visiting  Templates on Dashboard (https://dashboard.100ms.live/templates) or via API (see below). After updating a template or some part of its like permissions, you need to rejoin or restart the session for the template updates to take place.   Template (/docs/docs/v2/template.png)   Roles Role is a collection of permissions that allows you to perform certain set of operations while being part of the room. It has the following attributes:   Name Every role has a name that should be unique inside a template. This name will be used while generating app tokens and referencing inside a template.   Priority Priority will determine the order in which the roles will be degraded. A lower number represents a higher priority.   Publish Strategies Publish strategies will be used to determine the tracks and their quality which can be published by this role.  Strategy       Description                                                                                                                                                                                                                            Can share audio    Whether the role is allowed to publish the audio track or not.                                                                                                                  Can share video    Whether the role is allowed to publish the video track or not                                                                                                                   Can share screen   Whether the role is allowed to do screen share or not                                                                                                                       Video quality     Quality of the video track which is going to be published by the role. Currently, 6 video qualities 1080p , 720p , 480p , 360p , 240p and 120p are predefined and the user can select one out of these values. This option will be visible only if the   Can share video   is enabled.   Screenshare quality  Quality of the screen which is going to be shared by the role. Currently, 2 video qualities 720p and 1080p are predefined and the user can select one out of these values. This option will be visible only if the   Can share screen   is enabled.                       Subscribe Strategies Subscribe strategies will be used to determine what all roles, this role can subscribe to.  Strategy        Description                                                                                                                                                                                                      Subscribe to      You can select all the roles of the template which this role will subscribe                                                                                             Subscribe Degradation  When this flag is turned on, one or more remote video tracks will be muted automatically when the network condition worsens. Such tracks will be marked as degraded . When the network condition improves, the degraded tracks will automatically be unmuted.    Permissions Permissions will contain a list of additional privileges that this role will have.  Permission               Description                                                                                                                                              Can change any participant's role   With this permission, user will be able to change the role of the other participant's who are present in the room                                  Can mute any participant        With this permission, user will be able to mute any participant's audio and/or video.                                                Can ask participant to unmute     With this permission, user will be able to ask any participant to unmute their audio and/or video.                                         Can remove participant from the room  With this permission, user will be able to remove any participant from the current session of the room.                                       Can end current session of the room  With this permission, user will be able to end the current session of the room.                                                   Can receive room state         With this permission, user will be able to receive room state like peer-count and peer-list on the preview screen.                                 Can start/stop RTMP livestream     With this permission, user will be able to publish live audio/video livestream externally to social media and custom platforms (e.g Youtube/Facebook/Twitter).           Can start/stop HLS livestream     With this permission, user will be able to publish audio/video livestream in the HLS format.                                            Can start/stop Beam Recording     With this permission, user will be able to record meeting/livestream via the browser recording approach where a bot will join the room and record the meeting/livestream as is.    Advanced Settings As the name suggests, Advanced Settings section contains more settings and controls for the advanced user.   Template (/docs/docs/v2/advanced-settings.png)   Preview room state Preview room state enables you to build a \"preview\" screen which shows the state of the room before joining. This room state includes a list of peers, which can be used to show who is in the room. Preview room state also includes recording and streaming state. Preview room state settings define strategy of sending state updates to client SDKs.  Setting                  Description                                                                                                                                                                                                 Room-state Message Interval (in seconds)  Room-state data will be sent over a regular interval of these many seconds. Consequently, the room state displayed on the preview screen will refresh accordingly. This value must be a multiple of 5, between 5 and 3600 seconds, both inclusive.   Send Peer List in Room-state        Enabling this will send peer-list info of the room. If disabled, only the peer count is sent.                                                                             Enable Room-State             If enabled, room-state data will be sent to the preview screen. If disabled, no such room-state data will be sent.                                                                   Roles with room-state permission      This is the list of all the roles which will get the room-state data. You can also individually toggle these settings in the Roles tab under the Permissions section.                                           API reference Apart from the dashboard, a programmatic way to interact with templates is via  API (/server-side/v2/policy/template-object). "
    },
    {
        "title": "Flutter Quickstart Guide",
        "link": "/flutter/v2/guides/quickstart",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/guides/quickstart",
        "keywords": [],
        "content": "    Overview This guide will walk you through simple instructions to create a video conferencing app using 100ms Flutter SDK and test it using an emulator or your mobile phone. Please check our  basic concepts guide (./../foundation/basics) to understand the concepts like rooms, templates, peers, etc. This guide contains instructions for two approaches to get you started with 100ms Flutter SDK: 1.  Create a sample app ( create-a-sample-app) — instructions to create a flutter app quickly with a complete code sample. 2.  Building step-by-step ( building-step-by-step) — instructions to walk you through the implementation of the app in a step-by-step manner. You can also check our  basic sample app (https://github.com/100mslive/100ms-flutter/tree/main/sample%20apps/flutter-quickstart-app) on GitHub. Check out the full-fledged example app implementation in the 100ms Flutter SDK  GitHub repository (https://github.com/100mslive/100ms-flutter/tree/main/example) showcasing multiple features provided by 100ms. This uses the  provider package (https://pub.dev/packages/provider) as the state management library. We also have other  sample apps (https://github.com/100mslive/100ms-flutter/tree/main/sample%20apps) built using other popular state management libraries :    Bloc (https://github.com/100mslive/100ms-flutter/tree/main/sample%20apps/bloc)    Getx (https://github.com/100mslive/100ms-flutter/tree/main/sample%20apps/getx)    Riverpod (https://github.com/100mslive/100ms-flutter/tree/main/sample%20apps/riverpod)    Mobx (https://github.com/100mslive/100ms-flutter/tree/main/sample%20apps/mobx)   Create a sample app This section contains instructions to create a simple Flutter video conferencing app. We will help you with instructions to understand the project setup and complete code sample to implement this quickly.   Prerequisites To complete this implementation for the Android platform, you must have the following:   A  100ms account (https://dashboard.100ms.live/register) if you don't have one already.    Flutter (https://docs.flutter.dev/get-started/install) 3.3.0 or higher   Dart 2.12.0 or above   Use  VS code (https://code.visualstudio.com/),  Android Studio (https://developer.android.com/studio), or any other IDE that supports Flutter. For more information on setting up an IDE, check  Flutter's official guide (https://docs.flutter.dev/get-started/editor).   Create a Flutter app Once you have the prerequisites, follow the steps below to create a Flutter app. This guide will use VS code, but you can use any IDE that supports Flutter.   Create a Flutter app using the terminal; you can get the  Flutter SDK (https://docs.flutter.dev/get-started/install/macos get-sdk) and use the below command:     bash section=createFlutterApp   flutter create my_app       Once the app is created, open it in VS code.   Add 100ms SDK to your project Once you have created a Flutter app, you must add the 100ms Flutter SDK and  permission_handler package (https://pub.dev/packages/permission_handler) (to handle audio/video permissions from microphone and camera) to your app.   Add the below snippet to the pubspec.yaml .   yaml section=InstallingTheDependencies  100ms SDK and permissions_handler hmssdk_flutter: permission_handler:     Run flutter pub get to download these dependencies to your app.   Add permissions   s id=\"sample-app\" items=  'Android', 'iOS'       id=\"sample-app-0\"> Please follow the below instructions to test the app for the android target platform: 1. Allow camera, recording audio and internet permissions by adding the below snippet to the AndroidManifest.xml file (at the application tag level).   <br       xml section=androidPermissions   <uses-feature android:name=\"android.hardware.camera\"    <uses-feature android:name=\"android.hardware.camera.autofocus\"    <uses-permission android:name=\"android.permission.CAMERA\"    <uses-permission android:name=\"android.permission.CHANGE_NETWORK_STATE\"    <uses-permission android:name=\"android.permission.MODIFY_AUDIO_SETTINGS\"    <uses-permission android:name=\"android.permission.RECORD_AUDIO\"    <uses-permission android:name=\"android.permission.INTERNET\"    <uses-permission android:name=\"android.permission.ACCESS_NETWORK_STATE\"    <uses-permission android:name=\"android.permission.FOREGROUND_SERVICE\"     <uses-permission android:name=\"android.permission.BLUETOOTH\" android:maxSdkVersion=\"30\"     <uses-permission android:name=\"android.permission.BLUETOOTH_CONNECT\"       2. Add minimum SDK version ( minSdkVersion 21 ) in \"android/app/build.gradle\" file (inside \"defaultConfig\").   json ...  defaultConfig      ...    minSdkVersion 21    ...    ...   You will also need to request camera and record audio permissions at runtime before you join a call or display a preview. Please follow the  Android Documentation (https://developer.android.com/training/permissions/requesting request-permission) for runtime permissions. </Tab>   id=\"sample-app-1\"> Please follow the below instructions to test the app for iOS target platform: 1. Allow camera, recording audio and internet permissions by adding the below snippet to the info.plist file.   <br       xml section=iosPermissions   <key>NSMicrophoneUsageDescription</key>   <string> YourAppName  wants to use your microphone</string>   <key>NSCameraUsageDescription</key>   <string> YourAppName  wants to use your camera</string>   <key>NSLocalNetworkUsageDescription</key>   <string> YourAppName  App wants to use your local network</string>     2. Add the target platform version as ( platform :ios, '12.0' ) in \"ios/Podfile.\"   json platform :ios, '12.0'   </Tab>   Complete code example Now that your project setup is complete let's replace the code in the lib/main.dart file with the complete code sample below.   dart section=completeCodeExample import 'dart:io'; import 'package:flutter/cupertino.dart'; import 'package:flutter/material.dart'; import 'package:hmssdk_flutter/hmssdk_flutter.dart'; import 'package:permission_handler/permission_handler.dart'; void main() => runApp(const MaterialApp(home: HomePage())); class HomePage extends StatelessWidget    const HomePage( super.key );  Future<bool> getPermissions() async     if (Platform.isIOS) return true;   await Permission.camera.request();   await Permission.microphone.request();   await Permission.bluetoothConnect.request();   while ((await Permission.camera.isDenied))      await Permission.camera.request();       while ((await Permission.microphone.isDenied))      await Permission.microphone.request();       while ((await Permission.bluetoothConnect.isDenied))      await Permission.bluetoothConnect.request();       return true;    // UI to render join screen  @override  Widget build(BuildContext context)     return Scaffold(    body: Container(     color: Colors.black,     child: Center(      child: ElevatedButton(       style: ButtonStyle(        shape: MaterialStateProperty.all<RoundedRectangleBorder>(         RoundedRectangleBorder(          borderRadius: BorderRadius.circular(8.0),         ),        ),       ),       // Function to push to meeting page       onPressed: () async          await getPermissions();        Navigator.push(         context,         CupertinoPageRoute(builder: (_) => const MeetingPage()),        );        ,       child: const Padding(        padding: EdgeInsets.symmetric(vertical: 20, horizontal: 20),        child: Text(         'Join',         style: TextStyle(fontSize: 20),        ),       ),      ),     ),    ),   );      class MeetingPage extends StatefulWidget    const MeetingPage( super.key );  @override  State<MeetingPage> createState() => _MeetingPageState();   class _MeetingPageState extends State<MeetingPage>   implements HMSUpdateListener    //SDK  late HMSSDK hmsSDK;  // Variables required for joining a room  String authToken =    \"APP_TOKEN_FROM_DASHBOARD\";  String userName = \"test_user\";  // Variables required for rendering video and peer info  HMSPeer? localPeer, remotePeer;  HMSVideoTrack? localPeerVideoTrack, remotePeerVideoTrack;  // Initialize variables and join room  @override  void initState()     super.initState();   initHMSSDK();     void initHMSSDK() async     hmsSDK = HMSSDK();   await hmsSDK.build(); // ensure to await while invoking the build method   hmsSDK.addUpdateListener(listener: this);   hmsSDK.join(config: HMSConfig(authToken: authToken, userName: userName));     // Clear all variables  @override  void dispose()     remotePeer = null;   remotePeerVideoTrack = null;   localPeer = null;   localPeerVideoTrack = null;   super.dispose();     // Called when peer joined the room  get current state of room by using HMSRoom obj  @override  void onJoin( required HMSRoom room )     room.peers?.forEach((peer)      if (peer.isLocal)       localPeer = peer;     if (peer.videoTrack  = null)        localPeerVideoTrack = peer.videoTrack;           if (mounted)        setState(()   );               );     // Called when there's a peer update  use to update local & remote peer variables  @override  void onPeerUpdate( required HMSPeer peer, required HMSPeerUpdate update )     switch (update)      case HMSPeerUpdate.peerJoined:     if ( peer.isLocal)        if (mounted)         setState(()          remotePeer = peer;        );                  break;    case HMSPeerUpdate.peerLeft:     if ( peer.isLocal)        if (mounted)         setState(()          remotePeer = null;        );                  break;    case HMSPeerUpdate.networkQualityUpdated:     return;    default:     if (mounted)        setState(()         localPeer = null;       );               // Called when there's a track update  use to update local & remote track variables  @override  void onTrackUpdate(     required HMSTrack track,    required HMSTrackUpdate trackUpdate,    required HMSPeer peer )     if (track.kind == HMSTrackKind.kHMSTrackKindVideo)      switch (trackUpdate)       case HMSTrackUpdate.trackRemoved:      if (mounted)         setState(()          peer.isLocal          ? localPeerVideoTrack = null          : remotePeerVideoTrack = null;        );             return;     default:      if (mounted)         setState(()          peer.isLocal          ? localPeerVideoTrack = track as HMSVideoTrack          : remotePeerVideoTrack = track as HMSVideoTrack;        );                     // More callbacks  no need to implement for quickstart  @override  void onAudioDeviceChanged(     HMSAudioDevice? currentAudioDevice,    List<HMSAudioDevice>? availableAudioDevice )     @override  void onChangeTrackStateRequest(     required HMSTrackChangeRequest hmsTrackChangeRequest )     @override  void onHMSError( required HMSException error )     @override  void onMessage( required HMSMessage message )     @override  void onReconnected()     @override  void onReconnecting()     @override  void onRemovedFromRoom(     required HMSPeerRemovedFromPeer hmsPeerRemovedFromPeer )     @override  void onRoleChangeRequest( required HMSRoleChangeRequest roleChangeRequest )     @override  void onRoomUpdate( required HMSRoom room, required HMSRoomUpdate update )     @override  void onUpdateSpeakers( required List<HMSSpeaker> updateSpeakers )     // Widget to render a single video tile  Widget peerTile(Key key, HMSVideoTrack? videoTrack, HMSPeer? peer)     return Container(    key: key,    child: (videoTrack  = null &&  (videoTrack.isMute))      // Actual widget to render video      ? HMSVideoView(        track: videoTrack,       )      : Center(        child: Container(         decoration: BoxDecoration(          color: Colors.blue.withAlpha(4),          shape: BoxShape.circle,          boxShadow: const             BoxShadow(            color: Colors.blue,            blurRadius: 20.0,            spreadRadius: 5.0,           ),           ,         ),         child: Text(          peer?.name.substring(0, 1) ?? \"D\",          style: const TextStyle(            color: Colors.white,            fontSize: 24,            fontWeight: FontWeight.w600),         ),        ),       ),   );     // Widget to render grid of peer tiles and a end button  @override  Widget build(BuildContext context)     return WillPopScope(    // Used to call \"leave room\" upon clicking back button  in android     onWillPop: () async       hmsSDK.leave();     Navigator.pop(context);     return true;     ,    child: SafeArea(     child: Scaffold(      backgroundColor: Colors.black,      body: Stack(       children:          // Grid of peer tiles        Container(         height: MediaQuery.of(context).size.height,         child: GridView(          gridDelegate: SliverGridDelegateWithFixedCrossAxisCount(            mainAxisExtent: (remotePeerVideoTrack == null)              ? MediaQuery.of(context).size.height              : MediaQuery.of(context).size.height / 2,            crossAxisCount: 1),          children:             if (remotePeerVideoTrack  = null && remotePeer  = null)            peerTile(              Key(remotePeerVideoTrack?.trackId ?? \"\" \"mainVideo\"),              remotePeerVideoTrack,              remotePeer),           peerTile(             Key(localPeerVideoTrack?.trackId ?? \"\" \"mainVideo\"),             localPeerVideoTrack,             localPeer)           ,         ),        ),        // End button to leave the room        Align(         alignment: Alignment.bottomCenter,         child: RawMaterialButton(          onPressed: ()             hmsSDK.leave();           Navigator.pop(context);           ,          elevation: 2.0,          fillColor: Colors.red,          padding: const EdgeInsets.all(15.0),          shape: const CircleBorder(),          child: const Icon(           Icons.call_end,           size: 25.0,           color: Colors.white,          ),         ),        ),        ,      ),     ),    ),   );          Fetch token to join the room To test audio/video functionality, you need to connect to a 100ms room; please check the following steps for the same: 1. Navigate to your  100ms dashboard (https://dashboard.100ms.live/dashboard) or  create an account (https://dashboard.100ms.live/register) if you don't have one. 2. Use the Video Conferencing Starter Kit to create a room with a default template assigned to it to test this app quickly. 3. Go to the  Rooms page (https://dashboard.100ms.live/rooms) in your dashboard, click on the Room Id of the room you created above, and click on the Join Room button on the top right. 4. You will see 100ms demo URLs for the roles created when you deployed the starter kit; you can click on the 'key' icon to copy the token and update the authToken in \"lib/main.dart\" file.   > Token from 100ms dashboard is for testing purposes only, For production applications you must generate tokens on your own server. > Refer to the  Management Token section (./../foundation/security-and-tokens management-token) in Authentication and Tokens guide for more information. <video loop=\"true\" autoplay=\"autoplay\" controls=\"controls\" id=\"vid\" muted>   <source src=\"/docs/guides/update-token-flutter.mp4\" type=\"video/mp4\"   </video> <br     Test the app After adding the required code let's run the app    Build and run the app  s id=\"platform\" items=  'Android', 'iOS'       id=\"platform-0\">   Once you've made the above changes, your app is ready for testing. You can build the app and run it in an emulator or an actual android device.   Go to _Run > Start debugging > select a device to use_ (android emulator or android phone). Now, after you click join , you should be able to see yourself (android emulator doesn't support actual video, you can connect an actual device to see your video in real-time). You can join the room using a browser as the second peer to check audio/video transactions between two or more peers. <video loop=\"true\" autoplay=\"autoplay\" controls=\"controls\" id=\"vid\" muted>   <source src=\"/docs/guides/join-room-flutter-android.mp4\" type=\"video/mp4\"   </video> <br   </Tab>   id=\"platform-1\">   Once you've made the above changes, your app is ready for testing. You can build the app and run it in a simulator or an actual iOS device.   Go to _Run > Start debugging > select a device to use_ (iOS simulator or iPhone). Now, after you click join , you should be able to see yourself (iOS simulator doesn't support actual video, you can connect an actual device to see your video in real-time). You can join the room using a browser as the second peer to check audio/video transactions between two or more peers. <video loop=\"true\" autoplay=\"autoplay\" controls=\"controls\" id=\"vid\" muted>   <source src=\"/docs/guides/join-room-flutter-android.mp4\" type=\"video/mp4\"   </video> <br   </Tab>   Building step-by-step In this section, We'll walk through what the code does. <div className=\"steps-container\">   Add dependencies in pubspec.yaml  In your project pubspec.yaml dependencies add:   yaml section=InstallingTheDependencies  100ms SDK and permissions_handler hmssdk_flutter: permission_handler:      Add permissions for android and iOS  Add the permissions for microphone,camera and bluetooth for android and iOS follow the docs  above ( add-permissions)   Handle device runtime permissions We need permission from the user to access the media from the user's device. We must urge the user to grant permission to access camera, microphone, and bluetooth devices. We use the  permission_handler (https://pub.dev/packages/permission_handler) package that provides a cross-platform (iOS, Android) API to request permissions and check their status. Please ensure to update permissions in the AndroidManifest.xml file for android and info.plist file for iOS. Check  Add Permission section ( add-permissions) for more information. getPermissions takes required permission for microphone, camera and bluetooth.   dart section=stepByStepPermissions class HomePage extends StatelessWidget    const HomePage( super.key );  Future<bool> getPermissions() async     if (Platform.isIOS) return true;   await Permission.camera.request();   await Permission.microphone.request();   await Permission.bluetoothConnect.request();   while ((await Permission.camera.isDenied))      await Permission.camera.request();       while ((await Permission.microphone.isDenied))      await Permission.microphone.request();       while ((await Permission.bluetoothConnect.isDenied))      await Permission.bluetoothConnect.request();       return true;          Implement join screen This section will help you create the join screen user interface. To keep it simple for the quickstart, we have not created many UI elements; you can refer to the  sample app implementation (https://github.com/100mslive/100ms-flutter/tree/main/example/lib/preview) for a complete Preview/Join user interface. Add the below code in HomePage class.   dart section=stepByStepJoinScreen // UI to render join screen  @override  Widget build(BuildContext context)     return Scaffold(    body: Container(     color: Colors.black,     child: Center(      child: ElevatedButton(       style: ButtonStyle(        shape: MaterialStateProperty.all<RoundedRectangleBorder>(         RoundedRectangleBorder(          borderRadius: BorderRadius.circular(8.0),         ),        ),       ),       // Function to push to meeting page       onPressed: () async          await getPermissions();        Navigator.push(         context,         CupertinoPageRoute(builder: (_) => const MeetingPage()),        );        ,       child: const Padding(        padding: EdgeInsets.symmetric(vertical: 20, horizontal: 20),        child: Text(         'Join',         style: TextStyle(fontSize: 20),        ),       ),      ),     ),    ),   );        Implement meeting page You can check the below snippet to create a widget as the user interface to show the video tiles of local and remote peers. HMSUpdateListener plays a significant role in rendering video or displaying any information regarding the room. 100ms SDK provides callbacks to the client app about any change or update happening in the room after a user has joined by implementing HMSUpdateListener . To join a room, you need to create an HMSConfig instance and use that instance to call the join method of HMSSDK . >   Note:   An App token is required to authenticate a room join request from your client-side app. Please ensure to add the authToken by fetching it from your dashboard. Check  fetch token to join a room section ( fetch-token-to-join-the-room) for more information. <br  Read more about authentication and tokens in  this guide (./../foundation/security-and-tokens)   dart section=stepByStepMeetingPage class MeetingPage extends StatefulWidget    const MeetingPage( super.key );  @override  State<MeetingPage> createState() => _MeetingPageState();   class _MeetingPageState extends State<MeetingPage>   implements HMSUpdateListener    //SDK  late HMSSDK hmsSDK;  // Variables required for joining a room  String authToken =    \"APP_TOKEN_FROM_DASHBOARD\";  String userName = \"test_user\";  // Variables required for rendering video and peer info  HMSPeer? localPeer, remotePeer;  HMSVideoTrack? localPeerVideoTrack, remotePeerVideoTrack;  // Initialize variables and join room  @override  void initState()     super.initState();   initHMSSDK();     void initHMSSDK() async     hmsSDK = HMSSDK();   await hmsSDK.build(); // ensure to await while invoking the build method   hmsSDK.addUpdateListener(listener: this);   hmsSDK.join(config: HMSConfig(authToken: authToken, userName: userName));     // Clear all variables  @override  void dispose()     remotePeer = null;   remotePeerVideoTrack = null;   localPeer = null;   localPeerVideoTrack = null;   super.dispose();        Now in the same class we will override the HMSUpdateListener methods to listen to updates.   Listen to room and peer updates The 100ms SDK sends updates to the application about any change in HMSPeer and HMSRoom via the callbacks in HMSUpdateListener . Our application must listen to the corresponding updates in onPeerUpdate and onRoomUpdate . Check the  Update Listeners documentation (./../features/update-listener-enums) to understand the types of updates emitted by the SDK for room and peer updates. We will add these methods in MeetingPage class as they need to override the HMSUpdateListener methods.   dart section=stepByStepRoomUpdates // Called when peer joined the room  get current state of room by using HMSRoom obj  @override  void onJoin( required HMSRoom room )     room.peers?.forEach((peer)      if (peer.isLocal)       localPeer = peer;     if (peer.videoTrack  = null)        localPeerVideoTrack = peer.videoTrack;           if (mounted)        setState(()   );               );     // Called when there's a peer update  use to update local & remote peer variables  @override  void onPeerUpdate( required HMSPeer peer, required HMSPeerUpdate update )     switch (update)      case HMSPeerUpdate.peerJoined:     if ( peer.isLocal)        if (mounted)         setState(()          remotePeer = peer;        );                  break;    case HMSPeerUpdate.peerLeft:     if ( peer.isLocal)        if (mounted)         setState(()          remotePeer = null;        );                  break;    case HMSPeerUpdate.networkQualityUpdated:     return;    default:     if (mounted)        setState(()         localPeer = null;       );                  Listen to track updates 100ms SDK also sends updates to the application about any change in HMSTrack via the callbacks in HMSUpdateListener . Our application must listen to the corresponding updates in onTrackUpdate . Check the  Update Listeners documentation (./../features/update-listener-enums hms-track-update) to understand the types of updates emitted by the SDK for track updates.   dart section=stepByStepTrackUpdates // Called when there's a track update  use to update local & remote track variables  @override  void onTrackUpdate(     required HMSTrack track,    required HMSTrackUpdate trackUpdate,    required HMSPeer peer )     if (track.kind == HMSTrackKind.kHMSTrackKindVideo)      switch (trackUpdate)       case HMSTrackUpdate.trackRemoved:      if (mounted)         setState(()          peer.isLocal          ? localPeerVideoTrack = null          : remotePeerVideoTrack = null;        );             return;     default:      if (mounted)         setState(()          peer.isLocal          ? localPeerVideoTrack = track as HMSVideoTrack          : remotePeerVideoTrack = track as HMSVideoTrack;        );                        Other callbacks 100ms SDK provides various other callbacks to handle different scenarios in the app. For example, you can use onAudioDeviceChanged to get updates whenever a new audio device or an audio device is switched. Please check  here (./../features/update-listeners) for more information about these callbacks.   dart section=stepByStepOtherCallbacks  // More callbacks  no need to implement for quickstart  @override  void onAudioDeviceChanged(     HMSAudioDevice? currentAudioDevice,    List<HMSAudioDevice>? availableAudioDevice )     @override  void onChangeTrackStateRequest(     required HMSTrackChangeRequest hmsTrackChangeRequest )     @override  void onHMSError( required HMSException error )     @override  void onMessage( required HMSMessage message )     @override  void onReconnected()     @override  void onReconnecting()     @override  void onRemovedFromRoom(     required HMSPeerRemovedFromPeer hmsPeerRemovedFromPeer )     @override  void onRoleChangeRequest( required HMSRoleChangeRequest roleChangeRequest )     @override  void onRoomUpdate( required HMSRoom room, required HMSRoomUpdate update )     @override  void onUpdateSpeakers( required List<HMSSpeaker> updateSpeakers )        Render video in a tile We had initialized the HMSUpdateListener class in the  Implement meeting page section ( step-3-implement-meeting-page); now, we can use the same to render video tracks in a tile. To display a video track, first get the HMSVideoTrack & pass it on to HMSVideoView . Ensure to add the HMSVideoView to your app's Widget tree. Check the  render video guide (./../features/render-video) for more information.   dart section=stepByStepRenderTile // Widget to render a single video tile  Widget peerTile(Key key, HMSVideoTrack? videoTrack, HMSPeer? peer)     return Container(    key: key,    child: (videoTrack  = null &&  (videoTrack.isMute))      // Actual widget to render video      ? HMSVideoView(        track: videoTrack,       )      : Center(        child: Container(         decoration: BoxDecoration(          color: Colors.blue.withAlpha(4),          shape: BoxShape.circle,          boxShadow: const             BoxShadow(            color: Colors.blue,            blurRadius: 20.0,            spreadRadius: 5.0,           ),           ,         ),         child: Text(          peer?.name.substring(0, 1) ?? \"D\",          style: const TextStyle(            color: Colors.white,            fontSize: 24,            fontWeight: FontWeight.w600),         ),        ),       ),   );        Render video tiles for remote peer This section will help you build the user interface that renders the video tracks of local and remote peer in a grid. Add this in MeetingPage class.For more info about implementation check the complete code  above ( complete-code-example)  dart section=stepByStepRenderPeers // Widget to render grid of peer tiles and a end button  @override  Widget build(BuildContext context)     return WillPopScope(    // Used to call \"leave room\" upon clicking back button  in android     onWillPop: () async       hmsSDK.leave();     Navigator.pop(context);     return true;     ,    child: SafeArea(     child: Scaffold(      backgroundColor: Colors.black,      body: Stack(       children:          // Grid of peer tiles        Container(         height: MediaQuery.of(context).size.height,         child: GridView(          gridDelegate: SliverGridDelegateWithFixedCrossAxisCount(            mainAxisExtent: (remotePeerVideoTrack == null)              ? MediaQuery.of(context).size.height              : MediaQuery.of(context).size.height / 2,            crossAxisCount: 1),          children:             if (remotePeerVideoTrack  = null && remotePeer  = null)            peerTile(              Key(remotePeerVideoTrack?.trackId ?? \"\" \"mainVideo\"),              remotePeerVideoTrack,              remotePeer),           peerTile(             Key(localPeerVideoTrack?.trackId ?? \"\" \"mainVideo\"),             localPeerVideoTrack,             localPeer)           ,         ),        ),        // End button to leave the room        Align(         alignment: Alignment.bottomCenter,         child: RawMaterialButton(          onPressed: ()             hmsSDK.leave();           Navigator.pop(context);           ,          elevation: 2.0,          fillColor: Colors.red,          padding: const EdgeInsets.all(15.0),          shape: const CircleBorder(),          child: const Icon(           Icons.call_end,           size: 25.0,           color: Colors.white,          ),         ),        ),        ,      ),     ),    ),   );       </div> You can refer to the  test the app section ( test-the-app) to test your app for android or iOS platforms.   Next steps We have multiple example apps to get you started with 100ms Flutter SDK,   Basic example For a basic example, see the  sample app (https://github.com/100mslive/100ms-flutter/tree/main/sample%20apps/flutter-quickstart-app) on GitHub.   Full-fledged example You can also check out the full-fledged example app implementation in the 100ms Flutter SDK  GitHub repository (https://github.com/100mslive/100ms-flutter/tree/main/example) showcasing multiple features provided by 100ms. This uses the  provider package (https://pub.dev/packages/provider) as the state management library.   Examples with other state management libraries For implementations with other state management libraries, visit :    Bloc (https://github.com/100mslive/100ms-flutter/tree/main/sample%20apps/bloc)    Getx (https://github.com/100mslive/100ms-flutter/tree/main/sample%20apps/getx)    Riverpod (https://github.com/100mslive/100ms-flutter/tree/main/sample%20apps/riverpod)    Mobx (https://github.com/100mslive/100ms-flutter/tree/main/sample%20apps/mobx)   App store / Play store You can download & check out the 100ms Flutter app  🤖 Flutter Android app from  Google Play Store (https://play.google.com/store/apps/details?id=live.hms.flutter). 📱 Flutter iOS app from  Apple App Store (https://apps.apple.com/app/100ms-live/id1576541989). "
    },
    {
        "title": "Auth Token Endpoint Guide",
        "link": "/flutter/v2/guides/token-endpoint",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/guides/token-endpoint",
        "keywords": [],
        "content": "    Overview 100ms provides an option to get App Tokens without setting up a token generation backend service to simplify your integration journey while testing the  sample app (https://github.com/100mslive/100ms-web) or building integration with 100ms. You can find the token endpoint from the  developer page (https://dashboard.100ms.live/developer) in your 100ms dashboard.   Token endpoint (/guides/token-endpoint-dashboard.png) We recommend you move to your token generation service before you transition your app to production, as our token endpoint service will not scale in production. The \"Sample Apps\" built using 100ms client SDKs require an App Token to join a room to initiate a video conferencing or live streaming session. Please check the  Authentication and Tokens guide (./../foundation/security-and-tokens) Please note that you cannot use the token endpoint to create a Management Token for server APIs. Refer to the  Management Token section (./../foundation/security-and-tokens management-token) in Authentication and Tokens guide for more information.   Get an app token using token endpoint You can use the token endpoint from your 100ms dashboard while building integration with 100ms. This acts as a tool enabling front-end developers to complete the integration without depending on the backend developers to set up a token generation backend service.   URL format:   <YOUR_TOKEN_ENDPOINT>api/token  100ms token endpoint can generate an app token with the inputs passed, such as room_id, role, & user_id (optional  your internal user identifier as the peer's user_id). You can use  jwt.io (https://jwt.io/) to validate whether the app token contains the same input values. <PostRequest title=\"https://prod-in2.100ms.live/hmsapi/johndoe.app.100ms.live/api/token\"   <Request id=\"req-comp-0\">   bash curl location request POST 'https://prod-in2.100ms.live/hmsapi/johndoe.app.100ms.live/api/token'  header 'Content-Type: application/json'  data-raw '    \"room_id\":\"633fcdd84208780bf665346a\",   \"role\":\"host\",   \"user_id\":\"1234\"  '   </Request> <ResponseBox id=\"resp-0\" status=\"200 OK\">   json     \"token\": \"eyJ0eXAiOiJKV1QiLCJhbGciOi                           R3tT-Yk\",   \"msg\": \"token generated successfully\",   \"status\": 200,   \"success\": true,   \"api_version\": \"2.0.192\"     </ResponseBox>   Example client-side implementation You can directly add this to your client-side implementation, check our  sample app (https://github.com/100mslive/100ms-flutter/blob/0d4c3b5409003932d80cb19f67027a63424169e7/example/lib/service/room_service.dart L8) for reference.   Disable 100ms token endpoint Due to some security concerns, if you don't wish to use the token endpoint to generate app tokens, then you can disable it on the  Developers page (https://dashboard.100ms.live/developer) on your dashboard by disabling the option \"Disable &lt;room_id&gt;/&lt;role&gt; link format.\"   Disable Token endpoint (/guides/disable-token-endpoint.png)    Error Response Once you're disabled it on the dashboard, the requests to create an app token using the 100ms token endpoint will throw the below error:   json     \"success\": false,   \"msg\": \"Generating token using the room_id and role is disabled.\",   \"api_version\": \"2.0.192\"     "
    },
    {
        "title": "Auth Token Quickstart Guide",
        "link": "/flutter/v2/guides/token",
        "platformName": "Flutter",
        "objectID": "/flutter/v2/guides/token",
        "keywords": [],
        "content": "    Create a 100ms account  Create an account at  Dashboard of 100ms (https://dashboard.100ms.live/)   Create Account (/docs/guides/token/create-account.png)  After you have created your account you have to Confirm your Email , check the promotions tab in your Mail Box if you can't find it.  Login to the Dashboard and you will see an option to Create your first app. Click on it.   Create your first app (/docs/guides/token/starter-kit-initialize-first-step.png)  Then you would see this popup with multiple starter kits, hover over one of the starter kits and click Deploy . We will choose \"Video Conferencing\" for now.   Initialize Started Kit (/docs/guides/token/starter-kit-initialize.png)  In the Choose your deployment step, select 100ms and enter the subdomain you wish in the Subdomain field.   Domain details (/docs/guides/token/domain-details.png)  After you're App is set click on \"Go to Dashboard\" or  Go Here (https://dashboard.100ms.live/dashboard)   Dasboard (/docs/guides/token/go-to-dashboard.png)   Create a room  Go over to  Room in Dashboard (https://dashboard.100ms.live/rooms) and click on \"Create Room\" , While creating a room you can specify it's name, roles or enable recording.   Create Room (/docs/guides/token/create-room.png)  You will now see \"Room Details\" section and we have a room_id created, copy it somewhere.   Room Id (/docs/guides/token/room-id.png)   Get a temporary token from 100ms dashboard Any client connecting calling 100ms' service needs to authenticate using an auth token. In production you would have your own servers generating the tokens (see more  here (/docs/javascript/v2/foundation/security-and-tokens)), but for a quick start you can use the dashboard to create a token for you. The token will expire in 24 hours and should not be hard-coded into a production app.  To get a temporary token click on \"Join room\" button.   Join Room (/docs/guides/token/join-room.png)  In the popup that shows up click on icon with a key shape next to the role you want to join as.   Copy Token (/docs/guides/token/copy-token.png) The token will be copied to your clipboard. Use this along with the room_id to proceed with the quickstart guide. "
    },
    {
        "title": "HLS Timed Metadata",
        "link": "/ios/v2/advanced-features/hls-timed-metadata",
        "platformName": "iOS",
        "objectID": "/ios/v2/advanced-features/hls-timed-metadata",
        "keywords": [],
        "content": "  HLS Timed Metadata feature helps you synchronise certain events with the HLS stream. This can be useful for showing interactive quizzes / product overlays etc.   Requirements  100ms iOS SDK version 0.4.1 or higher  Active HLS stream   Sending HLS Timed Metadata To add HLS timed metadata cue to the currently running HLS stream use sendHLSTimedMetadata API like this:   swift let metadata = HMSHLSTimedMetadata(payload: \"quiz id: 3\") hmsSDK.sendHLSTimedMetadata( metadata , completion:    weak self  _, error in   if let error = error as? HMSError       print(\"Unable to send metadata: (error)\")     else       print(\"Metadata sent\")      )     Receiving HLS Timed Metadata To receive metadata on the HLS player side use AVPlayerItemMetadataCollector . Detailed documentation is available  here (https://developer.apple.com/documentation/avfoundation/avplayeritemmetadatacollector) but to briefly summarize: 1. Create a metadata collector and pass a delegate:   swift metadataCollector = AVPlayerItemMetadataCollector() metadataCollector.setDelegate(self, queue: DispatchQueue.main)   2. Add metadata collector to your AVPlayerItem item   swift playerItem.add(metadataCollector)   3. Conform to the AVPlayerItemMetadataCollectorPushDelegate protocol to save collected metadata   swift func metadataCollector(_ metadataCollector: AVPlayerItemMetadataCollector,              didCollect metadataGroups:  AVDateRangeMetadataGroup ,              indexesOfNewGroups: IndexSet,              indexesOfModifiedGroups: IndexSet)    self.metadataGroups = metadataGroups     4. Add playback observer to show the metadata when playback time reaches the timestamp defined by the metadata   swift player.addPeriodicTimeObserver(forInterval: CMTime(seconds: 1.0, preferredTimescale: CMTimeScale(NSEC_PER_SEC)), queue: .main, using:    weak self  time in   self?.updateMetadataView(for: time)  )     swift func updateMetadataView(for currentTime: CMTime)     hideCurrentMetadataViewIfNeeded()     guard currentMetadataGroup == nil, let playerItem = playerItem else   return       for group in metadataGroups       if group.shouldShow(for: playerItem)         showMetadataView(for: group)       break             func hideCurrentMetadataViewIfNeeded()     guard let currentMetadataGroup = currentMetadataGroup,      let playerItem = playerItem,       currentMetadataGroup.shouldShow(for: playerItem) else   return     self.currentMetadataGroup = nil   metadataView.isHidden = true   func showMetadataView(for group: AVDateRangeMetadataGroup)     guard currentMetadataGroup  = group else   return       currentMetadataGroup = group   metadataView.isHidden = false   metadataView.text = group.items.first?.stringValue       swift extension AVDateRangeMetadataGroup     func shouldShow(for item: AVPlayerItem) -> Bool       guard let endDate = endDate, let currentDate = item.currentDate() else   return false       return startDate <= currentDate && currentDate < endDate         👀 To see an example of HLS Timed Metadata implementation using 100ms SDK, checkout  our example project (https://github.com/100mslive/100ms-ios-sdk/blob/main/Example/HMSSDKExample/Meeting/HLS/HLSStreamViewController.swift). "
    },
    {
        "title": "Persistent Participant States (Peer Metadata)",
        "link": "/ios/v2/advanced-features/peer-metadata-update",
        "platformName": "iOS",
        "objectID": "/ios/v2/advanced-features/peer-metadata-update",
        "keywords": [],
        "content": "  Looking for persistent state that can be set on a peer and updated anytime, for everyone in the room? Peer metadata it is  Metadata can be set initially in the HMSConfig object that's passed into the join method. This section will show you how to: 1.  Read Peer Metadata ( reading-metadata). 2.  Respond to when a remote peer changes its metadata ( responding-to-updates). 3.  How to set a peer's metadata ( updating-metadata). The HMSPeer object prior to 0.2.1 contained customerDescription a read-only string whose value wouldn't change throughout a call. This has been replaced with metadata . The value is initially assigned as before but now can be changed by the peer who owns it. Any peer can change the metadata for themselves. Currently one peer cannot change another peer's metadata. To change their own metadata value the peer should call change(metadata:completion:) on their HMSSSDK instance. The data may be any arbitrary string.   Reading metadata To read metadata, read the metadata value on any HMSPeer instance.   Responding to updates Whenever a remote peer's metadata is updated a callback will be received in on(peer: HMSPeer, update: HMSPeerUpdate) of HMSUpdateListener where the update value will be of type HMSPeerUpdate.metadataUpdated . When this callback is received the UI for that peer should be updated as well.   Updating Metadata Here is how a peer can set their own metadata to a random string. In this case the string is stringified json.   swift let newMetadata = \"  \"ms \": 100  \" hmsSdk.change(metadata: newMetadata)   success, error in  ...     "
    },
    {
        "title": "Adaptive Bitrate (Simulcast)",
        "link": "/ios/v2/advanced-features/simulcast",
        "platformName": "iOS",
        "objectID": "/ios/v2/advanced-features/simulcast",
        "keywords": [],
        "content": "  Simulcast enables  Adaptive Bitrate (../foundation/adaptive-bitrate) (ABR) in video conferencing scenarios. This means 100ms SDKs can upgrade or downgrade video quality for remote tracks based on user preferences or network conditions.   Minimum Requirements  SDK version 0.5.1 or higher  Simulcast enabled in room template   100ms SDK Simulcast APIs You interact with the simulcast feature by setting HMSRemoteVideoTrack 's layer property. Let's look at the track interface:   swift class HMSRemoteVideoTrack     open var layerDefinitions:  HMSSimulcastLayerDefinition ?   open var layer: HMSSimulcastLayer   class HMSSimulcastLayerDefinition : NSObject     open var layer: HMSSimulcastLayer   open var resolution: CGSize   enum HMSSimulcastLayer     case high   case mid   case low     Here layerDefinitions property holds an array of currently available layers for the track and layer property is the currently selected layer. Within HMSSimulcastLayerDefinition you get its resolution and the layer name such as high/mid/low.   Auto Layer Selection  HMSVideoView has an automatic simulcast layer selection capability which is enabled by default. It will select a layer that best matches the current view frame size and reacts to frame updates. In case manual layer selection is preferred set disableAutoSimulcastLayerSelect property to true . By default, the track layer is set to high . "
    },
    {
        "title": "Release Notes",
        "link": "/ios/v2/changelog/release-notes",
        "platformName": "iOS",
        "objectID": "/ios/v2/changelog/release-notes",
        "keywords": [],
        "content": "description: Release Notes for 100ms iOS SDK    0.6.1  2023-01-19   Added  Error callback onPlaybackFailure to monitor HLS error events   Fixed  requested_by field not populated in role change webhook  RTC stats are not reported for screen share track  RTC stats are not reported if user joins with HLS role and then switches to WebRTC  In the first onRoom update during preview local peer's role is nil  In App Screen share sends bogus error first time with permission popup  Sometimes, in app screen share is stuck waiting for a visual change in the UI   Breaking change  RTC stats delegate callback signatures have been changed to return HMSVideoTrack and HMSAudioTrack types in order to support stats for screen share track.   0.5.5  2023-01-06   Added  Simulcast support for local RTC stats  Capture frame API for HMSVideoView   Changed  In-app screenshare will auto resume on app coming to foreground   Fixed  Switch output causes bluetooth devices to not show up in device list  Frames repeating in in-app screen share   0.5.4  2022-12-23   Added  Varying frame rate support for simulcast layers  API to share in app screen content  Log a helpful message to the console when provided app group id is wrong  Sending track removed update for each of peer's tracks before sending peerLeft update   Fixed  Redundant \"track restored\" event is sent whenever simulcast layer selection is changed  Changing role during audio file playback is causing a crash   0.5.3  2022-12-14   Fixed  iPhone 14 + bluetooth headset microphone issue when custom audio source is used  Video track settings not respecting back camera selection  HLS Stream playback not using bluetooth headset speaker   0.5.2  2022-12-07   Fixed  Microphone not captured on non publishing to publishing role change first time after app force quit.   0.5.1  2022-11-25   Added  Simulcast support  HMSHLSPlaybackMonitor class for easy retrieval of HLS playback stats from AVPlayer  Audio routing API to change audio output between earpiece and speaker  Local camera frame capture plugin   0.4.7  2022-11-04   Added  Tap to focus local camera API   Fixed  iOS 16 screen share failing if the app went into background without PIP mode  Sending chat message from REST API resulted in peer joined update getting fired   Changed  Bitcode support has been deprecated   0.4.6  2022-10-21   Added  HMSVideoTrackSettings and HMSAudioTrackSettings now have initialMuteState property that controls wether user will have tracks muted or unmuted on join  HMSAudioTrackSettingsBuilder and HMSVideoTrackSettingsBuilder are provided to simplify settings creation. Example:  swift sdk.trackSettings = HMSTrackSettings.build   videoSettingsBuilder, audioSettingsBuilder in  videoSettingsBuilder.initialMuteState = .mute  audioSettingsBuilder.initialMuteState = .mute       0.4.5  2022-10-06   Fixed  Screenshare compatibility for iOS 16   0.4.3  2022-09-28   Added  Session metadata API   Fixed  Potential fix for a race condition where WebSocket message arriving post cleanup leading to a crash   0.4.2  2022-09-21   Fixed  Audio coming from the earpiece instead of a speaker when joining with a non publishing role.  Fix missing headers compiler warning.   0.4.1  2022-09-09   Added  HLS Extended metadata support  Zoom and pan gesture support for HMSVideoView can be enabled via isZoomAndPanEnabled property.   Fixed  Headset microphone audio getting distorted when custom audio source is provided.   Breaking changes  Error reporting has been updating to match error codes on Android as well as to use iOS conventions for the error interface. See migration guide  here (https://www.100ms.live/docs/ios/v2/migration-guides/03to04) for details.   0.3.3  2022-07-27   Added  Support for local file playback and audio sharing  Support for Picture in Picture mode    Changed  Simplified HLS start API   0.3.2  2022-07-01   Added  Add resolution variable in HMSRTMPConfig  Support message sending from REST API  Join performance improvements for large rooms   Fixed  Reconnecting/reconnected callbacks triggered when new user joins  Fix HLS playback audio not routed to external speaker post role change   0.3.1  2022-05-31   Added  Screenshare support  Virtual background  Custom video plugins API  Analytics capturing improvements   Breaking change  Removed tracksErrorAudioShouldBeSilenced error that was used to signal that other app is playing unmixable audio, it is now up to the app to detect for example using CallKit   0.2.13  2022-05-16   Fixed  Local track state updates not being sent due to a bug in 0.2.12   0.2.12  2022-05-13   Fixed  Track will not recover if it was unsubscribed from post being degraded   0.2.11  2022-04-28   Added  Subscribe degradation improvements   Fixed  onReconnected callback not fired in some scenarios   0.2.10  2022-04-21   Fixed  Error 5000 returned when trying to rejoin from onError callback   Changed  Message in sendBroadcastMessage, sendDirectMessage and sendGroupMessage completion handlers will now have the timestamp set by the server instead of using local   0.2.9  2022-03-29   Added  Network health callbacks in preview and mid call  Fix camera light going on for muted track post role change   0.2.8  2022-03-08   Added  HMSHLSConfig constructor now takes HMSHLSRecordingConfig param to optionally enabled recording of HLS stream  hlsRecordingState property added to HMSRoom to show current recording state   0.2.7  2022-02-21   Added  Recording/Streaming info will now have \"started at\" date  Room state and peer list are now available in preview mode (if enabled from dashboard) via on(room:update:) and on(peer:update:) callbacks in HMSPreviewListener   Fixed  SDK interfering with apps querying device battery level   0.2.6  2022-01-28   Fixed  Fixed connection issues for EU based rooms.   0.2.5  2022-01-20   Added  Call stats API   0.2.4  2021-12-21   Added  HLS Streaming API    Changed  Improved track switching delay in HMSVideoView    Fixed  Track remove callback not fired in case of multiple auxiliary tracks were getting removed   0.2.3  2021-12-06   Fixed  Microphone permission requested for view only role  Subscribe degradation not working  onError callback fired on non main thread for no mic/video permission error   0.2.2  2021-11-26   Added  changeMetadata API  changeName API  setVolume API for HMSRemoteAudioTrack   0.2.1  2021-11-18   Breaking Change  requestedBy field has been made optional for HMSChangeTrackStateRequest, HMSRemovedFromRoomNotification, HMSRoleChangeRequest. It will be nil if request can originated from the REST API.   customerUserDescription field has been renamed to metadata in HMSPeer   0.1.6  2021-10-29   Changed  HMSLocalVideoTrack settings property is now properly reflecting cameraFacing state  Calling leave in preview mode no longer returns an error  Calling setMute on local video track stops camera access  Calling setMute on local tracks will now result in on(track: HMSTrack, update: HMSTrackUpdate, for peer: HMSPeer) callback fired for those tracks   0.1.5  2021-10-21  Removed Xcode 13 requirement   0.1.4  2021-10-13   Fixed  Fixed leave call not taking effect   0.1.3  2021-10-11   Added  hmsPeerStateDidChange notification added for convenience   Changed  Deprecated init(userName:userID:roomID:) of HMSConfig as same parameters are coming in the auth token   0.1.2  2021-09-30   Added  preview(role:completion:) API added to support showing preview screen mid call   0.1.1  2021-09-23   Added  changeRolesOfAllPeers API added to change roles of multiple peers at once  changeTrackState API variant added to mute/unmute multiple peers at once  startRTMPOrRecording and stopRTMPAndRecording APIs added to start/stop RTMP / browser recording   Changed  leave API now provides a completion handler that is called when leave operation has completed  HMSTrack source property type has been changed to String to allow custom track sources   Fixed  onReconnecting callback was not firing upon network disconnect   0.0.12  2021-09-02   Added  Swift package manager support  Bitcode support   Changed  Moved to xcframework   0.0.11  2021-08-25   Added  HMSVideoView now has a mirror property that controls mirroring of the video.   Changed  changeRole API now can be used to change role of a local peer.   0.0.10  2021-08-17   Added  Sending private and group messages APIs  Remove peer from a room API  Remote mute peer API  End room API  videoContentMode property added to HMSVideoView to allow selecting if content should fit or fill the view  videoTrack getter added HMSVideoView for convenience   Changed  send(message:) API renamed to sendBroadcastMessage   0.0.9  2021-08-05  Fixed individual track publish settings not respected during change role  Fixed wrong track source coming for trackRemoved update  Fixed button states not updating properly during role change in sample app   0.0.8  2021-07-26  HMSVideoView will auto pick the best fitting simulcast layer  Sample app will not show role change options if user does not have necessary permissions   0.0.7  2021-07-16  Added simulcast support  Added support for forced change role  HMSSpeaker now has a reference to HMSPeer instead of id   0.0.6  2021-07-09  Added capability to change role mid-call  Passing HMSRole object to peers  SDK checks for appropriate permissions before trying to publish audio & video  Corrected parsing of roles that do not subscribe to other roles   0.0.5  2021-07-02  Automatic video track switch off in bad network conditions.  Preview API added to the SDK, added preview screen to the sample app.   0.0.4  2021-06-25   Added Media Interruption Handling   Optimized network usage   Updated background to foreground transitions   Improved Error logging   0.0.3  2021-06-22   Removed private modules to fix installing failure   0.0.2  2021-06-16   Added Reconnection for reliability in bad network conditions   Handled network provider (WiFi to Mobile Data ) transitions mid-call   Upgraded Data Source which apps can use to easily create customized views   Added descriptive Error Codes   Added Analytics to track SDK performance   Added default handing to app background/foreground transitions   Handled joining meetings in different roles   Multiple Sample App enhancements   0.0.1  2021-06-04   Simplified SDK interfaces that can power a fully-featured video conferencing application     join     leave     listeners for tracks/peers/other data   Features not covered in this release     Handling network disconnections (disconnect from wi-fi to 4g ends the call )     Handling subscription network degradations     Handling second order publish network degradations     Detailed analytics     Recording "
    },
    {
        "title": "Bitcode Deprecation",
        "link": "/ios/v2/debugging/bitcode",
        "platformName": "iOS",
        "objectID": "/ios/v2/debugging/bitcode",
        "keywords": [],
        "content": "  100ms iOS SDK has removed support for bitcode starting following versions:  100ms SDK version 0.4.7 onwards  Broadcast Extension SDK version 0.0.6 onwards   Context Xcode 14 has deprecated bitcode. If you make a project with Xcode 14, the bitcode is disabled in all targets by default. And if you enable bitcode explicitly, Xcode generates a warning message: “Building with bitcode is deprecated. Please update your project and/or target settings to disable bitcode.\" Also, you can no longer submit apps with bitcode to App Store from Xcode 14.   Issue This means if you are using Xcode 13 or you have explicitly enabled bitcode on your target using the 100ms SDK, you may encounter an error saying: \"framework does not contain bitcode and you must rebuild it.\"   Resolution To resolve this issue, you should make sure that your targets have bitcode disabled. To disable bitcode on a target, you go to the target's build settings and search for \"bitcode\". Set 'Enable Bitcode' setting to 'NO'. "
    },
    {
        "title": "Audio Output Routing - Speaker and Earpiece (Beta)",
        "link": "/ios/v2/features/audio-output-routing",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/audio-output-routing",
        "keywords": [],
        "content": "  If you want to give your users option to switch audio between speaker and earpiece, you use Audio Output Routing API. For example, you can implement an in-call speaker button that toggles between speaker and earpiece audio.   Minimum Requirements   SDK version 0.5.1   How to switch audio to a particular output device You call   switchAudioOutput   method on HMSSDK instance passing in the HMSAudioOutputDevice that you would like to switch audio to. Like below:   swift // Switch audio to earpiece hmsSDK.switchAudioOutput(to: .earpiece) // Switch audio to speaker hmsSDK.switchAudioOutput(to: .speaker)     Currently, there are only 2 HMSAudioOutputDevices available: 1.   speaker    in-built speaker device 2.   earpiece    in-built earpiece device   Can I switch audio to audio output devices other than speaker and earpiece? No. Currently _in-built speaker_ and _in-built earpiece_ are the only two supported devices. Although there is getAudioOutputDeviceList method on HMSSDK that you call to get all available audio output devices, but currently it will return just these 2 in-built audio output devices.   swift let availableAudioOutputDevices = hmsSDK.getAudioOutputDeviceList()     Note: In future getAudioOutputDeviceList may return other available audio devices. 👀 To see an example iOS audio routing implementation using 100ms SDK, checkout  our example project (https://github.com/100mslive/100ms-ios-sdk/tree/main/Example). "
    },
    {
        "title": "Audio Share (Beta)",
        "link": "/ios/v2/features/audio-share",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/audio-share",
        "keywords": [],
        "content": "  iOS SDK provides support for sharing audio from audio files on your device or sharing audio that's playing on iOS device (for example, from another app like spotify) while sharing the screen of your device in the room.   Minimum Requirements   Minimum iOS version required to support Audio Share is iOS 13   Minimum 100ms SDK version required is 0.3.3   How audio sharing works in iOS SDK The audio that you share goes to other peers through the mic channel. To be able to share audio you need to setup the sdk to use a custom audio source instead of default mic. To do that you pass an instance of custom audio source to HMSAudioTrackSettings on your hmssdk instance.   How to use hmssdk to share audio from a file 1. You create an instance of HMSAudioFilePlayerNode and an instance of HMSMicNode like below:   swift   let audioFilePlayerNode = HMSAudioFilePlayerNode()   let micNode = HMSMicNode()    2. Next, you create an instance of HMSAudioMixerSource, passing an array of nodes that we created in the step above like below:   swift   let audioMixerSource = try HMSAudioMixerSource(nodes:  audioFilePlayerNode, micNode )     3. Next, you pass this custom audio source to the 'audioSource' parameter of HMSAudioTrackSettings that you set on hmssdk instance like so:   swift   let audioSettings = HMSAudioTrackSettings(..., audioSource: audioMixerSource)   hmsSDK.trackSettings = HMSTrackSettings(..., audioSettings: audioSettings)    That's all you need to setup the sdk to use your custom audio source. 4. You call play function on audioFilePlayerNode to play a file on local device with it's file url like below:   swift   try audioFilePlayerNode.play(fileUrl: ...)      How to know when file playback is finished You pass a completion handler to the play function. CompletionHandler gets called when file is finished playing.   swift   try audioFilePlayerNode.play(fileUrl: ...)       print(\"File finished playing\")           How to change mix volume of different nodes You can use volume property on nodes to control the volume.   swift   audioFilePlayerNode.volume = 0.5   micNode.volume = 0.9      How to schedule multiple audio files for back-to-back playback You can set 'interrupts' parameter to false to tell audioFilePlayerNode to not interrupt the current file playback, but schedule the file after the current file is finished. Like below:   swift   try audioFilePlayerNode.play(fileUrl: url to file 1)   try audioFilePlayerNode.play(fileUrl: url to file 2, interrupts: false)   try audioFilePlayerNode.play(fileUrl: url to file 3, interrupts: false)   ...      How to play multiple files concurrently You can pass multiple instances of audioFilePlayerNode and pass them as nodes when creating audioMixerSource like so:   swift   let backgroundMusicNode = HMSAudioFilePlayerNode()   backgroundMusicNode.volume = 0.2   let audioFilePlaybackNode = HMSAudioFilePlayerNode()   audioFilePlaybackNode.volume = 0.5   let micNode = HMSMicNode()     let audioMixerSource = try HMSAudioMixerSource(nodes:  backgroundMusicNode, audioFilePlaybackNode, micNode )    Now, you can play a looping background music at low volume and an audio file at the same time:   swift   try backgroundMusicNode.play(fileUrl: ..., loops: true)   try audioFilePlayerNode.play(fileUrl: ...)       How to pause, resume, stop playback and more You can use following interfaces on HMSAudioFilePlayerNode to pause, resume or stop playback and more:   swift   audioFilePlayerNode.pause()   audioFilePlayerNode.resume()   audioFilePlayerNode.stop()     let isPlaying = audioFilePlayerNode.isPlaying()   let currentPlaybackTime = audioFilePlayerNode.currentTime   let totalPlaybackDuration = audioFilePlayerNode.duration         How to share audio that's playing on your iPhone Note: iOS allows to get access to audio playing on iOS device (for example, from another app like spotify) only while broadcating your entire iPhone screen. So for this to work you should implement screen sharing in your app. You can follow along here to set it up  Screen Share (./../features/screen-share) Now once you have implemented the screen share feature from above. You can follow below steps to enable system audio broadcasting while sharing your screen: 1. You get an instance of HMSScreenBroadcastAudioNode and add it to your mixer.   swift   let screenAudioNode = try sdk.screenBroadcastAudioReceiverNode()   let audioMixerSource = try HMSAudioMixerSource(nodes:  audioFilePlaybackNode, micNode, screenAudioNode )     Note: you can pass only a single instance of HMSMicNode and HMSScreenBroadcastAudioNode to HMSAudioMixerSource, else you will receive an error. Now your mixer source is set to receive audio from your broadcast extension. 2. Next, you need to setup broadcast extension to send audio to the main app. Broadcast extension receives audio that's playing on your iOS device in processSampleBuffer function in your RPBroadcastSampleHandler class. To send audio from broadcast extension to main app, you call process(audioSampleBuffer) function on HMSScreenRenderer:   swift   let screenRenderer = HMSScreenRenderer(appGroup: \"group.live.100ms.videoapp\")     override func processSampleBuffer(_ sampleBuffer: CMSampleBuffer, with sampleBufferType: RPSampleBufferType)       ...         case RPSampleBufferType.audioApp:       self.screenRenderer.process(audioSampleBuffer: sampleBuffer)       break     ...         Now your broadcast extension is set to be send audio to the main app. And that's it. Now your custom mixer source in the main app can receive the audio from broadcast extension as well.   Advanced use cases   Play AVAudioPCMBuffer You add HMSAudioBufferPlayerNode to the mixer. And call play with passing AVAudioPCMBuffer.   swift   let streamPlayer = HMSAudioBufferPlayerNode()   let audioMixerSource = try HMSAudioMixerSource(nodes:  audioFilePlaybackNode, micNode, streamPlayer )   ...     try streamPlayer.play(buffer: ...)      You own custom audio implementation If you want to create your own custom audio implementation and just need hmssdk to forward your audio buffers to other peers in the room, you can use HMSAudioBufferSource (supported iOS 12+) as custom audio source. Use enqueue(buffer: AVAudioBuffer) function on HMSAudioBufferSource to send audio to remote peers.   swift   let audioBufferSource = HMSAudioBufferSource()     let audioSettings = HMSAudioTrackSettings(..., audioSource: audioBufferSource)   hmsSDK.trackSettings = HMSTrackSettings(..., audioSettings: audioSettings)   ...     audioBufferSource.enqueue(buffer: ...)    👀 To see an example audio shareing implementation using 100ms SDK, checkout  our example project (https://github.com/100mslive/100ms-ios-sdk/tree/main/Example). 📲 Download the 100ms fully-featured Sample iOS app here: https://testflight.apple.com/join/dhUSE7N8 "
    },
    {
        "title": "Background Modes",
        "link": "/ios/v2/features/background-modes",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/background-modes",
        "keywords": [],
        "content": "  When your app goes into the background, by default it can no longer access camera or microphone and publish it to other peers in the room. Also, you cannot hear audio of other participants in the call if your app is in the background without enabling Background Modes. This is the default iOS platform behaviour whereby it limits access to Microphone & Camera for capturing Audio & Video. By default, iOS also stops playing audio of the Room when your app is in Background. By Enabling Background Modes you can ensure that your iOS app has access to Microphone & can play incoming Audio from the Room. Following steps show how to add Background Modes in iOS: <div className=\"steps-container\">   Add Capability Click on your project in Navigator then select your app target under TARGETS and click on Signing & Capabilities and add Capability .   addCapability (/docs/v2/flutter-background-service-2.png)   Background Mode Search background mode and add it to project by clicking on it.   backgroundMode (/docs/v2/flutter-background-service-3.png)   Enable Background Mode Enable the checkbox under Background Modes named Audio, AirPlay, and Picture in Picture .   backgroundProcessing (/docs/v2/flutter-background-service-4.png) </div> Let's look at different scenarios on iOS with & without Enabling Background Modes.    Without Background Modes Enabled (Default iOS Behaviour) The table lists down iOS app behaviours when the app transitions to background when a 100ms Room is ongoing without enabling background processing. The \"  Scenario  \" on the left of the Table below implies the activity which is ongoing in the App when user has joined a 100ms Room. The \"  Behaviours  \" on the right of the Table shows what happens when the App transitions to Background.    Scenario       Behaviours     :     :                Mic is Unmuted  Mic will get   Muted     Camera is unmuted  Camera will get   Muted     Remote Peers are publishing audio  Incoming Audio from Room   Stops   Playing    Background Modes Enabled (Recommended) The table lists down iOS app behaviours when the app transitions to background when a 100ms Room is ongoing with Background Modes   Enabled  . The \"  Scenario  \" on the left of the Table below implies the activity which is ongoing in the App when user has joined a 100ms Room. The \"  Behaviours  \" on the right of the Table shows what happens when the App transitions to Background.    Scenario       Behaviours                     :     :                Mic is unmuted Mic will remain   Unmuted   and the user will able to publish audio without any restriction   Camera is unmuted  Camera will get   Muted     Remote Peers are publishing audio  Incoming Audio from Room   Continues   to Play  👀 To see an example iOS app that uses background mode and 100ms SDK, checkout  our example project (https://github.com/100mslive/100ms-ios-sdk/tree/main/Example). "
    },
    {
        "title": "Call Stats\r",
        "link": "/ios/v2/features/call-stats",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/call-stats",
        "keywords": [],
        "content": " -\r \r Sometimes you need a way to capture certain metrics related to a call. This may be helpful if you want to tailor the experience to your users or debug issues. Typical metrics of interest are audio/video bitrate, round trip time, total consumed bandwidth and packet loss. 100ms SDK provides this data via dedicated delegate callbacks found in HMSUpdateListener . These will be called with a fixed interval of one second after a room has been joined. You can get stats on a per track basis ( on(remoteAudioStats:track:peer) ) or as an overall summary ( on(rtcStats:) ) \r \r Here is the full list of callbacks:\r \r  swift\r /// This callback provides stats for a local audio track.\r optional func on(localAudioStats: HMSLocalAudioStats, track: HMSLocalAudioTrack, peer: HMSPeer)\r \r /// This callback provides stats for a local video track. In case of simulcast enabled room \r /// the localVideoStats param will contain an entry for each simulcast layer.\r optional func on(localVideoStats:  HMSLocalVideoStats , track: HMSLocalVideoTrack, peer: HMSPeer)\r \r /// This callback provides stats for a remote audio track.\r optional func on(remoteAudioStats: HMSRemoteAudioStats, track: HMSRemoteAudioTrack, peer: HMSPeer)\r \r /// This callback provides stats for a remote video track.\r optional func on(remoteVideoStats: HMSRemoteVideoStats, track: HMSRemoteVideoTrack, peer: HMSPeer)\r \r /// This callback provides combined stats for the session.\r optional func on(rtcStats: HMSRTCStatsReport)\r  \r \r  HMSRTCStatsReport\r \r This class will contain the combined stats for the room.\r \r  swift\r class HMSRTCStats  \r \t// Total bytes sent in the current session.\r \tvar bytesSent: UInt64\r \t// Total bytes received in the current session.\r \tvar bytesReceived: UInt64\r \t// Total packets received in the current session.\r \tvar packetsReceived: UInt64\r \t// Total packets lost in the current session.\r \tvar packetsLost: UInt64\r \t// Total outgoing bitrate observed since previous report.\r \tvar bitrateSent: Double\r \t// Total incoming bitrate observed since previous report in Kb/s.\r \tvar bitrateReceived: Double\r \t// Average round trip time observed since previous report in seconds.\r \tvar roundTripTime: Double\r  \r \r class HMSRTCStatsReport  \r \t// Combined audio + video values\r \tvar combined: HMSRTCStats\r \t// Summary of all audio tracks\r \tvar audio: HMSRTCStats\r \t// Summary of all video tracks\r \tvar video: HMSRTCStats\r  \r  \r \r  HMSLocalAudioStats\r \r This class contains stats related to local audio track.\r \r  swift\r class HMSLocalAudioStats  \r \t// Round trip time observed since previous report.\r \tvar roundTripTime: Double\r \t// Total bytes sent by this track in the current session.\r \tvar bytesSent: UInt64\r \t// Outgoing bitrate of this track observed since previous report in Kb/s.\r \tvar bitrate: Double\r  \r  \r \r  HMSLocalVideoStats\r \r This class contains stats related to local video track.\r \r  swift\r class HMSLocalVideoStats  \r \t// Round trip time observed since previous report.\r \tvar roundTripTime: Double\r \t// Total bytes sent by this track in the current session.\r \tvar bytesSent: UInt64\r \t// Outgoing bitrate of this track observed since previous report in Kb/s.\r \tvar bitrate: Double\r \t// Resolution of video frames being sent.\r \tvar resolution: HMSVideoResolution\r \t// Frame rate of video frames being sent (FPS).\r \tvar frameRate: Double\r \t// Simulcast layer id in case of simulcast enabled room. See HMSSimulcastLayer enum for possible values\r \tvar simulcastLayerId: NSNumber?\r  \r  \r \r  HMSRemoteAudioStats\r \r This class contains stats related to remote audio track.\r \r  swift\r class HMSRemoteAudioStats  \r \t// Packet Jitter measured in seconds for this track. Calculated as defined in section 6.4.1. of RFC3550.\r \tvar jitter: Double\r \t// Total bytes received by this track in the current session.\r \tvar bytesReceived: UInt64\r \t// Incoming bitrate of this track observed since previous report in Kb/s.\r \tvar bitrate: Double\r \t// Total packets received by this track in the current session.\r \tvar packetsReceived: UInt64\r \t// Total packets lost by this track in the current session.\r \tvar packetsLost: UInt64\r  \r  \r \r  HMSRemoteVideoStats\r \r This class contains stats related to remote video track.\r \r  swift\r class HMSRemoteVideoStats  \r \t// Packet Jitter measured in seconds for this track. Calculated as defined in section 6.4.1. of RFC3550.\r \tvar jitter: Double\r \t// Total bytes received by this track in the current session.\r \tvar bytesReceived: UInt64\r \t// Incoming bitrate of this track observed since previous report in Kb/s.\r \tvar bitrate: Double\r \t// Total packets received by this track in the current session.\r \tvar packetsReceived: UInt64\r \t// Total packets lost by this track in the current session.\r \tvar packetsLost: UInt64\r \t// Resolution of video frames being received.\r \tvar resolution: HMSVideoResolution\r \t// Frame rate of video frames being received (FPS).\r \tvar frameRate: Double\r  \r  "
    },
    {
        "title": "Callkit Integration (Beta)",
        "link": "/ios/v2/features/callkit",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/callkit",
        "keywords": [],
        "content": "  iOS SDK works well with Apple  Callkit framework (https://developer.apple.com/documentation/callkit). With Apple's Callkit framework, your VoIP apps can be integrated into iOS native calling experience.   Minimum Requirements   No minimum requirements   What is Apple Callkit framework? Callkit is generally used when you want to support making and receiving VoIP calls in your audio/video app using iOS system-calling UI. When one person calls another person, your backend can push a silent VoIP notification to the 'callee' on their iOS device using APNS (Apple Push Notification Service). You iOS VoIP app registers to listen for this notification using Apple's  Pushkit framework (https://developer.apple.com/documentation/pushkit). Upon receiving this notification, iOS calls your registered Pushkit delegate to handle the notification. You handle the VoIP notification by reporting an incoming call to the Callkit. Callkit shows the native incoming caller id screen for your app. Once user accepts the call, your Callkit delegate is called. At that point you join the meeting with the information in the notification payload.   swift   func pushRegistry(_ registry: PKPushRegistry, didReceiveIncomingPushWith payload: PKPushPayload, for type: PKPushType, completion: @escaping () -> Void)         if type == .voIP             let callId = extract from payload      let room = extract from payload                // Report the call to CallKit, and let it display the call UI.       let handle = CXHandle(type: .generic, value: roomName)       let update = CXCallUpdate()       update.remoteHandle = handle           provider.reportNewIncomingCall(with: callId, update: update, completion:   error in         if let error = error             assertionFailure(\"Error reporting new incoming call: (error.localizedDescription)\")                  )              Similarly, when you make a call, you request a CXStartCallAction transaction on CXCallController. And the system will show the appropriate native UI feedback to the user about the current call. iOS will show a green icon on top when a Callkit call is active.   swift   private let callController = CXCallController()   func startCall(id: UUID, roomName: String)         let handle = CXHandle(type: .generic, value: roomName)     let startCallAction = CXStartCallAction(call: id, handle: handle)     startCallAction.isVideo = true         let transaction = CXTransaction(action: startCallAction)         callController.request(transaction)   error in       if let error = error           assertionFailure(\"Error requesting CXStartCallAction transaction: (error)\")                        Does 100ms SDK provide special API's for Callkit? There are no APIs specially for Callkit. You can use 100ms SDK with Callkit as usual, while keeping some things in mind about how callkit's audio session works.   What to keep in mind when using 100ms SDK with Callkit? Callkit coordinates your app's calling services with other apps and the system. Callkit activates a prioritised audio session specifically for your call. Any audio session activated prior to Callkit's audio session is nullfied. Any audio session activated after Callkit's audio session activation becomes part of the call's audio. You need to make sure that you join the call using 100ms SDK only after callkit has activated the prioritised audio session. Callkit calls the following delegate to let you know when it has activated the audio session for your call:   swift   func provider(_ provider: CXProvider, didActivate audioSession: AVAudioSession)     If you join the meeting before callkit has activated the Callkit audio session, the audio session configured by 100ms SDK will be nullified, resulting in no audio in the call. "
    },
    {
        "title": "Capture Snapshot (Beta)",
        "link": "/ios/v2/features/capture-snapshot",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/capture-snapshot",
        "keywords": [],
        "content": "  You can capture a snapshot of the video stream of a local or remote peer's video.   Minimum Requirements   SDK version 0.5.5   How to capture the snapshot You show a peer's video track using HMSVideoView. To capture a snapshot, you call captureSnapshot method on HMSVideoView instance to get a UIImage containing a snapshot of the video stream:   swift var videoView = HMSVideoView() ... // Capture snapshot if let image = videoView.captureSnapshot()    // process the captured UIImage  ...      👀 To see an example iOS Capture Snapshot implementation using 100ms SDK, checkout  our example project (https://github.com/100mslive/100ms-ios-sdk/tree/main/Example). "
    },
    {
        "title": "Change Role\r",
        "link": "/ios/v2/features/change-role",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/change-role",
        "keywords": [],
        "content": " -\r \r Role is a powerful concept that takes a lot of complexity away in handling permissions and supporting features like breakout rooms.  Learn more about roles here. (../foundation/templates-and-roles)\r \r Each HMSPeer instance has a role property which returns an HMSRole instance. You can use this property to do following:\r \r 1. Check what this role is allowed to publish. I.e can it send video (and at what resolution)? can it send audio? can it share screen? Who can this role subscribe to? (I.e student can only see the teacher's video) This is can be discovered by checking publishSettings and subscribeSettings properties\r 2. Check what actions this role can perform. i.e can it change someone else current role, end meeting, remove someone from the room. This is can be discovered by checking permissions property\r \r In certain scenarios you may want to change someone's role. Imagine an audio room with 2 roles \"speaker\" and \"listener.\" Only someone with a \"speaker\" role can publish audio to the room while \"listener\" can only subscribe. Now at some point \"speaker\" may decide to nominate some \"listener\" to become a \"speaker.\" This is where the changeRole API comes in.\r \r To invoke the API you will need 2 things. An instance of HMSPeer of the peer who's role you want to change and the HMSRole instance for the target role. All the peers that are in the current room are accessible via peers property of HMSRoom instance that you can get via room property of HMSSDK instance after successful room join. A list of all available roles in the current room can be accessed via roles property of HMSSDK \r \r Once you have both you can invoke\r \r  swift\r hmsSDK.changeRole(for: targetPeer, role: targetRole)\r  \r \r If the change role succeeds you will get a\r \r  swift\r func on(peer: HMSPeer, update: HMSPeerUpdate)\r  \r \r delegate callback with the the same peer you passed as targetPeer and a roleUpdated update type.\r \r changeRole has an optional force parameter which is false by default meaning that changeRole is basically a polite request: \"Would you like to change you role from listener to speaker?\" which can be ignored by the other party. The way it works is the other party will first receive a\r \r  swift\r func on(roleChangeRequest: HMSRoleChangeRequest)\r  \r \r delegate callback. At which point app can choose to show a prompt to the user asking for permission. If the user accepts, app should call\r \r  swift\r hmsSDK.accept(changeRole: roleChangeRequest)\r  \r \r which completes the changeRole loop. Both parties will receive a roleUpdated callback so that they both can do necessary UI updates. Now the user actually becomes a speaker and the audio publishing will start automatically.\r \r Now lets imagine the newly nominated speaker is not behaving nicely and we want to move him back to listener without a prompt. This is where the force parameter comes in. When it is set to true the other party will not receive a confirmation roleChangeRequest but instead will straight away receive a new set of updated permissions and stop publishing. roleUpdated callback will still be fired so that the app can update the user's UI state.\r \r  Bulk Role Change\r Bulk role change is used when you want to change roles of all users from a list of roles, to another role.\r \r \r For example if peers join a room with a waiting role and you want to change them all to viewers then you'd use this API.\r \r Here is the method signature.\r \r \r  swift\r func changeRolesOfAllPeers(to role: HMSRole, limitToRoles:  HMSRole ? = nil, completion: ((Bool, Error?) -> Void)? = nil)\r  \r \r 1. role is the HMSRole they should be changed to.\r 2. limitToRoles is a list of HMSRole whose role should be changed.\r 3. completion is the optional completion handler to be invoked when the request succeeds or fails with an error.\r \r You should avoid omitting limitToRoles param as this can result in accidentally changing roles you may not have intended such as the bots that provide recording and streaming with the roles beam .\r \r Also bulk role changes are always forced, no dialog will be given for the peer to accept it, they will just be changed immediately.\r \r Here's how the method could be called to change all guest and waiting roles to host :\r \r  swift\r func changeRoles(hmsSdk: HMSSDK)  \r     var sourceRoles = roles.filter(  role in\r       role.name == \"guest\"  role.name == \"waiting\"\r      )\r     var toRole = roles.first   $0.name == \"host\"  \r \r     hmsSdk.changeRolesOfAllPeers(to: toRole, limitToRoles: sourceRoles)\r    \r  \r \r   Bulk Role Change Errors You may get the following errors for bulk role change:  Message                 Meaning                                                      invalid role           A role in the list of roles to change does not exist in this room.        target role clash with requested roles  the 'toRole' is also listed as one to change to 'toRole'   role does not have required permission  Peer does not have role change permission.          peer left                The peer who's role was to be changed has left.       role invalid               The 'toRole' is invalid.                 "
    },
    {
        "title": "Change User Name\r",
        "link": "/ios/v2/features/change-user-name",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/change-user-name",
        "keywords": [],
        "content": " -\r \r Any peer can change their own name before or after joining a room. Before joining, the name would have to be specified in HMSConfig that is passed to the join method. This document shows how the name can be changed after joining.\r \r  Responding to name changes\r \r Whenever a remote peer's name is changed a callback will be received in on(peer: HMSPeer, update: HMSPeerUpdate) of HMSUpdateListener where the update will be of type HMSPeerUpdate.nameUpdated .\r \r When this callback is received the UI for that peer should be updated as well.\r \r  Changing the name\r \r The peer who wants to change their name should call the following method on an HMSSDK instance.\r \r  swift\r let newName = \"Ashwini Shetty\"\r hmsSdk.change(name: newName)   success, error in\r  ...\r  \r  \r "
    },
    {
        "title": "Chat\r",
        "link": "/ios/v2/features/chat",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/chat",
        "keywords": [],
        "content": " -\r \r What's a video without being able to send messages to each other too? 100ms supports chat for every video/audio room you create.\r \r You can see an example of every way of sending messages and interpreting messages in the advanced  sample app (https://github.com/100mslive/100ms-ios-sdk).\r \r   Addressing messages\r \r    Broadcast messages ( sending-broadcast-messages) can be sent to Everyone in the chat hmssdk.sendBroadcastMessage .\r \r    Direct messages ( sending-direct-messages) let you send message to a specific person hmssdk.sendDirectMessage .\r \r    Group messages ( sending-group-messages) let you send a message to everyone with a particular HMSRole . Such as all hosts or all teachers or all students  hmsSdk.sendGroupMessage \r \r   Sending Chat Messages\r \r   Sending Broadcast Messages\r \r You want to let everyone in the chat know something? Call sendBroadcastMessage on the instance of HMSSDK to a send a broadcast.\r \r The text of the message, its type and a listener for whether the message reached the server or not are the parameters.\r \r > 💡 Note that the callback only lets you know if the server has received your request for the message or if there was some error. It does not convey whether the message was delivered to or read by the recipient.\r > also it's important to make a new callback per message because it will only contain the results of that particular call for sending a message.\r \r  swift\r hmssdk.sendBroadcastMessage(type: \"chat\", message: \"\")   message, error in\r \r  \r  \r \r   Sending Direct Messages\r \r Got secrets to share? Send a message directly to a single person in the chat with a direct message. Call sendDirectMessage on an instance of HMSSDK .\r \r The text of the message, its type and a listener for whether the message reached the server or not are the parameters.\r \r > 💡 Note that the callback only lets you know if the server has received your request for the message or if there was some error. It does not convey whether the message was delivered to or read by the recipient.\r > also it's important to make a new callback per message because it will only contain the results of that particular call for sending a message.\r \r  swift\r hmssdk.sendDirectMessage(type: \"chat\", message: \"\", peer: recipientPeer)   message, error in\r \r  \r  \r \r   Sending Group Messages\r \r Want to share with a group? Send a message directly to a group in the chat with a group message. Call sendGroupMessage on an instance of HMSSDK .\r \r The text of the message, its type and a listener for whether the message reached the server or not are the parameters.\r \r > 💡 Note that the callback only lets you know if the server has received your request for the message or if there was some error. It does not convey whether the message was delivered to or read by the recipient.\r > also it's important to make a new callback per message because it will only contain the results of that particular call for sending a message.\r \r  swift\r hmssdk.sendGroupMessage(type: \"chat\", message: \"\", roles: recipientRoles)   message, error in\r \r  \r  \r \r   Receiving Chat Messages\r \r When you called hmsSdk.join(config, delegate) to join a room, the HMSUpdateListener implementation that was passed in had the callback on(message:) .\r \r This where you'll receive new messages as HMSMessage during the call. It contains:\r \r  swift\r public class HMSMessage  \r   public let message: String\r   public let type: String\r   public var sender: HMSPeer?\r   public var recipient: HMSMessageRecipient\r   public let time: Date\r  \r  \r \r message : Content of the text message or the text description of the raw message.\r \r type : Type of message sent. Default value is chat .\r \r recipient : The intended recipient(s) of this message as a HMSMessageRecipient .\r \r time : Timestamp of when the messaging server receives this message. Update the time in your own messages when this comes back from the server in on(message:) for accurate ordering of your own messages.\r \r sender : The HMSPeer who is sending this message.\r \r Identifying Senders: The sender of a message is always contained in the sender field of HMSMessage. This lets you get the name and peer id for any message sender.\r \r Message Body: The body of the message is in message as a String.\r \r Time: The time the message was sent is contained in time .\r \r   Identifying who the message was for\r \r The HMSMessageRecipient contained in the recipient field of HMSMessage lets you know who the message was for.\r \r The HMSMessageRecipient contains:\r \r  swift\r public class HMSMessageRecipient  \r   public let type: HMSMessageRecipientType\r   public let peerRecipient: HMSPeer?\r   public let rolesRecipient:  HMSRole ?\r  \r  \r \r peerRecipient : Only contains a peer when a specific single peer is being direct messaged.\r \r rolesRecipient : Only contains values when a group message is being sent to one or many roles.\r \r type : Will be broadcast for a message being sent to everyone. If this is true, the other two field will be null, empty respectively.\r \r peer will be set when it's a direct message.\r \r roles will be set when it's a message to one or many roles.\r "
    },
    {
        "title": "End Room\r",
        "link": "/ios/v2/features/end-room",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/end-room",
        "keywords": [],
        "content": " -\r \r Done with talking and it's time to end the call for everyone not just yourself? You may be looking to end the room.\r \r   Permissions\r \r Can't let just anyone end a call. First you need to create a  role (../foundation/templates-and-roles) with the permissions to end a room.\r \r The permission to end a room is called endRoom and you should check for that within the permissions property of HMSRole of the peer to see if they have it.\r \r \r   Ending the Room\r \r Once you're sure the peer has the permissions to end the room they can call for the room to end when they're ready with hmsSdk.endRoom .\r \r endRoom takes three parameters.\r \r reason: Optional message you want to pass along the end room notification to other peers\r \r lock: Whether you want to prevent anyone from rejoining the room. If false, they will be allowed to enter the room again if the client called join . If this is false, they will not able to join this room again.\r \r completion handler: Lets you know whether the end room operation executed. was ended successfully or not. \r \r > 💡 After calling endRoom the local peer needs to dispose of the video calling UI as well.\r \r  swift\r hmssdk.endRoom(lock: false, reason: \"Meeting is over\")   success, error in\r if (success)  \r   // pop to previous screen\r  \r  \r  \r \r   How to handle an end room callback\r \r Once the peer with adequate permissions calls endRoom , all other peers in the room will receive a callback in HMSUpdateListener.onRemovedFromRoom .\r \r The on(removedFromRoom:) callback has a single parameter called HMSRemovedFromRoomNotification with the following structure.\r \r  swift\r class HMSRemovedFromRoomNotification  \r   public let requestedBy: HMSPeer\r   public let reason: String\r   public let roomEnded: Bool\r  \r  \r \r > 💡 This is the same callback that will be triggered if a peer was  removed from a room (remove-peer) as well. Except that roomEnded will be true when the entire room was ended.\r \r reason: The string message detailing why the room was ended.\r \r requestedBy: The details of the peer who called endRoom .\r \r roomEnded: True if the entire room was ended. False if only the receiving peer was removed.\r \r Clients should read this callback and show the appropriate UI."
    },
    {
        "title": "Error Handling",
        "link": "/ios/v2/features/error-handling",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/error-handling",
        "keywords": [],
        "content": "  Don't have time to dig into details? Here is a minimum error handling code to have in the app:   swift func on(error: Error)     guard let error = error as? HMSError else   return       if error.isTerminal       showErrorAndExitMeeting(errorMessage: error.localizedDescription)     else       print(error)         Errors happen. When they happen the app needs to react in a manner that makes the most sense to a user. There are 3 major categories of errors: 1. Connection errors (i.e connect lost, unable to connect) 2. Capture device errors (i.e user did not give permission to use microphone) 3. Lack of permissions/authentication errors (i.e. auth token is invalid/expired) While some errors like permissions/authentication errors usually happen during development and are not expected in production, others like connection/device errors are expected to come during normal app lifecycle and require handling in the application code.    Error callbacks There 2 places where a developer is expected to add error handling: 1. func on(error: HMSError) callback of the HMSUpdateListener 2. Completion handler of the various SDK APIs (i.e endRoom, changeRole etc)   Error Type The error type in the public SDK interface is  Error ( NSError in case of ObjC). In swift the error can be type casted to either HMSError or NSError for getting additional details like error code.    Error Handling Example   swift func on(error: Error)     guard let error = error as? HMSError else   return       //Example using error constants   switch error.code     case .websocketConnectionLost, .iceFailure, .endpointUnreachable:     retryConnection()   default: \t\tshowErrorAndExitMeeting(errorMessage: error.localizedDescription)     break         //Example using error codes   switch error.code.rawValue     case 1003, 4005, 2003:     retryConnection()   default: \t\tshowErrorAndExitMeeting(errorMessage: error.localizedDescription)     break           HMSError Properties   isTerminal The HMSError struct in swift will now have an isTerminal property which denotes wether error has caused the current session to terminate and the app will need to call join again to reconnect. Same will be available via is_terminal ( HMSIsTerminalUserInfoKey constant) key in userInfo dictionary of NSError    canRetry The HMSError struct in swift will now have an canRetry property which denotes wether app can call join again with the same configuration it has used before. The value be false in cases like token expiring or room getting locked. You can use this property while implementing infinite retry in your app. Same will be available via can_retry ( HMSCanRetryUserInfoKey constant) key in userInfo dictionary of NSError    Error Handling Example Using Properties This example shows the minimal error handling you can have in the app without worrying about error codes.   swift func on(error: Error)     guard let error = error as? HMSError else   return       if error.isTerminal       if error.canRetry && isInfiniteRetryEnabledInApp         retryConnection()       else         showErrorAndExitMeeting(errorMessage: error.localizedDescription)           else       logError(message: error.localizedDescription)          Connection Errors When for whatever reason the internet connection becomes unavailable the SDK will first try to automatically reconnect. Upon failing to reconnect within a timeframe of 50 seconds SDK will give up and send an error via onError callback of HMSUpdateListener  There are 3 error codes you can expect in this scenario:  1. websocketConnectionLost (1003) 2. iceFailure (4005) 3. endpointUnreachable (2003) When the application gets any of these it can react by calling join again as in an infinite retry, or it can bring user to the previous screen showing an error popup.  Capture Device Errors Typically when the SDK can’t get a hold of a capture device i.e the user has not given access permission it will send an onError callback with cantAccessCaptureDevice (3001). If the microphone/camera access are crucial to the experience you are trying to build the app should end the meeting and show an error popup, otherwise user can continue participating without camera/mic in a listen/watch only mode.  API Errors When issuing certain API calls like endRoom, changRole etc you might get en error in return. This might be due to lack of permissions for a current role or server getting overloaded etc.   Error Code List Following are the different error codes that are returned by the SDK.  Error Code  HMSError.Code Enum Case Name  Cause of the error  Action to be taken   :  :   :   :    1003  websocketConnectionLost  Websocket disconnected  Happens due to network issues  Mention user to check their network connection or try again after some time.   2002  invalidEndpointUrl  Invalid Endpoint URL  Check the endpoint provided while calling join on HMSSDK.   2003  endpointUnreachable  Endpoint is not reachable  Mention user to check their network connection or try again after some time.   2004  invalidTokenFormat  Token is not in proper JWT format  The token passed while calling join is not in correct format. Retry getting a new token.   3001  cantAccessCaptureDevice  Cant Access Capture Device  Ask user to check permission granted to audio/video capture devices.   3008  audiovideoSubsystemFailure  System media services were reset due to a failure.  Restart the session   4001  createOfferFailed  WebRTC error  Some webRTC error has occurred. Need more logs to debug.   4002  createOfferFailed  WebRTC error  Some webRTC error has occurred. Need more logs to debug.   4003  createOfferFailed  WebRTC error  Some webRTC error has occurred. Need more logs to debug.   4004  createOfferFailed  WebRTC error  Some webRTC error has occurred. Need more logs to debug.   4005  iceFailure  ICE Connection Failed due to network issue  Mention user to check their network connection or try again after some time.   5001  alreadyJoined  Trying to join a room which is already joined  Trying to join an already joined room.   6002  unknown  webRTC Error: Error while renegotiating  Please try again.   401  n/a  Token Error: Invalid Access Key  Access Key provided in the token is wrong.   401  n/a  Token Error: Invalid Room Id  RoomID provided in the token is wrong.   401  n/a  Token Error: Invalid Auth Id  AuthID provided in the token is wrong.   401  n/a  Token Error: Invalid App Id  App ID provided in the token is wrong.   401  n/a  Token Error: Invalid Customer Id  Customer Id provided in the token is wrong.   401  n/a  Token Error: Invalid User Id  User ID provided in the token is wrong.   401  n/a  Token Error: Invalid Role  The role provided in the token is wrong.   401  n/a  Token Error: Bad JWT Token  Bad JWT Token.   401  n/a  Generic Error  Need to debug further with logs.   400  n/a  Invalid Room  Room ID provided while fetching the token is an invalid room.   400  n/a  Room Mismatched with Token  Room ID provided while fetching the token does not match.   400  n/a  Peer already joined  Peer who is trying to join has already joined the room.   410  n/a  Peer is gone  The peer is no more present in the room.  "
    },
    {
        "title": "HLS Playback Stats (Beta)",
        "link": "/ios/v2/features/hls-stats",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/hls-stats",
        "keywords": [],
        "content": "  If you use Apple's AVPlayer to play HLS stream, you can utilise 100ms SDK to monitor statistics of your HLS playback. You can use these stats to help you debug issues or show useful statistics about your HLS playback.   Minimum Requirements   SDK version 0.5.1   You must be using AVPlayer for HLS Playback   How to monitor HLS playback statistics You create an instance of HMSHLSPlaybackMonitor class passing an instance of AVPlayer that's setup to play an HLS stream.   swift var statMonitor: HMSHLSStatsMonitor? ... let player = AVPlayer(playerItem: item) statMonitor = HMSHLSStatsMonitor(player: player)    That's all. Now you can check the properties exposed by HMSHLSStatsMonitor at any moment to get various stats about the current HLS playback.  HMSHLSStatsMonitor exposes following properties to give you stats of current HLS playback: 1.   estimatedBandwidth    The network bandwidth estimated by the player. This is the bitrate observed by the player in current network conditions (bits per second) 2.   bitrate    The bitrate of current HLS stream (layer) indicated by the server (bits per second) 3.   bytesDownloaded    Total number of bytes downloaded till this moment 4.   bufferedDuration    The duration of forward buffer that player has downloaded and ready to play (milliseconds) 5.   distanceFromLiveEdge    The distance of current playback position from the live edge of HLS stream (milliseconds) 6.   droppedFrames    The total number of frames dropped till this moment 7.   videoSize    The resolution of the HLS stream 8.   watchDuration    Total duration that the user has watched the stream includes any rewatch time (milliseconds)   How can I keep monitoring these stats continuously You use a timer to check the stats you are interested in like below:   swift Timer.scheduledTimer(withTimeInterval: 1.0, repeats: true)   _ in  print(statMonitor.estimatedBandwidth)  print(statMonitor.distanceFromLiveEdge)         Observing playback state Set the delegate property of HMSHLSStatsMonitor to an instance conforming to HMSHLSPlaybackDelegate protocol to listen to some important changes during HLS playback. Following delegate callbacks are available: 1.   playerDidStall    Player has stalled because it can't continue  this can happen when player does not have enough buffer for playback 2.   playerDidChangeResolution    Player has switched to different resolution layer of HLS stream  due to improved/degraded network condition or when user manually picks a different resolution   swift class YourClass: HMSHLSPlaybackDelegate    ...  statMonitor.delegate = self  ...  // HMSHLSPlaybackDelegate  func playerDidChangeResolution(videoSize: CGSize)     print(\"Resolution Changed: (videoSize)\")     func playerDidStall()     print(\"Player Stalled\")          👀 To see an example iOS HLS Stats implementation using 100ms SDK, checkout  our example project (https://github.com/100mslive/100ms-ios-sdk/tree/main/Example). "
    },
    {
        "title": "HLS Streaming",
        "link": "/ios/v2/features/hls",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/hls",
        "keywords": [],
        "content": "   HLS Streaming (./../foundation/live-streaming) allows for scaling to millions of viewers in near real time. You can give a link of your web-app which will be converted to a HLS feed by our server and can be played across devices for consumption. Behind the scenes, this will be achieved by having a bot join your room and stream what it sees and hears. Once the feed is ready, the server will give a URL which can be played using any HLS Player. > Note that the media server serving the content in this case is owned by 100ms. If you're looking for a way to stream > on YouTube, Twitch etc., please have a look at our RTMP streaming docs  here (./rtmp-recording).   Starting HLS HLS can be started in two ways depending on the level of customization you need. 1. Default View: The simplest view to just begin a stream with default UI and parameters. 2. Custom Views: To use your own UI for HLS streaming, you need to provide your own web-app URL for our bot to join and stream.    Default View Begins a stream with default parameters.   swift hmsSDK.startHLSStreaming()   didStart, error in   print(didStart, error)        Custom View To use your own web-app UI for HLS, you'll need to pass in a meeting URL. The 100ms bot will open this URL to join your room, so it must allow access without any user level interaction. In the future it'll be possible to start HLS for multiple such URLs for the same room. For this purpose the API supports taking in an array, although currently only the first element of the array will be used. To distinguish between multiple urls an additional field metadata can be optionally passed. The meetingURL and metadata are clubbed together to form what we'll call a variant . You can call hmsSDK.startHLSStreaming with HMSHLSConfig having an array of such variants. HMSHlsRecordingConfig is optional.   swift let config = HMSHLSConfig(variants:  HMSHLSMeetingURLVariant(meetingURL: meetingURL, metadata: \"tag for reference\") ) hmsSDK.startHLSStreaming(config: config)   didStart, error in   print(didStart, error)       Optional HLS Recording Optionally to record the HLS stream you may specify an HMSHLSRecordingConfig within the HMSHLSConfig . Here's what the HMSHlsRecordingConfig looks like   swift class HMSHLSRecordingConfig     let singleFilePerLayer: Bool   let enableVOD: Bool      1. singleFilePerLayer if the desired end result is a mp4 file per HLS layer, false by default. 2. enableVOD if the desired end result is a zip of m3u8 and all the chunks, false by default. Here's an example of how to create a recording config:   swift // Optional recording config let recordConfig = HMSHLSRecordingConfig(singleFilePerLayer: true, enableVOD: false) let config = HMSHLSConfig(variants:  variant , recording: recordConfig)     Stopping HLS You can call hmsSDK.stopHLSStreaming to stop HLS Streaming which will stop all the variants.   swift hmsSDK.stopHLSStreaming()   didStop, error in   ...       Current Room Status The current status for the room is always reflected in the HMSRoom object. Here are the relevant properties inside the HMSRoom object which you can read to get the current hls streaming status of the room namely: hlsStreamingState . The object contains a boolean running which lets you know if it's active on the room right now as well as list of active variants. 1.   hlsStreamingState   an instance of HMSHLSStreamingState , which looks like:   swift @objcMembers public class HMSHLSStreamingState : NSObject     public let running: Bool   public let variants:  HMSHLSVariant      This represents a livestream to one or more HLS urls. The room status should be checked in following two places  1. In the onJoin(room: HMSRoom) callback of HMSUpdateListener   The properties mentioned above will be on the HMSRoom object. 2. In the on(room: HMSRoom, update: HMSRoomUpdate) callback of HMSUpdateListener .  The HMSRoomUpdate type will be hlsStreamingStateUpdated .   Tips   If you're using the dashboard web-app from 100ms, please make sure to use a role which doesn't have publish permissions for beam tile to not show up.   If using your own web-app, do put in place retries for API calls like tokens etc. just in case any call fails. As human users we're   used to reloading the page in these scenarios which is difficult to achieve in the automated case.   Make sure to not disable the logs for the passed in meeting URL. This will allow for us to have more visibility into the room, refreshing the page   if join doesn't happen within a time interval. "
    },
    {
        "title": "Integrating The SDK",
        "link": "/ios/v2/features/integration",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/integration",
        "keywords": [],
        "content": "   HMSSDK is distributed via  Cocoapods (https://cocoapods.org/) and Swift Package Manager.   Swift Package Manager Use https://github.com/100mslive/100ms-ios-sdk.git as the package source.   Cocoapods Add the pod 'HMSSDK' to your Podfile as follows:      // Podfile   platform :ios, '12.0'  target 'MyAwesomeApp' do   use_frameworks    pod 'HMSSDK'  end    then run pod install repo-update  Add the entitlements for video, audio and network access to your Info.plist   xml <key>NSCameraUsageDescription</key> <string>Allow access to camera to enable video calling.</string> <key>NSLocalNetworkUsageDescription</key> <string>Allow access to local network to enable video calling.</string> <key>NSMicrophoneUsageDescription</key> <string>Allow access to mic to enable video calling.</string>     Enable Background Mode Enable the checkbox under Background Modes named Audio, AirPlay, and Picture in Picture.   backgroundProcessing (/docs/v2/flutter-background-service-4.png) To understand about the background modes check out  Background Modes (./background-modes) "
    },
    {
        "title": "Interruption Handling\r",
        "link": "/ios/v2/features/interruption-handling",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/interruption-handling",
        "keywords": [],
        "content": " -\r \r You're chatting away on your video call and, uh-oh, a phone call comes in. No worries, the 100ms SDK abstracts away handling this situation for you.\r \r When a telephonic voice call comes in and the user accepts the following happens.\r \r   You and all the other parties are muted.\r   If the app goes to background your video will be muted\r \r When the call ends the SDK will restore the audio, taking care not to un-mute you if you had muted yourself before the call came in. Neither will it un-mute all the peers if you had muted them from the app.\r \r No code required, this happens automatically for all calls.\r "
    },
    {
        "title": "Join Room\r",
        "link": "/ios/v2/features/join",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/join",
        "keywords": [],
        "content": " -\r \r To join and interact with others in audio or video call, the user needs to join a room .\r \r  Prerequisites\r \r When user indicates that they want to _join_ the room, your app should have -\r \r 1. User Name  the name which should be displayed to other peers in the room.\r 2.  Authentication Token (../guides/token)  the client side authentication token generated by the Token Service.\r \r  Join a Room\r \r We'll call the join method on HMSSDK object with a config containing above fields to join the room.\r \r <StepsContainer>\r \r   Create HMSSDK Instance\r \r First, create an instance of HMSSDK class. Store this instance as a property. Ensure that the SDK object is alive in memory so that you can receive event callbacks from SDK. Simplest way to do this is as follows -\r \r  swift\r import HMSSDK\r \r class MyMeetingClass  \r \r   let hmsSDK: HMSSDK // store instance of HMSSDK as a property in your class\r \r   init()  \r     hmsSDK = HMSSDK.build() // initialize the SDK using the build() class function\r    \r  \r  \r \r   Optional: Join with Muted Audio / Video  \r \r You can optionally set the initial audio/video state at the time of joining like below (they are unmute by default):\r \r  swift\r class MyMeetingClass  \r \r   let hmsSDK: HMSSDK // store instance of HMSSDK as a property in your class\r \r   init()  \r     // initialize the SDK using the build() class function\r     hmsSDK = HMSSDK.build()   sdk in\r       sdk.trackSettings = HMSTrackSettings.build   videoSettingsBuilder, audioSettingsBuilder in\r         videoSettingsBuilder.initialMuteState = .mute\r         audioSettingsBuilder.initialMuteState = .mute\r        \r      \r    \r  \r  \r \r   Create HMSConfig Instance\r \r Next, create an object of HMSConfig class using the available joining configurations\r \r  swift\r let config = HMSConfig(userName: \"John Doe\",\r            authToken: \"eyJH5c\")\r  \r \r   Optional: Set metadata for the user  \r \r Optionally, you can use metadata param that can be used to attach any additional data associated with this user. You can access this data on this peer later using peer's 'metadata' property.\r \r  swift\r let config = HMSConfig(userName: \"John Doe\",\r            authToken: \"eyJH5c\",\r            metadata: \" \"avatar\": \"location/on/amazon/storage\" \")\r  \r \r   Call join\r \r Now, we are primed to join the room. All you have to do is pass the config object to hmsSDK \r \r  swift\r hmsSDK.join(config: config, delegate: self)\r  \r \r Once on(join room: HMSRoom) callback is fired you have joined the room successfully. 🥳\r   \r  swift\r class MyMeetingClass: HMSUpdateListener  \r   func on(join room: HMSRoom)  \r     ...\r    \r  \r   \r Note: Make sure to wait for onJoin callback before calling any other SDK API (except leave) as they require an active connection and will fail with a “not connected” error.\r \r </StepsContainer>\r \r   Join API Signature\r \r Now, let's take a look at the signature of the Join API\r \r  swift\r func join(config: HMSConfig, delegate: HMSUpdateListener)\r  \r \r As evident, join accepts 2 arguments -\r \r   config : an object of type HMSConfig class, the room configuration object which encapsulates user & token data\r   delegate : a class conforming to HMSUpdateListener protocol.\r \r The methods of HMSUpdateListener are invoked to notify updates happening in the room like a peer joins/leaves, a track got muted/unmutes, etc.\r \r After calling join your app will be provided an update from the 100ms SDK.\r \r ✅ If successful, the func on(join room: HMSRoom) method of HMSUpdateListener will be invoked with information about the room encapsulated in the HMSRoom object.\r \r ❌ If failure, the func on(error: HMSError) method will be invoked with exact failure reason.\r "
    },
    {
        "title": "Leave Room\r",
        "link": "/ios/v2/features/leave",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/leave",
        "keywords": [],
        "content": " -\r \r Leaving a room should be quick. Once a user wishes to end their interaction in the room, they can choose to leave the meeting. 👋\r \r 100ms SDK provides a simple, no fuss API to do exactly that. Invoke the leave method on instance of HMSSDK .\r \r  swift\r hmsSDK.leave()\r  \r \r You should expect that the leave function always succeeds, even in scenarios like \"No Internet Connection\", \"Low Battery\", etc\r "
    },
    {
        "title": "Mute / Unmute\r",
        "link": "/ios/v2/features/mute",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/mute",
        "keywords": [],
        "content": " -\r \r An user should be able to mute or un-mute both audio & video as per their choice. 100ms SDK makes it simple to do just that in nifty way.\r \r 1. First, get the reference to local peer using the convenience method localPeer on HMSSDK class\r 2. Next, get the audio or video track of the local peer\r 3. Now, set it's mute status to true or false.\r \r   When mute is true, local peer won't transmit any audio or video. 🤫🤐\r   When mute is false, local peer can speak & show the world whatever they want to. 🕺💃\r \r   Mute Audio 🙊\r \r  swift\r hmsSDK.localPeer?.localAudioTrack()?.setMute(true)\r  \r \r   Ummute Audio 🐵\r \r  swift\r hmsSDK.localPeer?.localAudioTrack()?.setMute(false)\r  \r \r   Mute Video 🙈\r \r  swift\r hmsSDK.localPeer?.localVideoTrack()?.setMute(true)\r  \r \r   Unmute Video 🐒\r \r  swift\r hmsSDK.localPeer?.localVideoTrack()?.setMute(false)\r  \r \r Note: Ensure that you call these functions from the main queue.\r "
    },
    {
        "title": "Network Quality Reports\r",
        "link": "/ios/v2/features/network-quality-reports",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/network-quality-reports",
        "keywords": [],
        "content": " -\r \r Video/Audio conferencing is by nature a data intensive operation. The 100ms SDK attempts to stabilize connections especially if subscribe degradation is turned on in the template but it's possible for really bad connections that users will still have problems.\r \r It can be helpful to measure a user's connection speed before joining a room in order to set expectations or decide to have them join with video off etc.\r \r The 100ms SDK provides a way to measure the user's downlink speed in the preview screen.\r \r > ⚠️ The downlink speed is measured by having the SDK download a file (1mb as of this writing).\r \r The download will be continued for at most a fixed number of seconds (For example 10 seconds) and the speed during that interval is calculated. The entire file may not be downloaded if it exceeds the timeout.\r \r DNS time is not counted in this, only throughput is measured.\r \r  Receiving a Network Quality Update in Preview\r \r When creating an HMSConfig object to request a preview, set the captureNetworkQualityInPreview to true to measure the user's downlink network quality.\r \r When available, the information will be returned in the on(peer: HMSPeer, update: HMSPeerUpdate) callback of the HMSPreviewListener with the networkQualityUpdated update type. It can be retrieved out of the HMSPeer object's networkQuality property. In case of preview, the peer will be local peer.\r \r  swift\r let config = HMSConfig(userName: user, authToken: token, captureNetworkQualityInPreview: true)\r hmsSDK?.preview(config: config, delegate: self)\r  \r \r Here's the class definition of HMSNetworkQuality , which is a property on the HMSPeer .\r \r  swift\r public class HMSNetworkQuality  \r   public let downlinkQuality: Int\r  \r  \r \r  Receiving Network Quality Update During a Call\r \r After the room has been joined, it might be useful to give users an indication of network health of other participants in the room. In case of network issues this can reduce confusion regarding wether it is my network at fault or the other party's.\r No special configuration is required to enable network health callbacks. Starting with iOS SDK version 0.2.9 this information will be sent via on(peer: HMSPeer, update: HMSPeerUpdate) callback of HMSUpdateListener . The peer will signify which peer the network health is being reported for.\r \r  Interpreting the Values\r \r peer.networkQuality?.downlinkQuality will be a value between -1 and 5.\r \r   -1 -> Test timeout.\r   0 -> Very bad network or network check failure.\r   1 -> Poor network.\r   2 -> Bad network.\r   3 -> Average.\r   4 -> Good.\r   5 -> Best.\r "
    },
    {
        "title": "Picture in Picture Mode (Beta)",
        "link": "/ios/v2/features/pip-mode",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/pip-mode",
        "keywords": [],
        "content": "  iOS SDK provides support for creating Picture in Picture mode experience for video calls.   Minimum Requirements   Minimum iOS version required to support PiP is iOS 15   Minimum 100ms SDK version required is 0.3.3   Your app need to have  com.apple.developer.avfoundation.multitasking-camera-access (https://developer.apple.com/documentation/bundleresources/entitlements/com_apple_developer_avfoundation_multitasking-camera-access) entitlement to use PiP mode during video calls.   How Picture in Picture works during a video call When your app goes into the background, it can no longer access camera and publish it to other peers in the room. Also, you can't see video of other particiapants in the call if your app is in the background. Both of these issues are resolved by implementing PiP mode for video calling in your app.   How to add PiP support 1. You create an instance of AVPictureInPictureVideoCallViewController. And add the view that you want to show in PiP window as subview:   swift   let pipVideoCallViewController = AVPictureInPictureVideoCallViewController()   pipVideoCallViewController.view.addSubview(...)    2. Next, you create a content source from pipVideoCallViewController, passing the target view that PiP window will use as anchor view to animate from (you can optionally set PiP preferred size):   swift   let pipContentSource = AVPictureInPictureController.ContentSource(       activeVideoCallSourceView: targetView,       contentViewController: pipVideoCallViewController)         // Optionally set the target frame as preferred content size for PiP window   pipVideoCallViewController.preferredContentSize = targetView.frame.size     3. Then you create AVPictureInPictureController with the content source:   swift   let pipController = AVPictureInPictureController(contentSource: pipContentSource)     4. To start the PiP mode, you set pipController to automatically start PiP when the app goes to background or you can use startPictureInPicture function to start PiP manually:   swift   // To start PiP automatically when app goes to background   pipController.canStartPictureInPictureAutomaticallyFromInline = true         // Or you can start PiP manually   pipController.startPictureInPicture()       How to display participant's video in PiP AVPictureInPictureController requires source content to use AVSampleBufferDisplayLayer on it's subview. HMSVideoView uses Metal for rendering video of peers on the call. But because Metal is currently unsupported by AVPictureInPictureController, you can't directly use HMSVideoView to draw participants' video in PiP window. You need to use HMSSampleBufferDisplayView instead. HMSSampleBufferDisplayView is an UIImageView that uses AVSampleBufferDisplayLayer for drawing. 1. You create an instance of HMSSampleBufferDisplayView and set the track to display. You add this as subview to pipVideoCallViewController view:   swift   let trackVideoView = HMSSampleBufferDisplayView(frame: .zero)   trackVideoView.track = track     // Optionally set preferredSize and contentMode   trackVideoView.preferredSize = CGSize(width: 640.0, height: 480.0)   trackVideoView.contentMode = .scaleAspectFill     ...   // As in step 1 in 'How to add PiP support'   pipVideoCallViewController.view.addSubview(trackVideoView)     2. Set trackVideoView to beging drawing by making it enabled when PiP window is shown (Make it false when PiP window is closed to save resources)   swift   trackVideoView.isEnabled = true   ...     // When PiP window is hidden   trackVideoView.isEnabled = false       CPU budget in the background HMSSampleBufferDisplayView updates it's frame every 0.25 seconds (4 frames per second) to save CPU cycles. This is done to not exceed CPU budget assinged to a background app on iOS. You can experiment and change this update frequency using 'updateEvery' property on HMSSampleBufferDisplayView:   swift   // 10 frames per second   trackVideoView.updateEvery = 0.1    👀 To see an example iOS Picuture in Picture implementation using 100ms SDK, checkout  our example project (https://github.com/100mslive/100ms-ios-sdk/tree/main/Example). "
    },
    {
        "title": "Preview",
        "link": "/ios/v2/features/preview",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/preview",
        "keywords": [],
        "content": "  Preview screen is a frequently used UX element which allows users to check if their input devices are working properly and set the initial state (mute/unmute) of their audio and video tracks before joining. 100ms SDKs provide an easy-to-use API to back this feature. Additionally, the SDK will try to establish a connection to 100ms server to verify there are no network issues and that the auth credentials are valid so that if everything is in order the subsequent room join is instant. To invoke this API call   swift hmsSDK.preview(config: config, delegate: previewDelegate)   You would need the same config object that you would pass to  join API (join). The previewDelegate is an object conforming to HMSPreviewListener protocol that has four callbacks:   swift func onPreview(room: HMSRoom, localTracks:  HMSTrack )   Which is called when SDK has passed all its preflight checks and established the connection to 100ms server. This will pass an array of local tracks that you can display to the user (see  Render Video (render-video) and  Mute (mute) sections for more details). If however there was some error related to getting the input sources or some preflight check has failed   swift func on(error: HMSError)   delegate callback will be fired with the HMSError instance you can use to find what went wrong (see  Error Handling (error-handling)).   swift func on(room: HMSRoom, update: HMSRoomUpdate)   Can be used to get the room updates such as state of recording, peer count etc.   swift func on(peer: HMSPeer, update: HMSPeerUpdate)   Can be used to get peer updates i.e peer join and leave. "
    },
    {
        "title": "Remote Mute\r",
        "link": "/ios/v2/features/remote-mute",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/remote-mute",
        "keywords": [],
        "content": " -\r \r You're running a room and decide that someone who's currently talking shouldn't be talking. You'd prefer they'd stay mute. Or perhaps you want their video turned-off as well as their audio. Then you're looking for a remote mute feature.\r \r Muting can apply to both audio and video.\r \r   Unmuting\r \r You may also decide that you want to let someone else speak who was currently muted. Or may want to ask someone to turn on their video. You can request people to un-mute themselves as well.\r \r   Permissions\r \r You can't let just anyone mute others. First you need to create a  role (../foundation/templates-and-roles) with the permissions to mute other participants and unmute other participants. Please note that they are two separate permissions.\r \r   Permissions  Remote mute/unmute (/guides/permissions.png)\r \r The permission to mute others is called mute and you should check for that within the permissions property of HMSRole of the peer to see if they have it. Similarly permission to unmute others is called 'unmute'.\r \r   Muting/Unmuting\r \r Once you have checked that the caller has permissions to mute another peer's audio or video, you can use hmsSdk.changeTrackState to make a remote mute/unmute request.\r \r To mute a track:\r \r Get the audio or video track of the peer whom you want to mute and pass it as the first parameter to hmsSdk.changeTrackState .\r \r The second parameter is a boolean for muting. If true, the track will be muted. If false the track will be requested to un-mute.\r \r Finally you can pass a completion handler to check whether changeTrackState succeeded or failed as the third parameter.\r \r   Handling a mute callback\r \r Muting is automatically applied to the receiver tracks. No action is required.\r \r   Handling an un-mute callback\r \r Unmute callbacks are received in the target peer's on(changeTrackStateRequest:) callback of HMSUpdateListener .\r \r The target peer will receive an object of HMSChangeTrackStateRequest .\r \r Here's its structure.\r \r  swift\r class HMSChangeTrackStateRequest  \r   public let track: HMSTrack\r   public let mute: Bool\r   public let requestedBy: HMSPeer\r  \r  \r \r With the information here, show a dialog to the user to ask if they want to accept the change and then apply the settings locally. The same as in a regular user  Mute/Unmute (mute).\r "
    },
    {
        "title": "Remove Peer\r",
        "link": "/ios/v2/features/remove-peer",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/remove-peer",
        "keywords": [],
        "content": " -\r \r Someone's overstayed their welcome and now you need to remove a peer from the chat. Just call hmsSdk.removePeerRequest .\r \r \r   Permissions\r \r Can't let just anyone remove others from the room. First you need to create a  role (../foundation/templates-and-roles) with the permissions to end a room.\r \r The permission to end a room is called removeOthers and you should check for that within the permissions property of HMSRole of the peer to see if they have it.\r \r   Removing a peer\r \r Once the permissions are checked to ensure the caller has the permission to remove a peer, remove them by calling hmsSdk.removePeer .\r \r The parameters are:\r \r peer: The HMSRemotePeer that you'd like to be removed from the room.\r \r reason: Optional message you want to pass along the peer being removed\r \r completion handler: Let you know whether the remove operation executed successfully or not. \r \r \r  swift\r hmssdk.removePeer(targetPeer, reason: \"You are violating the community rules.\")   success, error in\r  \r  \r \r   Handling the remove peer callback\r \r The target peer of the removePeer will receive a callback in HMSUpdateListener of onRemovedFromRoom(notification : HMSRemovedFromRoom) .\r \r The HMSRemovedFromRoom object which is passed into the callback has the structure:\r \r  swift\r class HMSRemovedFromRoomNotification  \r   public let requestedBy: HMSPeer\r   public let reason: String\r   public let roomEnded: Bool\r  \r  \r \r > 💡 This is the same callback that will be triggered if a room was  ended (end-room) as well. Except that roomEnded will be false.\r \r reason: The string message detailing why the peer is being removed.\r \r requestedBy: The details of the peer who called removePeer .\r \r roomEnded: True if the entire room was ended. False if only the receiving peer was removed.\r \r Clients should read this callback and show the appropriate UI."
    },
    {
        "title": "Render Video\r",
        "link": "/ios/v2/features/render-video",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/render-video",
        "keywords": [],
        "content": " -\r \r It all comes down to this. All the setup so far has been done so that we can show live streaming video in our beautiful apps. \r \r How to do it? What should I be careful about? Is it easy? 😨😰😱\r \r It's simpler than you think. That's what we are here for. 🙌✌️🤝\r \r 1. First, you need to create a HMSVideoView object in your View hierarchy.\r HMSVideoView is a subclass of UIView so add it to your UI just as a regular view using Storyboard, XIB or in-code.\r \r 2. Next, you'll have a reference to a video track object. Specifically, a object of type HMSVideoTrack (or any of it's subclasses). For example, the local peer's video track can be accessed like so -\r  swift\r hmsSDK.localPeer?.localVideoTrack() // it's a HMSLocalVideoTrack, a subclass of HMSVideoTrack\r  \r \r 3. Now, pass the video   track   object to video   view   using setVideoTrack method of HMSVideoView \r  swift\r videoView.setVideoTrack(track) // videoView is HMSVideoView, track is HMSVideoTrack\r  \r Video Track to Video View. Remember this. You'll be doing this often.\r \r That's it. \r \r The video which the peer will be transmitting will now be rendered & start playing in your video view. 🥳 "
    },
    {
        "title": "RTMP Streaming / Recording\r",
        "link": "/ios/v2/features/rtmp-recording",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/rtmp-recording",
        "keywords": [],
        "content": " -\r \r Want to preserve your video call for posterity in a recording? Or live stream it out to millions of viewers on Twitch or YouTube or whatever gives you an RTMP ingest URL?\r \r Turn on RTMP Streaming or Recording\r \r In 100ms, recording and streaming is usually achieved by having a bot join your room and stream what it sees and hears to a file (recording) or to an RTMP ingest URL (streaming).\r \r The topics covered in this doc are:\r \r 1.  How to start streaming / recording. ( starting-streaming-recording)\r 2.  How to stop streaming / recording. ( stopping-streaming-recording)\r 3.  Checking the current status for streaming / recording. ( current-room-status)\r \r  Starting Streaming / Recording\r \r To start recording, streaming or both, create an instance of HMSRTMPConfig .\r \r HMSRTMPConfig takes the following:\r \r 1.   meetingURL  : _URL?_. The URL the 100ms bot user will open to join your room. It must allow access without any user level interaction.\r 2.    rtmpURLs   : _ URL ?_. If streaming is required, this has to be one or more RTMP ingest Urls with max limit of 3 urls where the stream should go. If only recording, this should be nil.\r \r    Format: rtmp://server.com/app/STREAM_KEY \r    Example: rtmp://a.rtmp.youtube.com/live2/k0jv-329m-1y7f-ktth-ck48 \r      \"rtmp://a.rtmp.youtube.com/live2/\"  RTMP stream URL.\r      \"k0jv-329m-1y7f-ktth-ck48\"  RTMP stream key.\r \r    Please refer to the platform specific documentation for details on how to obtain the stream URL and stream key. Here are some examples:      YouTube (https://support.google.com/youtube/answer/2907883?hl=en&ref_topic=9257892)      Facebook (https://www.facebook.com/help/587160588142067)      Instagram (https://about.instagram.com/blog/tips-and-tricks/instagram-live-producer)      Twitch (https://help.twitch.tv/s/article/twitch-stream-key-faq?language=en_US)      LinkedIn (https://www.linkedin.com/help/linkedin/answer/a564446/go-live-using-a-custom-stream-rtmp)\r \r 3.   record  : _Bool_. If recording is required, set true. If recording is not required, set false. This value has no effect on streaming.\r \r   If both rtmpURLs and record = true are provided, both streaming and recording will begin.\r   If only rtmpURLs are provided, only streaming will begin.\r   If only record true is provided, only recording will begin.\r \r If either one is started, the other can't be started without first stopping whatever is running.\r Eg: Only streaming is started. Recording can't be started unless streaming is stopped first.\r \r If both are required, they have to be started together by providing both RTMP ingest Urls and recording = true.\r \r The result of the action is sent in the completion handler ((Bool, HMSError?) -> Void)? . When attempt to start streaming/recording is successful then Bool will be true & HMSError will be nil. In case of failure, Bool will be false & a HMSError object will be passed with relevant error information.\r \r  swift\r let config = HMSRTMPConfig(meetingURL: < meetingURL >, rtmpURLs: < rtmpURLs >, record: < recordingEnabled >)\r hmsSDK?.startRTMPOrRecording(config: config)   didStart, error in\r   print(didStart, error)\r  \r  \r \r  Stopping Streaming / Recording\r \r To stop streaming AND recording. It is not currently possible to stop just one, whatever is running will be stopped.\r \r Here's how to stop both:\r \r The stopRTMPAndRecording only has a completion handler. When attempt to stop is successful then Bool will be true & HMSError will be nil. In case of failure to stop streaming AND recording, Bool will be false & a HMSError object will be passed with relevant error information.\r \r  swift\r hmsSDK?.stopRTMPAndRecording()   didStop, error in\r  \r  \r \r  Current Room Status\r \r The current status for the room is always reflected in the HMSRoom object that is returned from the HMSUpdateListener .\r \r Here are the relevant properties inside the HMSRoom object which you can read to get the current recording/streaming status of the room namely: browserRecordingState , serverRecordingState and rtmpStreamingState .\r \r Each of them are objects which contain a boolean running which lets you know if it's active on the room right now and error which lets you know if there was an error.\r \r Apart from the RTMP stream and the browser recording, which are ones you can start and stop, there is also a serverRecordingState , which can be turned on for the room for archival purposes and which cannot currently be stopped if enabled for the room from the dashboard.\r \r 1.   rtmpStreamingState   an instance of HMSRTMPStreamingState , which looks like:\r \r  swift\r @objcMembers public class HMSRTMPStreamingState : NSObject  \r \r   public let running: Bool\r \r   public let error: HMSError?\r  \r  \r \r This represents a livestream to one or more RTMP urls.\r \r 2.   browserRecordingState   an instance of HMSBrowserRecordingState , which looks like:\r \r  swift\r @objcMembers public class HMSBrowserRecordingState : NSObject  \r \r   public let running: Bool\r \r   public let error: HMSError?\r  \r  \r \r This represents the recording that can be requested to start.\r \r 3.   serverRecordingState   an instance of HMSServerRecordingState , which looks like:\r \r  swift\r @objcMembers public class HMSServerRecordingState : NSObject  \r \r   public let running: Bool\r \r   public let error: HMSError?\r  \r  \r \r This represents that the room was set to be recorded when it was created and all sessions within it will always be recorded for archival by the server.\r \r The room status should be checked in following two places -\r \r 1. In the onJoin(room: HMSRoom) callback of HMSUpdateListener \r  The properties mentioned above will be on the HMSRoom object.\r 2. In the on(room: HMSRoom, update: HMSRoomUpdate) callback of HMSUpdateListener .\r  The HMSRoomUpdate type will be browserRecordingStateUpdated , serverRecordingStateUpdated or rtmpStreamingStateUpdated .\r \r   Code sample\r \r Refer to the code example within the  100ms Sample App (https://github.com/100mslive/100ms-ios-sdk/blob/ebb6dab6b16472af405cdef5d74e59aea7e90300/Example/HMSSDKExample/Meeting/MeetingViewController.swift L350) for further information.\r "
    },
    {
        "title": "Screen Sharing",
        "link": "/ios/v2/features/screen-share",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/screen-share",
        "keywords": [],
        "content": "  iOS SDK provides support for sharing the entire screen of the device in the room or you can share just your App's Screen.  Share Just Your App's Screen You can use the hmsSDK instance to start sharing your app screen like below:   swift   hmsSDK.startAppScreenCapture()   error in     // handle any errors        You can stop stop sharing your screen like below:   swift   hmsSDK.stopAppScreenCapture()   error in     // handle any errors           How to check if user is sharing their screen There are 2 approaches that we suggest for checking if user is sharing screen: 1. Check if the screen is added as a local track for the user 2. Maintain a state variable when you successfully start and stop screen-sharing  Share The Entire Device Screen To share entire device screen the SDK uses  Replaykit (https://developer.apple.com/documentation/replaykit) framework from Apple. To understand the documentation better consider familiarising yourself with live screen broadcast in iOS: https://developer.apple.com/videos/play/wwdc2018/601 Please note that for a peer to share their screen, their role must have screenshare enabled in the dashboard. Also, select the appropriate resolution that you desire the screenshare track should be of.   ScreenshareDashboard (/docs/v2/screenshare-dashboard.png)   How to start screenshare from the app You create an iOS broadcast upload extension. It uses Apple's ReplayKit framework to record the device screen and delivers frame samples to your broadcast extension. You can share not only your own app but also the entire device screen including other apps on the device. HMSBroadcastExtensionSDK is made specifically to be used in the iOS broadcast upload extension target. After importing the SDK, you use HMSScreenRenderer class to process the sample buffer coming from RPBroadcastSampleHandler. These samples are then presented in your main meeting app as a screen recording track.   Video walk-through of implementation <iframe allow=\"fullscreen;\" width=\"800\" height=\"460\" src=\"https://www.youtube.com/embed/wMGfYtytAEc\"></iframe>   How to import   Using Swift Package Manager You can use  Swift Package Manager (https://www.swift.org/package-manager) (use https://github.com/100mslive/100ms-ios-broadcast-sdk.git as the package source) Add HMSBroadcastExtensionSDK to your main app target from Swift Package Manager. Then add HMSBroadcastExtensionSDK to your iOS broadcast upload extension target.   Using Cocoapods Get the HMSBroadcastExtensionSDK via  Cocoapods (https://cocoapods.org/). Add the pod 'HMSBroadcastExtensionSDK' to your broadcast upload extension target in Podfile as follows:    ruby   Podfile   platform :ios, '12.0'   target 'MainApp' do   pod 'HMSSDK'  end  target 'HMSScreenShare' do   pod 'HMSBroadcastExtensionSDK'  end      How to use HMSBroadcastExtensionSDK to share iOS screen with other participants in the room? iOS broadcast extension provides you with CMSampleBuffers that you can process using HMSBroadcastExtensionSDK. This enabled you to share the screen with other participants in the room in your main app that uses HMSSDK.   How to create Replaykit Broadcast extension? 1. Create a Broadcast Upload Extension target that is embedded in your app project target. 2. You create a subclass of RPBroadcastSampleHandler and override the processSampleBuffer function. This gives you access to the CMSampleBuffer provided by the Replaykit framework.   How to configure your main app and extension to enable screen sharing 1. Configure an  App Group (https://developer.apple.com/documentation/xcode/configuring-app-groups) for your main app. Choose the same App Group for your broadcast extension target. 2. In your main app set the app group string to the hmssdk instance like below:   swift   hmsSDK = HMSSDK.build   sdk in       sdk.appGroup = \"group.live.100ms.videoapp\"       ...           3. In your broadcast extension import HMSBroadcastExtensionSDK   swift   import HMSBroadcastExtensionSDK     4. Create an instance of HMSScreenRenderer passing the app group string like below:     swift   class SampleHandler: RPBroadcastSampleHandler     let screenRenderer = HMSScreenRenderer(appGroup: \"group.live.100ms.videoapp\")     ...      HMSScreenRenderer uses this app group string to talk to your main app.    How to share the screen in meeting  In your Broadcast Extension, you create a subclass of RPBroadcastSampleHandler class. This class handles screen frame samples produced by Replaykit. Replaykit delivers these frames as CMSampleBuffer to processSampleBuffer function.  In your processSampleBuffer function call process function on your screenRenderer instance passing the CMSampleBuffer like below:     swift   override func processSampleBuffer(_ sampleBuffer: CMSampleBuffer, with sampleBufferType: RPSampleBufferType)       switch sampleBufferType       case RPSampleBufferType.video:       // Handle video sample buffer       screenRenderer.process(sampleBuffer)     ...           How to invalidate the connection between main app and extension when the user stops screen recording When the user stops screen sharing broadcastFinished function is called on your RPBroadcastSampleHandler class. At that point, call invalidate on your screenRenderer instance to stop screen sharing.   swift   override func broadcastFinished()       // User has requested to finish the broadcast.     self.screenRenderer.invalidate()           Stop screen sharing when the user leaves the room When a user leaves the meeting room from the main app, you will receive an error HMSScreenRendererErrorCode.noActiveMeeting when calling process on screenRenderer instance. You can handle this error to stop the broadcast extension like below:   swift   override func processSampleBuffer(_ sampleBuffer: CMSampleBuffer, with sampleBufferType: RPSampleBufferType)       switch sampleBufferType       case RPSampleBufferType.video:       // Handle video sample buffer       if let error = screenRenderer.process(sampleBuffer)           if error.code == .noActiveMeeting             finishBroadcastWithError(NSError(domain: \"ScreenShare\", code: error.code.rawValue, userInfo:  NSLocalizedFailureReasonErrorKey : \"You are not in a meeting.\" ))                       ...            How to check if user is sharing their screen There are 2 approaches that we suggest for checking if user is sharing screen: 1. Check if the screen is added as a local track for the user 2. Use the app group user defaults to store the state of screen sharing from broadcast extension (using broadcastStarted and broadcastFinished functions)   Debugging issues in your screen-share implementation 1. If you use cocoapods with M1 mac and compile for simulator then you might run into an error like this: \"error framework not found Pods_hmsscreenshare\". This issue is specific to cocoapods method of integrating the 100ms SDK. If you use swift package manager, you wouldn't run into this error. As a workaround please use a snippet similar to at the end of this example podfile to exclude arm64 arch for simulator from your pod screen share target: Workaround for M1 simulator issue https://github.com/100mslive/100ms-ios-sdk/blob/main/Example/Podfile Note: Pods-HMSScreenShare in the snippet is your cocoapods framework target for broadcast extension (it's generally Pods Your Broadcast Extension target name ) 2. If you are getting a crash while running with screen share feature, please make sure that the app group identifier is not misspelled in both the main app and the broadcast extension. Also make sure your entitlement file has the correct app group name.   Example implementation 👀 To see an Example iOS broadcast upload extension implementation for screen sharing using 100ms Broadcast extension SDK, checkout  our Example project (https://github.com/100mslive/100ms-ios-sdk/tree/main/Example). 📲 Download the 100ms fully featured Sample iOS app here: https://testflight.apple.com/join/dhUSE7N8 "
    },
    {
        "title": "Tap-to-focus (Beta)",
        "link": "/ios/v2/features/tap-to-focus",
        "platformName": "iOS",
        "objectID": "/ios/v2/features/tap-to-focus",
        "keywords": [],
        "content": "  If you have a use-case where users should be able to tap on the screen to lock the camera focus on certain objects, you can use tap-to-focus feature provided by 100ms iOS SDK.   Minimum Requirements   SDK version 0.4.7   What is Tap to Focus? If you have used native iOS camera app, you know that you can tap on the camera preview screen to lock focus on certain object in the camera scene. The focus plane of the camera is set to the depth of the object in focus. This is called 'Tap to focus'.   What are different camera focus modes There are mainly 3 focus modes:   1. AutoFocus The default focus mode where camera continuously sets focus and exposure based on center point of the scene.   2. TapToAutoFocus This allows user to tap to focus on an object. Once user moves the camera enough to change the subject of the scene the camera switches to default   Autofocus   mode   3.TapToLockFocus This allows user to tap to lock focus on an object. The focus will not be changed until user taps again to focus on another object. By default iOS camera uses   autofocus   mode. That means it constantly tries to focus on the center object in the camera scene.   How to enable Tap-to-focus mode To enable 'Tap-to-focus' mode, you pass one of the two tap-to-focus modes (  HMSCameraFocusMode  ) as a param to the HMSVideoSettings like below:   swift sdk.trackSettings = HMSTrackSettings.build   videoSettingsBuilder, audioSettingsBuilder in   videoSettingsBuilder.cameraFocusMode = .focusModeTapToAutoFocus   ...        OR    swift sdk.trackSettings = HMSTrackSettings.build   videoSettingsBuilder, audioSettingsBuilder in   videoSettingsBuilder.cameraFocusMode = .focusModeTapToLockFocus   ...      That's all you need to do. These camera modes are integrated into the   HMSVideoView   that you use to create video tiles. Now whenever user taps on the local camera view (HMSVideoView), the tapped object in the camera scene gets focused according to the focus mode that has been set. HMSCameraFocusMode has following 3 modes: 1.   kHMSFocusModeAuto   2.   kHMSFocusModeTapToAutoFocus   3.   kHMSFocusModeTapToLockFocus   If you don't use   cameraFocusMode   video setting, the   autofocus   mode is used by default. 👀 To see an example iOS tap to focus implementation using 100ms SDK, checkout  our example project (https://github.com/100mslive/100ms-ios-sdk/tree/main/Example). "
    },
    {
        "title": "Adaptive Bitrate",
        "link": "/ios/v2/foundation/adaptive-bitrate",
        "platformName": "iOS",
        "objectID": "/ios/v2/foundation/adaptive-bitrate",
        "keywords": [],
        "content": "  Adaptive bitrate (ABR) refers to features that enable dynamic adjustments to video quality—to optimise for end-user experience under diverse network conditions. ABR ensures that every participant is able to consume the highest possible quality video in conferencing or streaming use-cases, based on their bandwidth constraints.\r \r In addition to network, ABR can also optimise for the right video quality based on the size of the video element. For example, a video call running on active speaker layout has larger video tiles that require higher quality video track. These adjustments can be made dynamically with adaptive bitrate.\r \r Learn about how 100ms enables adaptive bitrate in:\r \r    Conferencing scenarios ( abr-in-conferencing)\r    Live streaming scenarios ( abr-in-live-streaming)\r \r  ABR in conferencing\r \r Peers in 100ms rooms can publish multiple video quality levels simultaneously. This is called “simulcast” in 100ms. Peers that consume these video tracks can upgrade or downgrade video quality.\r \r You can enable simulcast on the publishing role's template, and use manual or automatic layer changes on the subscriber's side.\r \r   Publisher-side configuration\r \r Simulcast configuration is opt-in and can be enabled on the role's configuration inside your template. The role's publish video quality determines video quality layers on simulcast. For example, a role configured to publish at 720p can simulcast 180p, 360p and 720p layers.\r \r  Video publish quality  Possible simulcast layers \r               \r  1080p          1080p, 540p, 270p     \r  720p          720p, 360p, 180p      \r  480p          480p, 240p         \r  360p          360p, 180p         \r  240p          240p            \r  180p          180p            \r \r   Enable via dashboard\r \r Enable \"can publish simulcast\" on the template page for a particular role. You can also specify how many video quality layers will be simultaneously published by peers of this role. The peer will publish these layers assuming network bandwidth permits.\r \r   Simulcast configuration (/docs/guides/simulcast-on-dashboard.png)\r \r   Enable via API\r \r Update role configuration using the  server-side API (/docs/server-side/v2/policy/create-update-role). The simulcast config payload can include 2 or 3 layers that scale down the selected publish layer.\r \r In the example below, the role is configured to publish 720p with 3 simulcast layers:\r \r   f for full with scale down factor of 1 (= 720p)\r   h for half with scale down factor of 2 (= 360p)\r   q for quarter with scale down factor of 4 (= 180p)   js     \"publishParams\":       ...     \"simulcast\":         \"video\":           \"layers\":                           \"rid\": \"f\",             \"scaleResolutionDownBy\": 1,             \"maxBitrate\": 700,             \"maxFramerate\": 30            ,                         \"rid\": \"h\",             \"scaleResolutionDownBy\": 2,             \"maxBitrate\": 250,             \"maxFramerate\": 30            ,                         \"rid\": \"q\",             \"scaleResolutionDownBy\": 4,             \"maxBitrate\": 100,             \"maxFramerate\": 30                              ,       \"screen\":                  \r   Subscribe-side behavior\r \r   Manual layer selection\r \r The 100ms client-side SDKs provide methods to set a preferred quality layer for a remote peer's video track. See docs for your preferred platform:\r \r    JavaScript (/docs/javascript/v2/advanced-features/simulcast)\r    iOS (/docs/ios/v2/advanced-features/simulcast)\r    Android (/docs/android/v2/advanced-features/simulcast)\r \r   Automatic layer selection\r \r <br \r <video loop=\"true\" controls=\"controls\" id=\"vid\" muted>\r   <source src=\"/docs/guides/simulcast-tile-size-1.mp4\" type=\"video/mp4\"  \r </video>\r <br \r \r  Based on video tile size: The SDK automatically ensures appropriate video layer is subscribed to, as demonstrated in the video above. For example, if the video element is 360 px in width, 360p or the closest layer will be selected.\r      JavaScript  : The useVideo hook in the 100ms React SDK auto-selects the appropriate video quality layer.\r      iOS  : HMSVideoView can auto-select video quality layer.\r      Android  : HMSVideoView ( see docs (/docs/android/v2/migrations/surfaceview-migration)) can auto-select video quality layer.\r  Based on network quality: ABR will work alongside subscribe degradation and auto-downgrade video quality for peers. This is _coming soon_.\r \r  ABR in live streaming\r \r 100ms uses the HTTP Live Streaming (HLS) protocol in live streaming scenarios. HLS supports adaptive bitrate out of the box, and HLS video players can automatically or manually pick appropriate video quality levels.\r \r Learn more on  how HLS works on our blog (https://www.100ms.live/blog/hls-101-beginners-guide).\r "
    },
    {
        "title": "Basics\r",
        "link": "/ios/v2/foundation/basics",
        "platformName": "iOS",
        "objectID": "/ios/v2/foundation/basics",
        "keywords": [],
        "content": " -\r \r  Architecture 100ms is a cloud platform that allows developers to add video and audio conferencing to Web, Android and iOS applications. The platform provides REST APIs, SDKs, and a dashboard that makes it simple to capture, distribute, record, and render live interactive audio, video. Any application built using 100ms' SDK has 2 components.     Client:   Use 100ms android, iOS, Web SDKs to manage connections, room states, render audio/video.     Server:   Use 100ms' APIs or dashboard to create rooms, setup room templates, trigger recording or RTMP streaming, access events.   Architecture (/docs/docs/v2/arch.png)   Basic Concepts   Room A room is the basic object that 100ms SDKs return on successful connection. This contains references to peers, tracks and everything you need to render a live a/v or live streaming app.   Peer A peer is the object returned by 100ms SDKs that contains all information about a user  name, role, video track etc.   Session A session depicts activity in a room. A session is created when one or more peers join a room to communicate with each other. A single room can have multiple sessions; a unique ID will be assigned to each session. The maximum allowed duration for a session on the 100ms platform is 12 hours.   Track A track is a segment of media (audio/video) captured from the peer's camera and microphone. Peers in a session publish local tracks and subscribe to remote tracks from other peers.   Role A role defines who can a peer see/hear, the quality at which they publish their video, whether they have permissions to publish video/screenshare, mute someone, change someone's role.   Template A template is a collection of roles, room settings, recording and RTMP settings (if used), that are used by the SDK to decide which geography to connect to, which tracks to return to the client, whether to turn on recording when a room is created, etc. Each room is associated with a template.   Destinations Destinations is used to save audio/video calls for offline viewing. 100ms supports 2 kinds of recording   SFU recording (/docs/javascript/v2/foundation/recordings sfu-recording-advanced) and  Browser recording (/docs/javascript/v2/foundation/recordings browser-recording-recommended). Also, HLS enabled configuration will allow you to live stream your room over HLS.   RTMP RTMP streaming is used to live stream your video conferencing apps to platforms like YouTube, Twitch, Facebook, MUX, etc.   Webhooks Webhook is an HTTP(S) endpoint used for pushing the notifications to your application. It will be invoked by 100ms servers to notify events of your room.   Workspace A workspace is an isolated environment which contains account data like templates, rooms, room and session history, etc. You can use workspaces to represent environments like “Production” and “Development” and invite team members to a workspace.   What are the steps to build a live app with 100ms? 1. Sign up on 100ms using the   Try For Free   button in the top navbar.   Signup for 100ms account (/docs/docs/v2/signup.png) 2. Once you're logged in to the dashboard, click on Create Your First App    Signup for 100ms account (/docs/docs/v2/create-your-first-app.png) 3.   Hover   on one of the Starter Kits. Deploy one of them. (We will use the   Video Conference Starter Kit   for this example)   Dashboard _ 100ms.png (/docs/docs/v2/select-starter-kit.png) 4. Select your account type and fill in the details   Dashboard _ 100ms.png (/docs/docs/v2/personal-details.png) 5. Choose a deployment option. This could be 100ms or Vercel (based on the Starter Kit you are deploying)   Video Conferencing Starter Kit (/docs/docs/v2/choose-your-deployment.png) 6. Enter a subdomain of your choice. Please avoid entering https/http/www or dots while entering the subdomain. Select a region closest to you and hit Continue.   choose subdomain (/docs/docs/v2/choose-subdomain.png) 7. Join or Invite someone to your deployed app with one of the roles:    join or invite (/docs/docs/v2/demo-your-app.png)   Where should I start? \r \r   Quickstart\r \r If you want to see 100ms' SDKs in action in under 5 minutes, run one of our quickstart  apps (/ios/v2/guides/quickstart)\r "
    },
    {
        "title": "Handling audio-video edge cases\r",
        "link": "/ios/v2/foundation/handling-audio-video-edge-cases",
        "platformName": "iOS",
        "objectID": "/ios/v2/foundation/handling-audio-video-edge-cases",
        "keywords": [],
        "content": " -\r \r  Introduction 100ms handles a lot of standard audio/video issues internally without the developer needing to handle it explicitly. This page describes some common issues and how 100ms handles them. There are 3 major issues of issues that can occur in a audio/video conference  Device capture exceptions  Network disconnection/switching network exceptions  Network bandwidth limitation/large room exceptions   Device failure A common issue is a failure to capture mic/camera even though the user has all devices connected. Common causes include differences in OS/browser implementations of device capture APIs, permission not being granted by the user, or the device being in use by another program. The usual recourse in these exceptions is to prompt a user action  \"Grant permission\", \"Please close any other app using microphone\", \"Switch to Safari\" 100ms' SDKs come with a  preview method (../features/preview) that can be called before joining a room. This will test for device failures, network connectivity and throw errors with a recommended user action.   Network disconnection/Switching networks Another set of common issues are minor network blips. Common causes are when a user moves from one room to another, or switches from wifi to data. 100ms will send a notification within 10s of detecting a network disconnection and will automatically retry when connection is available upto 60s. After 60s, a terminal error is thrown to the client.   Network bandwidth limitation/large rooms A common occurrence in large rooms, or constrained networks is dropped frames. This results in robotic voices, frozen frames, pixelated screenshare or entire pieces of audio/video that are lost. 100ms will automatically prioritize connections if network limits are reached. This prioritization can be controlled by developers using the dashboard or 100ms APIs. eg. A developer can prioritize host's screenshare higher than guests' videos. In low bandwidth constraints, guests' videos will be turned off, while host's screenshare will remain. \r \r \r "
    },
    {
        "title": "Interactive Live Streaming",
        "link": "/ios/v2/foundation/live-streaming",
        "platformName": "iOS",
        "objectID": "/ios/v2/foundation/live-streaming",
        "keywords": [],
        "content": "    Overview Live video interactions can span various levels of interactivity. For example, in a virtual event, some participants can be on stage talking to each other, whereas participants in the audience can be listening to them. At 100ms, we think of this as the   3 levels of interactivity  . 100ms enables you to build live video use-cases by mixing and matching these 3 levels to get to your ideal solution.     Level 1  : Full duplex audio/video in real-time   Level 1 participants publish their audio/video, and interact with others in sub-second latency. This is real-time video conferencing, similar to Zoom or Google Meet.     Level 2  : Webinar-style audience in real-time   Level 2 participants consume audio/video from level 1 participants with sub-second latency, without publishing their own audio/video. Level 2 participants can engage with level 1 through messaging (chat, emojis, custom events). This is similar to a Zoom webinar.   Levels 1 and 2 are enabled using WebRTC.     Level 3  : Live stream audience consuming in near real-time   Level 3 participants consume a composite live stream in near real-time (<10 secs of latency) without publishing their audio/video. They can interact with other participants via messaging. This is similar to viewers on Twitch or YouTube Live, and is enabled via   100ms Interactive Live Streaming  .   Live streaming uses  HLS (https://www.100ms.live/blog/hls-101-beginners-guide) to achieve near real-time latency at scale. The  roles primitive (/docs/javascript/v2/foundation/templates-and-roles) can be used to define capabilities of a participant and associate them to an interaction level. A participant can move between levels using a single API call to change roles.   Try Interactive Live Streaming Use our  Live Streaming Starter Kit (https://www.100ms.live/marketplace/live-streaming-starter-kit) to try out the experience before you write a line of code. <StepsToc  parentId=\"try-live-streaming\"  descriptions=  \"Use our Live streaming starter kit to try out the experience before you write a line of code.\", \"Understand the difference between a stream broadcaster and stream viewer.\", \"Use the demo app link to go live for the first time as a broadcaster and join the stream as viewer.\", \"Use the 100ms self-serve dashboard to update the layout, aspect ratio, etc of the stream.\"   > <StepsContainer id=\"try-live-streaming\">   Create a new app   Live streaming starter kit (/docs/docs/v2/live-streaming-starter-kit.png) 1. Make sure that you have  an account with 100ms (https://dashboard.100ms.live/register) and can access the  100ms dashboard (https://dashboard.100ms.live/) 1. On the dashboard, create a new app using the Live Streaming Starter Kit 1. Specify a subdomain and region to deploy the app   Understand roles   Live Streaming roles (/docs/docs/v2/live-streaming-roles.png) This starter kit configures your new app with  two roles (/docs/javascript/v2/foundation/templates-and-roles):   broadcaster : This role represents a streamer who publishes their audio/video. There can be multiple peers who join as broadcasters   hls-viewer : This role represents a circle 3 audience, who subscribes to the composite live stream and can interact using messaging   Go live   Go live (/docs/docs/v2/live-streaming-go-live.gif) 1. To go live for the first time, join the room as a broadcaster and start the live stream 2. Once the stream has started, join the room as an hls-viewer and you should be able to see the ongoing live stream 3. Use chat messages to interact between the viewer and the broadcaster   Customize the stream   Go live (/docs/docs/v2/live-streaming-customise.png) By default, the live stream is composed in landscape mode for desktop viewers (with an aspect ratio of 16:9). You can customise the live stream for viewers on mobile or to support multiple broadcaster tiles. 1. On the 100ms dashboard, click the gear icon on your app to open configuration settings 2. Go to \"destinations\" and scroll down to find live stream (HLS) configuration 3. Update the configuration based on your needs:    If your viewers are on mobile, change the video aspect ratio to 9:16    If you have multiple broadcasters joining in, choose grid or active speaker based on your needs    In case of grid layout, choose the tile size that fits your use-case. For example, a stream with 2 streamers looks better with 1:1 tiles. </StepsContainer>   Integrate in your app To integrate 100ms Interactive Live Streaming in your app, follow these steps: 1.  Enable live streaming destination ( enable-destination) 2.  Integrate the 100ms SDK in your app ( sdk-integration) 3.  Integrate live stream playback ( live-stream-playback) 4.  Video on demand (VOD) use cases ( video-on-demand-vod-use-cases)   Enable destination   Enable HLS (/docs/docs/v2/live-streaming-enable.gif) If your app is based on the Live Streaming Starter Kit (as shown above), the live streaming destination is enabled out-of-the-box. For custom apps, you can enable the live streaming destination manually: 1. Open configuration for your existing app using the 100ms dashboard 1. In the \"destinations\" tab, enable \"Live Streaming with HLS\" 1. Ensure that you have roles for the broadcaster (who can publish their audio/video) and the viewer (who cannot publish audio/video)   SDK integration Use the 100ms client-side SDKs to integrate streaming in your application. See code snippets for the client-side SDK  here (/docs/javascript/v2/features/hls).   Live stream playback Using our client-side SDKs, you can enable live stream playback and add interactive experiences like chat, raise hand and other functionalities to your app using  peer metadata (/docs/javascript/v2/advanced-features/peer-metadata). The process is so simple: 1. Once you  start ( step-3-go-live) live streaming, you will get an HLS URL (M3U8 URL) which you can use for playback. 2. You can use the  client-side SDK (/docs/javascript/v2/features/hls) to get the HLS URL by checking the  current state (/docs/javascript/v2/features/hls current-room-status) of the room and start playback. If you need to only enable HLS playback and don't need interactivity, you can follow one of the below approaches to get the HLS URL:     Webhook:   You can listen to hls.started.success  webhook event (/docs/server-side/v2/introduction/webhook hls-started-success) and get the HLS URL from the url field. Please check the  webhooks guide (/docs/server-side/v2/introduction/webhook) to learn more about webhooks.     Static URL:   This configuration will help you get a static URL for playback. You can enable the Static playback URLs in your template from the  dashboard (https://dashboard.100ms.live/dashboard). You can go to Destination > enable \"Live streaming with HLS\" > under \"Customise stream video output\" section > enable \"Static playback URLs.\"     Enable Static URL (/docs/docs/v2/enable-static-url.png)     _Format_: https://cdn.100ms.live/beam/<customer_id>/<room_id>/master.m3u8     customer_id : replace this placeholder with your customer_id from  developer section (https://dashboard.100ms.live/developer) on your dashboard.     room_id : replace this placeholder with the room_id of the respective room from which the stream will be broadcasted.   Video on Demand (VOD) use cases If you wish to replay your HLS stream for Video on demand (VOD) use case, 100ms provides the capability to record the HLS stream which will be posted to your webhook as a ZIP file of M3U8 format (same playback format as HLS) with all the chunks once the stream ends. You can start recording a live stream using the  client-side SDK (/docs/javascript/v2/features/hls) or using the  server API (/docs/server-side/v2/Destinations/rtmp-streaming-and-browser-recording start-streaming-recording). Once the HLS recording is completed, you will get the details of recording as a callback to the webhook configured in your account. Check hls.recording.success  webhook event (/docs/server-side/v2/introduction/webhook hls-recording-success) for more information. "
    },
    {
        "title": "Recordings",
        "link": "/ios/v2/foundation/recordings",
        "platformName": "iOS",
        "objectID": "/ios/v2/foundation/recordings",
        "keywords": [],
        "content": "  Recordings are an important part of the live video stack as they convert live, ephemeral content into a long-term asset. But the use of this asset varies from business to business depending on their respective use case. For example, one of the common use cases for recording is for archival purposes versus, for some, its content to be publicized. Based on your end goal, you can choose one of the recording types and its implementation. You can understand some key differences using the comparison table below.   Recording types   Recording types ( recording-types)    Quick Comparison ( quick-comparison)    Browser Recording  Recommended   ( browser-recording-recommended)    SFU Recording  Advanced   ( sfu-recording-advanced)    Recordings for Live Streaming Use-cases ( recordings-for-live-streaming-use-cases)     Video-on-demand Recording ( video-on-demand-recording)     Multiresolution Recording ( multiresolution-recording)   Configure storage ( configure-storage)    How to configure recording storage? ( how-to-configure-recording-storage)   Quick Comparison  Recording Features           Browser Recording  Recommended   SFU Recording  Advanced                                      Resolution               Upto 1080p            Only 720p              Participant-level Audio/Video Tracks  Not Available           Available              Portrait/Landscape Mode        Available             Not Available            Start/Stop Recording          On-demand             Auto start/stop with the session   Custom Layout             Available             Not Available            Role-Specific Recording        Available             Not Available            Recording Output            MP4                MP4, WebM                Browser Recording  Recommended  Browser recording is built to give users a participant-first recording experience. When enabled, our browser-based bot Beam joins a room to record the viewport like any other participant. The output is an MP4 file that captures the room's published audio/video tracks together into one single file. This option removes the complexity of syncing various audio/video tracks and offers an intuitive, participant-first recording experience. An example use case is to record a sales meeting for later usage.   Resources     How to implement Browser Recording (https://www.100ms.live/docs/server-side/v2/Destinations/rtmp-streaming-and-browser-recording)   SFU Recording  Advanced  SFU recording is built for advanced use cases, which require individual audio and video tracks for each participant. This recording option allows you to isolate recording at a participant level. Track recording allows you to record audio and video streams separately, making it easier to edit, layer, or reuse each of them. An example use case is to record a live podcast and later edit it for publishing. You can get track recordings in two forms:   Individual: Media for each peer is provided as a separate mp4 file. This file will have both audio and video of the peer. These files can be used for offline review or in implementing custom composition.   Composite  currently in beta : Audio and video of all peers are composed as per their joining/leaving the meeting and provided as a single mp4. This file can be used for offline viewing of the meeting.   Resources      How to implement SFU Recording (https://www.100ms.live/docs/server-side/v2/Destinations/recording)   Recordings for Live Streaming Use-cases These are the types of live streaming recordings:    Video-on-demand Recording Video-on-demand recording is available for our Interactive Live Streaming capability. This recording will be a file with an M3U8 file (same playback format as HLS), which can be used for replaying your HLS stream. This option is more suitable for Video-on-Demand use cases. For the implementation of this type of recording, please  contact us (https://www.100ms.live/contact).    Multiresolution Recording A multi-resolution recording is available for Interactive Live Streaming capability. This type of recording will have a multi-file structure for all available resolutions of the stream. The output will be multiple MP4 files with these resolutions: 240p, 480p, 720p, and 1080p. For an implementation of this type of recording, please  contact us (https://www.100ms.live/contact).   Configure storage You can specify a cloud storage location for your recording files in your template. Our current offering allows you to store your recordings in Amazon S3 buckets. Once you configure the S3 config of your bucket in a template, all respective recordings of sessions created via those templates will be sent to your configured bucket. This holds true for all types of aforementioned recordings.   How to configure recording storage? 1. Generate your credentials; for this example, you can check out a  guide from AWS (https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html). You can skip this step if you already have credentials. Please note that if you are running a Browser recording, you need to give upload permission to your key, but if you are running an SFU recording, you need to give both upload and download permission. 2. Go to 100ms Dashboard and go to template   configuration by selecting the configure icon  .   Create your first app (/docs/docs/v2/recording-storage-settings-step2.png) 3. Head over to the   Destinations   tab.   Destinations (/docs/docs/v2/recording-storage-settings-step3.png) 1. Key in your credentials (using an example of an S3 bucket here):    Access Key: Access Key generated from AWS IAM Console    Secret Key: Secret Key generated from AWS IAM Console    Bucket: Name of the bucket in S3    Region: Name of the region, for example, ap-south1    Prefix for Upload Path: Define the directory name (optional)   Destinations (/docs/docs/v2/recording-storage-settings-step4.png) 5. Use the   Validate Config   button to test your storage setup.   Destinations (/docs/docs/v2/recording-storage-settings-step5.png) 6. You will see a message that the AWS   configuration was successfully validated  .   Destinations (/docs/docs/v2/recording-storage-settings-step6.png) The above message ensures that your configuration is successful now, and all your recordings will start collecting in your configured destination. "
    },
    {
        "title": "Authentication and Tokens",
        "link": "/ios/v2/foundation/security-and-tokens",
        "platformName": "iOS",
        "objectID": "/ios/v2/foundation/security-and-tokens",
        "keywords": [],
        "content": "    Introduction In 100ms, two types of tokens are used to authenticate requests coming from your Client apps and Backend application server into the 100ms platform.    App token ( app-token) : Used to authenticate and allow end-users (peers) to join 100ms rooms. An App Token controls Peer identity and Room permissions in your real-time or Interactive live-streaming video application.    Management token ( management-token) : Used to authenticate all the requests to 100ms REST API. You can set the expiry to both these tokens; if you follow the code samples from this guide, the expiry will be set as 24 hours. However, a best practice is to set the expiry as short as feasible for your application. You must host your server to generate these tokens while transitioning your app to production.   App Token 100ms _client-side SDKs_ use App Tokens to authenticate a peer (participant) while  joining a room (./../features/join). Generate this token on the server side and make it available for your client-side apps that use the 100ms SDKs. To create an App Token, you need to use app_access_key , app_secret , room_id , and user_id .   You can get the app_access_key and app_secret from the  developer section (https://dashboard.100ms.live/developer) in your 100ms dashboard. This key and secret differ across workspaces so please ensure you are in the intended workspace before copying these credentials.     room_id  : This is the unique identifier for your room. You can get it from the  rooms page (https://dashboard.100ms.live/rooms) in your dashboard or in the response payload of the  create room server-side API (/docs/server-side/v2/Rooms/create-via-api).     user_id  : This identifier can be used to map a 100ms peer to your own internal user object for business logic. Specify your internal user identifier as the peer's user_id. If not available, use any random string.   How to use? You can get App tokens using a couple of approaches based on your app's lifecycle stage. Please check the below sections for more information:    Set up your own authentication endpoint (./../foundation/security-and-tokens set-up-your-own-authentication-endpoint)    Get app tokens from the token endpoint (./../foundation/security-and-tokens get-app-tokens-from-the-token-endpoint)    Get app tokens from the dashboard (./../foundation/security-and-tokens get-app-tokens-from-the-dashboard)    Set up your own authentication endpoint When you have completed your integration with 100ms, and while transitioning your app to production, we recommend you create your backend service for app token generation; use the code snippet below and set up the token generation service as per your preferred programming language.    Code sample: Generate app token  s id=\"client-code-token\" items=  'Node.js', 'Python', 'Java', 'Ruby', 'PHP'       id='client-code-token-0'>   javascript var jwt = require('jsonwebtoken'); var uuid4 = require('uuid4'); var app_access_key = '<app_access_key>'; var app_secret = '<app_secret>'; var payload =     access_key: app_access_key,   room_id: '<room_id>',   user_id: '<user_id>',   role: '<role>',   type: 'app',   version: 2,   iat: Math.floor(Date.now() / 1000),   nbf: Math.floor(Date.now() / 1000)  ; jwt.sign(   payload,   app_secret,         algorithm: 'HS256',     expiresIn: '24h',     jwtid: uuid4()    ,   function (err, token)       console.log(token);     );   </Tab>   id='client-code-token-1'>   python  /usr/bin/env python3 import jwt import uuid import datetime import sys app_access_key = \"<app_access_key>\" app_secret = \"<app_secret>\" def generate(room_id, user_id, role):   expires = expires or 24   3600   now = datetime.datetime.utcnow()   exp = now+ datetime.timedelta(seconds=expires)   return jwt.encode(payload=          \"access_key\": app_access_key,         \"type\":\"app\",         \"version\":2,         \"room_id\": room_id,         \"user_id\": user_id,         \"role\":role,         \"jti\": str(uuid.uuid4()),         \"exp\": exp,         \"iat\": now,         \"nbf\": now,          , key=app_secret) if __name__ == \"__main__\":   if len(sys.argv) == 3:     room_id = sys.argv 0      user_id = sys.argv 1      role = sys.argv 2    print(generate(room_id=room_id, user_id=user_id, role=role))   </Tab>   id='client-code-token-2' >   java import java.time.Instant; import java.util.Date; import java.util.HashMap; import java.util.Map; import java.util.UUID; import io.jsonwebtoken.Jwts; import io.jsonwebtoken.SignatureAlgorithm; private void generateHmsClientToken()     Map<String, Object> payload = new HashMap<>();   payload.put(\"access_key\", \"<app_access_key>\");   payload.put(\"room_id\", \"<room_id>\");   payload.put(\"user_id\", \"<user_id>\");   payload.put(\"role\", \"<role>\");   payload.put(\"type\", \"app\");   payload.put(\"version\", 2);   String token = Jwts.builder().setClaims(payload).setId(UUID.randomUUID().toString())     .setExpiration(new Date(System.currentTimeMillis() + 86400   1000))     .setIssuedAt(Date.from(Instant.ofEpochMilli(System.currentTimeMillis()  60000)))     .setNotBefore(new Date(System.currentTimeMillis()))     .signWith(SignatureAlgorithm.HS256, \"<app_secret>\".getBytes()).compact();      </Tab>   id='client-code-token-3' >   ruby require 'jwt' require 'securerandom'  $app_access_key = \"<app_access_key>\" $app_secret = \"app_secret\" def generateAppToken(room_id, user_id, role)   now = Time.now   exp = now + 86400   payload =       access_key: $app_access_key,     room_id: room_id,     user_id: user_id,     role: role,     type: \"app\",     jti: SecureRandom.uuid,     version: 2,     iat: now.to_i,     nbf: now.to_i,     exp: exp.to_i       token = JWT.encode(payload, $app_secret, 'HS256') end puts generateAppToken \"<room_id>\", \"<user_id>\", \"<role>\"    </Tab>   id='client-code-token-4'>   php <?php use Firebase JWT JWT; use Ramsey Uuid Uuid; $issuedAt = new DateTimeImmutable(); $expire  = $issuedAt->modify('+24 hours')->getTimestamp(); $accessKey = \"<app_access_key>\"; $secret = \"<app_secret>\"; $version  = 2; $type   = \"app\"; $role   = \"<role>\"; $roomId  = \"<room_id>\"; $userId  = \"<user_id>\"; $payload =     'iat' => $issuedAt->getTimestamp(),   'nbf' => $issuedAt->getTimestamp(),   'exp' => $expire,   'access_key' => $accessKey,   'type' => \"app\",   'jti' => Uuid::uuid4()->toString()   'version' => 2,   'role' => $role,   'room_id' => $roomId,   'user_id' => $userId  ; $token = JWT::encode(   $payload,   $secret,   'HS256' );   </Tab> <Note type=\"warning\">   We strictly advise you not to post <i>app_access_key</i> and <i>app_secret</i> publicly; if you   need to store them in <strong>Git</strong>, please change the repository from public to private.   <br     <br   You cannot use an <strong>App token</strong> to trigger server API requests. </Note>    Get app tokens from the token endpoint If you are building integration with 100ms, you can get app tokens from the 100ms token endpoint without hosting a token generation backend service. Refer to  this guide (./../guides/token-endpoint get-an-app-token-using-token-endpoint) for more information.    Get app tokens from the dashboard If you are building your first app by following one of our  quickstart guides (/docs/javascript/v2/guides/javascript-quickstart), you can get the app token directly from 100ms dashboard to join a room for the first time. Refer to  this guide (./../guides/token get-a-temporary-token-from-100ms-dashboard) for more information.   Management Token 100ms uses management tokens to authenticate REST APIs.  If you're evaluating 100ms  server APIs (/docs/server-side/v2/introduction/basics), you can use our public  Postman collection (/docs/server-side/v2/introduction/postman-guide fork-the-collection), which doesn't require you to create a management token as we've managed it using a  pre-request script (/docs/server-side/v2/introduction/postman-guide simplified-token-generation) within the collection. If you're transitioning your app to production, we recommend you create your backend service for management token generation. You must use the app_access_key and app_secret from the  developer section (https://dashboard.100ms.live/developer) in your 100ms dashboard to create the management token.    Code sample: Generate management token  s id=\"test-code\" items=  'Node.js', 'Python', 'Java', 'Ruby', 'PHP'       id='test-code-0'>   js var jwt = require('jsonwebtoken'); var uuid4 = require('uuid4'); var app_access_key = '<app_access_key>'; var app_secret = '<app_secret>'; jwt.sign(         access_key: app_access_key,     type: 'management',     version: 2,     iat: Math.floor(Date.now() / 1000),     nbf: Math.floor(Date.now() / 1000)    ,   app_secret,         algorithm: 'HS256',     expiresIn: '24h',     jwtid: uuid4()    ,   function (err, token)       console.log(token);     );   </Tab>   id='test-code-1'>   py  /usr/bin/env python3 import jwt import uuid import datetime app_access_key = '<app_access_key>' app_secret = '<app_secret>'  def generateManagementToken():   expires = 24   3600   now = datetime.datetime.utcnow()   exp = now + datetime.timedelta(seconds=expires)   return jwt.encode(payload=      'access_key': app_access_key,     'type': 'management',     'version': 2,     'jti': str(uuid.uuid4()),     'iat': now,     'exp': exp,     'nbf': now      , key=app_secret) if __name__ == '__main__':   print(generateManagementToken())   </Tab>   id=\"test-code-2\">   java import java.time.Instant; import java.util.Date; import java.util.HashMap; import java.util.Map; import java.util.UUID; import io.jsonwebtoken.Jwts; import io.jsonwebtoken.SignatureAlgorithm; private void generateManagementToken()     Map<String, Object> payload = new HashMap<>();   payload.put(\"access_key\", \"<app_access_key>\");   payload.put(\"type\", \"management\");   payload.put(\"version\", 2);   String token = Jwts.builder().setClaims(payload).setId(UUID.randomUUID().toString())     .setExpiration(new Date(System.currentTimeMillis() + 86400   1000))     .setIssuedAt(Date.from(Instant.ofEpochMilli(System.currentTimeMillis()  60000)))     .setNotBefore(new Date(System.currentTimeMillis()))     .signWith(SignatureAlgorithm.HS256, \"<app_secret>\".getBytes()).compact();      </Tab>   id=\"test-code-3\">   ruby require 'jwt' require 'securerandom' $app_access_key = \"<app_access_key>\" $app_secret = \"<app_secret>\" def generateManagementToken()   now = Time.now   exp = now + 86400   payload =     access_key: $app_access_key,   type: \"management\",   version: 2,   jti: SecureRandom.uuid,   iat: now.to_i,   nbf: now.to_i,   exp: exp.to_i   token = JWT.encode(payload, $app_secret, 'HS256') return token end puts generateManagementToken   </Tab>   id=\"test-code-4\">   php <?php use Firebase JWT JWT; use Ramsey Uuid Uuid; $app_access_key = \"<app_access_key>\"; $app_secret = \"<app_secret>\"; $issuedAt  = new DateTimeImmutable(); $expire   = $issuedAt->modify('+24 hours')->getTimestamp(); $payload =     'access_key' => $app_access_key,   'type' => 'management',   'version' => 2,   'jti' => Uuid::uuid4()->toString(),   'iat' => $issuedAt->getTimestamp(),   'nbf' => $issuedAt->getTimestamp(),   'exp' => $expire,  ; $token = JWT::encode($payload, $app_secret, 'HS256'); ?>   </Tab> <Note type=\"warning\">   We strictly advise you not to post <i>app_access_key</i> and <i>app_secret</i> publicly; if you   need to store them in <strong>Git</strong>, please change the repository from public to private.   <br     <br     You cannot authenticate room join requests from your client-side apps with a <strong>     Management token   </strong>. </Note> "
    },
    {
        "title": "Templates and Roles",
        "link": "/ios/v2/foundation/templates-and-roles",
        "platformName": "iOS",
        "objectID": "/ios/v2/foundation/templates-and-roles",
        "keywords": [],
        "content": "    Introduction Template is the blueprint of the room. It defines the settings of the room along with the behavior of users who are part of it. Room will inherit the properties from a template that you have specified while creating it. If you have not specified any template then it will pick the default template. Each template will be identified by its id or name. For example default_videoconf_7e450ffc-8ef1-4572-ab28-b32474107b89  Users can see or modify the templates by visiting  Templates on Dashboard (https://dashboard.100ms.live/templates) or via API (see below). After updating a template or some part of its like permissions, you need to rejoin or restart the session for the template updates to take place.   Template (/docs/docs/v2/template.png)   Roles Role is a collection of permissions that allows you to perform certain set of operations while being part of the room. It has the following attributes:   Name Every role has a name that should be unique inside a template. This name will be used while generating app tokens and referencing inside a template.   Priority Priority will determine the order in which the roles will be degraded. A lower number represents a higher priority.   Publish Strategies Publish strategies will be used to determine the tracks and their quality which can be published by this role.  Strategy       Description                                                                                                                                                                                                                            Can share audio    Whether the role is allowed to publish the audio track or not.                                                                                                                  Can share video    Whether the role is allowed to publish the video track or not                                                                                                                   Can share screen   Whether the role is allowed to do screen share or not                                                                                                                       Video quality     Quality of the video track which is going to be published by the role. Currently, 6 video qualities 1080p , 720p , 480p , 360p , 240p and 120p are predefined and the user can select one out of these values. This option will be visible only if the   Can share video   is enabled.   Screenshare quality  Quality of the screen which is going to be shared by the role. Currently, 2 video qualities 720p and 1080p are predefined and the user can select one out of these values. This option will be visible only if the   Can share screen   is enabled.                       Subscribe Strategies Subscribe strategies will be used to determine what all roles, this role can subscribe to.  Strategy        Description                                                                                                                                                                                                      Subscribe to      You can select all the roles of the template which this role will subscribe                                                                                             Subscribe Degradation  When this flag is turned on, one or more remote video tracks will be muted automatically when the network condition worsens. Such tracks will be marked as degraded . When the network condition improves, the degraded tracks will automatically be unmuted.    Permissions Permissions will contain a list of additional privileges that this role will have.  Permission               Description                                                                                                                                              Can change any participant's role   With this permission, user will be able to change the role of the other participant's who are present in the room                                  Can mute any participant        With this permission, user will be able to mute any participant's audio and/or video.                                                Can ask participant to unmute     With this permission, user will be able to ask any participant to unmute their audio and/or video.                                         Can remove participant from the room  With this permission, user will be able to remove any participant from the current session of the room.                                       Can end current session of the room  With this permission, user will be able to end the current session of the room.                                                   Can receive room state         With this permission, user will be able to receive room state like peer-count and peer-list on the preview screen.                                 Can start/stop RTMP livestream     With this permission, user will be able to publish live audio/video livestream externally to social media and custom platforms (e.g Youtube/Facebook/Twitter).           Can start/stop HLS livestream     With this permission, user will be able to publish audio/video livestream in the HLS format.                                            Can start/stop Beam Recording     With this permission, user will be able to record meeting/livestream via the browser recording approach where a bot will join the room and record the meeting/livestream as is.    Advanced Settings As the name suggests, Advanced Settings section contains more settings and controls for the advanced user.   Template (/docs/docs/v2/advanced-settings.png)   Preview room state Preview room state enables you to build a \"preview\" screen which shows the state of the room before joining. This room state includes a list of peers, which can be used to show who is in the room. Preview room state also includes recording and streaming state. Preview room state settings define strategy of sending state updates to client SDKs.  Setting                  Description                                                                                                                                                                                                 Room-state Message Interval (in seconds)  Room-state data will be sent over a regular interval of these many seconds. Consequently, the room state displayed on the preview screen will refresh accordingly. This value must be a multiple of 5, between 5 and 3600 seconds, both inclusive.   Send Peer List in Room-state        Enabling this will send peer-list info of the room. If disabled, only the peer count is sent.                                                                             Enable Room-State             If enabled, room-state data will be sent to the preview screen. If disabled, no such room-state data will be sent.                                                                   Roles with room-state permission      This is the list of all the roles which will get the room-state data. You can also individually toggle these settings in the Roles tab under the Permissions section.                                           API reference Apart from the dashboard, a programmatic way to interact with templates is via  API (/server-side/v2/policy/template-object). "
    },
    {
        "title": "100ms SDK iOS Quickstart",
        "link": "/ios/v2/guides/quickstart",
        "platformName": "iOS",
        "objectID": "/ios/v2/guides/quickstart",
        "keywords": [],
        "content": "devTime: 15mins author: Dmitry Fedoseyev date: 2-8-2021 image: /guides/audio-room.png tags:    swift    ios    Prerequisites Familiarity with Xcode and iOS SDK, Cocoapods installed   Xcode Project Setup Create an Xcode project and select \"iOS\" for platform and \"App\" for application   Xcode (/guides/xcode1.png) Select \"Storyboard\" for interface and \"Swift\" for language Assuming your project name is \"basicvideocall\", create \"Podfile\" in the project folder with the following contents then run pod install   jsx section=XcodeProjectSetup sectionIndex=1 platform :ios, '13.0' target 'basicvideocall' do  use_frameworks   pod 'HMSSDK' end   Open .xcworkspace Add the entitlements for video, audio and network access to your Info.plist   xml section=XcodeProjectSetup sectionIndex=2 <key>NSCameraUsageDescription</key> <string>Please allow access to Camera to enable video conferencing.</string> <key>NSLocalNetworkUsageDescription</key> <string>Please allow access to network usage to enable video conferencing.</string> <key>NSMicrophoneUsageDescription</key> <string>Please allow access to Microphone to enable video conferencing.</string>     Initialising The SDK Open ViewController.swift  Add HMSSDK import   swift section=InitializingTheSDK sectionIndex=1 import HMSSDK   Conform to HMSUpdateListener and add stubs when Xcode offers to   swift section=InitializingTheSDK sectionIndex=2 extension ViewController: HMSUpdateListener       Create an instance of HMSSDK   swift section=InitializingTheSDK sectionIndex=3 class ViewController: UIViewController   var hmsSDK = HMSSDK.build() ...       Get Auth Token Before we proceed we need to obtain a room id and a token. In case you are not sure how to do this here is a quick guide:  Token Setup Guide (token)   Join Room Alright with the token and room id we are ready to proceed. Add joinRoom() function with room id and token you obtained in previous steps. Then call joinRoom() from viewDidLoad()   swift section=JoinRoom sectionIndex=1 func joinRoom()     let config = HMSConfig(userName: \"John Doe\", authToken: \"replace with token\")   hmsSDK.join(config: config, delegate: self)   override func viewDidLoad()     super.viewDidLoad()   joinRoom()     Congratulations  You have an audio conference app. Build and launch on device, then join same room on web app to try a call between web and iOS. (To join from the web, use one of the role url links from your room  Room in Dashboard (https://dashboard.100ms.live/rooms))   Adding Video Lets create and add a lazily initialized UIStackView that will hold our video views   swift 4-24  section=AddingVideo sectionIndex=1 class ViewController: UIViewController     var hmsSDK = HMSSDK.build()   lazy var stackView: UIStackView =       let result = UIStackView()     result.axis = .vertical     view.addSubview(result)     result.translatesAutoresizingMaskIntoConstraints = false     result.leadingAnchor.constraint(equalTo: view.leadingAnchor).isActive = true     result.trailingAnchor.constraint(equalTo: view.trailingAnchor).isActive = true     result.topAnchor.constraint(equalTo: view.topAnchor).isActive = true     let heightConstraint = result.heightAnchor.constraint(equalToConstant: 0)     heightConstraint.isActive = true     heightConstraint.priority = .defaultLow     return result    ()   Next step is to listen for trackAdded update so that we get notified when someone has published a video track. In the handler we will create an instance of HMSVideoView that allows us to render the HMSVideoTrack   swift section=AddingVideo sectionIndex=2 extension ViewController: HMSUpdateListener     ...   func on(track: HMSTrack, update: HMSTrackUpdate, for peer: HMSPeer)       switch update       case .trackAdded:       if let videoTrack = track as? HMSVideoTrack           addVideoView(for: videoTrack)             default:       break             func addVideoView(for track: HMSVideoTrack)       let videoView = HMSVideoView()     videoView.translatesAutoresizingMaskIntoConstraints = false     videoView.setVideoTrack(track)     videoView.heightAnchor.constraint(equalTo: videoView.widthAnchor, multiplier: 9.0/16.0).isActive = true     stackView.addArrangedSubview(videoView)       Build and run the app. Congratulations you have an a/v call running    Cleanup A peer can decide to stop publishing any of his tracks at any time (most frequent case is start/stop screen share), also peer may choose to leave a room. In any of these events we want to remove the corresponding video view to release resources. To start we will introduce a map of track to video view so that we can figure out which video view to remove   swift 4  section=Cleanup sectionIndex=1 class ViewController: UIViewController     var hmsSDK = HMSSDK.build()   var trackViewMap =  HMSTrack: HMSVideoView () ...   Next we want to add a map entry as a last step in our addVideoView function   jsx section=Cleanup sectionIndex=2 func addVideoView(for track: HMSVideoTrack)     let videoView = HMSVideoView()   videoView.translatesAutoresizingMaskIntoConstraints = false   videoView.setVideoTrack(track)   videoView.heightAnchor.constraint(equalTo: videoView.widthAnchor, multiplier: 9.0/16.0).isActive = true   stackView.addArrangedSubview(videoView)   trackViewMap track  = videoView     Now we will create a helper function that will remove the view for given track   swift section=Cleanup sectionIndex=3 func removeVideoView(for track: HMSVideoTrack)     trackViewMap track ?.removeFromSuperview()     With this we are ready to add handlers for trackRemoved and peerLeft events as follows:   swift section=Cleanup sectionIndex=4 func on(peer: HMSPeer, update: HMSPeerUpdate)     switch update     case .peerLeft:     if let videoTrack = peer.videoTrack         removeVideoView(for: videoTrack)         default:     break       func on(track: HMSTrack, update: HMSTrackUpdate, for peer: HMSPeer)     switch update     case .trackAdded:     if let videoTrack = track as? HMSVideoTrack         addVideoView(for: videoTrack)         case .trackRemoved:     if let videoTrack = track as? HMSVideoTrack         removeVideoView(for: videoTrack)         default:     break         And that's how you handle most common use case with the 100ms SDK    Final Points To control mute/unmute state of local video and audio tracks use   swift section=FinalPoints sectionIndex=1 hmsSDK.localPeer?.localAudioTrack()?.setMute(true) hmsSDK.localPeer?.localVideoTrack()?.setMute(true)   After you are done with the call it is a good idea to call   swift section=FinalPoints sectionIndex=2 hmsSDK.leave()     Ways to go from here Checkout complete project code on github:  https://github.com/100mslive/100ms-ios-sdk/tree/main/BasicExample (https://github.com/100mslive/100ms-ios-sdk/tree/main/BasicExample) Checkout a sample code for a full featured conferencing app:  https://github.com/100mslive/100ms-ios-sdk/tree/main/Example (https://github.com/100mslive/100ms-ios-sdk/tree/main/Example) "
    },
    {
        "title": "Get Token Using Token Endpoint",
        "link": "/ios/v2/guides/token-endpoint",
        "platformName": "iOS",
        "objectID": "/ios/v2/guides/token-endpoint",
        "keywords": [],
        "content": "    Overview 100ms provides an option to get App Tokens without setting up a token generation backend service to simplify your integration journey while testing the  sample app (https://github.com/100mslive/100ms-web) or building integration with 100ms. You can find the token endpoint from the  developer page (https://dashboard.100ms.live/developer) in your 100ms dashboard.   Token endpoint (/guides/token-endpoint-dashboard.png) We recommend you move to your token generation service before you transition your app to production, as our token endpoint service will not scale in production. The \"Sample Apps\" built using 100ms client SDKs require an App Token to join a room to initiate a video conferencing or live streaming session. Please check the  Authentication and Tokens guide (./../foundation/security-and-tokens) Please note that you cannot use the token endpoint to create a Management Token for server APIs. Refer to the  Management Token section (./../foundation/security-and-tokens management-token) in Authentication and Tokens guide for more information.   Get an app token using token endpoint You can use the token endpoint from your 100ms dashboard while building integration with 100ms. This acts as a tool enabling front-end developers to complete the integration without depending on the backend developers to set up a token generation backend service.   URL format:   <YOUR_TOKEN_ENDPOINT>api/token  100ms token endpoint can generate an app token with the inputs passed, such as room_id, role, & user_id (optional  your internal user identifier as the peer's user_id). You can use  jwt.io (https://jwt.io/) to validate whether the app token contains the same input values. <PostRequest title=\"https://prod-in2.100ms.live/hmsapi/johndoe.app.100ms.live/api/token\"   <Request id=\"req-comp-0\">   bash curl location request POST 'https://prod-in2.100ms.live/hmsapi/johndoe.app.100ms.live/api/token'  header 'Content-Type: application/json'  data-raw '    \"room_id\":\"633fcdd84208780bf665346a\",   \"role\":\"host\",   \"user_id\":\"1234\"  '   </Request> <ResponseBox id=\"resp-0\" status=\"200 OK\">   json     \"token\": \"eyJ0eXAiOiJKV1QiLCJhbGciOi                           R3tT-Yk\",   \"msg\": \"token generated successfully\",   \"status\": 200,   \"success\": true,   \"api_version\": \"2.0.192\"     </ResponseBox>   Example client-side implementation You can directly add this to your client-side implementation, check our  sample app (https://github.com/100mslive/100ms-ios-sdk/blob/23bd356ad03cb7369e16451a41d5923c6ceaf109/Example/HMSSDKExample/Meeting/RoomService.swift L40) for reference.   Disable 100ms token endpoint Due to some security concerns, if you don't wish to use the token endpoint to generate app tokens, then you can disable it on the  Developers page (https://dashboard.100ms.live/developer) on your dashboard by disabling the option \"Disable &lt;room_id&gt;/&lt;role&gt; link format.\"   Disable Token endpoint (/guides/disable-token-endpoint.png)    Error Response Once you're disabled it on the dashboard, the requests to create an app token using the 100ms token endpoint will throw the below error:   json     \"success\": false,   \"msg\": \"Generating token using the room_id and role is disabled.\",   \"api_version\": \"2.0.192\"     "
    },
    {
        "title": "Get Auth Token From Dashboard\r",
        "link": "/ios/v2/guides/token",
        "platformName": "iOS",
        "objectID": "/ios/v2/guides/token",
        "keywords": [],
        "content": " -\r \r  Create a 100ms account  Create an account at  Dashboard of 100ms (https://dashboard.100ms.live/)   Create Account (/docs/guides/token/create-account.png)  After you have created your account you have to Confirm your Email , check the promotions tab in your Mail Box if you can't find it.  Login to the Dashboard and you will see an option to Create your first app. Click on it.   Create your first app (/docs/guides/token/starter-kit-initialize-first-step.png)  Then you would see this popup with multiple starter kits, hover over one of the starter kits and click Deploy . We will choose \"Video Conferencing\" for now.   Initialize Started Kit (/docs/guides/token/starter-kit-initialize.png)  In the Choose your deployment step, select 100ms and enter the subdomain you wish in the Subdomain field.   Domain details (/docs/guides/token/domain-details.png)  After you're App is set click on \"Go to Dashboard\" or  Go Here (https://dashboard.100ms.live/dashboard)   Dasboard (/docs/guides/token/go-to-dashboard.png)   Create a room  Go over to  Room in Dashboard (https://dashboard.100ms.live/rooms) and click on \"Create Room\" , While creating a room you can specify it's name, roles or enable recording.   Create Room (/docs/guides/token/create-room.png)  You will now see \"Room Details\" section and we have a room_id created, copy it somewhere.   Room Id (/docs/guides/token/room-id.png)   Get a temporary token from 100ms dashboard Any client connecting calling 100ms' service needs to authenticate using an auth token. In production you would have your own servers generating the tokens (see more  here (/docs/javascript/v2/foundation/security-and-tokens)), but for a quick start you can use the dashboard to create a token for you. The token will expire in 24 hours and should not be hard-coded into a production app.  To get a temporary token click on \"Join room\" button.   Join Room (/docs/guides/token/join-room.png)  In the popup that shows up click on icon with a key shape next to the role you want to join as.   Copy Token (/docs/guides/token/copy-token.png) The token will be copied to your clipboard. Use this along with the room_id to proceed with the quickstart guide.\r "
    },
    {
        "title": "0.3.x to 0.4.x Migration Guide",
        "link": "/ios/v2/migration-guides/03to04",
        "platformName": "iOS",
        "objectID": "/ios/v2/migration-guides/03to04",
        "keywords": [],
        "content": "   Error Type Changes The error type in the public SDK interface has been changed to  Error ( NSError in case of ObjC) from  HMSError . This is to align with platform conventions. In swift the error can be type casted to either HMSError or NSError for getting additional details like error code.    Error Handling Example   swift func on(error: Error)     guard let error = error as? HMSError else   return       //Example using error constants   switch error.code     case .websocketConnectionLost, .iceFailure, .endpointUnreachable:     retryConnection()   default: \t\tshowErrorAndExitMeeting(errorMessage: error.localizedDescription)     break         //Example using error codes   switch error.code.rawValue     case 1003, 4005, 2003:     retryConnection()   default: \t\tshowErrorAndExitMeeting(errorMessage: error.localizedDescription)     break          Error Property Changes   isTerminal The HMSError struct in swift will now have an isTerminal property which denotes wether error has caused the current session to terminate and the app will need to call join again to reconnect. Same will be available via is_terminal ( HMSIsTerminalUserInfoKey constant) key in userInfo dictionary of NSError    canRetry The HMSError struct in swift will now have an canRetry property which denotes wether app can call join again with the same configuration it has used before. The value be false in cases like token expiring or room getting locked. You can use this property while implementing infinite retry in your app. Same will be available via can_retry ( HMSCanRetryUserInfoKey constant) key in userInfo dictionary of NSError    Removed properties Error properties: id, action, info, message, params have been removed. localizedDescription property will contain all the necessary info.    Error Handling Example Using Properties This example shows the minimal error handling you can have in the app without worrying about error codes.   swift func on(error: Error)     guard let error = error as? HMSError else   return       if error.isTerminal       if error.canRetry && isInfiniteRetryEnabledInApp         retryConnection()       else         showErrorAndExitMeeting(errorMessage: error.localizedDescription)           else       logError(message: error.localizedDescription)          Error Code Changes Previously upon loosing connection and failing to reconnect as well as in case initial connection could not be established SDK would return error code 2000 ( initServerError ). Starting from 0.4.1 SDK will return error 1003 ( websocketConnectionLost ) if the connection was previously there but got lost in the middle of the session. If the SDK can not connect in the first place it will return error code 2003 ( endpointUnreachable ). This change makes makes error codes consistent between platforms  Error Constant Name Changes    Code      Old Name      New Name     :   :   :    2002  initHTTPErrorInvalidEndpointURL  invalidEndpointUrl   2003  initHTTPErrorEndpointUnreachable  endpointUnreachable   3000  tracksErrorGeneric  genericTrack   3001  tracksErrorCantAccessCaptureDevice  cantAccessCaptureDevice   3005  tracksErrorNothingToReturn  nothingToReturn   3007  tracksErrorCodecChangeNotPermitted  codecChangeNotPermitted   3008  tracksErrorAudioVideoSubsystemFailure  audiovideoSubsystemFailure   3011  tracksErrorMicCaptureFailed  micCaptureFailed   4001  webrtcErrorCreateOfferFailed  createOfferFailed   4002  webrtcErrorCreateAnswerFailed  createAnswerFailed   4003  webrtcErrorSetLocalDescriptionFailed  setLocalDescriptionFailed   4004  webrtcErrorSetRemoteDescriptionFailed  setRemoteDescriptionFailed   4005  webrtcErrorICEFailure  iceFailure   5001  joinErrorAlreadyJoined  alreadyJoined   6000  genericErrorNotConnected  notConnected   6002  genericErrorUnknown  unknown   6004  genericErrorJsonParsingFailed  jsonParsingFailed "
    },
    {
        "title": "Custom Video Effects (Beta)",
        "link": "/ios/v2/plugins/custom-video-plugins",
        "platformName": "iOS",
        "objectID": "/ios/v2/plugins/custom-video-plugins",
        "keywords": [],
        "content": "  Custom video plugins allow you to hook into 100ms' video lifecycle and add your own video processing pipeline right before it gets sent to the other participants in the room. This allows for super cool things like building AR Filters, adding Virtual Background, Emojifying the streams and monitoring participant engagement. Checkout our  Virtual Background (./virtual-background) docs to see an example of such a plugin. Note that this page is about creating custom new plugins than using existing ones.   Prerequisites   Basic Knowledge about 100ms SDK, follow our quickstart guides.   Video Plugin Interface By a 'video plugin' here, we mean a custom class that you can write to to manipulate video frames. Your class should subclass HMSVideoPlugin class and override the 'process' function to implement the plugin functionality.   swift class GrayscaleVideoPlugin: HMSVideoPlugin     override func process(_ frame: CVPixelBuffer) -> CVPixelBuffer     ...         The process function provides you with an input frame that you can process. After processing, you return an output frame. HMSSDK calls the process function whenever a new frame is generated from the camera. The input frames are replaced by the output frames provided by you thus transforming the video stream. The general approach to write a video plugin that modifies the frames in the video is like below:   swift class GrayscaleVideoPlugin: HMSVideoPlugin     override func process(_ frame: CVPixelBuffer) -> CVPixelBuffer         // Process the 'frame' to transform it into a grayscal frame    ...       let outputFrame: CVPixelBuffer = ...       // return the output frame    return outputFrame         In case you are writing a plugin that doesn't modify the video frames but only analyse the frames for some purpose, we recommend you do your processing asynchronously while returning the input frame immediately like below:   swift class FaceAttendencePlugin: HMSVideoPlugin     override func process(_ frame: CVPixelBuffer) -> CVPixelBuffer       DispatchQueue.global().async       // Recognise face and mark attendence     ...          return frame           Adding the plugin Once a plugin implementation is ready, it can be added to the local peer's video track as below  1. Create an HMSVideoPlugin array   swift   var videoPlugins =  HMSVideoPlugin ()      2. Create an instance of HMSVirtualBackgroundPlugin and append it to videoPlugins array, like below   swift   let grayscalePlugin = GrayscaleVideoPlugin()   grayscalePlugin.activate()   videoPlugins.append(grayscalePlugin)      3. Next, create an instance of HMSVideoTrackSettings passing videoPlugins array   swift   let videoSettings = HMSVideoTrackSettings(... videoPlugins: self.videoPlugins)      4. Use this videoSettings instance while setting the trackSettings on HMSSDK   swift   hmsSDK = HMSSDK.build   sdk in       sdk.trackSettings = HMSTrackSettings(videoSettings: videoSettings, audioSettings: ...)       ...            That is all you need to do to add a custom video plugin to your HMSSDK session    Activating and deactivating the plugin When you subclass from HMSVideoPlugin, your custom class also gains access to activate(), deactivate() functions. You call these functions on your plugin instance to activate or deactivate your plugin. Note that when you add your plugin it's deactivated by defualt.   swift grayscalePlugin.deactivate() grayscalePlugin.activate()     Implementation Example  Grayscale Filter Below is a sample implementation of the above interface which converts the local video in grayscale.   swift class GrayscaleVideoPlugin: HMSVideoPlugin       static let defaultAttributes:  NSString: NSObject  =       kCVPixelBufferPixelFormatTypeKey: NSNumber(value: kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange),     kCVPixelBufferIOSurfacePropertiesKey :  :  as NSDictionary         private var attributes:  NSString: NSObject        var attributes:  NSString: NSObject  = Self.defaultAttributes     attributes kCVPixelBufferWidthKey  = NSNumber(value: Int(extent.width))     attributes kCVPixelBufferHeightKey  = NSNumber(value: Int(extent.height))     return attributes         private var _pixelBufferPool: CVPixelBufferPool?   private var pixelBufferPool: CVPixelBufferPool        get         if _pixelBufferPool == nil           var pixelBufferPool: CVPixelBufferPool?         CVPixelBufferPoolCreate(nil, nil, attributes as CFDictionary?, &pixelBufferPool)         _pixelBufferPool = pixelBufferPool               return _pixelBufferPool            set         _pixelBufferPool = newValue               private var extent = CGRect.zero       didSet         guard extent  = oldValue else   return         pixelBufferPool = nil               let ciContext = CIContext(options: nil)     override func process(_ frame: CVPixelBuffer) -> CVPixelBuffer       let inputImage = CIImage(cvPixelBuffer: frame)         guard let outputImage = inputImage.grayscale else   return frame           var outputBuffer: CVImageBuffer?         extent = outputImage.extent         CVPixelBufferPoolCreatePixelBuffer(nil, pixelBufferPool, &outputBuffer)     if let outputBuffer = outputBuffer         ciContext.render(outputImage, to: outputBuffer)           return outputBuffer ?? frame       extension CIImage     var grayscale: CIImage?       let context = CIContext(options: nil)     guard let currentFilter = CIFilter(name: \"CIPhotoEffectNoir\") else   return nil       currentFilter.setValue(self, forKey: kCIInputImageKey)     if let output = currentFilter.outputImage,       let cgImage = context.createCGImage(output, from: output.extent)         return CIImage(cgImage: cgImage)           return nil           Handler synchronisation between process and your own methods and properties When you subclass from HMSVideoPlugin, your custom class also gains access to a property called 'frameProcessingQueue'. The frameProcessingQueue is the serial dispatch queue on which the frames arrive in your process function. You can use this queue to serialise access to any shared resources in your class's properties and functions.   swift let serialGrayscalePluginProcessingQueue = grayscalePlugin.frameProcessingQueue   👀 To see an example video plugin implementation using 100ms SDK, checkout our  Example project (https://github.com/100mslive/100ms-ios-sdk/tree/main/Example). 📲 Download the 100ms fully featured Sample iOS app here: https://testflight.apple.com/join/dhUSE7N8   Plugin Guidelines   Feel free to implement more methods outside the interface, options passed in plugin's constructor etc. as required for the users of the plugin to give more interaction points. For example, our Virtual background plugin exposes a property to change the background as required. "
    },
    {
        "title": "High Quality Camera Capture (Beta)",
        "link": "/ios/v2/plugins/local-camera-capture",
        "platformName": "iOS",
        "objectID": "/ios/v2/plugins/local-camera-capture",
        "keywords": [],
        "content": "    Introduction You can use Camera Frame Capture Plugin to capture the current frame from the local camera in hight quality. This can be useful if you want to implement a remote KYC (know-your-customer) use-case to capture high quality photos of client's documents.   Minimum Requirements   SDK version 0.5.1   How to capture current camera frame in high quality 1. Create an HMSVideoPlugin array   swift   var videoPlugins =  HMSVideoPlugin ()   2. Create an instance of HMSFrameCapturePlugin and activate it. Then append it to videoPlugins array, like below   swift   let frameCapturePlugin = HMSFrameCapturePlugin()   frameCapturePlugin.activate()   videoPlugins.append(frameCapturePlugin)   3. Next, assign videoPlugins array to videoPlugins property of videoSettingsBuilder:   swift let hmsSDK = HMSSDK.build   sdk in    sdk.trackSettings = HMSTrackSettings.build   videoSettingsBuilder, audioSettingsBuilder in      videoSettingsBuilder.videoPlugins = videoPlugins      ...         That is all the setup you need to do. Now, you can call   capture   method on your frameCapturePlugin instance to get the current camera frame as a UIImage:   swift   if let image = frameCapturePlugin.capture()       // show/upload/process the uiimage       👀 To see an example iOS Local Camera Capture Plugin implementation using 100ms SDK, checkout  our example project (https://github.com/100mslive/100ms-ios-sdk/tree/main/Example). "
    },
    {
        "title": "Virtual Background (Beta)\r",
        "link": "/ios/v2/plugins/virtual-background",
        "platformName": "iOS",
        "objectID": "/ios/v2/plugins/virtual-background",
        "keywords": [],
        "content": " -\r \r  Introduction\r \r Virtual Background plugin helps in customising one’s background that replacing the background with a static image or blurring the background.\r This guide provides an overview of usage of the Virtual Background plugin of 100ms.\r \r <div style=  textAlign: 'center'  >\r \r   VirtualBackground (/docs/v2/ios-vb.gif)\r \r </div>\r \r  Minimum Requirements\r \r   Minimum iOS version required to support Virtual Background plugin is iOS 15\r   Minimum 100ms SDK version required is 0.3.1 \r   Virtual background plugin is in beta stage and may have performance issues on iPhone X, 8, 7, 6 and other older devices. We recommend that you use this feature on a high performance device for smooth experience.\r \r  How to enable virtual background in your app using HMSSDK\r \r 1. Create an HMSVideoPlugin array\r \r  swift\r   var videoPlugins =  HMSVideoPlugin ()\r  \r \r 2. Create an instance of HMSVirtualBackgroundPlugin and append it to videoPlugins array, like below\r \r  swift\r   let virtualBackgroundPlugin = HMSVirtualBackgroundPlugin(backgroundImage: UIImage(named: \"VB1\"))\r   videoPlugins.append(virtualBackgroundPlugin)\r  \r \r 3. Next, create an instance of HMSVideoTrackSettings passing videoPlugins array\r \r  swift\r   let videoSettings = HMSVideoTrackSettings(... videoPlugins: self.videoPlugins)\r  \r \r 4. Use this videoSettings instance while setting the trackSettings on HMSSDK\r \r  swift\r   hmsSDK = HMSSDK.build   sdk in\r       sdk.trackSettings = HMSTrackSettings(videoSettings: videoSettings, audioSettings: ...)\r       ...\r      \r  \r \r That is all you need to do to enable virtual background \r \r  How to enable and disable virtual background\r \r Hold on to a reference to the instance of HMSVirtualBackgroundPlugin and use activate and deactivate functions on it to enable/disable the virtual background.\r \r  swift\r   var virtualBackgroundPlugin: HMSVideoPlugin?\r \r   func setupPlugins()  \r     virtualBackgroundPlugin = HMSVirtualBackgroundPlugin(backgroundImage: UIImage(named: \"VB1\"))\r     ...\r    \r \r   func toggleVB()  \r     let isVBActivated = UserDefaults.standard.bool(forKey: \"virtualBackgroundPluginEnabled\")\r \r     if isVBActivated  \r       self?.interactor.virtualBackgroundPlugin?.deactivate()\r       UserDefaults.standard.set(false, forKey: \"virtualBackgroundPluginEnabled\")\r      \r     else  \r       _ = self?.interactor.virtualBackgroundPlugin?.activate()\r       UserDefaults.standard.set(true, forKey: \"virtualBackgroundPluginEnabled\")\r      \r    \r  \r \r  How to blur background instead of replacing it with an image\r \r If you pass nil as the backgroudImage parameter while initilising HMSVirtualBackgroundPlugin, it will blur the background instead of replaicng it with an image. You can optionally pass blurRadius parameter to control the amount of blur in the background. Default blurRadius used is 10.\r \r  swift\r   let virtualBackgroundPlugin = HMSVirtualBackgroundPlugin(backgroundImage: nil, blurRadius: 20)\r  \r \r  Change Background\r \r You can use backgroundImage property on HMSVirtualBackgroundPlugin to set a new background image.\r \r  swift\r   let virtualBackgroundPlugin = HMSVirtualBackgroundPlugin(backgroundImage: UIImage(named: \"VB1\"))\r   virtualBackgroundPlugin.backgroundImage = UIImage(named: \"VB2\")\r  \r \r  Recommendations for supporting older devices\r \r Built-in Virtual background plugin uses Apple's segementation APIs and is supported on iOS 15 and onwards.\r In out testing, the built-in Virtual background plugin that uses Apple's segementation API performs well on iPhone 13, 12, 11, and XS. It may not perform well on iPhone X, 8, 7, 6 and older devices.\r \r If you would like to support iOS version lower than iOS 15 or want to support older devices, you can write a custom virtual background video plugin. For example you can use Google's MLKit's segementer for replcing background. Below is an example of writing a custom video plugin called 'GoogleSegementor'\r \r  swift\r class CustomVirtualBackground: HMSVideoPlugin  \r \r   // MARK: Private\r   private static let DefaultFrameRate = 15\r \r   private lazy var rateLimiter: RateLimiter =  .init(limit: 1/Double(CustomVirtualBackground.DefaultFrameRate)) ()\r \r   private var coreBackgroundImage: CIImage?\r   private var blurRadius: CGFloat?\r \r   private let hmsVideoPersonSegmentationHandler = HMSMLKitPersonSegmentationHandler()\r \r   // MARK: Public\r   public var backgroundImage: UIImage?  \r     didSet  \r       frameProcessingQueue.sync  \r         if let cgImage = backgroundImage?.cgImage  \r           coreBackgroundImage = CIImage(cgImage: cgImage)\r          \r         else  \r           coreBackgroundImage = nil\r          \r        \r      \r    \r \r   public init(backgroundImage: UIImage?, blurRadius: NSNumber? = nil)  \r     self.backgroundImage = backgroundImage\r     self.blurRadius = blurRadius  = nil ? CGFloat(blurRadius .doubleValue) : nil\r     super.init()\r     // This defer is to make didSet get triggered for backgroundImage from init\r     defer  \r       self.backgroundImage = backgroundImage\r      \r    \r \r   public override func process(frame: CVPixelBuffer) -> CVPixelBuffer  \r \r     hmsVideoPersonSegmentationHandler.replaceBackground(in: frame,\r                               with: coreBackgroundImage,\r                               blurRadius: blurRadius ?? 10,\r                               shouldSkip:  \r       return  rateLimiter.shouldFeed()\r      )\r    \r  \r \r // Helper classes for custom video plugin above\r // MLKit is from GoogleMLKit/SegmentationSelfie\r import MLKit\r \r class HMSMLKitPersonSegmentationHandler  \r \r   private static let defaultAttributes:  NSString: NSObject  =  \r     kCVPixelBufferPixelFormatTypeKey: NSNumber(value: kCVPixelFormatType_420YpCbCr8BiPlanarVideoRange),\r     kCVPixelBufferIOSurfacePropertiesKey :  :  as NSDictionary\r    \r \r   private var extent = CGRect.zero  \r     didSet  \r       guard extent  = oldValue else   return  \r       pixelBufferPool = nil\r      \r    \r \r   private var attributes:  NSString: NSObject   \r     var attributes:  NSString: NSObject  = Self.defaultAttributes\r     attributes kCVPixelBufferWidthKey  = NSNumber(value: Int(extent.width))\r     attributes kCVPixelBufferHeightKey  = NSNumber(value: Int(extent.height))\r     return attributes\r    \r \r   private var _pixelBufferPool: CVPixelBufferPool?\r   private var pixelBufferPool: CVPixelBufferPool   \r     get  \r       if _pixelBufferPool == nil  \r         var pixelBufferPool: CVPixelBufferPool?\r         CVPixelBufferPoolCreate(nil, nil, attributes as CFDictionary?, &pixelBufferPool)\r         _pixelBufferPool = pixelBufferPool\r        \r       return _pixelBufferPool \r      \r     set  \r       _pixelBufferPool = newValue\r      \r    \r \r   private var ciContext = CIContext()\r   private var previousBuffer: CVImageBuffer?\r \r   // MARK: Public\r   func replaceBackground(in framePixelBuffer: CVPixelBuffer, with backgroundImage: CIImage?, blurRadius: CGFloat, shouldSkip: ()->Bool) -> CVImageBuffer  \r \r     guard  shouldSkip() else   return previousBuffer ?? framePixelBuffer  \r \r     var info = CMSampleTimingInfo()\r     info.presentationTimeStamp = CMTime.zero\r     info.duration = CMTime.invalid\r     info.decodeTimeStamp = CMTime.invalid\r \r     var formatDesc: CMFormatDescription? = nil\r     CMVideoFormatDescriptionCreateForImageBuffer(allocator: kCFAllocatorDefault, imageBuffer: framePixelBuffer, formatDescriptionOut: &formatDesc)\r \r     var sampleBuffer: CMSampleBuffer? = nil\r \r     CMSampleBufferCreateReadyWithImageBuffer(allocator: kCFAllocatorDefault,\r                         imageBuffer: framePixelBuffer,\r                         formatDescription: formatDesc ,\r                         sampleTiming: &info,\r                         sampleBufferOut: &sampleBuffer);\r \r     let image = VisionImage(buffer: sampleBuffer )\r     image.orientation = imageOrientation(\r      deviceOrientation: UIDevice.current.orientation,\r      cameraPosition: .front)\r \r     var mask: SegmentationMask?\r     do  \r       mask = try segmenter.results(in: image)\r       catch let error  \r       print(\"Failed to perform segmentation with error: (error.localizedDescription).\")\r \r       return framePixelBuffer\r      \r \r     // Get the pixel buffer that contains the mask image.\r     guard let maskPixelBuffer = mask?.buffer else   return framePixelBuffer  \r \r     // Blend the images and mask.\r     return blend(original: framePixelBuffer, mask: maskPixelBuffer, backgroundImage: backgroundImage, blurRadius: blurRadius) ?? framePixelBuffer\r    \r \r   let segmenter: Segmenter =  \r     let options = SelfieSegmenterOptions()\r     options.segmenterMode = .stream\r     options.shouldEnableRawSizeMask = true\r \r     let segmenter = Segmenter.segmenter(options: options)\r \r     return segmenter\r    ()\r \r   func imageOrientation(\r    deviceOrientation: UIDeviceOrientation,\r    cameraPosition: AVCaptureDevice.Position\r   ) -> UIImage.Orientation  \r    switch deviceOrientation  \r    case .portrait:\r     return cameraPosition == .front ? .leftMirrored : .right\r    case .landscapeLeft:\r     return cameraPosition == .front ? .downMirrored : .up\r    case .portraitUpsideDown:\r     return cameraPosition == .front ? .rightMirrored : .left\r    case .landscapeRight:\r     return cameraPosition == .front ? .upMirrored : .down\r    case .faceDown, .faceUp, .unknown:\r     return .up\r     \r    \r \r \r   // MARK: Private\r \r   private func blend(original framePixelBuffer: CVPixelBuffer, mask maskPixelBuffer: CVPixelBuffer, backgroundImage: CIImage? = nil, blurRadius: CGFloat) -> CVImageBuffer?  \r \r     var imageBuffer: CVImageBuffer?\r \r     let originalImage = CIImage(cvPixelBuffer: framePixelBuffer)\r \r     var maskImage = CIImage(cvPixelBuffer: maskPixelBuffer)\r \r     // Scale the mask image to fit the bounds of the video frame.\r     scaleToFit(image: &maskImage, originalImage: originalImage)\r \r     // Create a clear colored background image.\r     var background: CIImage\r \r     if let backgroundImage = backgroundImage  \r       background = backgroundImage.oriented(.left)\r      \r     else  \r       background = originalImage.clampedToExtent()\r         .applyingFilter(\r           \"CIBokehBlur\",\r           parameters:  \r             kCIInputRadiusKey: blurRadius,\r            \r         )\r         .cropped(to: originalImage.extent)\r      \r \r     // Scale the background image to fit the bounds of the video frame.\r     scaleToFit(image: &background, originalImage: originalImage)\r \r     // Blend the original, background, and mask images.\r     let blendFilter = CIFilter.blendWithRedMask()\r     blendFilter.inputImage = originalImage\r     blendFilter.backgroundImage = background\r     blendFilter.maskImage = maskImage\r \r     // Redner image to a new buffer.\r     if let finalImage = blendFilter.outputImage  \r \r       imageBuffer = renderToBuffer(image: finalImage)\r      \r \r     previousBuffer = imageBuffer\r \r     return imageBuffer\r    \r \r   private func renderToBuffer(image: CIImage) -> CVImageBuffer?  \r \r     var imageBuffer: CVImageBuffer?\r \r     extent = image.extent\r \r     CVPixelBufferPoolCreatePixelBuffer(nil, pixelBufferPool, &imageBuffer)\r \r     if let imageBuffer = imageBuffer  \r       ciContext.render(image, to: imageBuffer)\r      \r \r     return imageBuffer\r    \r \r   private func scaleToFit(image: inout CIImage, originalImage: CIImage)  \r     // Scale the image to fit the bounds of the video frame.\r     let scaleX = originalImage.extent.width / image.extent.width\r     let scaleY = originalImage.extent.height / image.extent.height\r     image = image.transformed(by: .init(scaleX: scaleX, y: scaleY))\r    \r  \r \r class RateLimiter  \r  private let limit: TimeInterval\r  private var lastExecutedAt: Date?\r \r  init(limit: TimeInterval)  \r    self.limit = limit\r   \r \r  func shouldFeed() -> Bool  \r    let now = Date()\r    let timeInterval = now.timeIntervalSince(lastExecutedAt ?? .distantPast)\r \r    if timeInterval > limit  \r      lastExecutedAt = now\r \r      return true\r     \r \r    return false\r   \r  \r  \r "
    },
    {
        "title": "Release Notes",
        "link": "/javascript/v2/changelog/release-notes",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/changelog/release-notes",
        "keywords": [],
        "content": "description: Release Notes for 100ms JavaScript SDK    Latest Versions  Package                 Version                                                                                                                 @100mslive/hms-video-store           npm version (https://badge.fury.io/js/%40100mslive%2Fhms-video-store.svg) (https://badge.fury.io/js/%40100mslive%2Fhms-video-store)   @100mslive/react-sdk              npm version (https://badge.fury.io/js/%40100mslive%2Freact-sdk.svg) (https://badge.fury.io/js/%40100mslive%2Freact-sdk)         @100mslive/hms-video-react(deprecated)     npm version (https://badge.fury.io/js/%40100mslive%2Fhms-video-react.svg) (https://badge.fury.io/js/%40100mslive%2Fhms-video-react)    2022-12-19 Released: @100mslive/hms-video-store@0.8.1 , @100mslive/react-sdk@0.6.1    Added:   Bulk role change API (https://www.100ms.live/docs/javascript/v2/features/change-role change-bulk-peer-role-api)    Fixed:  Analytics fixes  FPS for the HMSVBPlugin  Device change not working sometimes with some bluetooth devices  Audio track not received on remote end on role change due to no data being sent  Reestablishment of audio track after external call interuption   2022-11-25 Released: @100mslive/hms-video-store@0.8.0 , @100mslive/react-sdk@0.6.0    Added:  Simulcast support  New virtual background plugin with improved cpu usage compared to the previous one. Refer  usage (https://github.com/100mslive/100ms-web/blob/main/src/plugins/VirtualBackground/VirtualBackground.jsx)    Fixed:  Black tile when video tile is on screen only for a split second(Example scenario: pagination)  Join promise not getting resolved in some rare case  leave.failure webhook getting triggered when closing tab from preview    Breaking:  hmsActions.setVolume is a promise now  HMSStatsStore interface updated and some of the selectors are removed   2022-11-10 Released: @100mslive/hms-video-store@0.7.1 , @100mslive/react-sdk@0.5.1    Fixed:  @100mslive/react-sdk failing in nextjs environment  Fix dropdown from @100mslive/react-ui making the UI unresponsive when closed  Screenshare not working in macOS Ventura  Improved device change analytics    Added:  Stats for nerds for HLS  @alpha release    Breaking  customerDescription on   HMSPeer  (https://www.100ms.live/docs/api-reference/javascript/v2/interfaces/HMSPeer) is now removed. Use metadata field instead  Screenshare config is updated. Please refer  here (https://www.100ms.live/docs/api-reference/javascript/v2/interfaces/HMSScreenShareConfig)   2022-10-20 Released: @100mslive/hms-video-store@0.6.4 , @100mslive/react-sdk@0.4.4    Fixed:    hmsActions.setAudioOutputDevice  (https://www.100ms.live/docs/api-reference/javascript/v2/interfaces/HMSActions set-audio-output-device) is now async, the returned promise will resolve once the device change is applied to all the audio tracks in the room  Terminal errors, join promise not getting resolved on certain join failures  WebRTC stats not working on rejoining a room  new HMSReactiveStore() doesn't work in server side rendering  Handling null in track selectors  End room sending both peer.leave.success and peer.leave.failure events for same peer in webhook   2022-09-30 Released: @100mslive/hms-video-store@0.6.2 , @100mslive/react-sdk@0.4.2    Fixed:  setSessionMetadata API type   2022-09-28 Released: @100mslive/hms-video-store@0.6.1 , @100mslive/react-sdk@0.4.1    Added:  Session Metadata API (alpha release)  Strict track types    Added HMSVideoTrack, HMSScreenVideoTrack, HMSAudioTrack, HMSScreenAudioTrack  Added HMSReconnectionNotification to notification types    Fixed:  Type error when subscribing to all notifications.   2022-09-13 Released: @100mslive/hms-video-store@0.6.0 , @100mslive/react-sdk@0.4.0    Added:  join is now async  An error will be returned if validateVideoPluginSupport is called before video track is available   facingMode (https://developer.mozilla.org/en-US/docs/Web/API/MediaTrackSettings/facingMode) property for local videoTrack.(Note: This will be useful on mobile browsers)    Fixed:  Cleanup on leave  Improved logging for browser recording    Breaking:  A HMSNotificationTypes.PEER_LIST will be sent after join even if there are no peers already in the room  Specific types are added to Notifications   2022-08-01 Released: @100mslive/hms-video-store@0.5.0 , @100mslive/react-sdk@0.3.0    Added:   retry calls for all the websocket messages   params are now optional in startHLSStreaming    Breaking:   react-ui theme colors have been updated   react-ui, styling for components is updated   2022-07-11 Released: @100mslive/hms-video-store@0.4.0 , @100mslive/react-sdk@0.2.0    Added:   leaveOnUnload prop to HMSRoomProvider to make leave on page load optional. Defaults to true .   Video/Audio Plugins can be added in preview now.    Fixed:   Error in getting track stats when Audio/Video plugin is added in preview    Breaking:   react-ui theme colors have been updated   2022-06-22 Released: @100mslive/hms-video-store@0.3.4 , @100mslive/react-sdk@0.1.4    Added:   clearList and removeItem actions in playlist   getContextType in video plugin interface   joining without webrtc connection for hls stream viewer   2022-06-14 Released: @100mslive/hms-video-store@0.3.2 , @100mslive/react-sdk@0.1.2 , @100mslive/hms-video-react@0.4.2    Added:   Codec in track stats    Fixed:   System denied permission error for camera and microphone   Duplicate tile in server recording on role change from AV to audio to AV   Retry in join   2022-06-08 Released: @100mslive/hms-video-store@0.3.1 , @100mslive/react-sdk@0.1.1 , @100mslive/hms-video-react@0.4.1    Fixed:   Analytics events   Reconnected notification not being triggered in some cases   2022-05-31 Released: @100mslive/hms-video-store@0.3.0 , @100mslive/react-sdk@0.1.0 , @100mslive/hms-video-react@0.4.0    Fixed:   Video freezing when same video track is played in multiple video elements and one of them is removed   Incorrect device showing in device selection when initially joined as muted and then unmuted.    Breaking:   All the requests will have the sender as optional now.   Examples:     In chat message object, message.sender is optional     in unmute request, request.requestedBy is optional > NOTE: if you are planning to use api's to send chat messages, update metadata or change roles, the sdk version should be minimun at this version.   2022-05-11 Released: @100mslive/hms-video-store@0.2.100 , @100mslive/react-sdk@0.0.17 , @100mslive/hms-video-react@0.3.128    Upgrade to React 18   Added support for server-side subscribe degradation   Fix device list failing to be populated in preview on device error   Fix inconsistent peer list in preview   Fix auxiliary regular tracks not having their own video tiles   Fix token errors  made terminal   2022-04-29 Released: @100mslive/hms-video-store@0.2.98 , @100mslive/react-sdk@0.0.15 , @100mslive/hms-video-react@0.3.126    Added support for resolutions in RTMP.   Added methods to set your app data in 100ms store. Refer docs  here (../features/store-appdata)   Exposed Style-able Pagination component from react-ui   2022-04-21 Released: @100mslive/hms-video-store@0.2.96 , @100mslive/react-sdk@0.0.13 , @100mslive/hms-video-react@0.3.125    Add reconnections due to internet issues in preview phase   Add treeshaking for react-sdk and react-ui   Fix race condition when leave is called immediately after join before join could complete.   Fix race condition where preview complete could be notified before join was ready to be called.   Support for server side degradation   Fix bug in useVideo not leading to proper detach on unmounting   2022-03-29 Released: @100mslive/hms-video-store@0.2.94 , @100mslive/react-sdk@0.0.11 , @100mslive/hms-video-react@0.3.124    Get Connection quality score to show network bar in UI, docs  here (../advanced-features/connection-quality)   Measure internet quality score for local peer in preview   Errors occurred during screenshare(permission not given etc.) will be sent via notification as well now in   addition to being thrown on the method call   hmsActions.refreshDevices() to refresh the device list from UI side. This can be done for example if the UI is   taking control of asking user permission.   server timestamp for local peer's messages   changed interface to check plugin's support   Significant improvements and refactoring of our  public sample app (https://github.com/100mslive/100ms-web)   2022-03-21 Released: @100mslive/hms-video-store@0.2.93 , @100mslive/react-sdk@0.0.10 , @100mslive/hms-video-react@0.3.123    Fixed:   Redundant events sent to analytics   2022-03-17 Released: @100mslive/hms-video-store@0.2.91 , @100mslive/react-sdk@0.0.8 , @100mslive/hms-video-react@0.3.121    Added:   useCustomEvent hook in react-sdk   Subscribing to specific notification types   HLS recording status on joining room   Additional fields in disconnection events   Video Only mode for screenshare   joinedAt in HMSPeer    Fixed:   Fix turning on recording with hls   2022-02-24 Released: @100mslive/hms-video-store@0.2.90 , @100mslive/react-sdk@0.0.7 , @100mslive/hms-video-react@0.3.119    Added:   PeerList in preview   HLS recording status will be available    Breaking(react-sdk):   input/output interfaces of useVideo has been changed. check  react quickstart (../guides/react-quickstart) for reference.   2022-02-04 Released: @100mslive/hms-video-store@0.2.89 , @100mslive/react-sdk@0.0.6 , @100mslive/hms-video-react@0.3.116    Added:   startedAt fields for RTMP and recording   A exception will be thrown when preview/join is called if there is no WebRTC support now(for example disable via an extension)   APIs to get WebRTC Stats for the call   A new @100mslive/react-sdk beta package for primitive hooks to build React UI components   A new disconnecting room state which happens from the moment leave is clicked till it's successful    Fixed:   Fix for wrong video mute state in a corner case after high frequency updates   Fix for vb-background video overlaying   VB not enabling in firefox if turned on in muted state   Fixes related to device settings remembering in muted state   Distorted audio when both noise suppression and virtual background are on   2022-01-14 Released: @100mslive/hms-video@0.1.41 , @100mslive/hms-video-store@0.2.87 , @100mslive/hms-video-react@0.3.115    Added:   Terminal error for insecure contexts(http://)    Fixed:   Type errors in few selectors   iOS video getting stuck when custom plugins are enabled.   Preview audio level failing.   2022-01-07 Released: @100mslive/hms-video@0.1.40 , @100mslive/hms-video-store@0.2.86 , @100mslive/hms-video-react@0.3.114    Added:   server errors are marked as terminal, SDK will automatically leave and give a terminal error    Fixed:   process is not defined error when using with CRA5, webpack5 or with react-scripts@5   update recording/streaming status when they are actually started    Breaking:   requestedBy is now optional in HMSRoleChangeRequest, HMSChangeTrackStateRequest, HMSChangeMultiTrackStateRequest   2021-12-21 Released: @100mslive/hms-video@0.1.39 , @100mslive/hms-video-store@0.2.85 , @100mslive/hms-video-react@0.3.113    Added:    HLS streaming (../features/hls) for streaming a room to a large number of viewers   Support for Multiple  video plugins (../plugins/custom-video-plugins)   startedAt field in room to know since when the call is running   An additional boolean showTileForAllPeers in VideoList component to show peers with no tiles.    Fixed:   Type in custom events not getting passed for direct messages   Camera selection showing wrong device in some cases   2021-12-10 Released: @100mslive/hms-video@0.1.37 , @100mslive/hms-video-store@0.2.83 , @100mslive/hms-video-react@0.3.112    Added:   Support for Multiple Audio Plugins(alpha)   Notification when user updates peer info(name or metadata)    Fixed:   Throw proper error with error code and description for firefox when network is blocked/disconnected.   Resolve preview promise correctly.   (You can directly await the preview promise and perform the necessary actions after that.)   Device change with audio plugins > Add @100mslive/hms-video-react or @100mslive/hms-video-store as a sole dependency for using 100ms. They include the core sdk.   2021-11-26 Released: @100mslive/hms-video@0.1.36 , @100mslive/hms-video-store@0.2.82 , @100mslive/hms-video-react@0.3.111    Added:    Change Name (../features/peer-name) post Joining the room    Change Metadata (../advanced-features/peer-metadata) post Joining the room   Notifications for track  degraded and restored (../features/sub-degradation)    Breaking   peer.customerDescription is now renamed to peer.metadata   2021-11-12 Released: @100mslive/hms-video@0.1.32 , @100mslive/hms-video-store@0.2.79 , @100mslive/hms-video-react@0.3.105  Added:   Support to play next item from audio/video playlist after current track ended   Support for setting playbackrate on audio/video playlist   selectPeerSharingAudioPlaylist selector to get peer who is sharing audioplaylist   selectPeersByRole(role: string) selector to get peers by role    Fixed:   Not able to change respective devices when audio/video is muted   Devicelist empty if permissions are denied while joining and enabled later on unmuting.   Retry getusermedia when it fails with overconstrainedError.   Type errors for selectors   Role changes not persisent when the role change event occurs when a peer is reconnecting   2021-10-25 Released: @100mslive/hms-video@0.1.18 , @100mslive/hms-video-react@0.3.97    Fix an issue in VideoTile react component leading to blank tile   Fix unsuccesful tracks publish in case of user toggling video in preview   2021-10-21 Released: @100mslive/hms-video@0.1.16 , @100mslive/hms-video-store@0.2.71 , @100mslive/hms-video-react@0.3.96    Support resolution changes for screenshare on role change   Fix extra track getting added when joined with Audio/Video muted   Fix setVolume(0, trackId) not working on IOS when it is called on onTrackUpdate .   Screenshare error codes  3001 for user rejection, 3002 for OS level block.   Fix a bug in mute all where it behaves in opposite way on a request from Android/iOS.   Fix type error for setRemoteTracksEnabled   Add peerId for all tracks   Optimisations for tracksMap selector   Add HMSNotificationTypes.PEER_LIST notification which gives peers already in room when joined   Optimise video tile, participant list rerenders   Add feedback component    Breaking:   ParticipantsInView component removed. Use UiSettings component instead   2021-10-08 Released: @100mslive/hms-video@0.1.1 , @100mslive/hms-video-store@0.2.62 , @100mslive/hms-video-react@0.3.88    Single permission for audio and video   Bulk Remote mute per type, source and roles. Refer to  remote muting (/javascript/v2/features/remote-mute) section in the docs for more information.   Locally muting remote track on iOS Devices   Performance optimizations    Fixed   Role change takes video resolution into account as well now   2021-10-01 Released: @100mslive/hms-video@0.0.204 , @100mslive/hms-video-store@0.2.57 , @100mslive/hms-video-react@0.3.81    Support for starting/stopping RTMP/recording   Add bitrate capping for playlist tracks     AudioTrack  64kbps     VideoTrack  1mbps   Fix addSink issue in videoTrack.   Sometimes, addSink not sending correct data to backend, which leads to black tile    Breaking:   onPeerUpdate is called with HMSPeer   for initial peer list instead of calling onPeerUdpate for each peer    Experiemental:   Add/remove sink for audio element with feature flag.   Join before publish with feature flag   2021-09-17 Released: @100mslive/hms-video@0.0.185 , @100mslive/hms-video-store@0.2.43 , @100mslive/hms-video-react@0.3.64    Added   Support for Audio/Video Playlist from remote urls   Support for AudioOnly screenshare   2021-08-27 Released: @100mslive/hms-video@0.0.173 , @100mslive/hms-video-store@0.2.34 , @100mslive/hms-video-react@0.3.54    Added   Memorization of manual device selection.  Refer Usage (../features/device-change)   Self role change.  Refer Usage (../features/change-role)    Fixed   Autoplay error on ios safari   2021-08-16 Released: @100mslive/hms-video@0.0.148 , @100mslive/hms-video-store@0.2.22 , @100mslive/hms-video-react@0.3.32 .    Added   Mute/unmute remote peers.  Refer Usage (/api-reference/javascript/v2/interfaces/HMSActions set-remote-track-enabled)   Remove peer from room.  Refer Usage (/api-reference/javascript/v2/interfaces/HMSActions remove-peer)   End room.  Refer Usage (/api-reference/javascript/v2/interfaces/HMSActions remove-peer)   Private Messages to peers/roles.      Direct Message to Peer (/api-reference/javascript/v2/interfaces/HMSActions send-direct-message)      Group Message to Role (/api-reference/javascript/v2/interfaces/HMSActions send-group-message)      Broadcast Message to everyone in the room (/api-reference/javascript/v2/interfaces/HMSActions send-broadcast-message)    Breaking Change     sendMessage is now deprecated  , please use sendBroadcastMessage, sendGroupMessage and sendDirectMessage for messaging.  Refer Usage (/api-reference/javascript/v2/interfaces/HMSActions send-broadcast-message)   0.0.128  2021-08-07   Fix tracks not getting removed properly in Firefox in between role changes   0.0.127  2021-08-06   Fix black tile issue   0.0.124  2021-08-06   Export HMSLogLevel to set the HMSdk's log level.   0.0.123  2021-08-05    Added   Autoplay handling added.     on autoplay error, the onError callback is called     unblockAutoplay method is provided to resolve the autoplay error      Refer Usage (/javascript/v2/features/error-handling handling-autoplay-error)   Audio/video state is preserved when setSettings is called.   Device change handling improvements     Output device selection logic improved     Handled headphones with no microphone     Stereo headphones are handled correctly for input/output   Handle virtual background on role change   Cleanup video/audio elements to limit chrome 92 bug for mediaplayers    Fixed   Fix Settings component to show correct selected devices   Fix video elements not detached when not in view   2021-07-28    Added   Changing roles mid-call.  Refer Usage (/javascript/v2/features/change-role)   BREAKING CHANGE: The interface of HMSPeer has been modified for users using the JavaScript SDK without the  Reactive Store ( ).   jsx // Before peer.role; // returns a string // After peer.role; // returns HMSRole interface HMSRole     name: string;   publishParams: PublishParams;   subscribeParams:       subscribeToRoles: string  ,     maxSubsBitRate: number    ;   permissions:       endRoom: boolean,     removeOthers: boolean,     stopPresentation: boolean,     muteAll: boolean,     askToUnmute: boolean,     muteSelective: boolean,     changeRole: boolean    ;   priority: number;   export interface PublishParams     audio:       bitRate: number,     codec: string    ;   video:       bitRate: number,     codec: string,     frameRate: number,     width: number,     height: number    ;   screen:       bitRate: number,     codec: string,     frameRate: number,     width: number,     height: number    ;   allowed: string  ;       BREAKING CHANGE: For users of the  Reactive Store ( ) and  React SDK ( ), the HMSPeer interface has been modified. Use peer.roleName instead of peer.role to obtain the name of a peer's role.    Fixed   Improved automatic detection of input/output devices.   Audio level update in Firefox due to wrong track ID.    Known Issues   Safari on Mac gives echo with some wired headphones   With some bluetooth headphones(eg: OnePlus Bullets) with stereo output option, the default audioOutput doesn't work on windows.   (Quick Fix: Change device to another option of same device)   2021-07-17    Added   Added custom audio output management.  Refer Usage (/javascript/v2/advanced-features/volume-control)   Added audio output device and volume control using hmsActions.setAudioOutputDevice(\"a-valid-MediaDeviceInfo-object-id\") and hmsActions.setVolume(30) .  Refer Usage ( )   Audio Level at preview could be fetched by selectPeerAudioByID(localPeer.id) .    Fixed   Audio level update callback in preview.  Refer Usage (/javascript/v2/advanced-features/audio-level)     Experimental   Automatic device change on plug/unplug. Use track.setSettings to override input devices.  Refer Usage (/javascript/v2/features/device-change). Use audio output manager( hms.getAudioOutput().setDevice() ) to override output devices.  Refer Usage (/javascript/v2/features/device-change)   2021-07-14    Versions  Package           Version               @100mslive/hms-video     0.0.79   @100mslive/hms-video-react  0.2.57     Added   Adjust remote peer volume locally   Add HMSPeer, HMSTrack objects to HMSSpeaker for use in onAudioLevelUpdate callback.   ts interface HMSSpeaker     peer: HMSPeer;   track: HMSTrack;   audioLevel: number;       Add HMSPolicy to HMSPeer.    Fixed   Analytics   2021-07-09    Added   New preview API.   Screenshare with Audio    Fixed   Analytics   2021-07-08    Added   Preview method to enable showing preview UI.  Refer Usage (/javascript/v2/features/preview)   Audio in screen-share    Fixed   Analytics   2021-07-02    Added   Enable OpusDtx   Replace HMSPeer with HMSLocalPeer and HMSRemotePeer   Made @100mslive/hms-video as peer dependency. Install @100mslive/hms-video alongside @100mslive/hms-video-react to use 100ms web SDK in your app. For yarn,   yarn add @100mslive/hms-video @100mslive/hms-video-react   For npm,   npm i @100mslive/hms-video @100mslive/hms-video-react     Added transitionType property to hmsToast.    Fixed   Black tiles in Safari  Correctly queue messages when data channel not open   Retry Join Init API fetch failure and throw terminal error on retry failure   Silent Audio   Import problem in Angular  Replace Bowser with ua-parser-js   UI fixes in components.   2021-06-24    Added   Connection Reestablishment Mechanism.   On network switch, disconnection, Websocket signal, Publish and Subscribe ICE connections will be retried for reconnection   onReconnecting  called when reconnection starts   onReconnected  called when reconnection has succeeded   onError  called when reconnection has failed      Refer usage (/javascript/v2/features/notifications notification-types)   Internal Bug fixes   2021-06-21    Added   addTrack and removeTrack to add/remove auxiliary tracks to local peer.  See documentation (/javascript/v2/advanced-features/custom-tracks).   2021-06-17    Added   Notifications  See Documentation (/javascript/v2/features/notifications)   2021-06-13    Fixed   Join with camera/microphone device failures.   Join with camera/microphone failures.   Preview shows correct error message.   Start preview if either of camera/microphone is available.   Stop preview when page goes into background.   UI Fixes in modal, chat.   2021-06-11    Versions  Package           Version               @100mslive/hms-video     0.0.26   @100mslive/hms-video-react  0.2.14     Added   HMSPeer object will now have customerUserId property that stores the userId from  Client Token (/javascript/v2/foundation/security-and-tokens).    Fixed   Removed unsupported deviceId constraint in Safari.   Use EventEmitter instead of EventTarget to support older versions of some browsers.   selectDominantSpeaker replaces selectDominantSpeakerName . selectDominantSpeaker will return a HMSPeer object which contains customerUserId, name, role.   More responsive UI for smaller screens.   2021-06-10    Added   Handle device change on mid-call.    Fixed   Video tile going black when double/multiple clicked for unmute.   Block on preview on camera/mic failure.   2021-06-09    Added   Support for Firefox (&gt;=60 ).   Shim and polyfills for cross browser compatibility.   Exported isSupported method to check if browser and device are supported. import   isSupported   from \"@100mslive/hms-video\";    Fixed   Show dismissible warning for unsupported browsers in preview.   Export MessageModal component.   0.0.19  2021-06-02    Added   Error Codes  every error thrown from the SDK now will be of type HMSException, a wrapper over the native Error.   typescript interface HMSException     code: number; // A code number denoting the type of error   name: string; // Name of the error   action: string; // What action caused the error   message: string; // Message related to the error   description: string; // Additional information and steps to recover from the error       0.0.18  2021-06-02    Fixed   Screenshare permitted only when user's role is allowed to share screen.   Improved screenshare quality by removing maxBitrate limit.   Alpha Our alpha release is here. Thanks for your patience : ). This release might still have bugs, some interfaces can change (we will keep them to a minimum ) This release has   Simplified SDK interfaces that can power a fully-featured video conferencing application     join     leave     getPeers, getLocalPeer for tracks/peers/other data   Features not covered in this release     Handling network disconnections (disconnect from wi-fi to 4g ends the call )     Handling subscription network degradations     Handling second order publish network degradations     Detailed analytics     Recording   Our sample app is currently a React edtech app to demonstrate how roles work, and also how our UI-components work. Documentation for these roles will be released soon   Browser coverage is limited to latest versions of Chrome on desktop. We will improve this over the next few releases   Please contact @akash goswami in your Slack channel if you have any questions "
    },
    {
        "title": "Adaptive Bitrate",
        "link": "/javascript/v2/foundation/adaptive-bitrate",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/foundation/adaptive-bitrate",
        "keywords": [],
        "content": "  Adaptive bitrate (ABR) refers to features that enable dynamic adjustments to video quality—to optimise for end-user experience under diverse network conditions. ABR ensures that every participant is able to consume the highest possible quality video in conferencing or streaming use-cases, based on their bandwidth constraints.\r \r In addition to network, ABR can also optimise for the right video quality based on the size of the video element. For example, a video call running on active speaker layout has larger video tiles that require higher quality video track. These adjustments can be made dynamically with adaptive bitrate.\r \r Learn about how 100ms enables adaptive bitrate in:\r \r    Conferencing scenarios ( abr-in-conferencing)\r    Live streaming scenarios ( abr-in-live-streaming)\r \r  ABR in conferencing\r \r Peers in 100ms rooms can publish multiple video quality levels simultaneously. This is called “simulcast” in 100ms. Peers that consume these video tracks can upgrade or downgrade video quality.\r \r You can enable simulcast on the publishing role's template, and use manual or automatic layer changes on the subscriber's side.\r \r   Publisher-side configuration\r \r Simulcast configuration is opt-in and can be enabled on the role's configuration inside your template. The role's publish video quality determines video quality layers on simulcast. For example, a role configured to publish at 720p can simulcast 180p, 360p and 720p layers.\r \r  Video publish quality  Possible simulcast layers \r               \r  1080p          1080p, 540p, 270p     \r  720p          720p, 360p, 180p      \r  480p          480p, 240p         \r  360p          360p, 180p         \r  240p          240p            \r  180p          180p            \r \r   Enable via dashboard\r \r Enable \"can publish simulcast\" on the template page for a particular role. You can also specify how many video quality layers will be simultaneously published by peers of this role. The peer will publish these layers assuming network bandwidth permits.\r \r   Simulcast configuration (/docs/guides/simulcast-on-dashboard.png)\r \r   Enable via API\r \r Update role configuration using the  server-side API (/docs/server-side/v2/policy/create-update-role). The simulcast config payload can include 2 or 3 layers that scale down the selected publish layer.\r \r In the example below, the role is configured to publish 720p with 3 simulcast layers:\r \r   f for full with scale down factor of 1 (= 720p)\r   h for half with scale down factor of 2 (= 360p)\r   q for quarter with scale down factor of 4 (= 180p)   js     \"publishParams\":       ...     \"simulcast\":         \"video\":           \"layers\":                           \"rid\": \"f\",             \"scaleResolutionDownBy\": 1,             \"maxBitrate\": 700,             \"maxFramerate\": 30            ,                         \"rid\": \"h\",             \"scaleResolutionDownBy\": 2,             \"maxBitrate\": 250,             \"maxFramerate\": 30            ,                         \"rid\": \"q\",             \"scaleResolutionDownBy\": 4,             \"maxBitrate\": 100,             \"maxFramerate\": 30                              ,       \"screen\":                  \r   Subscribe-side behavior\r \r   Manual layer selection\r \r The 100ms client-side SDKs provide methods to set a preferred quality layer for a remote peer's video track. See docs for your preferred platform:\r \r    JavaScript (/docs/javascript/v2/advanced-features/simulcast)\r    iOS (/docs/ios/v2/advanced-features/simulcast)\r    Android (/docs/android/v2/advanced-features/simulcast)\r \r   Automatic layer selection\r \r <br \r <video loop=\"true\" controls=\"controls\" id=\"vid\" muted>\r   <source src=\"/docs/guides/simulcast-tile-size-1.mp4\" type=\"video/mp4\"  \r </video>\r <br \r \r  Based on video tile size: The SDK automatically ensures appropriate video layer is subscribed to, as demonstrated in the video above. For example, if the video element is 360 px in width, 360p or the closest layer will be selected.\r      JavaScript  : The useVideo hook in the 100ms React SDK auto-selects the appropriate video quality layer.\r      iOS  : HMSVideoView can auto-select video quality layer.\r      Android  : HMSVideoView ( see docs (/docs/android/v2/migrations/surfaceview-migration)) can auto-select video quality layer.\r  Based on network quality: ABR will work alongside subscribe degradation and auto-downgrade video quality for peers. This is _coming soon_.\r \r  ABR in live streaming\r \r 100ms uses the HTTP Live Streaming (HLS) protocol in live streaming scenarios. HLS supports adaptive bitrate out of the box, and HLS video players can automatically or manually pick appropriate video quality levels.\r \r Learn more on  how HLS works on our blog (https://www.100ms.live/blog/hls-101-beginners-guide).\r "
    },
    {
        "title": "Basic Concepts\r",
        "link": "/javascript/v2/foundation/basics",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/foundation/basics",
        "keywords": [],
        "content": " -\r \r  Architecture 100ms is a cloud platform that allows developers to add video and audio conferencing to Web, Android and iOS applications. The platform provides REST APIs, SDKs, and a dashboard that makes it simple to capture, distribute, record, and render live interactive audio, video. Any application built using 100ms' SDK has 2 components.     Client:   Use 100ms android, iOS, Web SDKs to manage connections, room states, render audio/video.     Server:   Use 100ms' APIs or dashboard to create rooms, setup room templates, trigger recording or RTMP streaming, access events.   Architecture (/docs/docs/v2/arch.png)   Basic Concepts   Room A room is the basic object that 100ms SDKs return on successful connection. This contains references to peers, tracks and everything you need to render a live a/v or live streaming app.   Peer A peer is the object returned by 100ms SDKs that contains all information about a user  name, role, video track etc.   Session A session depicts activity in a room. A session is created when one or more peers join a room to communicate with each other. A single room can have multiple sessions; a unique ID will be assigned to each session. The maximum allowed duration for a session on the 100ms platform is 12 hours.   Track A track is a segment of media (audio/video) captured from the peer's camera and microphone. Peers in a session publish local tracks and subscribe to remote tracks from other peers.   Role A role defines who can a peer see/hear, the quality at which they publish their video, whether they have permissions to publish video/screenshare, mute someone, change someone's role.   Template A template is a collection of roles, room settings, recording and RTMP settings (if used), that are used by the SDK to decide which geography to connect to, which tracks to return to the client, whether to turn on recording when a room is created, etc. Each room is associated with a template.   Destinations Destinations is used to save audio/video calls for offline viewing. 100ms supports 2 kinds of recording   SFU recording (/docs/javascript/v2/foundation/recordings sfu-recording-advanced) and  Browser recording (/docs/javascript/v2/foundation/recordings browser-recording-recommended). Also, HLS enabled configuration will allow you to live stream your room over HLS.   RTMP RTMP streaming is used to live stream your video conferencing apps to platforms like YouTube, Twitch, Facebook, MUX, etc.   Webhooks Webhook is an HTTP(S) endpoint used for pushing the notifications to your application. It will be invoked by 100ms servers to notify events of your room.   Workspace A workspace is an isolated environment which contains account data like templates, rooms, room and session history, etc. You can use workspaces to represent environments like “Production” and “Development” and invite team members to a workspace.   What are the steps to build a live app with 100ms? 1. Sign up on 100ms using the   Try For Free   button in the top navbar.   Signup for 100ms account (/docs/docs/v2/signup.png) 2. Once you're logged in to the dashboard, click on Create Your First App    Signup for 100ms account (/docs/docs/v2/create-your-first-app.png) 3.   Hover   on one of the Starter Kits. Deploy one of them. (We will use the   Video Conference Starter Kit   for this example)   Dashboard _ 100ms.png (/docs/docs/v2/select-starter-kit.png) 4. Select your account type and fill in the details   Dashboard _ 100ms.png (/docs/docs/v2/personal-details.png) 5. Choose a deployment option. This could be 100ms or Vercel (based on the Starter Kit you are deploying)   Video Conferencing Starter Kit (/docs/docs/v2/choose-your-deployment.png) 6. Enter a subdomain of your choice. Please avoid entering https/http/www or dots while entering the subdomain. Select a region closest to you and hit Continue.   choose subdomain (/docs/docs/v2/choose-subdomain.png) 7. Join or Invite someone to your deployed app with one of the roles:    join or invite (/docs/docs/v2/demo-your-app.png)   Where should I start? \r \r   Quickstart\r \r If you want to see 100ms' SDKs in action in under 5 minutes, run one of our quickstart  apps (/javascript/v2/guides/javascript-quickstart)\r "
    },
    {
        "title": "Handling audio-video edge cases",
        "link": "/javascript/v2/foundation/handling-audio-video-edge-cases",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/foundation/handling-audio-video-edge-cases",
        "keywords": [],
        "content": "    Introduction 100ms handles a lot of standard audio/video issues internally without the developer needing to handle it explicitly. This page describes some common issues and how 100ms handles them. There are 3 major issues of issues that can occur in a audio/video conference  Device capture exceptions  Network disconnection/switching network exceptions  Network bandwidth limitation/large room exceptions   Device failure A common issue is a failure to capture mic/camera even though the user has all devices connected. Common causes include differences in OS/browser implementations of device capture APIs, permission not being granted by the user, or the device being in use by another program. The usual recourse in these exceptions is to prompt a user action  \"Grant permission\", \"Please close any other app using microphone\", \"Switch to Safari\" 100ms' SDKs come with a  preview method (../features/preview) that can be called before joining a room. This will test for device failures, network connectivity and throw errors with a recommended user action.   Network disconnection/Switching networks Another set of common issues are minor network blips. Common causes are when a user moves from one room to another, or switches from wifi to data. 100ms will send a notification within 10s of detecting a network disconnection and will automatically retry when connection is available upto 60s. After 60s, a terminal error is thrown to the client.   Network bandwidth limitation/large rooms A common occurrence in large rooms, or constrained networks is dropped frames. This results in robotic voices, frozen frames, pixelated screenshare or entire pieces of audio/video that are lost. 100ms will automatically prioritize connections if network limits are reached. This prioritization can be controlled by developers using the dashboard or 100ms APIs. eg. A developer can prioritize host's screenshare higher than guests' videos. In low bandwidth constraints, guests' videos will be turned off, while host's screenshare will remain.  "
    },
    {
        "title": "Interactive Live Streaming",
        "link": "/javascript/v2/foundation/live-streaming",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/foundation/live-streaming",
        "keywords": [],
        "content": "    Overview Live video interactions can span various levels of interactivity. For example, in a virtual event, some participants can be on stage talking to each other, whereas participants in the audience can be listening to them. At 100ms, we think of this as the   3 levels of interactivity  . 100ms enables you to build live video use-cases by mixing and matching these 3 levels to get to your ideal solution.     Level 1  : Full duplex audio/video in real-time   Level 1 participants publish their audio/video, and interact with others in sub-second latency. This is real-time video conferencing, similar to Zoom or Google Meet.     Level 2  : Webinar-style audience in real-time   Level 2 participants consume audio/video from level 1 participants with sub-second latency, without publishing their own audio/video. Level 2 participants can engage with level 1 through messaging (chat, emojis, custom events). This is similar to a Zoom webinar.   Levels 1 and 2 are enabled using WebRTC.     Level 3  : Live stream audience consuming in near real-time   Level 3 participants consume a composite live stream in near real-time (<10 secs of latency) without publishing their audio/video. They can interact with other participants via messaging. This is similar to viewers on Twitch or YouTube Live, and is enabled via   100ms Interactive Live Streaming  .   Live streaming uses  HLS (https://www.100ms.live/blog/hls-101-beginners-guide) to achieve near real-time latency at scale. The  roles primitive (/docs/javascript/v2/foundation/templates-and-roles) can be used to define capabilities of a participant and associate them to an interaction level. A participant can move between levels using a single API call to change roles.   Try Interactive Live Streaming Use our  Live Streaming Starter Kit (https://www.100ms.live/marketplace/live-streaming-starter-kit) to try out the experience before you write a line of code. <StepsToc  parentId=\"try-live-streaming\"  descriptions=  \"Use our Live streaming starter kit to try out the experience before you write a line of code.\", \"Understand the difference between a stream broadcaster and stream viewer.\", \"Use the demo app link to go live for the first time as a broadcaster and join the stream as viewer.\", \"Use the 100ms self-serve dashboard to update the layout, aspect ratio, etc of the stream.\"   > <StepsContainer id=\"try-live-streaming\">   Create a new app   Live streaming starter kit (/docs/docs/v2/live-streaming-starter-kit.png) 1. Make sure that you have  an account with 100ms (https://dashboard.100ms.live/register) and can access the  100ms dashboard (https://dashboard.100ms.live/) 1. On the dashboard, create a new app using the Live Streaming Starter Kit 1. Specify a subdomain and region to deploy the app   Understand roles   Live Streaming roles (/docs/docs/v2/live-streaming-roles.png) This starter kit configures your new app with  two roles (/docs/javascript/v2/foundation/templates-and-roles):   broadcaster : This role represents a streamer who publishes their audio/video. There can be multiple peers who join as broadcasters   hls-viewer : This role represents a circle 3 audience, who subscribes to the composite live stream and can interact using messaging   Go live   Go live (/docs/docs/v2/live-streaming-go-live.gif) 1. To go live for the first time, join the room as a broadcaster and start the live stream 2. Once the stream has started, join the room as an hls-viewer and you should be able to see the ongoing live stream 3. Use chat messages to interact between the viewer and the broadcaster   Customize the stream   Go live (/docs/docs/v2/live-streaming-customise.png) By default, the live stream is composed in landscape mode for desktop viewers (with an aspect ratio of 16:9). You can customise the live stream for viewers on mobile or to support multiple broadcaster tiles. 1. On the 100ms dashboard, click the gear icon on your app to open configuration settings 2. Go to \"destinations\" and scroll down to find live stream (HLS) configuration 3. Update the configuration based on your needs:    If your viewers are on mobile, change the video aspect ratio to 9:16    If you have multiple broadcasters joining in, choose grid or active speaker based on your needs    In case of grid layout, choose the tile size that fits your use-case. For example, a stream with 2 streamers looks better with 1:1 tiles. </StepsContainer>   Integrate in your app To integrate 100ms Interactive Live Streaming in your app, follow these steps: 1.  Enable live streaming destination ( enable-destination) 2.  Integrate the 100ms SDK in your app ( sdk-integration) 3.  Integrate live stream playback ( live-stream-playback) 4.  Video on demand (VOD) use cases ( video-on-demand-vod-use-cases)   Enable destination   Enable HLS (/docs/docs/v2/live-streaming-enable.gif) If your app is based on the Live Streaming Starter Kit (as shown above), the live streaming destination is enabled out-of-the-box. For custom apps, you can enable the live streaming destination manually: 1. Open configuration for your existing app using the 100ms dashboard 1. In the \"destinations\" tab, enable \"Live Streaming with HLS\" 1. Ensure that you have roles for the broadcaster (who can publish their audio/video) and the viewer (who cannot publish audio/video)   SDK integration Use the 100ms client-side SDKs to integrate streaming in your application. See code snippets for the client-side SDK  here (/docs/javascript/v2/features/hls).   Live stream playback Using our client-side SDKs, you can enable live stream playback and add interactive experiences like chat, raise hand and other functionalities to your app using  peer metadata (/docs/javascript/v2/advanced-features/peer-metadata). The process is so simple: 1. Once you  start ( step-3-go-live) live streaming, you will get an HLS URL (M3U8 URL) which you can use for playback. 2. You can use the  client-side SDK (/docs/javascript/v2/features/hls) to get the HLS URL by checking the  current state (/docs/javascript/v2/features/hls current-room-status) of the room and start playback. If you need to only enable HLS playback and don't need interactivity, you can follow one of the below approaches to get the HLS URL:     Webhook:   You can listen to hls.started.success  webhook event (/docs/server-side/v2/introduction/webhook hls-started-success) and get the HLS URL from the url field. Please check the  webhooks guide (/docs/server-side/v2/introduction/webhook) to learn more about webhooks.     Static URL:   This configuration will help you get a static URL for playback. You can enable the Static playback URLs in your template from the  dashboard (https://dashboard.100ms.live/dashboard). You can go to Destination > enable \"Live streaming with HLS\" > under \"Customise stream video output\" section > enable \"Static playback URLs.\"     Enable Static URL (/docs/docs/v2/enable-static-url.png)     _Format_: https://cdn.100ms.live/beam/<customer_id>/<room_id>/master.m3u8     customer_id : replace this placeholder with your customer_id from  developer section (https://dashboard.100ms.live/developer) on your dashboard.     room_id : replace this placeholder with the room_id of the respective room from which the stream will be broadcasted.   Video on Demand (VOD) use cases If you wish to replay your HLS stream for Video on demand (VOD) use case, 100ms provides the capability to record the HLS stream which will be posted to your webhook as a ZIP file of M3U8 format (same playback format as HLS) with all the chunks once the stream ends. You can start recording a live stream using the  client-side SDK (/docs/javascript/v2/features/hls) or using the  server API (/docs/server-side/v2/Destinations/rtmp-streaming-and-browser-recording start-streaming-recording). Once the HLS recording is completed, you will get the details of recording as a callback to the webhook configured in your account. Check hls.recording.success  webhook event (/docs/server-side/v2/introduction/webhook hls-recording-success) for more information. "
    },
    {
        "title": "Recordings",
        "link": "/javascript/v2/foundation/recordings",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/foundation/recordings",
        "keywords": [],
        "content": "  Recordings are an important part of the live video stack as they convert live, ephemeral content into a long-term asset. But the use of this asset varies from business to business depending on their respective use case. For example, one of the common use cases for recording is for archival purposes versus, for some, its content to be publicized. Based on your end goal, you can choose one of the recording types and its implementation. You can understand some key differences using the comparison table below.   Recording types   Recording types ( recording-types)    Quick Comparison ( quick-comparison)    Browser Recording  Recommended   ( browser-recording-recommended)    SFU Recording  Advanced   ( sfu-recording-advanced)    Recordings for Live Streaming Use-cases ( recordings-for-live-streaming-use-cases)     Video-on-demand Recording ( video-on-demand-recording)     Multiresolution Recording ( multiresolution-recording)   Configure storage ( configure-storage)    How to configure recording storage? ( how-to-configure-recording-storage)   Quick Comparison  Recording Features           Browser Recording  Recommended   SFU Recording  Advanced                                      Resolution               Upto 1080p            Only 720p              Participant-level Audio/Video Tracks  Not Available           Available              Portrait/Landscape Mode        Available             Not Available            Start/Stop Recording          On-demand             Auto start/stop with the session   Custom Layout             Available             Not Available            Role-Specific Recording        Available             Not Available            Recording Output            MP4                MP4, WebM                Browser Recording  Recommended  Browser recording is built to give users a participant-first recording experience. When enabled, our browser-based bot Beam joins a room to record the viewport like any other participant. The output is an MP4 file that captures the room's published audio/video tracks together into one single file. This option removes the complexity of syncing various audio/video tracks and offers an intuitive, participant-first recording experience. An example use case is to record a sales meeting for later usage.   Resources     How to implement Browser Recording (https://www.100ms.live/docs/server-side/v2/Destinations/rtmp-streaming-and-browser-recording)   SFU Recording  Advanced  SFU recording is built for advanced use cases, which require individual audio and video tracks for each participant. This recording option allows you to isolate recording at a participant level. Track recording allows you to record audio and video streams separately, making it easier to edit, layer, or reuse each of them. An example use case is to record a live podcast and later edit it for publishing. You can get track recordings in two forms:   Individual: Media for each peer is provided as a separate mp4 file. This file will have both audio and video of the peer. These files can be used for offline review or in implementing custom composition.   Composite  currently in beta : Audio and video of all peers are composed as per their joining/leaving the meeting and provided as a single mp4. This file can be used for offline viewing of the meeting.   Resources      How to implement SFU Recording (https://www.100ms.live/docs/server-side/v2/Destinations/recording)   Recordings for Live Streaming Use-cases These are the types of live streaming recordings:    Video-on-demand Recording Video-on-demand recording is available for our Interactive Live Streaming capability. This recording will be a file with an M3U8 file (same playback format as HLS), which can be used for replaying your HLS stream. This option is more suitable for Video-on-Demand use cases. For the implementation of this type of recording, please  contact us (https://www.100ms.live/contact).    Multiresolution Recording A multi-resolution recording is available for Interactive Live Streaming capability. This type of recording will have a multi-file structure for all available resolutions of the stream. The output will be multiple MP4 files with these resolutions: 240p, 480p, 720p, and 1080p. For an implementation of this type of recording, please  contact us (https://www.100ms.live/contact).   Configure storage You can specify a cloud storage location for your recording files in your template. Our current offering allows you to store your recordings in Amazon S3 buckets. Once you configure the S3 config of your bucket in a template, all respective recordings of sessions created via those templates will be sent to your configured bucket. This holds true for all types of aforementioned recordings.   How to configure recording storage? 1. Generate your credentials; for this example, you can check out a  guide from AWS (https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html). You can skip this step if you already have credentials. Please note that if you are running a Browser recording, you need to give upload permission to your key, but if you are running an SFU recording, you need to give both upload and download permission. 2. Go to 100ms Dashboard and go to template   configuration by selecting the configure icon  .   Create your first app (/docs/docs/v2/recording-storage-settings-step2.png) 3. Head over to the   Destinations   tab.   Destinations (/docs/docs/v2/recording-storage-settings-step3.png) 1. Key in your credentials (using an example of an S3 bucket here):    Access Key: Access Key generated from AWS IAM Console    Secret Key: Secret Key generated from AWS IAM Console    Bucket: Name of the bucket in S3    Region: Name of the region, for example, ap-south1    Prefix for Upload Path: Define the directory name (optional)   Destinations (/docs/docs/v2/recording-storage-settings-step4.png) 5. Use the   Validate Config   button to test your storage setup.   Destinations (/docs/docs/v2/recording-storage-settings-step5.png) 6. You will see a message that the AWS   configuration was successfully validated  .   Destinations (/docs/docs/v2/recording-storage-settings-step6.png) The above message ensures that your configuration is successful now, and all your recordings will start collecting in your configured destination. "
    },
    {
        "title": "Authentication and Tokens\r",
        "link": "/javascript/v2/foundation/security-and-tokens",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/foundation/security-and-tokens",
        "keywords": [],
        "content": " -\r \r  Introduction In 100ms, two types of tokens are used to authenticate requests coming from your Client apps and Backend application server into the 100ms platform.    App token ( app-token) : Used to authenticate and allow end-users (peers) to join 100ms rooms. An App Token controls Peer identity and Room permissions in your real-time or Interactive live-streaming video application.    Management token ( management-token) : Used to authenticate all the requests to 100ms REST API. You can set the expiry to both these tokens; if you follow the code samples from this guide, the expiry will be set as 24 hours. However, a best practice is to set the expiry as short as feasible for your application. You must host your server to generate these tokens while transitioning your app to production.   App Token 100ms _client-side SDKs_ use App Tokens to authenticate a peer (participant) while  joining a room (./../features/join). Generate this token on the server side and make it available for your client-side apps that use the 100ms SDKs. To create an App Token, you need to use app_access_key , app_secret , room_id , and user_id .   You can get the app_access_key and app_secret from the  developer section (https://dashboard.100ms.live/developer) in your 100ms dashboard. This key and secret differ across workspaces so please ensure you are in the intended workspace before copying these credentials.     room_id  : This is the unique identifier for your room. You can get it from the  rooms page (https://dashboard.100ms.live/rooms) in your dashboard or in the response payload of the  create room server-side API (/docs/server-side/v2/Rooms/create-via-api).     user_id  : This identifier can be used to map a 100ms peer to your own internal user object for business logic. Specify your internal user identifier as the peer's user_id. If not available, use any random string.   How to use? You can get App tokens using a couple of approaches based on your app's lifecycle stage. Please check the below sections for more information:    Set up your own authentication endpoint (./../foundation/security-and-tokens set-up-your-own-authentication-endpoint)    Get app tokens from the token endpoint (./../foundation/security-and-tokens get-app-tokens-from-the-token-endpoint)    Get app tokens from the dashboard (./../foundation/security-and-tokens get-app-tokens-from-the-dashboard)    Set up your own authentication endpoint When you have completed your integration with 100ms, and while transitioning your app to production, we recommend you create your backend service for app token generation; use the code snippet below and set up the token generation service as per your preferred programming language. \r \r   Code sample: Generate app token\r \r  s id=\"client-code-token\" items=  'Node.js', 'Python', 'Java', 'Ruby', 'PHP'    \r \r   id='client-code-token-0'>\r \r  javascript\r var jwt = require('jsonwebtoken');\r var uuid4 = require('uuid4');\r \r var app_access_key = '<app_access_key>';\r var app_secret = '<app_secret>';\r \r var payload =  \r   access_key: app_access_key,\r   room_id: '<room_id>',\r   user_id: '<user_id>',\r   role: '<role>',\r   type: 'app',\r   version: 2,\r   iat: Math.floor(Date.now() / 1000),\r   nbf: Math.floor(Date.now() / 1000)\r  ;\r \r jwt.sign(\r   payload,\r   app_secret,\r    \r     algorithm: 'HS256',\r     expiresIn: '24h',\r     jwtid: uuid4()\r    ,\r   function (err, token)  \r     console.log(token);\r    \r );\r  \r \r </Tab>\r \r   id='client-code-token-1'>\r \r  python\r  /usr/bin/env python3\r import jwt\r import uuid\r import datetime\r import sys\r \r app_access_key = \"<app_access_key>\"\r app_secret = \"<app_secret>\"\r \r def generate(room_id, user_id, role):\r   expires = expires or 24   3600\r   now = datetime.datetime.utcnow()\r   exp = now+ datetime.timedelta(seconds=expires)\r   return jwt.encode(payload= \r         \"access_key\": app_access_key,\r         \"type\":\"app\",\r         \"version\":2,\r         \"room_id\": room_id,\r         \"user_id\": user_id,\r         \"role\":role,\r         \"jti\": str(uuid.uuid4()),\r         \"exp\": exp,\r         \"iat\": now,\r         \"nbf\": now,\r          , key=app_secret)\r if __name__ == \"__main__\":\r   if len(sys.argv) == 3:\r     room_id = sys.argv 0 \r     user_id = sys.argv 1 \r     role = sys.argv 2 \r   print(generate(room_id=room_id, user_id=user_id, role=role))\r  \r \r </Tab>\r \r   id='client-code-token-2' >\r \r  java\r import java.time.Instant;\r import java.util.Date;\r import java.util.HashMap;\r import java.util.Map;\r import java.util.UUID;\r import io.jsonwebtoken.Jwts;\r import io.jsonwebtoken.SignatureAlgorithm;\r \r private void generateHmsClientToken()  \r   Map<String, Object> payload = new HashMap<>();\r   payload.put(\"access_key\", \"<app_access_key>\");\r   payload.put(\"room_id\", \"<room_id>\");\r   payload.put(\"user_id\", \"<user_id>\");\r   payload.put(\"role\", \"<role>\");\r   payload.put(\"type\", \"app\");\r   payload.put(\"version\", 2);\r   String token = Jwts.builder().setClaims(payload).setId(UUID.randomUUID().toString())\r     .setExpiration(new Date(System.currentTimeMillis() + 86400   1000))\r     .setIssuedAt(Date.from(Instant.ofEpochMilli(System.currentTimeMillis()  60000)))\r     .setNotBefore(new Date(System.currentTimeMillis()))\r     .signWith(SignatureAlgorithm.HS256, \"<app_secret>\".getBytes()).compact();\r   \r  \r \r </Tab>\r \r   id='client-code-token-3' >\r \r  ruby\r require 'jwt'\r require 'securerandom'\r \r \r \r $app_access_key = \"<app_access_key>\"\r $app_secret = \"app_secret\"\r \r def generateAppToken(room_id, user_id, role)\r   now = Time.now\r   exp = now + 86400\r   payload =  \r     access_key: $app_access_key,\r     room_id: room_id,\r     user_id: user_id,\r     role: role,\r     type: \"app\",\r     jti: SecureRandom.uuid,\r     version: 2,\r     iat: now.to_i,\r     nbf: now.to_i,\r     exp: exp.to_i\r    \r \r   token = JWT.encode(payload, $app_secret, 'HS256')\r end\r \r puts generateAppToken \"<room_id>\", \"<user_id>\", \"<role>\"\r \r  \r \r </Tab>\r \r   id='client-code-token-4'>\r \r  php\r <?php\r \r use Firebase JWT JWT;\r use Ramsey Uuid Uuid;\r \r $issuedAt = new DateTimeImmutable();\r $expire  = $issuedAt->modify('+24 hours')->getTimestamp();\r $accessKey = \"<app_access_key>\";\r $secret = \"<app_secret>\";\r $version  = 2;\r $type   = \"app\";\r $role   = \"<role>\";\r $roomId  = \"<room_id>\";\r $userId  = \"<user_id>\";\r \r $payload =  \r   'iat' => $issuedAt->getTimestamp(),\r   'nbf' => $issuedAt->getTimestamp(),\r   'exp' => $expire,\r   'access_key' => $accessKey,\r   'type' => \"app\",\r   'jti' => Uuid::uuid4()->toString()\r   'version' => 2,\r   'role' => $role,\r   'room_id' => $roomId,\r   'user_id' => $userId\r  ;\r \r $token = JWT::encode(\r   $payload,\r   $secret,\r   'HS256'\r );\r  \r \r </Tab>\r \r <Note type=\"warning\">\r   We strictly advise you not to post <i>app_access_key</i> and <i>app_secret</i> publicly; if you\r   need to store them in <strong>Git</strong>, please change the repository from public to private.\r   <br  \r   <br   You cannot use an <strong>App token</strong> to trigger server API requests.\r </Note>\r \r   Get app tokens from the token endpoint If you are building integration with 100ms, you can get app tokens from the 100ms token endpoint without hosting a token generation backend service. Refer to  this guide (./../guides/token-endpoint get-an-app-token-using-token-endpoint) for more information.    Get app tokens from the dashboard If you are building your first app by following one of our  quickstart guides (/docs/javascript/v2/guides/javascript-quickstart), you can get the app token directly from 100ms dashboard to join a room for the first time. Refer to  this guide (./../guides/token get-a-temporary-token-from-100ms-dashboard) for more information.   Management Token 100ms uses management tokens to authenticate REST APIs.  If you're evaluating 100ms  server APIs (/docs/server-side/v2/introduction/basics), you can use our public  Postman collection (/docs/server-side/v2/introduction/postman-guide fork-the-collection), which doesn't require you to create a management token as we've managed it using a  pre-request script (/docs/server-side/v2/introduction/postman-guide simplified-token-generation) within the collection. If you're transitioning your app to production, we recommend you create your backend service for management token generation. You must use the app_access_key and app_secret from the  developer section (https://dashboard.100ms.live/developer) in your 100ms dashboard to create the management token.\r \r   Code sample: Generate management token\r \r  s id=\"test-code\" items=  'Node.js', 'Python', 'Java', 'Ruby', 'PHP'    \r \r   id='test-code-0'>\r \r  js\r var jwt = require('jsonwebtoken');\r var uuid4 = require('uuid4');\r \r var app_access_key = '<app_access_key>';\r var app_secret = '<app_secret>';\r \r jwt.sign(\r    \r     access_key: app_access_key,\r     type: 'management',\r     version: 2,\r     iat: Math.floor(Date.now() / 1000),\r     nbf: Math.floor(Date.now() / 1000)\r    ,\r   app_secret,\r    \r     algorithm: 'HS256',\r     expiresIn: '24h',\r     jwtid: uuid4()\r    ,\r   function (err, token)  \r     console.log(token);\r    \r );\r  \r \r </Tab>\r \r   id='test-code-1'>\r \r  py\r  /usr/bin/env python3\r import jwt\r import uuid\r import datetime\r \r app_access_key = '<app_access_key>'\r app_secret = '<app_secret>'\r \r \r def generateManagementToken():\r   expires = 24   3600\r   now = datetime.datetime.utcnow()\r   exp = now + datetime.timedelta(seconds=expires)\r   return jwt.encode(payload= \r     'access_key': app_access_key,\r     'type': 'management',\r     'version': 2,\r     'jti': str(uuid.uuid4()),\r     'iat': now,\r     'exp': exp,\r     'nbf': now\r      , key=app_secret)\r \r if __name__ == '__main__':\r   print(generateManagementToken())\r  \r \r </Tab>\r \r   id=\"test-code-2\">\r \r  java\r import java.time.Instant;\r import java.util.Date;\r import java.util.HashMap;\r import java.util.Map;\r import java.util.UUID;\r import io.jsonwebtoken.Jwts;\r import io.jsonwebtoken.SignatureAlgorithm;\r \r private void generateManagementToken()  \r   Map<String, Object> payload = new HashMap<>();\r   payload.put(\"access_key\", \"<app_access_key>\");\r   payload.put(\"type\", \"management\");\r   payload.put(\"version\", 2);\r   String token = Jwts.builder().setClaims(payload).setId(UUID.randomUUID().toString())\r     .setExpiration(new Date(System.currentTimeMillis() + 86400   1000))\r     .setIssuedAt(Date.from(Instant.ofEpochMilli(System.currentTimeMillis()  60000)))\r     .setNotBefore(new Date(System.currentTimeMillis()))\r     .signWith(SignatureAlgorithm.HS256, \"<app_secret>\".getBytes()).compact();\r   \r  \r \r </Tab>\r \r   id=\"test-code-3\">\r \r  ruby\r require 'jwt'\r require 'securerandom'\r \r $app_access_key = \"<app_access_key>\"\r $app_secret = \"<app_secret>\"\r \r def generateManagementToken()\r   now = Time.now\r   exp = now + 86400\r   payload =  \r   access_key: $app_access_key,\r   type: \"management\",\r   version: 2,\r   jti: SecureRandom.uuid,\r   iat: now.to_i,\r   nbf: now.to_i,\r   exp: exp.to_i\r  \r token = JWT.encode(payload, $app_secret, 'HS256')\r return token\r end\r \r puts generateManagementToken\r  \r \r </Tab>\r \r   id=\"test-code-4\">\r \r  php\r <?php\r \r use Firebase JWT JWT;\r use Ramsey Uuid Uuid;\r \r $app_access_key = \"<app_access_key>\";\r $app_secret = \"<app_secret>\";\r \r $issuedAt  = new DateTimeImmutable();\r $expire   = $issuedAt->modify('+24 hours')->getTimestamp();\r \r $payload =  \r   'access_key' => $app_access_key,\r   'type' => 'management',\r   'version' => 2,\r   'jti' => Uuid::uuid4()->toString(),\r   'iat' => $issuedAt->getTimestamp(),\r   'nbf' => $issuedAt->getTimestamp(),\r   'exp' => $expire,\r  ;\r \r $token = JWT::encode($payload, $app_secret, 'HS256');\r ?>\r  \r \r </Tab>\r \r <Note type=\"warning\">\r   We strictly advise you not to post <i>app_access_key</i> and <i>app_secret</i> publicly; if you\r   need to store them in <strong>Git</strong>, please change the repository from public to private.\r   <br  \r   <br  \r   You cannot authenticate room join requests from your client-side apps with a <strong>\r     Management token\r   </strong>.\r </Note>\r "
    },
    {
        "title": "Templates and Roles",
        "link": "/javascript/v2/foundation/templates-and-roles",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/foundation/templates-and-roles",
        "keywords": [],
        "content": "    Introduction Template is the blueprint of the room. It defines the settings of the room along with the behavior of users who are part of it. Room will inherit the properties from a template that you have specified while creating it. If you have not specified any template then it will pick the default template. Each template will be identified by its id or name. For example default_videoconf_7e450ffc-8ef1-4572-ab28-b32474107b89  Users can see or modify the templates by visiting  Templates on Dashboard (https://dashboard.100ms.live/templates) or via API (see below). After updating a template or some part of its like permissions, you need to rejoin or restart the session for the template updates to take place.   Template (/docs/docs/v2/template.png)   Roles Role is a collection of permissions that allows you to perform certain set of operations while being part of the room. It has the following attributes:   Name Every role has a name that should be unique inside a template. This name will be used while generating app tokens and referencing inside a template.   Priority Priority will determine the order in which the roles will be degraded. A lower number represents a higher priority.   Publish Strategies Publish strategies will be used to determine the tracks and their quality which can be published by this role.  Strategy       Description                                                                                                                                                                                                                            Can share audio    Whether the role is allowed to publish the audio track or not.                                                                                                                  Can share video    Whether the role is allowed to publish the video track or not                                                                                                                   Can share screen   Whether the role is allowed to do screen share or not                                                                                                                       Video quality     Quality of the video track which is going to be published by the role. Currently, 6 video qualities 1080p , 720p , 480p , 360p , 240p and 120p are predefined and the user can select one out of these values. This option will be visible only if the   Can share video   is enabled.   Screenshare quality  Quality of the screen which is going to be shared by the role. Currently, 2 video qualities 720p and 1080p are predefined and the user can select one out of these values. This option will be visible only if the   Can share screen   is enabled.                       Subscribe Strategies Subscribe strategies will be used to determine what all roles, this role can subscribe to.  Strategy        Description                                                                                                                                                                                                      Subscribe to      You can select all the roles of the template which this role will subscribe                                                                                             Subscribe Degradation  When this flag is turned on, one or more remote video tracks will be muted automatically when the network condition worsens. Such tracks will be marked as degraded . When the network condition improves, the degraded tracks will automatically be unmuted.    Permissions Permissions will contain a list of additional privileges that this role will have.  Permission               Description                                                                                                                                              Can change any participant's role   With this permission, user will be able to change the role of the other participant's who are present in the room                                  Can mute any participant        With this permission, user will be able to mute any participant's audio and/or video.                                                Can ask participant to unmute     With this permission, user will be able to ask any participant to unmute their audio and/or video.                                         Can remove participant from the room  With this permission, user will be able to remove any participant from the current session of the room.                                       Can end current session of the room  With this permission, user will be able to end the current session of the room.                                                   Can receive room state         With this permission, user will be able to receive room state like peer-count and peer-list on the preview screen.                                 Can start/stop RTMP livestream     With this permission, user will be able to publish live audio/video livestream externally to social media and custom platforms (e.g Youtube/Facebook/Twitter).           Can start/stop HLS livestream     With this permission, user will be able to publish audio/video livestream in the HLS format.                                            Can start/stop Beam Recording     With this permission, user will be able to record meeting/livestream via the browser recording approach where a bot will join the room and record the meeting/livestream as is.    Advanced Settings As the name suggests, Advanced Settings section contains more settings and controls for the advanced user.   Template (/docs/docs/v2/advanced-settings.png)   Preview room state Preview room state enables you to build a \"preview\" screen which shows the state of the room before joining. This room state includes a list of peers, which can be used to show who is in the room. Preview room state also includes recording and streaming state. Preview room state settings define strategy of sending state updates to client SDKs.  Setting                  Description                                                                                                                                                                                                 Room-state Message Interval (in seconds)  Room-state data will be sent over a regular interval of these many seconds. Consequently, the room state displayed on the preview screen will refresh accordingly. This value must be a multiple of 5, between 5 and 3600 seconds, both inclusive.   Send Peer List in Room-state        Enabling this will send peer-list info of the room. If disabled, only the peer count is sent.                                                                             Enable Room-State             If enabled, room-state data will be sent to the preview screen. If disabled, no such room-state data will be sent.                                                                   Roles with room-state permission      This is the list of all the roles which will get the room-state data. You can also individually toggle these settings in the Roles tab under the Permissions section.                                           API reference Apart from the dashboard, a programmatic way to interact with templates is via  API (/server-side/v2/policy/template-object). "
    },
    {
        "title": "JavaScript Quickstart Guide",
        "link": "/javascript/v2/get-started/javascript-quickstart",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/get-started/javascript-quickstart",
        "keywords": [],
        "content": "    Overview This guide will walk you through step-by-step instructions to create a video conferencing application using the 100ms JavaScript SDK and test it locally on your device. If you plan to use React, do check our  React quickstart (https://www.100ms.live/docs/javascript/v2/guides/react-quickstart) as well. Please check our  basic concepts (/javascript/v2/foundation/basics) guide to understand the concepts like rooms, templates, peers, etc. TL;DR  Try out this complete example in  CodeSandbox (https://codesandbox.io/s/hms-js-quickstart-5rmes). <Codesandbox id=\"5rmes\"     Create a sample app locally In this section, we will walk you through a complete code example that will enable you to create a local video-conferencing application in under 10 minutes and test audio/video functionality along with a couple of other essential functionalities like mute/unmute audio and video. We will use  ParcelJS (https://parceljs.org/getting-started/webapp/), a web application bundler that requires zero configuration. If you wish to use any other library or framework, you can set up your app using that. <StepsContainer id=\"sample-app-locally\">   Initialize a project You can initialize a project using npm init or yarn init . While initializing the project you can enter the details for your project as you wish.   Install ParcelJS & 100ms SDK Then, install ParcelJS and 100ms SDK into your app like below   using yarn:    bash section=InstallingTheDependencies sectionIndex=1   yarn add dev parcel   yarn add @100mslive/hms-video-store       using npm:    bash section=InstallingTheDependencies sectionIndex=2   npm install save-dev parcel   npm install @100mslive/hms-video-store       Project setup Now that the dependencies are installed, let's create basic source files for our video-conferencing application. Let's create the files and folders in the below manner:   100ms-test   index.html   src    index.js    styles.css     Complete code example Next, copy the below code snippets and paste it into the respective files created in the above step.  s id=\"complete-code\" items=  'JS', 'HTML', 'CSS'       id=\"complete-code-0\">   jsx:index.js section=completeCodeExample sectionIndex=1 tab=JS import    HMSReactiveStore,  selectIsLocalAudioEnabled,  selectIsLocalVideoEnabled,  selectPeers,  selectIsConnectedToRoom,  selectVideoTrackByID,   from \"@100mslive/hms-video-store\"; // Initialize HMS Store const hmsManager = new HMSReactiveStore(); hmsManager.triggerOnSubscribe(); const hmsStore = hmsManager.getStore(); const hmsActions = hmsManager.getActions(); // HTML elements const form = document.getElementById(\"join\"); const joinBtn = document.getElementById(\"join-btn\"); const conference = document.getElementById(\"conference\"); const peersContainer = document.getElementById(\"peers-container\"); const leaveBtn = document.getElementById(\"leave-btn\"); const muteAudio = document.getElementById(\"mute-aud\"); const muteVideo = document.getElementById(\"mute-vid\"); const controls = document.getElementById(\"controls\"); // store peer IDs already rendered to avoid re-render on mute/unmute const renderedPeerIDs = new Set(); // Joining the room joinBtn.onclick = () =>    hmsActions.join(    userName: document.getElementById(\"name\").value,   authToken: document.getElementById(\"token\").value   );  ; // Leaving the room function leaveRoom()    hmsActions.leave();   // Cleanup if user refreshes the tab or navigates away window.onunload = window.onbeforeunload = leaveRoom; leaveBtn.onclick = leaveRoom; // Helper function to create html elements function createElementWithClass(tag, className)    const newElement = document.createElement(tag);  newElement.className = className;  return newElement;   // Render a single peer function renderPeer(peer)    const peerTileDiv = createElementWithClass(\"div\", \"peer-tile\");  const videoElement = createElementWithClass(\"video\", \"peer-video\");  const peerTileName = createElementWithClass(\"div\", \"peer-name\");  videoElement.autoplay = true;  videoElement.muted = true;  videoElement.playsinline = true;  peerTileName.textContent = peer.name;  hmsStore.subscribe((track) =>     if ( track)      return;       if (track.enabled)      hmsActions.attachVideo(track.id, videoElement);     else      hmsActions.detachVideo(track.id, videoElement);       , selectVideoTrackByID(peer.videoTrack));  peerTileDiv.append(videoElement);  peerTileDiv.append(peerTileName);  renderedPeerIDs.add(peer.id);  return peerTileDiv;   // display a tile for each peer in the peer list function renderPeers()    const peers = hmsStore.getState(selectPeers);  peers.forEach((peer) =>     if ( renderedPeerIDs.has(peer.id) && peer.videoTrack)      peersContainer.append(renderPeer(peer));       );   // Reactive state  renderPeers is called whenever there is a change in the peer-list hmsStore.subscribe(renderPeers, selectPeers);  // Mute and unmute audio muteAudio.onclick = async () =>    const audioEnabled =  hmsStore.getState(selectIsLocalAudioEnabled);  await hmsActions.setLocalAudioEnabled(audioEnabled);  muteAudio.textContent = audioEnabled ? \"Mute\" : \"Unmute\";  ; // Mute and unmute video muteVideo.onclick = async () =>    const videoEnabled =  hmsStore.getState(selectIsLocalVideoEnabled);  await hmsActions.setLocalVideoEnabled(videoEnabled);  muteVideo.textContent = videoEnabled ? \"Hide\" : \"Unhide\";  ; // Showing the required elements on connection/disconnection function onConnection(isConnected)    if (isConnected)     form.classList.add(\"hide\");   conference.classList.remove(\"hide\");   leaveBtn.classList.remove(\"hide\");   controls.classList.remove(\"hide\");    else     form.classList.remove(\"hide\");   conference.classList.add(\"hide\");   leaveBtn.classList.add(\"hide\");   controls.classList.add(\"hide\");      // Listen to the connection state hmsStore.subscribe(onConnection, selectIsConnectedToRoom);   </Tab>   id=\"complete-code-1\">   html:index.html section=completeCodeExample sectionIndex=1 tab=HTML < DOCTYPE html> <html>  <head>   <title>Quickstart JS</title>   <link rel=\"stylesheet\" href=\"src/styles.css\"     <meta charset=\"UTF-8\"    </head>  <body>   <header>    <img class=\"logo\" src=\"https://www.100ms.live/assets/logo.svg\"      <button id=\"leave-btn\" class=\"btn-danger hide\">Leave Room</button>   </header>   <form id=\"join\">    <h2>Join Room</h2>    <div class=\"input-container\">     <input id=\"name\" type=\"text\" name=\"username\" placeholder=\"Your name\"      </div>    <div class=\"input-container\">     <input id=\"token\" type=\"text\" name=\"token\" placeholder=\"Auth token\"      </div>    <button type=\"button\" class=\"btn-primary\" id=\"join-btn\">     Join    </button>   </form>   <div id=\"conference\" class=\"conference-section hide\">    <h2>Conference</h2>    <div id=\"peers-container\"></div>   </div>   <div id=\"controls\" class=\"control-bar hide\">    <button id=\"mute-aud\" class=\"btn-control\">Mute</button>    <button id=\"mute-vid\" class=\"btn-control\">Hide</button>   </div>   <script type=\"module\" src=\"src/index.js\"></script>  </body> </html>   </Tab>   id=\"complete-code-2\">   css:styles.css section=completeCodeExample sectionIndex=1 tab=CSS       margin: 0;   padding: 0;   box-sizing: border-box;     body     font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Oxygen,    Ubuntu, Cantarell, \"Open Sans\", \"Helvetica Neue\", sans-serif;   background-color: 263238;   color: white;     h1,  h2,  h3,  h4,  h5     font-weight: normal;     header     padding: 10px;   display: flex;   align-items: end;   justify-content: space-between;     .btn-danger     border: 1px solid transparent;   border-radius: 4px;   padding: 6px 14px;   background-color: f44336;   color: white;   font-family: inherit;   font-size: 14px;     .hide     display: none  important;     form     max-width: 450px;   margin: 30px auto;   box-shadow: 0 20px 40px rgba(0, 0, 0, 0.4);   border-radius: 8px;   padding: 20px;     input     display: block;   width: 100%;   border-radius: 8px;   border: 2px solid transparent;   height: 34px;   padding: 5px;   background: 37474f;   color: inherit;   font-family: inherit;     input::placeholder     color: aaa;     .input-container     margin-bottom: 20px;     .btn-primary     border: 1px solid transparent;   border-radius: 4px;   padding: 6px 14px;   background-color: 1565c0;   color: white;   font-family: inherit;   font-size: 14px;     form h2,  .conference-section h2     margin-bottom: 20px;     .conference-section     padding: 20px 30px;   max-width: 960px;   margin: 0 auto;     .conference-section h2     text-align: center;   font-size: 32px;   padding-bottom: 10px;   border-bottom: 1px solid 546e7a;      peers-container     display: grid;   grid-template-columns: repeat(3, minmax(min-content, 1fr));   place-items: center;   grid-gap: 10px;     .peer-video     height: 250px;   width: 250px;   border-radius: 40%;   object-fit: cover;   margin-bottom: 10px;     .local.peer-video     transform: scaleX(-1);     .peer-name     font-size: 14px;   text-align: center;     .peer-tile     padding: 10px;     .control-bar     display: flex;   position: fixed;   bottom: 0;   width: 100%;   padding: 15px;   justify-content: center;   z-index: 10;     .control-bar >  :not(:first-child)     margin-left: 8px;     .btn-control     font-size: 12px;   text-transform: uppercase;   letter-spacing: 1px;   border: 2px solid 37474f;   width: 64px;   height: 64px;   border-radius: 50%;   text-align: center;   background-color: 607d8b;   box-shadow: 0 0 10px rgba(0, 0, 0, 0.4);   color: white;      </Tab>   Test the app To test the application, run the application using:   using Yarn yarn parcel 100ms-test/index.html port 3000 or   when using npm, run npx parcel index.html port 3000 . This will start your development server in http://localhost:3000/. Open this in your browser to test the video conferencing app you created above.    Fetch token to join the room To test audio/video functionality, you need to connect to a 100ms room; please check following steps for the same: 1. Navigate to your  100ms dashboard (https://dashboard.100ms.live/dashboard) or  create an account (https://dashboard.100ms.live/register) if you don't have one. 2. Use the Video Conferencing Starter Kit to create a room with a default template assigned to it to test this app quickly. 3. Go to  Rooms page (https://dashboard.100ms.live/rooms) in your dashboard, click on the Room Id of the room you created above, and click on the Join room button on the top right. 4. You will see two 100ms demo URLs for two roles created when you deployed the starter kit; you can click on the 'key' icon to copy the token and use the same to join the room in your browser.     Join room key (/guides/join_room_key.png) Now, after you click join , you should be able to see yourself  <video loop=\"true\" autoplay=\"autoplay\" controls=\"controls\" id=\"vid\" muted>   <source src=\"/docs/guides/peer-1.mp4\" type=\"video/mp4\"   </video> <br   You can open the app in another tab in the browser to join as the second peer to check audio/video transactions between two or more peers. <video loop=\"true\" autoplay=\"autoplay\" controls=\"controls\" id=\"vid\" muted>   <source src=\"/docs/guides/peer-2.mp4\" type=\"video/mp4\"   </video> <br     We have a fully functional video conferencing application now   🎉🚀 </StepsContainer>   Building step-by-step In this section, let's walk through what the code does. <StepsToc   parentId=\"initialize-sdk-steps\"   descriptions=       'Initialize HMSStore and HMSActions from 100ms SDK for state management and to perform actions like join, leave, etc. in the app.',     'Include the HTML elements such as tiles and buttons to render peers and trigger actions like join, leave, etc.',     'Pass config object with userName and authToken to join method from HMSActions. Use leave method from HMSActions to leave the room.',     'Use attachVideo method from HMSActions to add video and selectPeers selector from hmsStore to display tile for peers.',     'Use setLocalAudioEnabled and setLocalVideoEnabled from HMSActions to mute/unmute local audio/video tracks.',     'Use selectIsConnectedToRoom selector from hmsStore to manage UI elements based on connection/disconnection.'        <StepsContainer id=\"initialize-sdk-steps\">   Initialize the SDK Let us start with initializing the libraries. We need two instances to get started:   An instance of  HMSStore (/api-reference/javascript/v2/interfaces/HMSStore) that holds the complete state of the application such as details of all the participants. We can also visualize this state at any time using the  DevTools (https://chrome.google.com/webstore/detail/redux-devtools/lmhkpmbekcpmknklioeibfkpmmfibljd?hl=en) extension.     This HMSStore is a single source of truth for the entire SDK. To get access to the data from this store, you have to call  hmsStore.subscribe() (/api-reference/javascript/v2/interfaces/IHMSStore subscribe). It takes two parameters. A callback and a selector which tells the store which slice of state you want to subscribe to.     The subscribe() also returns a reference to unsubscribe() which could be used later to ubsubscribe from the subscription.   And an instance of  HMSActions (/api-reference/javascript/v2/interfaces/HMSActions) that will help us perform actions such as joining the room, muting our audio, and sending messages.   jsx:index.js section=InitializingTheSDK sectionIndex=1 import    HMSReactiveStore,  selectIsLocalAudioEnabled,  selectIsLocalVideoEnabled,  selectPeers,  selectIsConnectedToRoom   from \"@100mslive/hms-video-store\"; // Initialize HMS Store const hmsManager = new HMSReactiveStore(); hmsManager.triggerOnSubscribe(); const hmsStore = hmsManager.getStore(); const hmsActions = hmsManager.getHMSActions();     Initialize the HTML elements Now, let's initialize all the HTML elements required such as tile for local and remote peers, join and leave button, mute and unmute button, etc.  s id=\"initialize-html-elements\" items=  'JS', 'HTML', 'CSS'       id=\"initialize-html-elements-0\">   jsx:index.js section=InitializingTheSDK sectionIndex=2 tab=JS // HTML elements const form = document.getElementById(\"join\"); const joinBtn = document.getElementById(\"join-btn\"); const conference = document.getElementById(\"conference\"); const peersContainer = document.getElementById(\"peers-container\"); const leaveBtn = document.getElementById(\"leave-btn\"); const muteAudio = document.getElementById(\"mute-aud\"); const muteVideo = document.getElementById(\"mute-vid\"); const controls = document.getElementById(\"controls\");   </Tab>   id=\"initialize-html-elements-1\">   html:index.html section=InitializingTheSDK sectionIndex=2 tab=HTML < DOCTYPE html> <html>  <head>   <title>Quickstart JS</title>   <link rel=\"stylesheet\" href=\"src/styles.css\"     <meta charset=\"UTF-8\"    </head>  <body>   <header>    <button id=\"leave-btn\" class=\"btn-danger hide\">Leave Room</button>   </header>   <form id=\"join\">    <h2>Join Room</h2>    <div class=\"input-container\">     <input id=\"name\" type=\"text\" name=\"username\" placeholder=\"Your name\"      </div>    <div class=\"input-container\">     <input id=\"token\" type=\"text\" name=\"token\" placeholder=\"Auth token\"      </div>    <button type=\"button\" class=\"btn-primary\" id=\"join-btn\">     Join    </button>   </form>   <div id=\"conference\" class=\"conference-section hide\">    <h2>Conference</h2>    <div id=\"peers-container\"></div>   </div>   <div id=\"controls\" class=\"control-bar hide\">    <button id=\"mute-aud\" class=\"btn-control\">Mute</button>    <button id=\"mute-vid\" class=\"btn-control\">Hide</button>   </div>   <script type=\"module\" src=\"src/index.js\"></script>  </body> </html>   </Tab>   id=\"initialize-html-elements-2\">   css:styles.css section=InitializingTheSDK sectionIndex=2 tab=CSS       margin: 0;   padding: 0;   box-sizing: border-box;     body     font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Oxygen,    Ubuntu, Cantarell, \"Open Sans\", \"Helvetica Neue\", sans-serif;   background-color: 263238;   color: white;     h1,  h2,  h3,  h4,  h5     font-weight: normal;     header     padding: 10px;   display: flex;   align-items: end;   justify-content: space-between;     .btn-danger     border: 1px solid transparent;   border-radius: 4px;   padding: 6px 14px;   background-color: f44336;   color: white;   font-family: inherit;   font-size: 14px;     .hide     display: none  important;     form     max-width: 450px;   margin: 30px auto;   box-shadow: 0 20px 40px rgba(0, 0, 0, 0.4);   border-radius: 8px;   padding: 20px;     input     display: block;   width: 100%;   border-radius: 8px;   border: 2px solid transparent;   height: 34px;   padding: 5px;   background: 37474f;   color: inherit;   font-family: inherit;     input::placeholder     color: aaa;     .input-container     margin-bottom: 20px;     .btn-primary     border: 1px solid transparent;   border-radius: 4px;   padding: 6px 14px;   background-color: 1565c0;   color: white;   font-family: inherit;   font-size: 14px;     form h2,  .conference-section h2     margin-bottom: 20px;     .conference-section     padding: 20px 30px;   max-width: 960px;   margin: 0 auto;     .conference-section h2     text-align: center;   font-size: 32px;   padding-bottom: 10px;   border-bottom: 1px solid 546e7a;      peers-container     display: grid;   grid-template-columns: repeat(3, minmax(min-content, 1fr));   place-items: center;   grid-gap: 10px;     .peer-video     height: 250px;   width: 250px;   border-radius: 40%;   object-fit: cover;   margin-bottom: 10px;     .local.peer-video     transform: scaleX(-1);     .peer-name     font-size: 14px;   text-align: center;     .peer-tile     padding: 10px;     .control-bar     display: flex;   position: fixed;   bottom: 0;   width: 100%;   padding: 15px;   justify-content: center;   z-index: 10;     .control-bar >  :not(:first-child)     margin-left: 8px;     .btn-control     font-size: 12px;   text-transform: uppercase;   letter-spacing: 1px;   border: 2px solid 37474f;   width: 64px;   height: 64px;   border-radius: 50%;   text-align: center;   background-color: 607d8b;   box-shadow: 0 0 10px rgba(0, 0, 0, 0.4);   color: white;      </Tab>   Join/leave a room To join a room (a video call), we need to call the  join (/api-reference/javascript/v2/interfaces/HMSActions join) method on hmsActions and it requires us to pass a config object. The config object must be passed the following fields:   userName : The name of the user. This is the value that will be set on the peer object and be visible to everyone connected to the room.   authToken : A client-side token that is used to authenticate the user. You can read about how to generate this token  here (./token). Let's use a form to request both of these fields from the user for our application. Let's include a way for people to leave the room as well. To leave the room, we can use the hmsActions  leave (/api-reference/javascript/v2/interfaces/HMSActions leave) method. We'll set up the leave method to be called whenever the user closes the window or refreshes the tab.  s id=\"join-form\" items=  'JS', 'HTML', 'CSS'       id=\"join-form-0\">   jsx:index.js section=JoiningRoom sectionIndex=1 tab=JS // Joining the room joinBtn.onclick = () =>    hmsActions.join(    userName: document.getElementById(\"name\").value,   authToken: document.getElementById(\"token\").value   );  ; // Leaving the room function leaveRoom()    hmsActions.leave();   // Cleanup if user refreshes the tab or navigates away window.onunload = window.onbeforeunload = leaveRoom; leaveBtn.onclick = leaveRoom;   </Tab>   id=\"join-form-1\">   html:index.html section=JoiningRoom sectionIndex=1 tab=HTML < DOCTYPE html> <html>  <head>   <title>Quickstart JS</title>   <link rel=\"stylesheet\" href=\"src/styles.css\"     <meta charset=\"UTF-8\"    </head>  <body>   <header>    <button id=\"leave-btn\" class=\"btn-danger hide\">Leave Room</button>   </header>   <form id=\"join\">    <h2>Join Room</h2>    <div class=\"input-container\">     <input id=\"name\" type=\"text\" name=\"username\" placeholder=\"Your name\"      </div>    <div class=\"input-container\">     <input id=\"token\" type=\"text\" name=\"token\" placeholder=\"Auth token\"      </div>    <button type=\"button\" class=\"btn-primary\" id=\"join-btn\">     Join    </button>   </form>   <script type=\"module\" src=\"src/index.js\"></script>  </body> </html>   </Tab>   id=\"join-form-2\">   css:styles.css section=JoiningRoom sectionIndex=1 tab=CSS       margin: 0;   padding: 0;   box-sizing: border-box;     body     font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Oxygen,    Ubuntu, Cantarell, \"Open Sans\", \"Helvetica Neue\", sans-serif;   background-color: 263238;   color: white;     h1,  h2,  h3,  h4,  h5     font-weight: normal;     header     padding: 10px;   display: flex;   align-items: end;   justify-content: space-between;     .btn-danger     border: 1px solid transparent;   border-radius: 4px;   padding: 6px 14px;   background-color: f44336;   color: white;   font-family: inherit;   font-size: 14px;     .hide     display: none  important;     form     max-width: 450px;   margin: 30px auto;   box-shadow: 0 20px 40px rgba(0, 0, 0, 0.4);   border-radius: 8px;   padding: 20px;     input     display: block;   width: 100%;   border-radius: 8px;   border: 2px solid transparent;   height: 34px;   padding: 5px;   background: 37474f;   color: inherit;   font-family: inherit;     input::placeholder     color: aaa;     .input-container     margin-bottom: 20px;     .btn-primary     border: 1px solid transparent;   border-radius: 4px;   padding: 6px 14px;   background-color: 1565c0;   color: white;   font-family: inherit;   font-size: 14px;      </Tab>   Render video tiles for peers Let us next add a way to show a tile for every participant in the room. We'll need a list of peers connected to the room.   We will use the  attachVideo (/api-reference/javascript/v2/interfaces/HMSActions attach-video) method on hmsActions to add the video from an element for a track ID.   And  selectPeers (/api-reference/javascript/v2/home/content select-peers) selector from hmsStore   s id=\"render-peers\" items=  'JS', 'HTML', 'CSS'       id=\"render-peers-0\">   jsx:index.js section=SubscribingToTheState sectionIndex=1 tab=JS // Helper function to create html elements function createElementWithClass(tag, className)    const newElement = document.createElement(tag);  newElement.className = className;  return newElement;   // Render a single peer function renderPeer(peer)    const peerTileDiv = createElementWithClass(\"div\", \"peer-tile\");  const videoElement = createElementWithClass(\"video\", \"peer-video\");  const peerTileName = createElementWithClass(\"span\", \"peer-name\");  videoElement.autoplay = true;  videoElement.muted = true;  videoElement.playsinline = true;  peerTileName.textContent = peer.name;  hmsActions.attachVideo(peer.videoTrack, videoElement);  peerTileDiv.append(videoElement);  peerTileDiv.append(peerTileName);  return peerTileDiv;   // Display a tile for each peer in the peer list function renderPeers()    peersContainer.innerHTML = \"\";  const peers = hmsStore.getState(selectPeers);  peers.forEach((peer) =>     if (peer.videoTrack)      peersContainer.append(renderPeer(peer));       );   // Reactive state  renderPeers is called whenever there is a change in the peer-list hmsStore.subscribe(renderPeers, selectPeers);   </Tab>   id=\"render-peers-1\">   html:index.html section=SubscribingToTheState sectionIndex=1 tab=HTML <div id=\"conference\" class=\"conference-section hide\">   <h2>Conference</h2>   <div id=\"peers-container\"></div> </div>   </Tab>   id=\"render-peers-2\">   css:styles.css section=SubscribingToTheState sectionIndex=1 tab=CSS  form h2,  .conference-section h2     margin-bottom: 20px;     .conference-section     padding: 20px 30px;   max-width: 960px;   margin: 0 auto;     .conference-section h2     text-align: center;   font-size: 32px;   padding-bottom: 10px;   border-bottom: 1px solid 546e7a;      peers-container     display: grid;   grid-template-columns: repeat(3, minmax(min-content, 1fr));   place-items: center;   grid-gap: 10px;     .peer-video     height: 250px;   width: 250px;   border-radius: 40%;   object-fit: cover;   margin-bottom: 10px;     .local.peer-video     transform: scaleX(-1);     .peer-name     font-size: 14px;   text-align: center;     .peer-tile     padding: 10px;       </Tab>   Mute/unmute local tracks Right now we are publishing both audio and video feed of the user whenever they join the room. We may want to allow the user to mute/unmute their own tracks  both audio and video. Let's add 2 buttons on the bottom of the page and call the  setLocalAudioEnabled (/api-reference/javascript/v2/interfaces/HMSActions set-local-audio-enabled) method on hmsActions to mute/unmute the local audio tracks and  setLocalVideoEnabled (/api-reference/javascript/v2/interfaces/HMSActions set-local-video-enabled) method on hmsActions to mute/unmute the local video tracks.  s id=\"mute-unmute\" items=  'JS', 'HTML', 'CSS'       id=\"mute-unmute-0\">   jsx:index.js section=MutingUnmutingLocalTracks sectionIndex=1 tab=JS // Mute and unmute audio muteAudio.onclick = () =>    const audioEnabled =  hmsStore.getState(selectIsLocalAudioEnabled);  hmsActions.setLocalAudioEnabled(audioEnabled);  muteAudio.textContent = audioEnabled ? \"Mute\" : \"Unmute\";  ; // Mute and unmute video muteVideo.onclick = () =>    const videoEnabled =  hmsStore.getState(selectIsLocalVideoEnabled);  hmsActions.setLocalVideoEnabled(videoEnabled);  muteVideo.textContent = videoEnabled ? \"Hide\" : \"Unhide\";  // Re-render video tile  renderPeers();  ;   </Tab>   id=\"mute-unmute-1\">   html:index.html section=MutingUnmutingLocalTracks sectionIndex=1 tab=HTML <div id=\"controls\" class=\"control-bar\">   <button id=\"mute-aud\" class=\"btn-control\">Mute</button>   <button id=\"mute-vid\" class=\"btn-control\">Hide</button> </div>   </Tab>   id=\"mute-unmute-2\">   css:styles.css section=MutingUnmutingLocalTracks sectionIndex=1 tab=CSS  .control-bar     display: flex;   position: fixed;   bottom: 0;   width: 100%;   padding: 15px;   justify-content: center;   z-index: 10;     .control-bar >  :not(:first-child)     margin-left: 8px;     .btn-control     font-size: 12px;   text-transform: uppercase;   letter-spacing: 1px;   border: 2px solid 37474f;   width: 64px;   height: 64px;   border-radius: 50%;   text-align: center;   background-color: 607d8b;   box-shadow: 0 0 10px rgba(0, 0, 0, 0.4);   color: white;      </Tab>   Change UI based on connection state Right now, our join form will show up even after we have joined the room. We need a way to know the connection state of the room and hide the form after we've connected. We can do this by subscribing to the store with the  selectIsConnectedToRoom (/api-reference/javascript/v2/home/content select-is-connected-to-room) selector from hmsStore .   jsx:index.js section=ChangingUIBasedOnConnectionState sectionIndex=1 // Showing the required elements on connection/disconnection function onConnection(isConnected)    if (isConnected)     form.classList.add(\"hide\");   conference.classList.remove(\"hide\");   leaveBtn.classList.remove(\"hide\");   controls.classList.remove(\"hide\");    else     form.classList.remove(\"hide\");   conference.classList.add(\"hide\");   leaveBtn.classList.add(\"hide\");   controls.classList.add(\"hide\");      // Listen to the connection state hmsStore.subscribe(onConnection, selectIsConnectedToRoom);   Refer to  Test the app section (/javascript/v2/guides/javascript-quickstart test-the-app) above to test your sample app locally. </StepsContainer>   Next steps Here's the complete example. <Codesandbox id=\"5rmes\"   There is also a version of the above quickstart using CDN  here (https://codepen.io/triptu/pen/dymxmoR?editors=0010). That wraps it up for this guide. We hope you had fun. There are plenty of cool stuff which can be done with our SDK, be sure to check the  features section (/javascript/v2/features/integration) for more information. You can also check out our  React demo app (https://github.com/100mslive/100ms-web) built with more features like screenshare, virtual background, noise suppression, audio/video playlist, etc. "
    },
    {
        "title": "React Quickstart Guide",
        "link": "/javascript/v2/get-started/react-quickstart",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/get-started/react-quickstart",
        "keywords": [],
        "content": "    Getting started Hello there  In this guide, we'll build a video conferencing application using our React SDK. We'll be using functional components with the powerful hooks provided by our SDK and build an app where you can have a video call with your friends. TL;DR  You can find the link to the complete example over  here (https://codesandbox.io/s/100ms-react-quickstart-kh0hy?file=/src/App.jsx). > If you get stuck at any point or need help in understanding a concept, you can put your query in our  Discord Channel (https://100ms.live/discord).   Prerequisites To get started you should be familiar with the basics of  React (https://reactjs.org/).   Installing the dependencies   bash section=InstallingTheDependencies sectionIndex=1  npm npm install save @100mslive/react-sdk@latest  yarn yarn add @100mslive/react-sdk@latest     Initializing the SDK Let us start with initializing the libraries. We need to wrap the entire application with <HMSRoomProvider   component. This will let us use the hooks for state and actions.   jsx:src/index.js section=InitializingTheSDK sectionIndex=1 import   StrictMode   from \"react\"; import ReactDOM from \"react-dom\"; import   HMSRoomProvider   from \"@100mslive/react-sdk\"; import App from \"./App\"; const rootElement = document.getElementById(\"root\"); ReactDOM.render(  <StrictMode>   <HMSRoomProvider>    <App     </HMSRoomProvider>  </StrictMode>,  rootElement );    hmsStore will hold the complete state of the application such as details of all the participants. We can also visualize this state at any time using the  devtools (https://chrome.google.com/webstore/detail/redux-devtools/lmhkpmbekcpmknklioeibfkpmmfibljd?hl=en) extension.  hmsActions will help us perform actions such as joining the room, muting our audio/video, and sending messages.   Concepts   Room : When we join a conference call, the participants are said to be in a video call room .   Peer : A participant in the video call. You are the local peer while others are remote peers .   Track : Media. There are two types of tracks a peer can have  audio and video.   Joining a room To join a room (a video call), we need to call the join method on hmsActions and it requires us to pass a config object. The config object must be passed the following fields:   userName : The name of the user. This is the value that will be set on the peer object and be visible to everyone connected to the room.   authToken : A client-side token that is used to authenticate the user. You can read about how to generate this token  here (./token). For our application, let us collect both of these fields from the user using a form. Let's create a JoinForm component and add it to the application. > Note: join is async from this  version (../changelog/release-notes 2022-09-13)   jsx:src/JoinForm.jsx section=JoiningRoom sectionIndex=1 import   useState   from \"react\"; import   useHMSActions   from \"@100mslive/react-sdk\"; function JoinForm()    const hmsActions = useHMSActions();  const  inputValues, setInputValues  = useState(    name: \"\",   token: \"\"   );  const handleInputChange = (e) =>     setInputValues((prevValues) => (     ...prevValues,     e.target.name : e.target.value    ));   ;  const handleSubmit = async (e) =>     e.preventDefault();   await hmsActions.join(     userName: inputValues.name,    authToken: inputValues.token    );   ;  return (   <form onSubmit= handleSubmit >    <h2>Join Room</h2>    <div className=\"input-container\">     <input      required      value= inputValues.name       onChange= handleInputChange       id=\"name\"      type=\"text\"      name=\"name\"      placeholder=\"Your name\"          </div>    <div className=\"input-container\">     <input      required      value= inputValues.token       onChange= handleInputChange       id=\"token\"      type=\"text\"      name=\"token\"      placeholder=\"Auth token\"          </div>    <button className=\"btn-primary\">Join</button>   </form>  );   export default JoinForm;   Let's add this form to our <App> component.   jsx:src/App.jsx section=JoiningRoom sectionIndex=2 export default function App()     return (     <div className=\"App\">       <JoinForm       </div>   );       Join form (/guides/join-room.png) Congratulations    you now have an audio-only conference ready to use  . If you have a friend join from another device, you can have a nice friendly chat with them.   Leaving the room Before we go ahead with adding video, let us add a way to leave the room as well. We can call the leave method on hmsActions to leave the room. We'll set up the leave method to be called whenever the user closes the window or refreshes the tab.   jsx 2-8 :src/App.jsx section=LeavingTheRoom sectionIndex=1 export default function App()    const hmsActions = useHMSActions();  useEffect(() =>     window.onunload = () =>      hmsActions.leave();    ;   ,  hmsActions );  return (   <div className=\"App\">    <JoinForm     </div>  );       Adding video tiles Let us next add a way to show a tile for every participant in the room. We'll need a list of peers connected to the room.   Fetching information from state At any point in time we can get a list of peers with:   jsx section=FetchingInformation sectionIndex=1 import   useHMSStore, selectPeers   from '@100mslive/react-sdk'; function Component()     const peers = useHMSStore(selectPeers);   return <Peers peers= peers   ;     Let's take a moment to discuss how hmsStore works. The store maintains the state of the video call which includes the list of peers, the connection state of the room, the tracks, track states, etc. We can use \"selectors\" that return a slice of the state to get the data piece we're interested in. The hmsStore is also reactive, which means any component using the HMSStore hook will re-render when the slice of the state, it listens to, changes. This allows us to write  declarative (https://en.wikipedia.org/wiki/Declarative_programming) code. Let us now create a Conference component that will render a list of peers. We'll get the peers from the store and render them. Our component will get rerendered if this list of peers changes.   jsx:src/Conference.jsx section=FetchingInformation sectionIndex=2 import   selectPeers, useHMSStore   from \"@100mslive/react-sdk\"; import React from \"react\"; import Peer from \"./Peer\"; function Conference()    const peers = useHMSStore(selectPeers);  return (   <div className=\"conference-section\">    <h2>Conference</h2>    <div className=\"peers-container\">      peers.map((peer) => (      <Peer key= peer.id  peer= peer        ))     </div>   </div>  );   export default Conference;   For each peer, we'll render a <video> element and a <div> element with their name. To render the video, we need to call attachVideo method of hmsActions , which accepts a trackId and a DOM element. But we have abstracted this implementation inside useVideo hook for ease. This hook will return an object videoRef given a video trackId . The returned videoRef can be used to set on a video element meant to display the video. The hook will take care of attaching and detaching video, and will automatically detach when the video goes out of view to save on bandwidth.   jsx:src/Peer.jsx section=FetchingInformation sectionIndex=3 import   useVideo   from \"@100mslive/react-sdk\"; function Peer(  peer  )    const   videoRef   = useVideo(    trackId: peer.videoTrack   );  return (   <div className=\"peer-container\">    <video     ref= videoRef      className=  peer-video $ peer.isLocal ? \"local\" : \"\"        autoPlay     muted     playsInline         <div className=\"peer-name\">      peer.name   peer.isLocal ? \"(You)\" : \"\"     </div>   </div>  );   export default Peer;   > Note that height and width CSS properties need to be set on the <video> element to render the peer's video. > In case you aren't seeing the remote video after this step, do ensure that the role is  subscribing (https://www.100ms.live/docs/javascript/v2/foundation/templates-and-roles subscribe-strategies) to itself in the  dashboard (https://dashboard.100ms.live/dashboard) template.   Changing UI based on connection state Right now, our join form shows even after we have joined the room. We need a way to know the connection state of the room and hide the form after we've connected. We can do this with selectIsConnectedToRoom selector.   jsx 5,8,13,17,22-26 :src/App.jsx section=ChangingUIBasedOnConnectionState sectionIndex=1 import \"./styles.css\"; import JoinForm from \"./JoinForm\"; import Conference from \"./Conference\"; import   useEffect   from \"react\"; import    selectIsConnectedToRoom,  useHMSActions,  useHMSStore   from \"@100mslive/react-sdk\"; export default function App()    const isConnected = useHMSStore(selectIsConnectedToRoom);  const hmsActions = useHMSActions();  useEffect(() =>     window.onunload = () =>      if (isConnected)       hmsActions.leave();         ;   ,  hmsActions, isConnected );  return (   <div className=\"App\">    <Header       isConnected ? (     <Conference      ) : (     <JoinForm      )    </div>  );       Muting/unmuting local tracks Right now we are publishing both audio and video feed of the user whenever they join the room. We may want to allow the user to mute/unmute their own tracks  both audio and video. If you specifically need granular data like knowing the current video status you can use selectIsLocalVideoEnabled and for audio selectIsLocalAudioEnabled instead. In this case, we can use useAVToggle hook which will give us the current audio/video status of the user and also give us functions to toggle them.   jsx:src/Footer.jsx section=MutingUnmutingLocalTracks sectionIndex=1 import   useAVToggle   from \"@100mslive/react-sdk\"; function Footer()    const     isLocalAudioEnabled,   isLocalVideoEnabled,   toggleAudio,   toggleVideo    = useAVToggle();  return (   <div className=\"control-bar\">    <button className=\"btn-control\" onClick= toggleAudio >      isLocalAudioEnabled ? \"Mute\" : \"Unmute\"     </button>    <button className=\"btn-control\" onClick= toggleVideo >      isLocalVideoEnabled ? \"Hide\" : \"Unhide\"     </button>   </div>  );   export default Footer;   We fetch the current state of the local audio and video and toggle them whenever the buttons are clicked.   Mute/Unmute (/guides/mute-unmute.gif) And finally, let's add this component to the <App> component to render the buttons only when we are connected to the room.   jsx 1,19-22 :src/App.js section=MutingUnmutingLocalTracks sectionIndex=2 import Footer from './Footer'; export default function App()    const isConnected = useHMSStore(selectIsConnectedToRoom);  const hmsActions = useHMSActions();  useEffect(() =>     window.onunload = () =>      if (isConnected)       hmsActions.leave();         ;   ,  hmsActions, isConnected );  return (   <div className=\"App\">    <Header       isConnected ? (     <>      <Conference        <Footer       <     ) : (     <JoinForm      )    </div>  );     That wraps it up for this guide. We hope you had fun. There is plenty of cool stuff which can be done with our SDK, be sure to check the  features section (/javascript/v2/features/integration) for more information. > Here's the complete example. <Codesandbox id=\"100ms-react-quickstart-kh0hy\"   "
    },
    {
        "title": "Svelte Quickstart Guide",
        "link": "/javascript/v2/get-started/svelte-quickstart",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/get-started/svelte-quickstart",
        "keywords": [],
        "content": "    Introduction We have written a SvelteKit quickstart at Github  here (https://github.com/100mslive/svelte-100ms). It details all the steps towards building the app using the JS SDK. It follows the same flow as the  JS Quickstart (./javascript-quickstart), with the README file having details of all steps as well as link to the commit where the change was done to make it easier to follow. When you open a commit in Github, there is an option to browse files which will allow you see the full state of the repository right after that commit was done. This may be helpful if you're following the steps and need to compare local changes vs the quickstart repository changes at that time.   Trying out You can also try out the full code alongside a live demo at Codesandbox. <Codesandbox id=\"github/100mslive/svelte-100ms?module=/src/routes/JoinForm.svelte,/src/routes/Conference.svelte,/src/routes/Peer.svelte,/src/routes/Video.svelte,/src/routes/Footer.svelte&runonclick=1&forcerefresh=1\"     Deploy    Deploy with Vercel (https://vercel.com/button) (https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2F100mslive%2Fsvelte-100ms&project-name=svelte-100ms-video-call&repo-name=svelte-100ms-video-call&demo-title=Svelte%20%2B%20100ms&demo-description=A%20video%20call%20app%20built%20with%20SvelteKit%20and%20100ms&demo-url=https%3A%2F%2Fsvelte-100ms.vercel.app%2F&demo-image=https%3A%2F%2Fraw.githubusercontent.com%2F100mslive%2Fsvelte-100ms%2Fmain%2Fstatic%2Fconference.png) You can use the above link to deploy on Vercel. As part of the deployment, there will be a prompt to create a clone of the repository in your own Github/Gitlab account too.   Next steps Once you have been through the quickstart in the main branch, you can check out  this branch (https://github.com/100mslive/svelte-100ms/tree/features) which showcases building different features on top of the quickstart. You can also try out the final code in Codesandbox below. <Codesandbox id=\"github/100mslive/svelte-100ms/tree/features?module=/src/routes/JoinForm.svelte,/src/routes/Conference.svelte,/src/routes/Peer.svelte,/src/routes/Video.svelte,/src/routes/Footer.svelte&runonclick=1&forcerefresh=1\"   Or deploy it to Vercel     Deploy with Vercel (https://vercel.com/button) (https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2F100mslive%2Fsvelte-100ms%2Ftree%2Ffeatures&project-name=svelte-100ms-video-call&repo-name=svelte-100ms-video-call&demo-title=Svelte%20%2B%20100ms&demo-description=A%20video%20call%20app%20built%20with%20SvelteKit%20and%20100ms&demo-url=https%3A%2F%2Fsvelte-100ms.vercel.app%2F&demo-image=https%3A%2F%2Fraw.githubusercontent.com%2F100mslive%2Fsvelte-100ms%2Fmain%2Fstatic%2Fconference.png) "
    },
    {
        "title": "Auth Token Endpoint Guide",
        "link": "/javascript/v2/get-started/token-endpoint",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/get-started/token-endpoint",
        "keywords": [],
        "content": "    Overview 100ms provides an option to get App Tokens without setting up a token generation backend service to simplify your integration journey while testing the  sample app (https://github.com/100mslive/100ms-web) or building integration with 100ms. You can find the token endpoint from the  developer page (https://dashboard.100ms.live/developer) in your 100ms dashboard.   Token endpoint (/guides/token-endpoint-dashboard.png) We recommend you move to your token generation service before you transition your app to production, as our token endpoint service will not scale in production. The \"Sample Apps\" built using 100ms client SDKs require an App Token to join a room to initiate a video conferencing or live streaming session. Please check the  Authentication and Tokens guide (./../foundation/security-and-tokens) Please note that you cannot use the token endpoint to create a Management Token for server APIs. Refer to the  Management Token section (./../foundation/security-and-tokens management-token) in Authentication and Tokens guide for more information.   Get an app token using token endpoint You can use the token endpoint from your 100ms dashboard while building integration with 100ms. This acts as a tool enabling front-end developers to complete the integration without depending on the backend developers to set up a token generation backend service.   URL format:   <YOUR_TOKEN_ENDPOINT>api/token  100ms token endpoint can generate an app token with the inputs passed, such as room_id, role, & user_id (optional  your internal user identifier as the peer's user_id). You can use  jwt.io (https://jwt.io/) to validate whether the app token contains the same input values. <PostRequest title=\"https://prod-in2.100ms.live/hmsapi/johndoe.app.100ms.live/api/token\"   <Request id=\"req-comp-0\">   bash curl location request POST 'https://prod-in2.100ms.live/hmsapi/johndoe.app.100ms.live/api/token'  header 'Content-Type: application/json'  data-raw '    \"room_id\":\"633fcdd84208780bf665346a\",   \"role\":\"host\",   \"user_id\":\"1234\"  '   </Request> <ResponseBox id=\"resp-0\" status=\"200 OK\">   json     \"token\": \"eyJ0eXAiOiJKV1QiLCJhbGciOi                           R3tT-Yk\",   \"msg\": \"token generated successfully\",   \"status\": 200,   \"success\": true,   \"api_version\": \"2.0.192\"     </ResponseBox>   Example client-side implementation You can directly add this to your client-side implementation, check our  sample app (https://github.com/100mslive/100ms-web/blob/f559293779c0f496cc0cd6afa9236eefde2857d8/src/services/tokenService.js L20) for reference.   Disable 100ms token endpoint Due to some security concerns, if you don't wish to use the token endpoint to generate app tokens, then you can disable it on the  Developers page (https://dashboard.100ms.live/developer) on your dashboard by disabling the option \"Disable &lt;room_id&gt;/&lt;role&gt; link format.\"   Disable Token endpoint (/guides/disable-token-endpoint.png)    Error Response Once you're disabled it on the dashboard, the requests to create an app token using the 100ms token endpoint will throw the below error:   json     \"success\": false,   \"msg\": \"Generating token using the room_id and role is disabled.\",   \"api_version\": \"2.0.192\"     "
    },
    {
        "title": "Auth Token Quickstart Guide",
        "link": "/javascript/v2/get-started/token",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/get-started/token",
        "keywords": [],
        "content": "    Create a 100ms account  Create an account at  Dashboard of 100ms (https://dashboard.100ms.live/)   Create Account (/docs/guides/token/create-account.png)  After you have created your account you have to Confirm your Email , check the promotions tab in your Mail Box if you can't find it.  Login to the Dashboard and you will see an option to Create your first app. Click on it.   Create your first app (/docs/guides/token/starter-kit-initialize-first-step.png)  Then you would see this popup with multiple starter kits, hover over one of the starter kits and click Deploy . We will choose \"Video Conferencing\" for now.   Initialize Started Kit (/docs/guides/token/starter-kit-initialize.png)  In the Choose your deployment step, select 100ms and enter the subdomain you wish in the Subdomain field.   Domain details (/docs/guides/token/domain-details.png)  After you're App is set click on \"Go to Dashboard\" or  Go Here (https://dashboard.100ms.live/dashboard)   Dasboard (/docs/guides/token/go-to-dashboard.png)   Create a room  Go over to  Room in Dashboard (https://dashboard.100ms.live/rooms) and click on \"Create Room\" , While creating a room you can specify it's name, roles or enable recording.   Create Room (/docs/guides/token/create-room.png)  You will now see \"Room Details\" section and we have a room_id created, copy it somewhere.   Room Id (/docs/guides/token/room-id.png)   Get a temporary token from 100ms dashboard Any client connecting calling 100ms' service needs to authenticate using an auth token. In production you would have your own servers generating the tokens (see more  here (/docs/javascript/v2/foundation/security-and-tokens)), but for a quick start you can use the dashboard to create a token for you. The token will expire in 24 hours and should not be hard-coded into a production app.  To get a temporary token click on \"Join room\" button.   Join Room (/docs/guides/token/join-room.png)  In the popup that shows up click on icon with a key shape next to the role you want to join as.   Copy Token (/docs/guides/token/copy-token.png) The token will be copied to your clipboard. Use this along with the room_id to proceed with the quickstart guide. "
    },
    {
        "title": "Useful Selectors",
        "link": "/javascript/v2/get-started/useful-selectors",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/get-started/useful-selectors",
        "keywords": [],
        "content": "  Here's a list of useful Selector hooks that you might want to use while building your application. Before using these make sure you have initialized the SDK, you can refer to this  guide (/javascript/v2/guides/javascript-quickstart initializing-the-sdk). Selector functions would fetch you information from the state at any point in time, it can be anything ranging from \"how many people are in the room?\" to \"is my audio on or not?\". The answer to all these questions is the store.   Am I connected to the Room? Knowing if you're in the room or not is the barest model information you need. selectIsConnectedToRoom returns a boolean flag denoting whether you've joined a room. NOTE: Returns true only after join, returns false during the preview.  s id=\"connect\" items=  'Javascript', 'React'       id='connect-0'>   js section=AmIConnectedToTheRoom sectionIndex=1 tab=JS const isConnected = hmsStore.getState(selectIsConnectedToRoom);   </Tab>   id='connect-1'>   jsx section=AmIConnectedToTheRoom sectionIndex=1 tab=React const isConnected = useHMSStore(selectIsConnectedToRoom);   </Tab>   Am I in the Preview?  selectIsInPreview returns a boolean denoting whether the room is in Preview state.  s id=\"preview\" items=  'Javascript', 'React'       id='preview-0'>   js section=AmIInThePreview sectionIndex=1 tab=JS const isInPreview = hmsStore.getState(selectIsInPreview);   </Tab>   id='preview-1'>   jsx section=AmIInThePreview sectionIndex=1 tab=React const isInPreview = useHMSStore(selectIsInPreview);   </Tab>   How many people have joined the room?  selectPeerCount returns the number of peers inside the room. If you have turned on peer list in preview, this count won't include the local peer if they're still in preview.  s id=\"peerCount\" items=  'Javascript', 'React'       id='peerCount-0'>   js section=HowManyPeopleHaveJoinedTheRoom sectionIndex=1 tab=JS const count = hmsStore.getState(selectPeerCount);   </Tab>   id='peerCount-1'>   jsx section=HowManyPeopleHaveJoinedTheRoom sectionIndex=1 tab=React const count = useHMSStore(selectPeerCount);    </Tab>   Who all are in my room?  selectPeers returns you an array of peers(remote peers and your local peer) present in the room. It will include the local peer in preview phase.  s id=\"peers\" items=  'Javascript', 'React'       id='peers-0'>   js section=WhoAllAreInMyRoom sectionIndex=1 tab=JS const peers = hmsStore.getState(selectPeers);   </Tab>   id='peers-1'>   jsx section=WhoAllAreInMyRoom sectionIndex=1 tab=React const peers = useHMSStore(selectPeers);   </Tab>   What is my current state of room? What if you fail to join the room? How do you know if the user is reconnecting? This is where selectRoomState comes in handy, it returns you the current state of your room. You can refer to the  ENUMS (/api-reference/javascript/v2/enums/HMSRoomState) types for reference.  s id=\"roomState\" items=  'Javascript', 'React'       id='roomState-0'>   js section=WhatIsMyCurrentStateOfRoom sectionIndex=1 tab=JS const roomState = hmsStore.getState(selectRoomState);   </Tab>   id='roomState-1'>   jsx section=WhatIsMyCurrentStateOfRoom sectionIndex=1 tab=React const roomState = useHMSStore(selectRoomState);   </Tab>   How do I know if my mic/video is on? Mic/Video being turned on can lead to a lot of blunders, therefore it's critical to show it to the end-user. selectIsLocalAudioEnabled & selectIsLocalVideoEnabled helps you know if the local audio/video is enabled or not.  s id=\"av\" items=  'Javascript', 'React'       id='av-0'>   js section=HowDoIKnowIfMyMicVideoIsOn sectionIndex=1 tab=JS const audioOn = hmsStore.getState(selectIsLocalAudioEnabled); const videoOn = hmsStore.getState(selectIsLocalVideoEnabled);   </Tab>   id='av-1'>   jsx section=HowDoIKnowIfMyMicVideoIsOn sectionIndex=1 tab=React const audioOn = useHMSStore(selectIsLocalAudioEnabled); const videoOn = useHMSStore(selectIsLocalVideoEnabled);   </Tab>   How do I know the status of remote mic/video status?  selectIsPeerAudioEnabled & selectIsPeerVideoEnabled returns a boolean denoting the peer's Audio/Video status.  s id=\"rm\" items=  'Javascript', 'React'       id='rm-0'>   js section=HowDoIKnowTheStatusOfRemoteMicVideoStatus sectionIndex=1 tab=JS const audioOn = hmsStore.getState(selectIsPeerAudioEnabled(peerId)); const videoOn = hmsStore.getState(selectIsPeerVideoEnabled(peerId));   </Tab>   id='rm-1'>   jsx section=MicVideoStatus sectionIndex=1 tab=React const audioOn = useHMSStore(selectIsPeerAudioEnabled(peerId)); const videoOn = useHMSStore(selectIsPeerVideoEnabled(peerId));   </Tab>   How do I get the video stream of a user? You can get video stream of a user using selectCameraStreamByPeer it returns  HMSVideoTrack (/api-reference/javascript/v2/interfaces/HMSVideoTrack) which you can use to call  attach/detach (/javascript/v2/features/render-video) actions.  s id=\"cam\" items=  'Javascript', 'React'       id='cam-0'>   js section=HowDoIGetTheVideoStreamOfAUser sectionIndex=1 tab=JS const videoTrack = hmsStore.getState(selectCameraStreamByPeer(peerId));   </Tab>   id='cam-1'>   jsx section=HowDoIGetTheVideoStreamOfAUser sectionIndex=1 tab=React const videoTrack = useHMSStore(selectCameraStreamByPeer(peerId));   </Tab> "
    },
    {
        "title": "Peer Metadata",
        "link": "/javascript/v2/how--to-guides/build-interactive-features/peer-metadata",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/build-interactive-features/peer-metadata",
        "keywords": [],
        "content": "  Looking for persistent state that can be set on a peer and updated anytime, for everyone in the room? Peer metadata it is. Metadata can be set initially in the HMSConfig object that's passed into the join method and update post join by following the API below. You can imagine metadata as a persistent object attached to the peer which has more details about them. We'll explain the API using an example of implementing raise hand. For this, we'll use the below interface for metadata    ts interface CustomMetadata     isHandRaised: boolean;       Getting and Setting metadata Note that peer.metadata is a string, it can be used to keep a stringified JSON. The string will be converted properly to a json object however if you use the selector described below. Metadata update post join can be achieved by calling hmsActions.changeMetadata(metadata) , the below example shows implementing a toggle raise hand function for the local peer.  s id=\"meta\" items=  'Javascript', 'React'       id='meta-0'>   js async function toggleRaiseHand()     const localPeerId = hmsStore.getState(selectLocalPeerID);   const metadata = hmsStore.getState(selectPeerMetadata(localPeerId));   const newMetadata =   ...metadata, isHandRaised:  metadata.isHandRaised  ;   await hmsActions.changeMetadata(newMetadata);     </Tab>   id='meta-1'>   jsx const RaiseHand = () =>     const localPeerId = useHMSStore(selectLocalPeerID);   const metaData = useHMSStore(selectPeerMetadata(localPeerId));   const hmsActions = useHMSActions();   const toggleRaiseHand = useCallback(async () =>       const newMetadata =   ...metaData, isHandRaised:  metaData.isHandRaised  ;     await hmsActions.changeMetadata(newMetadata);    ,  hmsActions, metaData );   return (     <button onClick= toggleRaiseHand >        metaData.isHandRaised ? 'Hand Raised' : 'Hand not Raised'      </button>   );  ;   </Tab>   Peer Metadata update Notification Once the metadata is updated, all the peers will receive a notification with type METADATA_UPDATED and  HMSPeer (../../../api-reference/javascript/v2/interfaces/HMSPeer) object as notification data.  s id=\"meta-notif\" items=  'Javascript', 'React'       id='meta-notif-0'>   js hmsNotifications.onNotification((notification) =>     const peer = notification.data;   const   isHandRaised   = hmsStore.getState(selectPeerMetadata(peer.id));   if (isHandRaised &&  peer.isLocal)       toast( $ peer.name  raised their hand. );      , HMSNotificationsTypes.METADATA_UPDATED);   </Tab>   id='meta-notif-1'>   jsx const notification = useHMSNotifications(HMSNotificationsTypes.METADATA_UPDATED) const peer = notification?.data const isHandRaised = useHMSStore(selectPeerMetadata(peer?.id ?? \"\"))?.isHandRaised useEffect(() =>     if (isHandRaised && peer &&  peer.isLocal)       toast( $ peer.name  raised their hand. );      ,  isHandRaised, peer )   </Tab>   Updating Remote Peer Metadata Say if a guest has raised hand, host might want to lower their hand after a while. We don't have a direct method available to do this currently but it can be done using  custom events (../features/chat custom-events). The host can send a message to the peer who raised their hand, and the peer can lower their hand on getting the message.  s id=\"meta-update\" items=  'Javascript', 'React'       id='meta-update-0'>   js const LOWER_HAND = 'LOWER_HAND'; // don't save messages with this type in store hmsActions.ignoreMessageTypes( LOWER_HAND ); // host can send a custom message to the peer who has raised hand await hmsActions.sendDirectMessage('', peerIdWithRaisedHand, LOWER_HAND); // the peer on receiving the event can lower their hand hmsNotifications.onNotification((notification) =>     const msg = notification.data;   if (msg && msg.type === LOWER_HAND)       const localPeerId = hmsStore.getState(selectLocalPeerID);     const metadata = hmsStore.getState(selectPeerMetadata(localPeerId));     const newMetadata =   ...metadata, isHandRaised: false  ;     hmsActions.changeMetadata(newMetadata);      , HMSNotificationTypes.NEW_MESSAGE);   </Tab>   id='meta-update-1'>   jsx const LOWER_HAND = 'LOWER_HAND'; // don't save messages with this type in store hmsActions.ignoreMessageTypes( LOWER_HAND ); // host can send a custom message to the peer who has raised hand await hmsActions.sendDirectMessage('', peerIdWithRaisedHand, LOWER_HAND); // the peer on receiving the event can lower their hand const hmsActions = useHMSActions(); const notification = useHMSNotifications(HMSNotificationsTypes.NEW_MESSAGE); const localPeerId = useHMSStore(selectLocalPeerID); const metadata = useHMSStore(selectPeerMetadata(localPeerId)); useEffect(() =>     if ( notification)       return;       const message = notification.data;   if (message?.type === LOWER_HAND)       const newMetadata =   ...metadata, isHandRaised: false  ;     hmsActions.changeMetadata(newMetadata);      ,  notification )   </Tab>   Ideas   Implementing raise hand   Multiplayer games like chess, scribble, quizzes etc.   Store extra information such as profile picture for every peer which can be updated mid call "
    },
    {
        "title": "Change Role",
        "link": "/javascript/v2/how--to-guides/control-remote-peers/change-role",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/control-remote-peers/change-role",
        "keywords": [],
        "content": "    Role Change (/guides/role-change.png)   Introduction Role is a powerful concept that takes a lot of complexity away in handling permissions and supporting features like breakout rooms.  Learn more about roles here. (../foundation/templates-and-roles) Every peer is associated with a role. The   HMSRole  (/api-reference/javascript/v2/interfaces/HMSRole) object can be used to know the following: 1. Check what this role is allowed to publish. i.e can it send video (and at what resolution)? can it send audio? can it share screen? This can be discovered by using selectIsAllowedToPublish to display the UI appropriately. 2. Check which other roles can this role subscribe to. This is internally taken care of by the 100ms infra and sdk, and your UI will only get tracks as per allowed subscriptions. role.subscribeParams can be used to get details. 3. Check what actions this role can perform. i.e can it change someone else's current role, mute others, end meeting, remove someone from the room. This can be discovered by using the selectPermissions selector.   Example Imagine an audio room with 2 roles \"speaker\" and \"listener\". Only someone with a \"speaker\" role can publish audio to the room while \"listener\" can only subscribe. Now at some point \"speaker\" may decide to nominate some \"listener\" to become a \"speaker\". This is where the changeRole API comes in.   Know Thy Permissions The below selectors can be used to know what the local peer is allowed to publish, and the permissions they have. To know about all the permissions please check   selectPermissions'  (../../../api-reference/javascript/v2/home/content select-permissions) api reference.   js const role = hmsStore.getState(selectLocalPeerRole); const permissions = hsmStore.getState(selectPermissions); console.log('can I end room  ', permissions.endRoom); console.log('can I end change role  ', permissions.changeRole); const   video, audio, screen   = hmsStore.getState(selectIsAllowedToPublish);     Change Role APIs There are two way of chaning peer's role using our api: 1.   Single Peer Role API  ( change-single-peer-role-api)  It is useful when we need to change role of a single peer. 2.   Bulk Peer Role API  ( change-bulk-peer-role-api)  We could change a all peers of particular roles to a given role. It will be helpful to make breakout rooms, waiting rooms, etc.   Change Single Peer Role API If the local peer has permissions to change role( permissions.changeRole ), they can change either their or someone else's role using the below interface.  s id=\"singleRoleChange\" items=  'Javascript', 'React'     ' '    id='singleRoleChange-0'>   js hmsActions.changeRoleOfPeer(forPeerId, toRoleName, force);   </Tab>   id='singleRoleChange-1'>   jsx import   useHMSActions   from '@100mslive/react-sdk'; function RoleChange()    const actions = useHMSActions();  return (<button onClick= () => hmsActions.changeRoleOfPeer(forPeerId, toRoleName, force) > )     </Tab>   forPeerId : The peer ID whose role needs to be changed, the peer can be either of local or remote.   toRoleName : The target role name.   force (optional): The other peer gets a request by default to accept a role change, by setting force to true, the role can be changed   without requesting for approval.   Change Bulk Peer Role API If the local peer has permissions to change role( permissions.changeRole ), they can convert all roles from a list of roles, to another role using the below interface.  s id=\"bulkRoleChange\" items=  'Javascript', 'React'     ' '    id='bulkRoleChange-0'>   js hmsActions.changeRoleOfPeersWithRoles(ofRoles, toRole);   </Tab>   id='bulkRoleChange-1'>   jsx import   useHMSActions   from '@100mslive/react-sdk'; function BulkRoleChange()    const actions = useHMSActions();  return (<button onClick= () => hmsActions.changeRoleOfPeersWithRoles(ofRoles, toRole) > )     </Tab>   ofRoles : It accepts the list of roles name.   toRole : The target role name. Note that if an empty list is sent to ofRoles, no roles will be changed. This is to avoid accidentally changing roles you may not have intended such as the bots that provide recording and streaming with the roles beam. It will throw exception when list ofRoles consist of target toRole. Bulk role changes are always forced, no dialog will be given for the peer to accept it, they will just be changed immediately. > 💡 A list of all available role names in the current room can be accessed via the selectAvailableRoleNames selector. Further the selectRoleByRoleName > selector can be used to get the full  HMSRole (/api-reference/javascript/v2/interfaces/HMSRole) object for a role name.   Handling a Role Change Request The force parameter in changeRole , when false, is a polite request: \"Would you like to change your role from listener to speaker?\" which can be ignored by the other party. The way it works is the other party will first receive a  request (/api-reference/javascript/v2/interfaces/HMSRoleChangeRequest) which they can accept or reject.  s id=\"roleChange\" items=  'Javascript', 'React'     ' '    id='roleChange-0'>   js function handleRoleChangeRequest(request)     if ( request)       return;       console.log( $ request.requestedBy.name  requested role change to  $ request.role.name  );   // shouldAccept can for example present a pop up to the user for deciding how to act on the request   const accept = shouldAccept(request);   if (accept)       hmsActions.acceptChangeRole(request);     else       hmsActions.rejectChangeRole(request);       hmsStore.subscribe(handleRoleChangeRequest, selectRoleChangeRequest);   </Tab>   id='roleChange-1'>   jsx function handleRoleChangeRequest(request)     const request = useHMSStore(selectRoleChangeRequest);   const hmsActions = useHMSActions();   useEffect(() =>       console.log( $ request.requestedBy.name  requested role change to  $ request.role.name  );     // shouldAccept can for example present a pop up to the user for deciding how to act on the request     const accept = shouldAccept(request);     if (accept)         hmsActions.acceptChangeRole(request);       else         hmsActions.rejectChangeRole(request);          ,  request, hmsActions );   return null;     </Tab> If the request is accepted, the peer.roleName field in the store will update re rendering a declarative UI as required.   Forced Role Change Imagine that the newly nominated speaker is not behaving nicely and we want to move him back to listener without a prompt. This is where the force parameter comes in. When it is set to true the other party will not receive a confirmation roleChangeRequest but instead will straight away receive a new set of updated permissions and stop publishing.   Listener Role (/guides/role-listener.png)   Notifications You'll get a notification of type ROLE_UPDATED when role change happens for the local peer. When a peer's role change happens in a way where the updated role has new publish permissions, the track will be muted by default. So for example, if a viewer(no audio/video) is changed to a host, their tracks will be muted right after the role change. They'll be able to manually unmute post role change though. If you want this unmuting to happen automatically, you can listen to the notification to call the action.   Ideas   Breakout Rooms/Groups You can have multiple roles which act as breakout rooms subscribing only to peers within the role. A peer can join another breakout room or group easily using the change single peer role API.   Waiting Room You can create a specific role for a waiting room which doesn't subscribe to anyone else. When someone joins waiting room, hosts get a notification, and they can let the person join by changing their role to attendees.   Audio Rooms An audio room can be designed with moderators, speakers and listeners where a moderator can promote a listener to speaker and vice versa.   Classroom Roles can be created for muted-student(not allowed to unmute), student-spotlight(publishes higher quality video), student-screen(allowed to screenshare), prefect(can mute others) etc. to create an enriching feature rich classroom experience. "
    },
    {
        "title": "End Room for All",
        "link": "/javascript/v2/how--to-guides/control-remote-peers/end-room",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/control-remote-peers/end-room",
        "keywords": [],
        "content": "    End Room (/guides/end-room.png) If the local peer has   endRoom  (./change-role know-thy-permissions) permission they can end a room using the below api. It's also possible to lock the room to prevent any future rejoins.  s id=\"endroomcall\" items=  'JavaScript', 'React'       id='endroomcall-0'>   js function renderEndRoomButton(permissions)     const endRoomButton = document.getElementById('end-room-button');   if (permissions.endRoom)       endRoomButton.addEventListener('click', function ()         try           const lock = false; // set to true to disallow rejoins         const reason = 'party is over';         await hmsActions.endRoom(lock, reason);         catch (error)           // Permission denied or not connected to room         console.error(error);              );     endRoomButton.style.display = 'inline-block';     else       endRoomButton.style.display = 'none';       // subscribe to the permissions, so render is called whenever there is a change due to role change hmsStore.subscribe(renderEndRoomButton, selectPermissions);   </Tab>   id='endroomcall-1'>   jsx function EndRoomButton()     const permissions = useHMSStore(selectPermissions);   const hmsActions = useHMSActions();   const endRoom = () =>       try         const lock = false; // set to true to disallow rejoins       const reason = 'party is over';       await hmsActions.endRoom(lock, reason);       catch (error)         // Permission denied or not connected to room       console.error(error);          ;   return permissions.endRoom ? <button onClick= endRoom >End Room</button> : null;     </Tab> > 🚧 If the local peer doesn't have the required endRoom permission, the hmsActions.endRoom call will throw an  HMSException error (error-handling).   End Room Notification Once the peer with adequate permissions calls endRoom , all other peers in the room will receive a  notification (../features/notifications) with type ROOM_ENDED with a  HMSLeaveRoomRequest (../../../api-reference/javascript/v2/interfaces/HMSLeaveRoomRequest) object as the data. The SDK automatically calls leave and performs necessary clean ups immediately after this notification is sent, clients should show the appropriate UI(show a toast, redirect to a 'good-bye' page) within this period.  s id=\"endroomhandle\" items=  'JavaScript', 'React'       id='endroomhandle-0'>   js hmsNotifications.onNotification((notification) =>     if ( notification)       return;       switch (notification.type)       // ...Other notification type cases     case 'ROOM_ENDED':       // Redirect or Show toast to user       toast(notification.data.reason);       break;      );   </Tab>   id='endroomhandle-1'>   jsx import   useHMSNotifications   from '@100mslive/react-sdk'; function Notification()     const notification = useHMSNotifications();   useEffect(() =>       if ( notification)         return;           switch (notification.type)         // ...Other notification type cases       case 'ROOM_ENDED':         // Redirect or Show toast to user         toast(notification.data.reason);         break;          ,  notification );   return <div>Notification:  notification?.type </div>;     </Tab> "
    },
    {
        "title": "Remote Mute/Unmute",
        "link": "/javascript/v2/how--to-guides/control-remote-peers/remote-mute",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/control-remote-peers/remote-mute",
        "keywords": [],
        "content": "  You're running a room and decide that someone who's currently talking shouldn't be talking, or perhaps you want their video turned off. You're looking for a remote mute/unmute in this case.   Muting/Unmuting Can't let just anyone mute others. First you need to create a  role (../foundation/templates-and-roles) with the permissions to end a room.   Permissions  Remote mute/unmute (/guides/permissions.png) Use the  selectPermissions (./change-role know-thy-permissions) selector to check whether the local peer has the permission to mute or unmute others and then call one of the APIs below. 1. Muting Single Track To mute a single track, call   hmsActions.setRemoteTrackEnabled  (../../../api-reference/javascript/v2/interfaces/HMSActions set-remote-track-enabled) with the trackID of the track you want to mute/unmute and enable ( true for unmuting, false for muting).   js try     await hmsActions.setRemoteTrackEnabled(peer.audioTrack, true);   catch (error)     // Permission denied or invalid track ID or not connected to room   console.error(error);     2. Muting Multiple Tracks You can use this API to mute mute/unmute multiple peers at the same time either by their roles, track source, track type or any combination of the above. Note that this API always excludes the local peer from the operation and applies only to remote peers.   js try     const options =       enabled: false, // false to mute, true to unmute     roles:  'student', 'parents' , // optional, array of roles to mute, mutes everyone if not passed     type: 'audio', // optional, audio/video, mutes both if not passed     source: 'regular' // optional, mutes all sources(regular, screen etc.) if not passed    ;   await hmsActions.setRemoteTracksEnabled(options);   catch (error)     // Permission denied or invalid track ID or not connected to room   console.error(error);     > 🚧 If the local peer doesn't have the required mute / unmute permission, the actions will throw an  HMSException error (error-handling).   Handling a mute/unmute request   Remote Unmute Request (/guides/remote-unmute.png) Once the peer with adequate permissions calls setRemoteTrackEnabled , the local peer will receive a  notification (notifications). In case of single track mute it will be of type CHANGE_TRACK_STATE_REQUEST with a  HMSChangeTrackStateRequest (../../../api-reference/javascript/v2/interfaces/HMSChangeTrackStateRequest) object as the data.   Mute requests are automatically applied to the receiver by the SDK. No action is required.   For unmute request, use the information in notification.data to show a dialog to the user to ask if they want to accept   the change and then apply the settings locally. The same as in a regular user  Mute/Unmute (mute).  s id=\"remotemutehandle\" items=  'JavaScript', 'React'       id='remotemutehandle-0'>   js hmsNotifications.onNotification((notification) =>     if ( notification)       return;       switch (notification.type)       // For Single Track Remote Mute     case HMSNotificationTypes.CHANGE_TRACK_STATE_REQUEST:       const   requestedBy, track, enabled   = notification.data;       // Unmute Request       if (enabled)           // Ask for consent using dialog or any other appropriate UI         await hmsActions.setEnabledTrack(track.id, enabled);         else           // Mute Request         // Show toast to user               break;     // For bulk remote mute, notification will contain all the tracks which will be impacted by this request     case HMSNotificationTypes.CHANGE_MULTI_TRACK_STATE_REQUEST:       const   requestedBy, tracks, type, source, enabled   = notification.data;       // Unmute Request       if (enabled)           // Ask for consent using dialog or any other appropriate UI         tracks.forEach(           async (track) => await hmsActions.setEnabledTrack(track.id, enabled)         );         else           // Mute Request         // Show toast to user               break;      );   </Tab>   id='remotemutehandle-1'>   jsx import   useHMSNotifications   from '@100mslive/react-sdk'; function Notification()     const notification = useHMSNotifications();   useEffect(() =>       if ( notification)         return;           switch (notification.type)         // For Single Track Remote Mute       case HMSNotificationTypes.CHANGE_TRACK_STATE_REQUEST:         const   requestedBy, track, enabled   = notification.data;         // Unmute Request         if (enabled)             // Ask for consent using dialog or any other appropriate UI           await hmsActions.setEnabledTrack(track.id, enabled);           else             // Mute Request           // Show toast to user                   break;       // For bulk remote mute, notification will contain all the tracks which will be impacted by this request       case HMSNotificationTypes.CHANGE_MULTI_TRACK_STATE_REQUEST:         const   requestedBy, tracks, type, source, enabled   = notification.data;         // Unmute Request         if (enabled)             // Ask for consent using dialog or any other appropriate UI           tracks.forEach(             async (track) => await hmsActions.setEnabledTrack(track.id, enabled)           );           else             // Mute Request           // Show toast to user                   break;          ,  notification );   return <div>Notification:  notification?.type </div>;     </Tab> "
    },
    {
        "title": "Remove Peer",
        "link": "/javascript/v2/how--to-guides/control-remote-peers/remove-peer",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/control-remote-peers/remove-peer",
        "keywords": [],
        "content": "  If the local peer has   removeOthers  (./change-role know-thy-permissions) permission they can remove a peer using the below api.   js try     const reason = 'Good Bye';   await hmsActions.removePeer(peer.id, reason);   catch (error)     // Permission denied or invalid peer ID or not connected to room   console.error(error);     > 🚧 If the local peer doesn't have the required removeOthers permission, a  HMSException (error-handling) will be thrown.   Remove Peer Notification Once the peer with adequate permissions calls removePeer for the local peer, the local peer will receive a  notification (notifications) with type REMOVED_FROM_ROOM with a  HMSLeaveRoomRequest (../../../api-reference/javascript/v2/interfaces/HMSLeaveRoomRequest) object as the data. The SDK automatically calls leave and performs necessary clean ups immediately after this notification is sent, clients should show the appropriate UI(show a dialog, redirect to a 'good-bye' page) on receiving this notification.  s id=\"removepeerhandle\" items=  'JavaScript', 'React'       id='removepeerhandle-0'>   js hmsNotifications.onNotification((notification) =>     if ( notification)       return;       switch (notification.type)       // ...Other notification type cases     case 'REMOVED_FROM_ROOM':       // Redirect or Show toast to user       toast('Reason: ', notification.data.reason);       break;      );   </Tab>   id='removepeerhandle-1'>   jsx import   useHMSNotifications   from '@100mslive/react-sdk'; function Notification()     const notification = useHMSNotifications();   useEffect(() =>       if ( notification)         return;           switch (notification.type)         // ...Other notification type cases       case 'REMOVED_FROM_ROOM':         // Redirect or Show toast to user         toast('Reason: ', notification.data.reason);         break;          ,  notification );   return <div>Notification:  notification?.type </div>;     </Tab> "
    },
    {
        "title": "Manage Audio Volume",
        "link": "/javascript/v2/how--to-guides/control-remote-peers/volume-control",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/control-remote-peers/volume-control",
        "keywords": [],
        "content": "    Change Volume (/guides/volume.png)   Change Volume API Audio volume refers to the volume of an audio track as perceived on the receiving end. This is a number between 0 to 100. To modify audio volume    s id=\"volume\" items=  'Javascript', 'React'       id='volume-0'>   js const volume = 70; // set the volume across whole room for each track hmsActions.setVolume(volume); // set the volume for a specific track hmsActions.setVolume(volume, \"<track_id>\"); // get the currrent volume for a specific track hmsStore.getState(selectAudioTrackVolume(\"<track_id>\"));   </Tab>   id='volume-1'>   jsx function PeerVolumeController( peerID )    const track = useHMSStore(selectAudioTrackByPeerID(peerID));  const currVolume = useHMSStore(selectAudioTrackVolume(track?.id));  const hmsActions = useHMSActions();  const setVolume = (volume) =>     if (track)      console.log( setting volume for track=$ track.id , prev=$ currVolume , new=$ volume  );    hmsActions.setVolume(volume, track.id);         return <Slider onChange= setVolume  ;     </Tab>   Gotchas Availability of peer doesn't mean availability of their audio track. The below code can silence the whole room if peer's audio track is not yet present.   js const peer = hmsStore.getState(selectPeerById(\"peer-id\")); hmsActions.setVolume(0, peer?.audioTrack); //    If you want to set volume for specific track, always do a check before calling setVolume     js const peer = hmsStore.getState(selectPeerById(\"peer-id\")); if (peer?.audioTrack)    hmsActions.setVolume(0, peer.audioTrack);       Ideas   Proximity based audio for the Metaverse If you're building a 2D/3D world, where peers can navigate around and you want the audio to be based on how close two people are, you can use this action to decide an appropriate audio level based on proximity.    Silence a role To silence a particular role, you can get all the peers for the role and silence the tracks in a loop    js function setVolumeByRole(roleName, volume)    const peers = hmsStore.getState(selectPeersByRole(roleName));  for (const peer of peers)     if (peer.audioTrack)      hmsActions.setVolume(volume, peer.audioTrack);          // silence a role setVolumeByRole(\"role-name\", 0); // revert the silence setVolumeByRole(\"role-name\", 100);   "
    },
    {
        "title": "Error Handling",
        "link": "/javascript/v2/how--to-guides/debugging/error-handling",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/error-handling",
        "keywords": [],
        "content": "  When you make an API call to access an HMS SDK, the SDK may return error codes. ErrorCodes are usually returned when a problem that cannot be recovered without app intervention has occurred. These are returned as type='error' through the  notifications interface (/javascript/v2/features/notifications). The type of notification.data is HMSException . Find the  HMSException interface here (/api-reference/javascript/v2/interfaces/HMSException).   Handling Autoplay Error   Device Change Settings (/guides/autoplay-error.png) Most browsers have a restriction on audio autoplay where audio is allowed only if the user has interacted with the page. You can read more about the restrictions here for  chrome (https://developer.chrome.com/blog/autoplay/) and  safari (https://webkit.org/blog/7734/auto-play-policy-changes-for-macos/). If an autoplay error is received(code=3008), UI can show a popup or a notification with button. Once a user click has happened confirming an interaction point, unblockAudio can be called.   js await hmsActions.unblockAudio();     Error Codes Following are the different error codes that are returned by the SDK. Before returning any error code, SDK retries the errors wherever possible.    Error Code      Cause of the error                                               Action to be taken                                               :     :                            :                               1000       Failed to connect to Websocket  Happens due to network issues or firewall blocking Websocket connections  Mention user to check their network connection and firewall/anti-virus/VPN or try again after some time.      1003       Websocket disconnected  Happens due to network issues                           Mention user to check their network connection or try again after some time.                    2002       Invalid Endpoint URL                                            Check the endpoint provided while calling join on HMSSDK .                            2003       Endpoint is not reachable                                          Mention user to check their network connection or try again after some time.                    2004       Token is not in proper JWT format                                      The token passed while calling join is not in correct format. Retry getting a new token.             3000       Overconstrained error                                            No Action needed from end user, we will be internally retrying.                           3001       User denied permission to access capture device at browser level                      Ask user to check permission granted to audio/video capture devices in browser(in address bar).           3002       Capture Device is not Available                                       Ask user to check if the audio/video capture device is connected or not.                      3003       Capture device is in use by some other application                             Show notification to user mentioning that the capturing device is used by some other application currently.     3005       There is no media to return. Please select either video or audio or both.                  There is no media to return. Please select either video or audio or both.                      3008       Browser has throw an autoplay exception                                   Show notification to user mentioning that the browser blocked autoplay                       3009       Device unable to satisfy requested media constraints                            Reduce media constraints for the role in dashboard or upgrade device hardware.                   3010       No audio detected from track, indicates problem with device hardware                    Notify user to try using a headphone for microphone.                                3011       System denied permission to access capture device                              Ask user to check permission granted to audio/video/screen capture devices in system settings.           4001       WebRTC error                                                Some webRTC error has occured. Need more logs to debug.                               4002       WebRTC error                                                Some webRTC error has occured. Need more logs to debug.                               4003       WebRTC error                                                Some webRTC error has occured. Need more logs to debug.                               4004       WebRTC error                                                Some webRTC error has occured. Need more logs to debug.                               4005       ICE Connection Failed due to network issue                                 Mention user to check their network connection or try again after some time.                    5001       Trying to join a room which is already joined                                Trying to join an already joined room.                                       5002       Trying to join a room when preview is in progress                              Call join action only when room state is \"Preview\" and not \"Connecting\".                      6000       Calling actions when not connected/joined to a room                             Call the required action after joining a room.                                   6002       WebRTC Error: Error while renegotiating                                   We can add check error.message for more info.                                   6003       WebRTC engine is not ready yet                                       Wait for preview to complete before calling join.                                  6008       Invalid arguments(peer ID, track ID, or others) passed to actions                      Check if the corresponding arguments are valid by checking the store when calling the action            6009       Trying to join a room when preview is in progress                              Call join action only when room state is \"Preview\" and not \"Connecting\".                      6010       navigator.mediaDevices is undefined in insecure contexts served over HTTP protocol             Deploy the app in a secure context with HTTPS protocol.                               6011       RTCPeerConnection is undefined in browser                                  Switch/Upgrade browser to WebRTC compatible version/check for browser extenstion blocking WebRTC.          7001       Platform Not Supported                                           The platform is not supported for plugin                                      7002       Plugin Init Failed                                             Plugin initialization has failed                                          7003       Plugin Processing Failed                                          Plugin processing failed                                              7004       Plugin Add Already Going on                                         Plugin add is already in progress                                          8001       Playlist: Invalid action call                                        Check current playing entry, next entry, previous entry when calling seek/seekTo, playNext, playPrevious.   "
    },
    {
        "title": "What does HMS stand for in the SDK name?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#what-does-hms-stand-for-in-the-sdk-name",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#what-does-hms-stand-for-in-the-sdk-name-0",
        "keywords": [],
        "content": "---\ntitle: Frequently Asked\nnav: 10.5\n---\n\nThis page lists down frequently asked questions. If you want to add a new question or edit an older one, feel free to\n[send us a PR](https://github.com/100mslive/100ms-docs/blob/main/docs/javascript/v2/debugging/faq.mdx).\n\n## What does HMS stand for in the SDK name?\n\nHundred(100) Milliseconds 😊. [Why?](https://www.nngroup.com/articles/response-times-3-important-limits/)\n\n## "
    },
    {
        "title": "Can I listen to webhooks on server side?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#can-i-listen-to-webhooks-on-server-side",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#can-i-listen-to-webhooks-on-server-side-1",
        "keywords": [],
        "content": "Can I listen to webhooks on server side?\n\nYes, please check [webhooks](/server-side/v2/introduction/webhook).\n\n## "
    },
    {
        "title": "How do I record a room?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#how-do-i-record-a-room",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#how-do-i-record-a-room-2",
        "keywords": [],
        "content": "How do I record a room?\n\nWe have two types of recordings available, [SFU](/server-side/v2/Destinations/recording) and [Browser](/server-side/v2/Destinations/rtmp-streaming-and-browser-recording).\nYou can also start the latter from the [SDK](../features/rtmp-recording).\n\n## "
    },
    {
        "title": "How do I debug blank video tile while rendering?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#how-do-i-debug-blank-video-tile-while-rendering",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#how-do-i-debug-blank-video-tile-while-rendering-3",
        "keywords": [],
        "content": "How do I debug blank video tile while rendering?\n\nStart with making sure that attach video is being called for the correct track and video\nelement. Also ensure that there is no bug leading to detach call just after or around the same time as attach. These calls\nwhen done will also show up in the redux DevTools [extension](../debugging/debugging#redux-devtools-integration). Some things we have seen in the past -\n\n-   The role was not subscribed properly in the dashboard's templates section\n-   [React] Calling detach as a cleanup function of the same useEffect which calls attach and has video track as dependency. Instead of this\n    please have a separate useEffect with no dependencies to call detach on component unmount.\n-   [React] Not using the [key](https://reactjs.org/docs/lists-and-keys.html) field properly while rendering the list of components displaying\n    the track. This should ideally be the `trackId` or `peerId-trackType`, where track type is video or screen.\n-   [Angular] Not using the [trackBy](https://angular.io/api/core/TrackByFunction) field properly while rendering the list of components displaying\n    the track. This should ideally be the `trackId` or `peerId-trackType`, where track type is video or screen.\n\n## "
    },
    {
        "title": "Why is the video not auto-playing on page load?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#why-is-the-video-not-auto-playing-on-page-load",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#why-is-the-video-not-auto-playing-on-page-load-4",
        "keywords": [],
        "content": "Why is the video not auto-playing on page load?\n\nFor the video to auto-play please make sure these fields are set on the video element - `auto-play`, `muted`, `playsinline`. Please check\nthe docs for [render video](../features/render-video) for more details.\n\n## "
    },
    {
        "title": "(Angular) Why is video not auto-playing even though muted is set as true?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#angular-why-is-video-not-auto-playing-even-though-muted-is-set-as-true",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#angular-why-is-video-not-auto-playing-even-though-muted-is-set-as-true-5",
        "keywords": [],
        "content": "(Angular) Why is video not auto-playing even though muted is set as true?\n\nAngular 2+ is sometimes not able to translate the `muted` field correctly. Instead of setting the muted and auto-play property as\n`<video muted>` doing `<video [muted]=\"true\">` should work.\nPlease check this [Stack Overflow answer](https://stackoverflow.com/questions/48856562/chrome-android-video-autoplay-inside-angular-2-component) for more details.\n\n## "
    },
    {
        "title": "How do I debug no audio coming in the room?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#how-do-i-debug-no-audio-coming-in-the-room",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#how-do-i-debug-no-audio-coming-in-the-room-6",
        "keywords": [],
        "content": "How do I debug no audio coming in the room?\n\n-   Check that the role is being [subscribed to](../foundation/templates-and-roles#subscribe-strategies) properly in [dashboard's](https://dashboard.100ms.live/) templates section.\n\n-   If your web-app doesn't require a user click to join the room, you might run into auto-play issues. Browsers don't allow a website to play audio if user hasn't interacted with the page till that point in time. Fortunately, we have inbuilt support to detect and resolve this given in more details [here](../features/error-handling#handling-autoplay-error).\n\n-   If you're using the [setVolume](../advanced-features/volume-control) API, it's possible that even though the audio is available it has been locally muted.\n\n## "
    },
    {
        "title": "Is it possible to do RTMP out, live stream a room to YouTube, Twitch, Wowza?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#is-it-possible-to-do-rtmp-out-live-stream-a-room-to-youtube-twitch-wowza",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#is-it-possible-to-do-rtmp-out-live-stream-a-room-to-youtube-twitch-wowza-7",
        "keywords": [],
        "content": "Is it possible to do RTMP out, live stream a room to YouTube, Twitch, Wowza?\n\nYes, you can achieve it both from [server-side APIs](/server-side/v2/Destinations/rtmp-streaming-and-browser-recording) or [SDK](../features/rtmp-recording).\n\n## "
    },
    {
        "title": "What should I do to hide the beam tile showing up in 100ms web-app for browser based recording/streaming?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#what-should-i-do-to-hide-the-beam-tile-showing-up-in-100ms-web-app-for-browser-based-recordingstreaming",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#what-should-i-do-to-hide-the-beam-tile-showing-up-in-100ms-web-app-for-browser-based-recordingstreaming-8",
        "keywords": [],
        "content": "What should I do to hide the beam tile showing up in 100ms web-app for browser based recording/streaming?\n\nYou can use a viewer role which doesn't have any publish permissions.\n\n## "
    },
    {
        "title": "How do I make the beam bot join with a custom role for dashboard web-app?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#how-do-i-make-the-beam-bot-join-with-a-custom-role-for-dashboard-web-app",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#how-do-i-make-the-beam-bot-join-with-a-custom-role-for-dashboard-web-app-9",
        "keywords": [],
        "content": "How do I make the beam bot join with a custom role for dashboard web-app?\n\nYou can append a query param in the end of the URL for the custom role - `<custom_role_url>?skip_preview=true`. This\nwill tell the web-app to skip preview screen and join directly.\n\n## "
    },
    {
        "title": "Why does YouTube dashboard shows that the video bitrate is less than the recommended bitrate when using RTMP Out?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#why-does-youtube-dashboard-shows-that-the-video-bitrate-is-less-than-the-recommended-bitrate-when-using-rtmp-out",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#why-does-youtube-dashboard-shows-that-the-video-bitrate-is-less-than-the-recommended-bitrate-when-using-rtmp-out-10",
        "keywords": [],
        "content": "Why does YouTube dashboard shows that the video bitrate is less than the recommended bitrate when using RTMP Out?\n\nYou can safely ignore this, this will happen if there is no activity happening on the URL being streamed. For example, there is nobody in\nthe room with their video turned on.\n\n## "
    },
    {
        "title": "Can I get HLS out for a room?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#can-i-get-hls-out-for-a-room",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#can-i-get-hls-out-for-a-room-11",
        "keywords": [],
        "content": "Can I get HLS out for a room?\n\nNot yet, but we're working on this.\n\n## "
    },
    {
        "title": "How do I join an API created room from dashboard web-app?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#how-do-i-join-an-api-created-room-from-dashboard-web-app",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#how-do-i-join-an-api-created-room-from-dashboard-web-app-12",
        "keywords": [],
        "content": "How do I join an API created room from dashboard web-app?\n\nYou won't see the join room button on the dashboard, but it's possible to form an URL which you can use. The format is\n`https://<subdomain>.app.100ms.live/preview/<room_id>/<role>`, For example `https://myroomlink.app.100ms.live/preview/123456/teacher`.\nAll of these, the subdomain, room_id and role are available on the dashboard.\n\n## "
    },
    {
        "title": "Do I need to do anything to handle poor internet connection?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#do-i-need-to-do-anything-to-handle-poor-internet-connection",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#do-i-need-to-do-anything-to-handle-poor-internet-connection-13",
        "keywords": [],
        "content": "Do I need to do anything to handle poor internet connection?\n\nNot much, just turn on a flag in dashboard, and show a proper UI when a video gets degraded/unsubscribed.\nMore details [here](../features/sub-degradation).\n\n## "
    },
    {
        "title": "Can I store extra information with a peer?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#can-i-store-extra-information-with-a-peer",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#can-i-store-extra-information-with-a-peer-14",
        "keywords": [],
        "content": "Can I store extra information with a peer?\n\nYes you can store [peer metadata](../advanced-features/peer-metadata) for a peer. The initial value can be provided at the time of join,\nand can be modified post join.\n\n## "
    },
    {
        "title": "How do I implement Raise Hand?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#how-do-i-implement-raise-hand",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#how-do-i-implement-raise-hand-15",
        "keywords": [],
        "content": "How do I implement Raise Hand?\n\nYou can do using [peer metadata](../advanced-features/peer-metadata).\n\n## "
    },
    {
        "title": "Why do I see videos getting stuck or frozen?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#why-do-i-see-videos-getting-stuck-or-frozen",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#why-do-i-see-videos-getting-stuck-or-frozen-16",
        "keywords": [],
        "content": "Why do I see videos getting stuck or frozen?\n\nIf you have enabled subscribe degradation from the dashboard, the SDK might go in the degradation mode on poor internet connection\nturning off some videos to ensure good call quality. When this is done, a flag on the track will be turned on to let the UI know. The UI\nshould treat it similar to the track turning off for purpose of displaying avatar etc. More details [here](../features/sub-degradation).\n\n## "
    },
    {
        "title": "Do you have UI components?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#do-you-have-ui-components",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#do-you-have-ui-components-17",
        "keywords": [],
        "content": "Do you have UI components?\n\nNot yet, but it's work in progress for react. Do let us know on discord if you want to sign up for beta and we'll hit you up soon.\n\n## "
    },
    {
        "title": "Can I use the SDK with NextJS, Angular, Svelte, VueJS etc.?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#can-i-use-the-sdk-with-nextjs-angular-svelte-vuejs-etc",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#can-i-use-the-sdk-with-nextjs-angular-svelte-vuejs-etc-18",
        "keywords": [],
        "content": "Can I use the SDK with NextJS, Angular, Svelte, VueJS etc.?\n\nYes, the core SDK is framework agnostic, you can follow the [JS Quickstart](../guides/javascript-quickstart) to learn the basics. The quickstart\nguide is with vanilla JS and doesn't assume any framework.\n\n## "
    },
    {
        "title": "I want to suggest a new feature.",
        "link": "/javascript/v2/how--to-guides/debugging/faq#i-want-to-suggest-a-new-feature",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#i-want-to-suggest-a-new-feature-19",
        "keywords": [],
        "content": "I want to suggest a new feature.\n\nAwesome, we're always on the lookout for new ideas and feature. Please reach out to us over [discord](https://100ms.live/discord).\n\n## "
    },
    {
        "title": "I'm facing an issue, how do I reach out?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#im-facing-an-issue-how-do-i-reach-out",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#im-facing-an-issue-how-do-i-reach-out-20",
        "keywords": [],
        "content": "I'm facing an issue, how do I reach out?\n\nPlease see [reaching out](../debugging/debugging#reaching-out).\n\n## "
    },
    {
        "title": "Can I create room using API?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#can-i-create-room-using-api",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#can-i-create-room-using-api-21",
        "keywords": [],
        "content": "Can I create room using API?\n\n[Yes](/server-side/v2/Rooms/create-via-api).\n\n## "
    },
    {
        "title": "Can I disable a room?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#can-i-disable-a-room",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#can-i-disable-a-room-22",
        "keywords": [],
        "content": "Can I disable a room?\n\n[Yes](/server-side/v2/Rooms/disable-or-enable). You can also do this while ending the room using the [SDK](../features/end-room).\n\n## "
    },
    {
        "title": "Is it possible to create and manage roles using APIs?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#is-it-possible-to-create-and-manage-roles-using-apis",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#is-it-possible-to-create-and-manage-roles-using-apis-23",
        "keywords": [],
        "content": "Is it possible to create and manage roles using APIs?\n\n[Yes](/server-side/v2/policy/template-object).\n\n## "
    },
    {
        "title": "Does the SDK remembers input/output device selection for future joins?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#does-the-sdk-remembers-inputoutput-device-selection-for-future-joins",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#does-the-sdk-remembers-inputoutput-device-selection-for-future-joins-24",
        "keywords": [],
        "content": "Does the SDK remembers input/output device selection for future joins?\n\nYes, just make sure you pass the `rememberDeviceSelection` as true in the [join config](../features/join).\n\n## "
    },
    {
        "title": "Can I implement custom events to broadcast or sent to a specific person in the room?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#can-i-implement-custom-events-to-broadcast-or-sent-to-a-specific-person-in-the-room",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#can-i-implement-custom-events-to-broadcast-or-sent-to-a-specific-person-in-the-room-25",
        "keywords": [],
        "content": "Can I implement custom events to broadcast or sent to a specific person in the room?\n\nYes, you can do so using our [messaging system](../features/chat#custom-events).\n\n## "
    },
    {
        "title": "How can I access the user id field used while creating the token after joining?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#how-can-i-access-the-user-id-field-used-while-creating-the-token-after-joining",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#how-can-i-access-the-user-id-field-used-while-creating-the-token-after-joining-26",
        "keywords": [],
        "content": "How can I access the user id field used while creating the token after joining?\n\nIt will be available as `peer.customerUserId` for any peer in the room.\n\n## "
    },
    {
        "title": "How can I implement break out rooms?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#how-can-i-implement-break-out-rooms",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#how-can-i-implement-break-out-rooms-27",
        "keywords": [],
        "content": "How can I implement break out rooms?\n\nThis can be done using [roles](https://www.youtube.com/watch?v=aO0KA2w03io).\n\n## "
    },
    {
        "title": "Can I locally mute a remote audio track?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#can-i-locally-mute-a-remote-audio-track",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#can-i-locally-mute-a-remote-audio-track-28",
        "keywords": [],
        "content": "Can I locally mute a remote audio track?\n\n[Yes](../advanced-features/volume-control).\n\n## "
    },
    {
        "title": "Can I process video before sending over to others in the room?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#can-i-process-video-before-sending-over-to-others-in-the-room",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#can-i-process-video-before-sending-over-to-others-in-the-room-29",
        "keywords": [],
        "content": "Can I process video before sending over to others in the room?\n\nYes, you can write [custom video plugins](../plugins/custom-video-plugins).\n\n## "
    },
    {
        "title": "How to disable console logs if I'm using the web SDK?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#how-to-disable-console-logs-if-im-using-the-web-sdk",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#how-to-disable-console-logs-if-im-using-the-web-sdk-30",
        "keywords": [],
        "content": "How to disable console logs if I'm using the web SDK?\n\nPlease follow [setting log level](../features/log-level).\n\n## "
    },
    {
        "title": "What is the maximum allowed duration for a session?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#what-is-the-maximum-allowed-duration-for-a-session",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#what-is-the-maximum-allowed-duration-for-a-session-31",
        "keywords": [],
        "content": "What is the maximum allowed duration for a session?\n\nThe maximum allowed duration for a session on the 100ms platform is 12 hours.\n\n## "
    },
    {
        "title": "How can I get the HLS URL to enable live stream playback?",
        "link": "/javascript/v2/how--to-guides/debugging/faq#how-can-i-get-the-hls-url-to-enable-live-stream-playback",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#how-can-i-get-the-hls-url-to-enable-live-stream-playback-32",
        "keywords": [],
        "content": "How can I get the HLS URL to enable live stream playback?\n\nYou can get the HLS URL in several ways based on whether you use 100ms client SDKs or a custom integration for playback; please check the [HLS guide](./../foundation/live-streaming#live-stream-playback) for more information.\n\n## "
    },
    {
        "title": "I get type errors related to WebRTC Stats, for example, 'Cannot find name RTCInboundRtpStreamStats'.",
        "link": "/javascript/v2/how--to-guides/debugging/faq#i-get-type-errors-related-to-webrtc-stats-for-example-cannot-find-name-rtcinboundrtpstreamstats",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/faq#i-get-type-errors-related-to-webrtc-stats-for-example-cannot-find-name-rtcinboundrtpstreamstats-33",
        "keywords": [],
        "content": "I get type errors related to WebRTC Stats, for example, 'Cannot find name RTCInboundRtpStreamStats'.\n\nThe minimum supported version of TypeScript for the SDK is '4.4'. You could update to any version above 4.4 or if you are on an old version of TypeScript and cannot do a major upgrade you can set `skipLibCheck: true` in your tsconfig file.\n"
    },
    {
        "title": "Known Issues",
        "link": "/javascript/v2/how--to-guides/debugging/known-issues",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/known-issues",
        "keywords": [],
        "content": "  We'll list any issues related to our SDK or browsers/devices here along with any possible workarounds. 1. In iOS Safari, the audio is at times routed to the phone earpiece instead of the speaker. For the end user, this appears as if  the volume is too low. This is an issue in Safari 15, please track the  WebKit bug (https://bugs.webkit.org/show_bug.cgi?id=230902) for more info. We recommend  showing a prompt for using earphones to the user till this is fixed by Safari. 2. If there is no audio in the room for a while iOS Safari can take away autoplay permission requiring a user interaction  to play future audio. Check our section on handling autoplay errors  here (../features/error-handling handling-autoplay-error). 3. Controlling the volume of audio elements is not supported by iOS. So,  locally setting remote peer volume (../advanced-features/volume-control) won't work in iOS.  However, hmsActions.setVolume(0, trackId) and hmsActions.setVolume(100, trackId) will mute and unmute audio respectively. 4. Mac Safari has an issue playing canvas stream in video. This will result in video playlist not visible to the peer who is playing it in safari. This is happening  since Safari 15.2. Please track  this (https://bugs.webkit.org/show_bug.cgi?id=181663) and  this (https://bugs.webkit.org/show_bug.cgi?id=230621). "
    },
    {
        "title": "Log Level",
        "link": "/javascript/v2/how--to-guides/debugging/log-level",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/log-level",
        "keywords": [],
        "content": "  Console logging from sdk can be controlled by using the   setLogLevel  (https://docs.100ms.live/api-reference/javascript/v2/interfaces/HMSActions set-log-level) api.   js hmsActions.setLogLevel(HMSLogLevel.WARN); // log warnings and errors hmsActions.setLogLevel(HMSLogLevel.ERROR); // only errors will be logged hmsActions.setLogLevel(HMSLogLevel.NONE); // nothing will be logged by our sdk   "
    },
    {
        "title": "Overview",
        "link": "/javascript/v2/how--to-guides/debugging/overview",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/debugging/overview",
        "keywords": [],
        "content": "     Debugging (https://imgs.xkcd.com/comics/debugging.png) (https://xkcd.com/1722/) No code ever runs in the first go. When things go wrong, debugging becomes an important aspect for a developer first SDK. We have designed our APIs and error messages with developer experience and ease of debugging in mind. We hope that you'll have a delightful experience walking through the features and integrating our sdk in your app.   Console Logs In case of any issues, we print error or warning logs in console to help with faster debugging.   types (/guides/role-error.png) > The  log level (../features/log-level) can be configured to control sdk logging.   Fully Typed API Since our Web SDK is written in TypeScript, you will get the amazing Intellisense / Autocompletion out of box. This means that even if you use JavaScript, editors will be able to give a significantly improved experience.   types (/guides/chat-types.png)   Redux Devtools Integration We take the debugging experience one step further by connecting our store to  redux devtools (https://chrome.google.com/webstore/detail/redux-devtools/lmhkpmbekcpmknklioeibfkpmmfibljd?hl=en) extension by default. This gives you transparency and visibility into the whole room state while building your UI.   Redux Devtools (/guides/redux-devtools.png) We highly recommend installing the  devtools (https://chrome.google.com/webstore/detail/redux-devtools/lmhkpmbekcpmknklioeibfkpmmfibljd?hl=en) extension to visualize this state along with all the activities taking place in a single view. This makes it easier to understand when and how the UI should change. You can use the time travel feature of the redux devtools to recreate state changes for debugging specific issues as well.   Feature Rich Sample App We also give a ready to use UI without writing any code which can be accessed from the  dashboard account (https://dashboard.100ms.live). The code for this sample is open sourced  here (https://github.com/100mslive/100ms-web). You can either start with this codebase or write your own, and use it for testing your web-app in the initial phase of development.   Error Codes Do check the detailed page for listening to and handling errors  here (../features/error-handling).   Reaching Out If nothing else works, feel free to create an issue  here (https://github.com/100mslive/100ms-web/issues) or reach out to us on  discord (https://100ms.live/discord), there is a channel there for web-sdk-discussion. If you're reporting a bug, we would appreciate as many details possible. We might further ask you for the following information to debug the issues better    Steps to reproduce Detailed steps to reproduce the issue. You can also give us a  codesandbox (https://codesandbox.io/) where the issue can be seen, feel free to base off the code over one of our quickstart examples.   Relevant code snippet This would be the React Component or the JS Function in/around which the bug/issue happens.   Store Dump You'll need to install the redux devtools  extension (https://chrome.google.com/webstore/detail/redux-devtools/lmhkpmbekcpmknklioeibfkpmmfibljd?hl=en) for this. Once you open the devtools there would be a download option on bottom panel. "
    },
    {
        "title": "Add custom tracks",
        "link": "/javascript/v2/how--to-guides/extend-capabilities/custom-tracks/custom-tracks",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/extend-capabilities/custom-tracks/custom-tracks",
        "keywords": [],
        "content": "  You can also add custom video or audio tracks to the sdk which will be published to others. Possible use cases    a synthetic track which doesn't use camera   screenshare with custom-built annotations in an electron app.   audio track from a source other than microphone To add a custom/auxiliary track, you need to get hold of a MediaStreamTrack object and add it using the addTrack method on hmsActions .   js /     track: MediaStreamTrack  Track to be added   source?: 'regular'  'screen'  'plugin'  Source of track  default: 'regular'  / await hmsActions.addTrack(track, source); // to remove the track await hmsActions.removeTrack(trackId);     Accessing custom/auxiliary tracks   js // if you have hold of the peer object, it'll have ids of all the auxiliary tracks console.log(peer.auxiliaryTracks); // to get the track from a track id you can do this track = hmsStore.getState(selectTrackByID(trackId)); // all auxiliary tracks hmsStore.getState(selectAuxiliaryTracksByPeerID(peer.id)); // audio auxiliary track hmsStore.getState(selectAuxiliaryAudioByPeerID(peer.id)); // track with source as screenshare hmsStore.getState(selectScreenShareByPeerID(peer.id)); hmsStore.getState(selectScreenShareAudioByPeerID(peer.id));   "
    },
    {
        "title": "Playlist - Audio/Video",
        "link": "/javascript/v2/how--to-guides/extend-capabilities/custom-tracks/playlist",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/extend-capabilities/custom-tracks/playlist",
        "keywords": [],
        "content": "  Say you have a bunch of hosted audio/video files you want to play for everyone in the room. The playlist feature gives a first class support for this. Supported actions    Initialise with a list of video or audio playlist items which can be played   Choose the currently playing one   Play, Pause, Next, Previous   Seeking forward/backward or to a specific position   Letting sdk play in sequence or shuffling per your logic   Volume control   Playback rate control  slow down or speed up   API The SDK deals with an array of what we'll call HMSPlaylistItem . This is how the interface looks for it looks like. There are 4 mandatory fields described in the schema below.   ts enum HMSPlaylistType     audio = 'audio',   video = 'video'   interface HMSPlaylistItem<T>     /       uniquely identifies a playlist item    /   id: string;   name: string;   type: HMSPlaylistType;   /       the url to play from, can also be a url created from local file eg: URL.createObjectURL(file)    /   url: string;   /       any additional info, for eg. composer, musician etc.    /   metadata?: T;   /       duration in seconds    /   duration?: number;     Once you have made such an array of your playlist items, you can then give it to sdk as below, and you'll be ready to go.   ts const audioPlayList: HMSPlaylistItem   = getMyAudioPlaylist(); const videoPlayList: HMSPlaylistItem   = getMyVideoPlaylist(); hmsActions.audioPlaylist.setList(audioPlayList); hmsActions.videoPlaylist.setList(videoPlayList);     Actions on the Playlist Once the playlist is ready and given to the sdk, play/pause etc. can be done through calling different functions on hmsActions.audioPlaylist or hmsActions.videoPlaylist . The below interface shows all the functions which can be called. Do note that these will work only if you have already joined the room.   ts export interface IHMSPlaylistActions     /       Plays the item whose id is passed.     @param  string  id  the id as given while initialising the list    /   play(id: string): Promise<void>;   /       Pauses the current playing item    /   pause(): Promise<void>;   /       Plays the next item in the queue(order decided by original array)    /   playNext(): Promise<void>;   /       Plays the previous item in the queue(order decided by original array)    /   playPrevious(): Promise<void>;   /       seek forward or backward relative to current position     @param  number  seekValue  number in seconds to move forward(pass negative values to move backwards)    /   seek(seekValue: number): void;   /       seek to a specific position  seekValue will be absolute     @param  number  seekValue  value in seconds of the absolute position in the playlist item duration    /   seekTo(seekValue: number): void;   /       set volume passing volume     @param  number  volume  number between 0-100    /   setVolume(volume: number): void;   /       pass list of playlist items to set playlist     @param  HMSPlaylistItem     list of playlist items    /   setList<T>(list: HMSPlaylistItem<T>  ): void;   /       Stop the current playback and unpublish the tracks.    /   stop(): Promise<void>;   /       set whether to autoplay next item in playlist after the current one ends. You can disable this     if you want to implement any custom ordering or shuffling on your side. By default sdk will play the next     item in the queue once the current ends.     @param  boolean  autoplay    /   setIsAutoplayOn(autoplay: boolean): void;   /       Control the playback speed  1.0 being normal, less than 1.0 will play it slowly     and more than 1.0 will play it faster.     @param playbackRate  value from 0.25 and 2.0    /   setPlaybackRate(playbackRate: number): void;   /       Remove a single item from the playlist. This will stop the track if it is the current playing one and remove the item     returns a promise that resolves to true if the item was removed    /   removeItem(id: string): Promise<boolean>;   /       Clear the entire playlist. This will stop the playing track(if any)    /   clearList(): Promise<void>;       Accessing Audio/Video Playlist information Once audio/video playlist item is played, a track will be obtained and added to peer.auxiliaryTracks . We also expose a bunch of helpful selectors similar to those for screenshare tracks for getting the current state.  s id=\"playlist\" items=  'JavaScript', 'React'     ' '    id='playlist-0'>   js:playlist.js import   hmsActions, hmsStore   from './hms'; // audioListItems, videoListItems should have HMSPlaylistItem   interface hmsActions.audioPlaylist.setList(audioListItems); hmsActions.videoPlaylist.setList(videoListItems);  // Accessing playlist list set above const audioList = hmsStore.getState(selectAudioPlaylist.list); const videoList = hmsStore.getState(selectVideoPlaylist.list); // Accessing current selectedItem const selectedAudio = hmsStore.getState(selectAudioPlaylist.selectedItem); const selectedVideo = hmsStore.getState(selectVideoPlaylist.selectedItem); // Accessing current selection    id, hasNext, hasPrevious   const audioSelection = hmsStore.getState(selectAudioPlaylist.selection); const videoSelection = hmsStore.getState(selectVideoPlaylist.selection); // Accessing current volume const audioVolume = hmsStore.getState(selectAudioPlaylist.volume); const videoVolume = hmsStore.getState(selectVideoPlaylist.volume); // Accessing current progress percentage  value between 0-100 const audioProgress = hmsStore.getState(selectAudioPlaylist.progress); const videoProgress = hmsStore.getState(selectVideoPlaylist.progress); // Accessing current time in seconds, 0-duration const audioCurrentTime = hmsStore.getState(selectAudioPlaylist.currentTime); const videoCurrentTime = hmsStore.getState(selectVideoPlaylist.currentTime); // Accessing current playback rate  value between 0.25-2.0 const audioPlaybackRate = hmsStore.getState(selectAudioPlaylist.playbackRate); const videoPlaybackRate = hmsStore.getState(selectVideoPlaylist.playbackRate); // Accessing Tracks for playlist peer = hmsStore.getState(selectPeerSharingAudioPlaylist); peer = hmsStore.getState(selectPeerSharingVideoPlaylist); const videoTrack = hmsStore.getState(  selectVideoPlaylistVideoTrackByPeerID(peer.id), ); const videoPlaylistAudioTrack = hmsStore.getState(  selectVideoPlaylistAudioTrackByPeerID(peer.id), ); const audioPlaylistAudioTrack = hmsStore.getState(  selectAudioPlaylistTrackByPeerID(peer.id), );    </Tab>   id='playlist-1'>   jsx:playlist.jsx import   useHMSActions, useHMSStore   from '@100mslive/react-sdk'; function Playlist()    const hmsActions = useHMSActions();  const audioList = useHMSStore(selectAudioPlaylist.list);  const videoList = useHMSStore(selectVideoPlaylist.list);  const audioVolume = useHMSStore(selectAudioPlaylist.volume);  const videoVolume = useHMSStore(selectVideoPlaylist.volume);  const audioProgress = useHMSStore(selectAudioPlaylist.progress);  const videoProgress = useHMSStore(selectVideoPlaylist.progress);  const videoTrack = useHMSStore(   selectVideoPlaylistVideoTrackByPeerID(peer.id),  );  const videoPlaylistAudioTrack = useHMSStore(   selectVideoPlaylistAudioTrackByPeerID(peer.id),  );  const audioPlaylistAudioTrack = useHMSStore(   selectAudioPlaylistTrackByPeerID(peer.id),  );  const peerSharingAudioPlaylist = useHMSStore(selectPeerSharingAudioPlaylist);  setList()     // audioListItems, videoListItems should have HMSPlaylistItem interface   hmsActions.audioPlaylist.setList(audioListItems);   hmsActions.videoPlaylist.setList(videoListItems);     return < ;     </Tab>   Playing Next item in the playlist By default, when the current item ends, the next item is played automatically. if you wish to control what you play next, you can do the following.   hmsActions.audioPlaylist.setIsAutoplayOn(false); hmsActions.videoPlaylist.setIsAutoplayOn(false);   You can then listen to a notification of type HMSNotificationTypes.PLAYLIST_TRACK_ENDED when current track ends and play the next track. The notification also contains the playlist item that has ended. <Note type=\"success\">   If you're also using SDKs of any other platform do note that the playlist tracks will come with   source as \"audioplaylist\"/\"videoplaylist\". </Note>   Ideas   Play background music for everyone in the room from a playlist.   Allow for teachers to play educations videos for students from a pre selected list and without worrying   about screenshare etc.   Watch videos together with friends, with some help from the  Chat API (/javascript/v2/features/chat), controls like play/pause can also   be shared between the peers.   Play videos from mobile web browsers which don't support screenshare.   Limitations   No custom encoders/decoders, only file formats supported by native audio and video tags will work. "
    },
    {
        "title": "Collaborative Whiteboard",
        "link": "/javascript/v2/how--to-guides/extend-capabilities/plugins/collaborative-whiteboard",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/extend-capabilities/plugins/collaborative-whiteboard",
        "keywords": [],
        "content": "  <Note type=\"warning\">   This feature is still in Beta and only available for Web platform. To know more or report any   issues, feel free to reach out to us over ' '    <a href= 'https://discord.com/invite/kGdmszyzq2' >Discord</a>. </Note>   Overview This guide will walk you through instructions to build a Collaborative whiteboard by integrating Pusher with our  sample app (https://github.com/100mslive/100ms-web). <video loop=\"true\" autoplay=\"autoPlay\" controls=\"controls\" id=\"vid\" muted>   <source src=\"/docs/guides/whiteboard-demo.mp4\" type=\"video/mp4\"   </video> <br   You need three components to complete the Collaborative whiteboard setup: 1.  Pusher Channels app ( usher-channels-app) 2.  Whiteboard server ( whiteboard-server) — this guide follows Vercel deployment. 3.  Whiteboard client ( whiteboard-client) — this guide uses the  100ms-web sample app (https://github.com/100mslive/100ms-web).   Pusher Channels App   Sign up for a  Pusher account (https://pusher.com/).   Click on Create app to create a new Channels app .   Provide a friendly name for your app and select a cluster based on your preference.   Go to the App Keys section on the sidebar, and copy your app_id, key, secret, and cluster.   Go to the App Settings section on the sidebar and turn on enable client events .   Whiteboard Server   Fork the  Whiteboard Pusher server (https://github.com/100mslive/whiteboard-server) and deploy it using your preferred hosting provider.   Add the Pusher keys noted earlier to environment variables.   bash APP_ID=\"app_id\" APP_KEY=\"key\" APP_SECRET=\"secret\" APP_CLUSTER=\"cluster\"     The API path is api/pusher/auth ; say your deployment URL is whiteboard-server.vercel.app , then the Pusher Auth Endpoint will be https://whiteboard-server.vercel.app/api/pusher/auth .   Whiteboard Client   From the  100ms-web sample app (https://github.com/100mslive/100ms-web) repository, copy the whole folder at /src/plugins/whiteboard into your live video conferencing or live streaming apps using 100ms' SDKs.   Add the Pusher app key and Pusher auth endpoint to REACT_APP_PUSHER_APP_KEY and REACT_APP_PUSHER_AUTHENDPOINT environment variables.   The useWhiteboardMetadata hook returns a state such as the whiteboard owner( whiteboardOwner ) and action to toggle the whiteboard( toggleWhiteboard ). Refer to usage in ToggleWhiteboard.jsx  an icon button to toggle the whiteboard based on the active state.   When the whiteboard is active( whiteboardOwner from useWhiteboardMetadata is not null), render the Whiteboard component on your UI to let your users draw on the whiteboard. Refer mainView.jsx and WhiteboardView.jsx . "
    },
    {
        "title": "Custom Audio Plugins",
        "link": "/javascript/v2/how--to-guides/extend-capabilities/plugins/custom-audio-plugins",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/extend-capabilities/plugins/custom-audio-plugins",
        "keywords": [],
        "content": "  <Note type=\"warning\">   This feature is still in Beta. To know more or report any issues, feel free to reach out to us   over <a href= 'https://discord.com/invite/kGdmszyzq2' >Discord</a>. </Note> Custom audio plugins allow you to hook into 100ms' video lifecycle and add your own audio processing pipeline right before it gets sent to the other participants in the room. This allows for various audio effects like noise suppression, reverb, echo, delay, chorus, robotic voice etc. Note that this page is about creating custom new plugins than using existing ones.   Prerequisites   Basic Knowledge about 100ms SDK, follow our quickstart guides.   Knowledge about  Audio nodes (https://developer.mozilla.org/en-US/docs/Web/API/AudioNode)   Audio Plugin Interface The plugin needs to implement the below interface post which it can be added to the processing pipeline for the local peer's audio.   ts section=AudioPluginInterface sectionIndex=1 export interface HMSPluginSupportResult     isSupported: boolean;   errType?: HMSPluginUnsupportedTypes;   errMsg?: string;   export enum HMSPluginUnsupportedTypes     PLATFORM_NOT_SUPPORTED = 'PLATFORM_NOT_SUPPORTED', // when particular os or browser is not supported   DEVICE_NOT_SUPPORTED = 'DEVICE_NOT_SUPPORTED' // when particular device is not supported, for example Bluetooth headphones   interface HMSAudioPlugin     /       The name is meant to uniquely specify a plugin instance. This will be used to track number of plugins     added to the track, and same name won't be allowed twice for the same audio track.    /   getName(): string;   /       This function will be called before the call to init, it is used to check whether the plugin supports current     OS, browser and audio device or not. An error object will be thrown back to the user if they try to use an unsupported plugin.    /   checkSupport(ctx?: AudioContext): HMSPluginSupportResult;   /       This function will be called in the beginning for initialization which may include tasks like setting up     variables, loading ML models etc. This can be used by a plugin to ensure it's prepared at the time     processAudio is called.    /   init(): Promise<void>;   /       @see HMSAudioPluginType below    /   getPluginType(): HMSAudioPluginType;   /       This function will be called by the SDK for audio track which the plugin needs to process.     The reason audio context is also part of the interface is that it's recommeneded to reuse on audio context     instead of creating new for every use  https://developer.mozilla.org/en-US/docs/Web/API/AudioContext    /   processAudioTrack(ctx: AudioContext, source: AudioNode): Promise<AudioNode>;   /       The plugin can implement this function to dispose off its resources. It'll be called when the processor instance is     no longer needed and the plugin is removed.    /   stop(): void;   /     Specifies the type of the plugin a transforming plugin will get an output audio node to give the resulting   transformation. While an analyzing plugin will only be passed the input node.   For analyse plugins, you can return the source node passed to plugin.processTrack to not modify anything  / export enum HMSAudioPluginType     TRANSFORM = 'TRANSFORM',   ANALYZE = 'ANALYZE'       Adding and Removing Plugins Once a plugin implementation is ready, it can be added and removed from local peer's audio track as below   s id=\"add-remove-plugin\" items=  'Usage', 'CustomPlugin'       id='add-remove-plugin-0'>   js section=AddingAndRemovingPlugins sectionIndex=1 tab=Usage const myPlugin = new MyCustomPlugin(); const checkSupport = myPlugin.checkSupport(); if (checkSupport.isSupported)     // myPlugin.init(); // optional, recommended if plugin implements it, you can show a loader here in the UI   const isPluginAdded = hmsStore.getState(     selectIsLocalAudioPluginPresent(myPlugin.getName());   )   if ( isPluginAdded)       hmsActions.addPluginToAudioTrack(myPlugin);     else       hmsActions.removePluginFromAudioTrack(myPlugin);       else     console.log('plugin not supported', checkSupport.errMsg, checkSupport.errType)     </Tab>   id='add-remove-plugin-1'>   ts section=AddingAndRemovingPlugins sectionIndex=1 tab=CustomPlugin class MyCustomPlugin implements HMSAudioPlugin      </Tab>   Implementation Example  Gain effect Plugin Below is a sample implementation of the above interface which can be used to control the overall gain (or volume) of the audio.   ts section=ImplementationExample sectionIndex=1 class GainPlugin implements HMSAudioPlugin     private gainNode?: GainNode;   private gainValue = 0.25;   private name = 'gain-plugin';   constructor(gainValue?: number, name?: string)       if (gainValue  == undefined)         this.gainValue = gainValue;           if (name)         this.name = name;             async processAudioTrack(ctx: AudioContext, source: AudioNode)       if ( ctx)         throw new Error('Audio context is not created');           if ( source)         throw new Error('source is not defined');           this.gainNode = ctx.createGain();     this.gainNode.gain.value = this.gainValue;     source.connect(this.gainNode);     return this.gainNode;        checkSupport()       // This is when plugin is supported.     return         isSupported: true,           // if plugin is not supported in some browser, you can send error here.     return         isSupported: false,       errType: HMSPluginUnsupportedTypes.PLATFORM_NOT_SUPPORTED;       errMsg: 'Error message you want to share'              init()      getName()       return this.name;       getPluginType()       return HMSAudioPluginType.TRANSFORM;       stop()       this.gainNode?.disconnect();     this.gainNode = undefined;       // to add the plugin to local audio hmsActions.addPluginToAudioTrack(new GainPlugin());     Plugin Guidelines   If your plugin involves a CPU intensive task such as initialising a ML model, ensure that it's done as part of the init method.   As a bonus, ensure that the plugin remembers the model initialisation across instances, so in case user leaves and joins the room again   without closing the tab they don't see a delay the second time. "
    },
    {
        "title": "Custom Video Plugins",
        "link": "/javascript/v2/how--to-guides/extend-capabilities/plugins/custom-video-plugins",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/extend-capabilities/plugins/custom-video-plugins",
        "keywords": [],
        "content": "  <Note type=\"warning\">   This feature is still in Beta. To know more or report any issues, feel free to reach out to us   over <a href= 'https://discord.com/invite/kGdmszyzq2' >Discord</a>. </Note> Custom video plugins allow you to hook into 100ms' video lifecycle and add your own video processing pipeline right before it gets sent to the other participants in the room. This allows for super cool things like building AR Filters, adding Virtual Background, Emojifying the streams and monitoring participant engagement. Checkout our  Virtual Background (./virtual-background) docs to see an example of such a plugin. Note that this page is about creating custom new plugins than using existing ones.   Prerequisites   Basic Knowledge about 100ms SDK, follow our quickstart guides.   Knowledge about  HTML Canvas (https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API)   Video Plugin Interface The plugin needs to implement the below interface post which it can be added to the processing pipeline for the local peer's video.   ts section=VideoPluginInterface sectionIndex=1 interface HMSVideoPlugin     /       The name is meant to uniquely specify a plugin instance. This will be used to track number of plugins     added to the track, and same name won't be allowed twice for the same video track.    /   getName(): string;   /       This function will be called before the call to init, it is used to check whether the plugin supports current     OS and device or not. An error will be thrown back to the user if they try to use an unsupported plugin.    /   checkSupport(): HMSPluginSupportResult;   /       @deprecated. Will be deleted in future updates. Use checkSupport instead.    /   isSupported(): boolean;   /       This function will be called in the beginning for initialization which may include tasks like setting up     variables, loading ML models etc. This can be used by a plugin to ensure it's prepared at the time     processVideoFrame is called.    /   init(): Promise<void>;   /       @see HMSVideoPluginType below    /   getPluginType(): HMSVideoPluginType;   /       By default a 2d rendering context is used for the output canvas. if you want to change it to webgl use this.     @see HMSVideoPluginCanvasContextType below    /   getContextType?(): HMSVideoPluginCanvasContextType;   /       This function will be called by the SDK for every video frame which the plugin needs to process.     PluginFrameRate  the rate at which the plugin is expected to process the video frames. This is not necessarily     equal to the capture frame rate. The user can specify this rate, and the sdk might also change it on basis of     device type, or CPU usage.     For an analyzing plugin, the below function will be called at plugin framerate.     For a transforming plugin, the sdk will pass in the input and output at real frame rate with an additional boolean     pass. The expectation is that the plugin should use results of previous runs instead of doing a complex processing     again when skipProcessing is set to true. This helps in maintaining the framerate of the video as well as bringing down     CPU usage in case of complex processing.     @param input input canvas containing the input frame     @param output the output canvas which should contain the output frame     @param skipProcessing use results from previous run if true    /   processVideoFrame(     input: HTMLCanvasElement,     output?: HTMLCanvasElement,     skipProcessing?: boolean   ): Promise<void>  void;   /       The plugin can implement this function to dispose off its resources. It'll be called when the processor instance is     no longer needed and the plugin is removed.    /   stop(): void;   /     Specifies the type of the plugin, a transforming plugin will get an output canvas to give the resulting   transformation. While an analyzing plugin will only be passed the input canvas.  / export enum HMSVideoPluginType     TRANSFORM = 'TRANSFORM',   ANALYZE = 'ANALYZE'   /     Specifies the type of canvas rendering context.  / export enum HMSVideoPluginCanvasContextType     '2D' = '2d',   WEBGL = 'webgl',   'WEBGL2' = 'webgl2'   export interface HMSPluginSupportResult     isSupported: boolean;   errType?: HMSPluginUnsupportedTypes;   errMsg?: string;   export enum HMSPluginUnsupportedTypes     PLATFORM_NOT_SUPPORTED = 'PLATFORM_NOT_SUPPORTED',   DEVICE_NOT_SUPPORTED = 'DEVICE_NOT_SUPPORTED'       Adding and Removing Plugins Once a plugin implementation is ready, it can be added and removed from local peer's video track as below   s id=\"add-remove-plugin\" items=  'Usage', 'CustomPlugin'       id='add-remove-plugin-0'>   js section=AddingAndRemovePlugins sectionIndex=1 tab=Usage import   hmsActions   from './hms'; const myPlugin = new MyCustomPlugin(); const pluginSupport = hmsActions.validateAudioPluginSupport(myPlugin); if (myPlugin.isSupported)     // myPlugin.init(); // optional, recommended if plugin implements it, you can show a loader here in the UI   const isPluginAdded = hmsStore.getState(     selectIsLocalVideoPluginPresent(myPlugin.getName());   )   if ( isPluginAdded)       hmsActions.addPluginToVideoTrack(myPlugin);     else       hmsActions.removePluginFromVideoTrack(myPlugin);       else    const err = pluginSupport.errMsg;   console.error(err);     </Tab>   id='add-remove-plugin-1'>   ts section=AddingAndRemovePlugins sectionIndex=1 tab=CustomPlugin class MyCustomPlugin implements HMSVideoPlugin      </Tab>   Implementation Example  Grayscale Filter Below is a sample implementation of the above interface which converts the local video in grayscale.   js section=ImplementationExample sectionIndex=1 class GrayscalePlugin    getName()     return \"grayscale-plugin\";     checkSupport()     let result =    as HMSPluginSupportResult;   result.isSupported = true;   return result;     async init()     getPluginType()     return HMSVideoPluginType.TRANSFORM;     stop()     /      @param input  HTMLCanvasElement     @param output  HTMLCanvasElement    /  processVideoFrame(input, output)     const width = input.width;   const height = input.height;   output.width = width;   output.height = height;   const inputCtx = input.getContext(\"2d\");   const outputCtx = output.getContext(\"2d\");   const imgData = inputCtx.getImageData(0, 0, width, height);   const pixels = imgData.data;   for (let i = 0; i < pixels.length; i += 4)      const red = pixels i ;    const green = pixels i + 1 ;    const blue = pixels i + 2 ;    // https://en.wikipedia.org/wiki/Grayscale Luma_coding_in_video_systems    const lightness = Math.floor(red   0.299 + green   0.587 + blue   0.114);    pixels i  = pixels i + 1  = pixels i + 2  = lightness;       outputCtx.putImageData(imgData, 0, 0);      // to add the plugin to local video hmsActions.addPluginToVideoTrack(new GrayscalePlugin());   > To see a complete example of above plugin in use with our react quickstart, please check  this repo (https://github.com/triptu/100ms-face-filters) or > the  codesandbox (https://codesandbox.io/s/github/triptu/100ms-face-filters).   Plugin Guidelines   Feel free to implement more methods outside the interface, options passed in plugin's constructor etc. as required for   the users of the plugin to give more interaction points. For example, our Virtual background plugin exposes a method to change   the background as required.   If your plugin involves a CPU intensive task such as initialising a ML model, ensure that it's done as part of the init method.   As a bonus, ensure that the plugin remembers the model initialisation across instances, so in case user leaves and joins the room again   without closing the tab they don't see a delay the second time.   The addPlugin method takes a second parameter to control the framerate. This can be used to reduce the amount of processing   on low-end devices. CPU usage by the plugin would be proportional to the video resolution and frame rate. "
    },
    {
        "title": "Noise Suppression",
        "link": "/javascript/v2/how--to-guides/extend-capabilities/plugins/noise-suppression",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/extend-capabilities/plugins/noise-suppression",
        "keywords": [],
        "content": "  <Note type=\"warning\">   This feature is still in Beta. To know more or report any issues, feel free to reach out to us   over <a href= 'https://discord.com/invite/kGdmszyzq2' >Discord</a>. </Note>   Introduction Background noise always rears its ugly head while live streaming outside, on site, or in any untreated setting. Sounds can ruin anyone's listening or watching experience, whether it's from passing cars, construction noise, or mechanical whirs. Noise suppression plugin helps in eliminating one’s background sound. This is trained for removing babble noise, car noise and street noise. This guide provides an overview of usage of the noise suppression plugin of 100ms and below is the demo of it.   Original audio <audio controls>   <source src=\"/docs/audio/test-music.mp3\" type=\"audio/mpeg\"     <source src=\"/docs/audio/test-music.ogg\" type=\"audio/ogg\"   </audio>   Denoised audio <audio controls>   <source src=\"/docs/audio/testdenoised-music.mp3\" type=\"audio/mpeg\"     <source src=\"/docs/audio/testdenoised-music.ogg\" type=\"audio/ogg\"   </audio> <br   <br     Supported Devices/SampleRate   Noise suppression is currently only supported on web in Chrome, Brave and Edge browsers.   Noise suppression has limited support on Firefox with bluetooth devices.   Pre-requisites   Get the 100ms NoiseSuppression Package     bash section=GetHMSNoiseSuppressionPackage sectionIndex=1 npm install save @100mslive/hms-noise-suppression     Import plugin   jsx section=ImportPlugin sectionIndex=1 import   HMSNoiseSuppressionPlugin   from '@100mslive/hms-noise-suppression';     Instantiate Noise Suppression This accepts durationInMs as a parameter   durationInMs  (optional)It maps to the audio samples bufferSize we need to process, by default we are using 80ms equivalent to bufferSize=4096   jsx section=InstantiateNoiseSuppresion sectionIndex=1 const noiseSuppressionPlugin = new HMSNoiseSuppressionPlugin(durationInMs);     Interfaces   Check if plugin is supported validateAudioPluginSupport can be used to check if the browser/ input device is supported or not. This accepts plugin instance as a parameter It will return True in case of plugin is supported and return error message if it is not.   jsx section=CheckIfPluginIsSupported sectionIndex=1 import   hmsActions   from './hms'; const pluginSupport = hmsActions.validateAudioPluginSupport(noiseSuppressionPlugin); if (pluginSupport.isSupported)     console.log('Plugin is supported');   else     const err = pluginSupport.errMsg;   console.error(err);       Init(Optional) Init is used to load model of background noise suppression for the first time. It takes on an average ..50.. milliseconds.<br  Calling init is handled internally by SDK if not done by user, in this case addPlugin call will take around 50 milliseconds for the first time and then less than 1 milliseconds in the subsequent calls. Check  this section ( start-and-stop-noise-suppression) for addPlugin API usage   jsx section=Init sectionIndex=1 noiseSuppressionPlugin.init();     Add and Remove Background Noise Suppression  s id=\"plugin-code\" items=  'JavaScript', 'React'       id='plugin-code-0'>   js section=AddAndRemoveBackgroundNoiseSuppresion sectionIndex=1 tab=JS import   hmsActions, hmsStore   from './hms'; import   selectIsLocalAudioPluginPresent   from '@100mslive/hms-video-store'; async function toggleNoiseSuppression()     const isNoiseSuppressed = hmsStore.getState(     selectIsLocalAudioPluginPresent(noiseSuppressionPlugin.getName())   );   try       if ( isNoiseSuppressed)         // add background noise suppression       await hmsActions.addPluginToAudioTrack(noiseSuppressionPlugin);       else         // remove background noise suppression       await hmsActions.removePluginFromAudioTrack(noiseSuppressionPlugin);           catch (err)       console.log('noise suppression failure  ', isNoiseSuppressed, err);         </Tab>   id='plugin-code-1'>   jsx section=AddAndRemoveBackgroundNoiseSuppresion sectionIndex=1 tab=React import    useHMSStore,  useHMSActions,  selectIsLocalAudioPluginPresent   from '@100mslive/hms-video-react'; function NoiseSuppression()    const isNoiseSuppressed = useHMSStore(selectIsLocalAudioPluginPresent(noiseSuppressionPlugin.getName());  const hmsActions = useHMSActions();  const toggleNoiseSuppression = async() =>    try     if ( isNoiseSuppressed)      // add background noise suppression    await hmsActions.addPluginToAudioTrack(noiseSuppressionPlugin);     else      // remove background noise suppression    await hmsActions.removePluginFromAudioTrack(noiseSuppressionPlugin);        catch (err)     console.log('noise suppression failure  ', isNoiseSuppressed, err);     return <Button onClick= toggleNoiseSuppression   ;     </Tab>   Enable/disable background noise suppression The function setEnabled can be used to enable/disable background noise suppression without removing the plugin from audio track. It accepts boolean as a parameter.   jsx section=EnableDisableBackgroundNoiseSuppresion sectionIndex=1 try     noiseSuppressionPlugin.setEnabled(true); // true/false   catch (err)     console.log('Failed to suppress noise', err);     "
    },
    {
        "title": "Virtual Background",
        "link": "/javascript/v2/how--to-guides/extend-capabilities/plugins/virtual-background",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/extend-capabilities/plugins/virtual-background",
        "keywords": [],
        "content": "  <Note type=\"warning\">   This feature is still in Beta. To know more or report any issues, feel free to reach out to us   over <a href= 'https://discord.com/invite/kGdmszyzq2' >Discord</a>. </Note>   Introduction Virtual background plugin helps in customising one’s background. The customising options are blurring the background or replacing it with a static image. This guide provides an overview of usage of the virtual background plugin of 100ms.   VirtualBackground (/docs/v2/virtual.gif)   Supported Devices Virtual background is currently only supported on web in Chrome, Firefox, Brave and Edge browsers.<br  We have not extended support for Mobile browsers in this release.   Basic Concepts   blur background  This is an action where the background of the video is blurred. No external image is used/required.   image background  This is where the plugin will replace the video background with the image provided by user.   plugin load time  The time taken by plugin to load model. This is in the range of 3-5 seconds for first time. Subsequent loads take less than 100 milliseconds.   Pre-requisites   Get the 100ms VirtualBackground Package     bash section=GetHMSVirtualBackgroundPackage sectionIndex=1 npm install save @100mslive/hms-virtual-background@latest     Usage   jsx section=ImportPlugin sectionIndex=1 import   HMSVirtualBackgroundPlugin   from '@100mslive/hms-virtual-background';     Instantiate Virtual Background This accepts background as a parameter, It can be of 3 types:   blur  String  This will set the background to blur   image  HTMLImageElement  This will replace the background to the image provide   none  String  This will remove the background effect from the video   jsx section=InstantiateVirtualBackground sectionIndex=1 // background :  'blur'  image  'none'  const virtualBackground = new HMSVirtualBackgroundPlugin(background :  'blur'  image  'none' );     Interfaces   Check if plugin is supported validateVideoPluginSupport can be used to check if the browser/input device is supported or not. It will return True in case of plugin is supported and return error message if it is not.   jsx section=CheckIfPluginIsSupported sectionIndex=1 import   hmsActions   from './hms'; const pluginSupport = hmsActions.validateVideoPluginSupport(virtualBackground); if (pluginSupport.isSupported)     console.log('Platform is supported');   else     const err = pluginSupport.errMsg;   console.error(err);       Init(Optional) Init is used to load model of virtual background for the first time. It takes on an average 3-5 seconds.<br  Calling init is handled internally by SDK if not done by user, in this case addPlugin call will take 3-5 seconds for the first time and then less than 100 milliseconds in the subsequent calls. Check  this section ( start-and-stop-virtual-background) for addPlugin API usage   jsx section=Init sectionIndex=1 virtualBackground.init();   Init can also be used by user to show a loader icon for background selector component during the plugin loading stage   jsx section=Init sectionIndex=2 virtualBackground.init().then(() => console.log('background can be changed now'));     Start and Stop Virtual Background  s id=\"plugin-code\" items=  'JavaScript', 'React'     Check  this section ( tuning-plugin-frame-rate-optional) to use custom pluginFrameRate   id='plugin-code-0'>   js section=StartAndStopVirtualBackground sectionIndex=1 tab=JS import   hmsActions, hmsStore   from './hms'; import   selectIsLocalVideoPluginPresent   from '@100mslive/hms-video-store'; async function toggleVB()     const isVirtualBackgroundEnabled = hmsStore.getState(     selectIsLocalVideoPluginPresent(virtualBackground.getName())   );   try       if ( isVirtualBackgroundEnabled)         // Recommended value       const pluginFrameRate = 15;       // add virtual background       await hmsActions.addPluginToVideoTrack(virtualBackground, pluginFrameRate);       else         // remove virtual background       await hmsActions.removePluginFromVideoTrack(virtualBackground);           catch (err)       console.log('virtual background failure  ', isVirtualBackgroundEnabled, err);         </Tab>   id='plugin-code-1'>   jsx section=StartAndStopVirtualBackground sectionIndex=1 tab=React import   useHMSStore, useHMSActions, selectIsLocalVideoPluginPresent   from '@100mslive/react-sdk'; function VirtualBackground()     const isVirtualBackgroundEnabled = useHMSStore(     selectIsLocalVideoPluginPresent(virtualBackground.getName())   );   const hmsActions = useHMSActions();   const toggleVB = async () =>       try         if ( isVirtualBackgroundEnabled)           // Recommended value         const pluginFrameRate = 15;         await hmsActions.addPluginToVideoTrack(virtualBackground, pluginFrameRate);         else           await hmsActions.removePluginFromVideoTrack(virtualBackground);               catch (err)         console.log('virtual background failure  ', isVirtualBackgroundEnabled, err);          ;   return <Button onClick= toggleVB   ;     </Tab> > Note that hmsActions.attachVideo should be called on toggling virtual background. Refer  Render Video (../features/render-video when-to-attach-detach-videos) section to see how to handle it.   Change Background The function setBackground can be used to update the background again later. It accepts string or HTMLImageElement as a parameter Image will get fit to the video by maintaining the aspect ratio. If the aspect ratio of the background image is not the same as the video, the image will be cropped to fit in the background.   jsx section=ChangeBackground sectionIndex=1 // background parameter is explained in Instantiate Virtual Background section try   virtualBackground.setBackground(background :  'blur'  image  'none' );  catch(err)   console.log(\"Failed to update background\", err);       Tuning pluginFrameRate(Optional)  pluginFrameRate  number  pluginFrameRate helps in controlling the performance and experience of Virtual background plugin. pluginFrameRate translates to the number of frames for which background is detected. Higher value will use more resources (cpu/memory/battery), while making the virtual background experience smooth. Lower value will be generous on resources, while lowering the virtual background smoothness. Recommended value is 15. Values higher than this will not significantly improve virtual background smoothness but will be heavy on resources. For lower end devices value can be in the range of 7-10.   Recommendations For Better User Experience For enhancing the performance of virtual background plugin we recommend enabling SIMD on the browser. With SIMD enabled, browsers can process multiple data points with each instruction, resulting in a performance boost of more than 2x.   Chrome  Open chrome://flags/ in browser, search for simd, enable WebAssembly SIMD support and restart.   Firefox  Open about:config in browser, search for simd, set javascript.options.wasm_simd=true and restart.   Edge  Open about:flags in browser, search for simd, enable WebAssembly SIMD support and restart.   Brave  Open about:flags in browser, search for simd, enable WebAssembly SIMD support and restart.   New VirtualBackground Plugin: The new virtual background plugin comes with improved performance(reduced cpu usage) and updated interfaces with proper types. It is part of the same package   Supported Devices Virtual background is currently only supported on web in Chrome, Firefox, Brave and Edge browsers.<br    Usage:  s id=\"vb-plugin-code\" items=  'JavaScript', 'React'     Check  this section ( tuning-plugin-frame-rate-optional) to use custom pluginFrameRate   id='vb-plugin-code-0'>   js import   hmsActions, hmsStore   from './hms'; import   selectIsLocalVideoPluginPresent   from '@100mslive/hms-video-store'; import   HMSVBPlugin, HMSVirtualBackgroundTypes   from '@100mslive/hms-virtual-background'; const virtualBackground = new HMSVBPlugin(   HMSVirtualBackgroundTypes.NONE,   HMSVirtualBackgroundTypes.NONE ); const checkSupport = () =>     const   isPluginSupported, errType, errMsg   = virtualBackground.checkSupport();   // errType and errMsg will be available if the plugin does not support the browser or os etc.   return isPluginSupported;   const changeBackground = async () =>     // for blurring the background   await virtualBackground.current.setBackground(HMSVirtualBackgroundTypes.BLUR, HMSVirtualBackgroundTypes.BLUR);   // for setting an image   const image = document.createElement('img');   image.src = <image url>;   await virtualBackground.current.setBackground(image, HMSVirtualBackgroundTypes.IMAGE)   async function toggleVB()     const isVirtualBackgroundEnabled = hmsStore.getState(     selectIsLocalVideoPluginPresent(virtualBackground.getName())   );   try       if ( isVirtualBackgroundEnabled)         // Recommended value       const pluginFrameRate = 15;       // add virtual background       await hmsActions.addPluginToVideoTrack(virtualBackground, pluginFrameRate);       else         // remove virtual background       await hmsActions.removePluginFromVideoTrack(virtualBackground);           catch (error)       console.log('failed to set virtual background -', error);         </Tab>   id='vb-plugin-code-1'> This is a very basic implementation. Refer the sample app  implementation (https://github.com/100mslive/100ms-web/blob/main/src/plugins/VirtualBackground/VirtualBackground.jsx) for more detail.   jsx import   useRef   from \"react\"; import   useHMSStore, useHMSActions, selectIsLocalVideoPluginPresent   from '@100mslive/react-sdk'; import   HMSVBPlugin, HMSVirtualBackgroundTypes   from '@100mslive/hms-virtual-background';  function VirtualBackground()     const virtualBackground = useRef(new HMSVBPlugin(     HMSVirtualBackgroundTypes.NONE,     HMSVirtualBackgroundTypes.NONE   ));   const isVirtualBackgroundEnabled = useHMSStore(     selectIsLocalVideoPluginPresent(virtualBackground.current.getName())   );   const hmsActions = useHMSActions();   const changeBackground = async () =>       // for blurring the background     await virtualBackground.current.setBackground(HMSVirtualBackgroundTypes.BLUR, HMSVirtualBackgroundTypes.BLUR);     // for setting an image     const image = document.createElement('img');     image.src = <image url>;     await virtualBackground.current.setBackground(image, HMSVirtualBackgroundTypes.IMAGE)       const checkSupport = () =>       const   isPluginSupported, errType, errMsg   = virtualBackground.current.checkSupport();     // errType and errMsg will be available if the plugin does not support the browser or os etc.     return isPluginSupported;       const toggleVB = useCallback(async () =>       try       if ( isVirtualBackgroundEnabled)         // Recommended value       const pluginFrameRate = 15;       // call changeBackground if it is not initialised with the right background can be done after adding the plugin as well       await hmsActions.addPluginToVideoTrack(virtualBackground.current, pluginFrameRate);       else         await hmsActions.removePluginFromVideoTrack(virtualBackground.current);             catch (error)       console.log(\"virtual background failure  \", error);          ,  hmsActions );   return checkSupport() ? <Button onClick= toggleVB    : null;     </Tab> "
    },
    {
        "title": "SDK for app state",
        "link": "/javascript/v2/how--to-guides/extend-capabilities/store-appdata",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/extend-capabilities/store-appdata",
        "keywords": [],
        "content": "    Introduction If your app doesn't already use a global store, you can use a part of hmsStore to store simple states that needs sharing between your components. You might want to do this for improved performance and cleaner code on the app layer(avoiding prop drilling). Note that this is not a replacement for external global store. It's limited by design and only suitable for straightforward use cases. > Note that this is not suitable for  >  storing large amount of data >  data changing at a high frequency >  complex interlinked state   Usage   Setting up initial appData   initAppData  (/api-reference/javascript/v2/interfaces/HMSActions init-app-data) takes an object of key value pairs to set the intial app data in store.   ts const data: Record<string,any> =  uiMode: \"activeSpeaker\", toasts:  message: false, errors: true  ; hmsActions.initAppData(data);     Updating State   setAppData  (/api-reference/javascript/v2/interfaces/HMSActions set-app-data) takes in key and value and update the state in store. If key doesn't already exist, it is added.   ts hmsActions.setAppData(\"uiMode\", \"gallery\"); // new value =>  uiMode: \"gallery\", toasts:  message: false, errors: true     If your value is also an object, there is an additional boolean option which can be used to merge(instead of replace) the new value with existing value.    With Merge as true   ts hmsActions.setAppData(\"toasts\",  message: true , true); // new value =>  uiMode: \"gallery\", toasts:  message: true, errors: true        Without merge as true   ts hmsActions.setAppData(\"toasts\",  message: true ); // new value =>  uiMode: \"gallery\", toasts:  message: true       Reading App Data The selector   selectAppData  (/api-reference/javascript/v2/home/content select-app-data) can be used for this. You can pass it the key you want to get, and it'll give the value. The component using it will only be rendered if the value actually changes.   ts console.log(hmsStore.getState(\"uiMode\")); // \"gallery\" console.log(hmsStore.getState(\"toasts\")); //  message: false, errors: true      Example  s id=\"appData\" items=  'JavaScript', 'React'     ' '    id='appData-0'>   js:app.js function initAppData()     const initialAppData =      uiMode: \"activeSpeaker\",    toasts:        message: false,      errors: true,         ;   hmsActions.initAppData(initialAppData);   console.log(\"inital toasts to show\", hmsStore.getState(selectAppData(\"toasts\"))); //   message: false, errors: true     hmsActions.setAppData(\"toasts\",  message: true , true); // merge this value in current value   console.log(\"merged new toasts\", hmsStore.getState(selectAppData(\"toasts\"))); //   message: true, errors: true     hmsActions.setAppData(\"toasts\",  message: true , true); // replace current value   console.log(\"replaced new toasts, hmsStore.getState(selectAppData(\"toasts\"))); //   message: true       console.log(\"uiMode\", hmsStore.getState(selectAppData(\"uiMode\")); // activeSpeaker   hmsActions.setAppData(\"uiMode\", \"gallery\");   console.log(\"uiMode\", hmsStore.getState(selectAppData(\"uiMode\")); // gallery      </Tab>   id='appData-1'>   jsx:app.jsx export function myComponent()    const toasts = useHMSStore(selectAppData(\"toasts\"));  const uiMode = useHMSStore(selectAppData(\"uiMode\"));    const hmsActions = useHMSActions();  useEffect(() =>     const initialAppData =      uiMode: \"activeSpeaker\",    toasts:        message: false,      errors: true,         ;   hmsActions.initAppData(initialAppData);   ,  hmsActions ); // runs only once  // toasts becomes   message: true, errors: true   after first call to toggle  toggleMessageToast = useCallback(()=>     hmsActions.setAppData(\"toasts\",   message:  toasts.message  , true);   ,  hmsActions );  // toasts becomes   message: false, errors: true   after call to this method  resetToasts = useCallback((newValue)=>     hmsActions.setAppData(\"toasts\",       message: false,      errors: true,     );   ,  hmsActions );  return <>  /  components  /  < ;     </Tab> "
    },
    {
        "title": "Supported Devices",
        "link": "/javascript/v2/how--to-guides/going-live/supported-devices",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/going-live/supported-devices",
        "keywords": [],
        "content": "   OS      Browsers:     Features     Minimum Browser Version  Minimum OS Version                                Windows   Google Chrome   Screen Sharing  78            7                           Audio      78            7                           Video      78            7                  Mozilla Firefox  Screen Sharing  NA            7                           Audio      78            7                           Video      78            7                  Edge       Screen Sharing  83            7                           Audio      83            7                           Video      83            7            Mac OS    Google Chrome   Screen Sharing  78            OS X Mojave                      Audio      78            OS X Mojave                      Video      78            OS X Mojave             Mozilla Firefox  Screen Sharing  NA            OS X Mojave                      Audio      78            OS X Mojave                      Video      78            OS X Mojave             Safari      Screen Sharing  13.1           OS X Mojave                      Audio      13.1           OS X Mojave                      Video      13.1           OS X Mojave             Edge       Screen Sharing  83            OS X Mojave                      Audio      83            OS X Mojave                      Video      83            OS X Mojave       Android OS  Google Chrome   Screen Sharing  78            Android 5.0, API 21                  Audio      78            Android 5.0, API 21                  Video      78            Android 5.0, API 21         Mozilla Firefox  Screen Sharing  78            Android 5.0, API 21                  Audio      78            Android 5.0, API 21                  Video      78            Android 5.0, API 21         Edge       Screen Sharing  83            Android 5.0, API 21                  Audio      83            Android 5.0, API 21                  Video      83            Android 5.0, API 21         SDK        Screen Sharing  API 21          Android 5.0, API 21                  Audio      API 21          Android 5.0, API 21                  Video      API 21          Android 5.0, API 21   iOS Mobile  Google Chrome   Screen Sharing  78            iOS 14                         Audio      78            iOS 14                         Video      78            iOS 14                Mozilla Firefox  Screen Sharing  34            iOS 14                         Audio      34            iOS 14                         Video      34            iOS 14                Safari      Screen Sharing  13.1           iOS 14                         Audio      13.1           iOS 14                         Video      13.1           iOS 14                Edge       Screen Sharing  46            iOS 14                         Audio      46            iOS 14                         Video      46            iOS 14                SDK        Screen Sharing  10            iOS 10                         Audio      10            iOS 10                         Video      10            iOS 10        "
    },
    {
        "title": "Integrating The SDK",
        "link": "/javascript/v2/how--to-guides/install-the-sdk/integration",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/install-the-sdk/integration",
        "keywords": [],
        "content": "    Installing our libraries You can use either npm or yarn to install the dependencies. Please install the respective libraries depending on whether you're planning to use with react or other framework/plain JavaScript.  s id=\"packages-install\" items=  'JavaScript', 'React', 'JavaScript(CDN)'       id=\"packages-install-0\">   bash  npm npm install save @100mslive/hms-video-store  yarn yarn add @100mslive/hms-video-store   </Tab>   id=\"packages-install-1\">   bash  npm npm install save @100mslive/react-sdk  yarn yarn add @100mslive/react-sdk   </Tab>   id=\"packages-install-2\">   https://cdn.skypack.dev/@100mslive/hms-video-store   </Tab>   Setting up the sdk Our core SDK can be used with plain JavaScript or any UI framework. We also provide a convenient hooks based interface in case you're planning to integrate our SDK in a React app. Please follow the appropriate section below. In case you want to use the CDN link, you can change the imports as present in the example  here (https://codepen.io/triptu/pen/dymxmoR?editors=0010). It's recommended to use a fixed version with CDN to avoid unintentional retrieval of a new release with breaking changes. Although CDN is great for quickstart and prototyping, we highly recommend setting up a bundler for production to improve page load speed and avoid a dependency on cdn provider.    JavaScript There are three entities which we need to be familiar of    hmsStore  this contains the complete state of the room at any given time. This includes for example, participant details,   messages and track states.   hmsActions  this is used to perform any action such as joining, muting and sending a message.   hmsNotifications  this can be used to get notified on peer join/leave and new messages in order to show toast notifications to   the user. Let's create a hms.js file where we initialize and export the above entities, so they can be used as required. We'll assume that this setup is in place in other sections of the documentation.  s id=\"store\" items=  'Setup and Export', 'Import and use'     ' '    id='store-0'>   js:hms.js import   HMSReactiveStore   from '@100mslive/hms-video-store'; const hms = new HMSReactiveStore(); // by default subscriber is notified about store changes post subscription only, this can be // changed to call it right after subscribing too using this function. hms.triggerOnSubscribe(); // optional, recommended const hmsActions = hms.getActions(); const hmsStore = hms.getStore(); const hmsNotifications = hms.getNotifications(); export   hmsActions, hmsStore, hmsNotifications  ;   </Tab>   id='store-1'>   js:some_other_file.js import   hmsActions, hmsStore, hmsNotifications   from './hms';   </Tab>   React Hooks If you're planning to use our SDK with React, we provide three friendly hooks as a wrapper over our plain JavaScript interface. You can wrap your UI in HMSRoomProvider and these hooks will become available to all the UI components. We'll learn more about these hooks, and their use as we navigate through further sections.  s id=\"react\" items=  'Wrap In Provider', 'Use hooks'     ' '    id='react-0'>   jsx:app.jsx import   HMSRoomProvider   from '@100mslive/react-sdk'; export function App()    return (   <HMSRoomProvider>    <MyApp     </HMSRoomProvider>  );      </Tab>   id='react-1'>   jsx:component.jsx import    useHMSStore,  useHMSActions,  useHMSNotifications   from '@100mslive/react-sdk'; export function MyComponent()    const hmsStore = useHMSStore();  const hmsActions = useHMSActions();  const hmsNotifications = useHMSNotifications();  return < ;     </Tab>   Debugging Do checkout our debugging page  here (../debugging/debugging) for an easy debugging experience while integrating our sdk. "
    },
    {
        "title": "Notifications",
        "link": "/javascript/v2/how--to-guides/listen-to-room-events/notifications",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/listen-to-room-events/notifications",
        "keywords": [],
        "content": "    Join Notification (/guides/notification-join.png)   Poor Connection Notification (/guides/notification-internet.png) Notifications can be used to display toast to user about activities happening in the room, such as peer join/leave or new messages. It can also be used to know about any mid call sdk level error. In order to subscribe for notifications we can use the hmsNotifications object created in the integration step.  s id=\"notification-initialize\" items=  'JavaScript', 'React'       id=\"notification-initialize-0\">   jsx import   hmsNotifications   from './hms'; hmsNotifications.onNotification((notification) =>     // This function will be called when a notification is received   console.log('notification type', notification.type);   // The data in notification depends on the notification type   console.log('data', notification.data);  );   </Tab>   id=\"notification-initialize-1\">   jsx import   useHMSNotifications   from '@100mslive/react-sdk'; function Notification()     const notification = useHMSNotifications();   useEffect(() =>       if ( notification)         return;           // notification is a reactive object     // this function will run everytime there is a new notification     console.log('notification type', notification.type);     // The data in notification depends on the notification type     console.log('data', notification.data);    ,  notification );   return <div>Notification:  notification?.type </div>;     </Tab>   Notification types The notification type is a string and can only take values from the  list here (/api-reference/javascript/v2/enums/HMSNotificationTypes). You can use it to check the type of the notification. The data in notification depends on the notification type. You can use the switch construct to the handle the notification type.  s id=\"notification-types\" items=  'JavaScript', 'React'       id=\"notification-types-0\">   jsx import   hmsNotifications   from './hms'; const unsubscribe = hmsNotifications.onNotification((notification) =>     console.log('notification type', notification.type);   console.log('data', notification.data);   // you can use the following to show appropriate toast notifications for eg.   switch (notification.type)       case HMSNotificationTypes.PEER_LIST:       console.log( $ notification.data  are the peers in the room ); // received right after join       break;     case HMSNotificationTypes.PEER_JOINED:       console.log( $ notification.data.name  joined );       break;     case HMSNotificationTypes.PEER_LEFT:       console.log( $ notification.data.name  left );       break;     case HMSNotificationTypes.NEW_MESSAGE:       console.log(         $ notification.data.message  received from $ notification.data.senderName        );       break;     case HMSNotificationTypes.ERROR:       console.log(' Error ', notification.data);       console.log(' Error Code ', notfication.data.code);       break;     case HMSNotificationTypes.RECONNECTING:       console.log(' Reconnecting ', notification.data);       break;     case HMSNotificationTypes.RECONNECTED:       console.log(' Reconnected ');       break;     case HMSNotificationTypes.NAME_UPDATED:     case HMSNotificationTypes.METADATA_UPDATED:     case HMSNotificationTypes.ROLE_UPDATED:       console.log( peer updated($ notification.type ), new peer= , notification.data);       break;     case HMSNotificationTypes.TRACK_DEGRADED:       console.log( track  $ notification.data  degraded due to poor network );       break;     case HMSNotificationTypes.TRACK_RESTORED:       console.log( track  $ notification.data  recovered );       break;     case HMSNotificationTypes.ROOM_ENDED:       console.log( room ended, reason  $ notification.data.reason  );       break;     case HMSNotificationTypes.REMOVED_FROM_ROOM:       console.log( removed from room, reason  $ notification.data.reason  );       break;     case HMSNotificationTypes.DEVICE_CHANGE_UPDATE:       console.log( device changed  $ notification.data  );       break;     default:       break;      );   </Tab>   id=\"notification-types-1\">   jsx import   useHMSNotifications   from '@100mslive/react-sdk'; function Notification()     const notification = useHMSNotifications();   React.useEffect(() =>       if ( notification)         return;           console.log('notification type', notification.type);     console.log('data', notification.data);     // you can use the following to show appropriate toast notifications for eg.     switch (notification.type)         case HMSNotificationTypes.PEER_JOINED:         console.log( $ notification.data.name  joined );         break;       case HMSNotificationTypes.PEER_LEFT:         console.log( $ notification.data.name  left );         break;       case HMSNotificationTypes.NEW_MESSAGE:         console.log(           $ notification.data.message  received from $ notification.data.senderName          );         break;       case HMSNotificationTypes.ERROR:         console.log(' Error ', notification.data);         console.log(' Error Code ', notfication.data.code);         break;       case HMSNotificationTypes.RECONNECTING:         console.log(' Reconnecting ', notification.data);         break;       case HMSNotificationTypes.RECONNECTED:         console.log(' Reconnected ');         break;       case HMSNotificationTypes.NAME_UPDATED:       case HMSNotificationTypes.METADATA_UPDATED:       case HMSNotificationTypes.ROLE_UPDATED:         console.log( peer updated($ notification.type ), new peer= , notification.data);         break;       case HMSNotificationTypes.TRACK_DEGRADED:         console.log( track  $ notification.data  degraded due to poor network );         break;       case HMSNotificationTypes.TRACK_RESTORED:         console.log( track  $ notification.data  recovered );         break;       case HMSNotificationTypes.ROOM_ENDED:         console.log( room ended, reason  $ notification.data.reason  );         break;       case HMSNotificationTypes.REMOVED_FROM_ROOM:         console.log( removed from room, reason  $ notification.data.reason  );         break;       case HMSNotificationTypes.DEVICE_CHANGE_UPDATE:         console.log( device changed  $ notification.data  );         break;       default:         break;          ,  notification );   return <div>Notification:  notification?.type </div>;     </Tab>   Listening to specific notifications You can pass a single notification or array of notifications from one of HMSNotificationTypes .  s id=\"notification-specific-types\" items=  'JavaScript', 'React'       id=\"notification-specific-types-0\">   jsx import   hmsNotifications   from './hms'; const unsubscribe = hmsNotifications.onNotification((notification) =>     console.log('notification type', notification.type);   console.log('data', notification.data);  , HMSNotificationTypes.PEER_JOINED); const unsubscribePeerNotifications = hmsNotifications.onNotification(   (notification) =>       if ( notification)         return;           // this will be called when one of HMSNotificationTypes.PEER_JOINED, HMSNotificationTypes.PEER_LEFT is arrived     console.log('notification type', notification.type);     console.log('data', notification.data);    ,    HMSNotificationTypes.PEER_JOINED, HMSNotificationTypes.PEER_LEFT  );   </Tab>   id=\"notification-specific-types-1\">   jsx import   useHMSNotifications   from '@100mslive/react-sdk'; function Notification()     const notification = useHMSNotifications(HMSNotificationTypes.PEER_JOINED);   const peerNotification = useHMSNotifications(      HMSNotificationTypes.PEER_JOINED,     HMSNotificationTypes.PEER_LEFT    );   React.useEffect(() =>       if ( notification)         return;           console.log('notification type', notification.type);     console.log('data', notification.data);    ,  notification );   React.useEffect(() =>       if ( peerNotification)         return;           console.log('notification type', notification.type);     console.log('data', notification.data);    ,  peerNotification );   return <div>Notification:  notification?.type </div>;     </Tab>   Unsubscribing from notifications You can unsubscribe to the notifications by calling the method returned by the hmsNotifications.onNotification function.  s id=\"notification-unsubscribe\" items=  'JavaScript', 'React'       id=\"notification-unsubscribe-0\">   jsx const unsubscribe = hmsNotifications.onNotification((notification) =>     //...  ); unsubscribe();   </Tab>   id=\"notification-unsubscribe-1\">   jsx // No code is required to unsubscribe from notifications // Unmount the component that calls the useHMSNotifications hook   </Tab>   Ideas You can create notifications with buttons in them, to make it easier for the end user to respond to those notifications. For example    Screenshare Notification (/guides/notification-screen.png) "
    },
    {
        "title": "Connection Quality",
        "link": "/javascript/v2/how--to-guides/measure-network-quality-and-performance/connection-quality",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/measure-network-quality-and-performance/connection-quality",
        "keywords": [],
        "content": "    Network Icons (/docs/v2/network-icons.png) Video/Audio conferencing is by nature a data intensive operation. Our SDK attempts to stabilize connections especially if subscribe degradation is turned on in the template but it's possible for really bad connections that users will still have problems. It can be helpful to measure a user's connection speed before joining a room in order to set expectations or decide to have them join with video off etc. Letting the participants know of each other's connection status is also a great value addition. Once you have joined the room, you can get a network quality score for all the peers including yourself. You can also get current user's connection quality score when in preview. The score ranges from -1 to 5, 5 being the hightest(Good Network) and 0 being the lowest(No Network). -1 indicates an undefined state that is either the score hasn't yet been determined or it couldn't be determined.   Getting peer connection quality Below is the interface for connection quality.   ts interface HMSConnectionQuality     peerID: string;   downlinkQuality: number;      s id=\"connection-quality\" items=  'JavaScript', 'React'       id='connection-quality-0'>   js hmsStore.subscribe((connectionQuality) =>     if (connectionQuality)       const quality = connectionQuality.downlinkQuality;     // use the score to show some UI       , selectConnectionQualityByPeerID(peerId));   </Tab>   id='connection-quality-1'>   jsx function ConnectionQuality(peerId)     const downlinkQuality = useHMSStore(selectConnectionQualityByPeerID(peerId))?.downlinkQuality;   // use the score to show some UI    return <span>downlinkQuality</span>;     </Tab>   Connection Quality Score in Preview To receive the score in  preview (../features/preview) as well, you can pass in the captureNetworkQualityInPreview flag as true in preview config while calling preview. The interface to get the quality score stays same as above which is for post joining the room. > ⚠️ The downlink speed is measured by having the user download a file (1mb as of this writing) after the websocket connection is established during a preview. > The download will be continued for at most a fixed number of seconds (eg: 10 seconds) and the speed during that interval is calculated. The entire file may not be downloaded if it exceeds the timeout.   js actions.preview( ..., captureNetworkQualityInPreview: true )     Score Interpretation The networkQuality score will be a number between -1 and 5.  -1 -> Undefined  yet to be determined or not enough data to determine  0 -> Disconnected or error in measuring score(in preview)  1 -> Very Bad Connection  2 -> Bad Connection  3 -> Moderate Connection  4 -> Good Connection  5 -> Excellent Connection  > 0 score in preview could also mean a failure in measuring the network due to firewall block if the internet otherwise looks good.   Showing in the UI You can show this as a network icon on every peer tile or show only a list of peers not having good connection. Feel free to checkout how the code from our dashboard app's implementation  here (https://github.com/100mslive/100ms-web/tree/main/src/components/Connection) where we show a network bar on each peer's tile as well in the participant list with a tooltip describing the connection state.  "
    },
    {
        "title": "HLS Stats",
        "link": "/javascript/v2/how--to-guides/measure-network-quality-and-performance/hls-stats",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/measure-network-quality-and-performance/hls-stats",
        "keywords": [],
        "content": "  <Note type=\"warning\">   This feature is still in Beta. To know more or report any issues, feel free to reach out to us   over <a href= 'https://discord.com/invite/kGdmszyzq2' >Discord</a>. </Note>  @100ms-live/hls-stats is a simple library that provides easy to use APIs for acquiring stats for your HLS Stream.   Initialization Initializing is very simple. Just initialize Hls.js and pass that reference as well as a video element to HLSStats and you are already done.  s id=\"stats-init\" items=  'Javascript', 'React'       id=\"stats-init-0\">   js import Hls from 'hls.js'; /     Initialize Hls.js and attach the video element.  / const hlsUrl = 'http://my-domain/stream.m3u8'; const hlsInstance = new Hls(); hlsInstance.loadSource(hlsUrl); hlsInstance.attachMedia(videoEl); /     initialize HlsStats  / const hlsStats = new HlsStats(hlsInstance, videoEl);   </Tab>   id=\"stats-init-1\">   jsx import Hls from \"hls.js\"; export function MyComponent()     const hls = new Hls(this.getHLSConfig());   hls.loadSource(hlsUrl);   hls.attachMedia(videoRef.current);   hlsStats = new HlsStats(hls, videoEl);   return (    // something   );     </Tab>   Subscribing to Stats  hlsStats have a subscribe function which takes two parameter. a callbackFn and an interval in ms. The interval tells how frequent you want hls-stats to report back to you. Default is 2000ms.  s id=\"stats-subscribe\" items=  'Javascript', 'React'       id=\"stats-subscribe-0\">   js const unsubscribe = hlsStats.subscribe((state) =>     // ...  );   </Tab>   id=\"stats-subscribe-1\">   jsx import Hls from \"hls.js\"; export function MyComponent()    ...  ...   useEffect(() =>     const unsubscribe = hlsStats.subscribe(state =>       setHlsStatsState(state);    );   return () =>      unsubscribe?.();    ;   ,   );   ...  ...     </Tab> the subscribe() also returns a reference to unsubscribe() function which could later be used to unsubscribe from your subscription   Exposed Stats hls-stats currently exposes the following stats  Name                Description                        Unit       Usage                                                                                                    bandwidthEstimate         The current bandwidth, as seen by the player       bits per second  Use this to show the current network speed of the user                      bitrate              server indicated bitrate of current layer of HLS stream  bits per second  Use to know the bitrate required for current layer                        bufferedDuration          buffered duration from the current position        ms        This can be used to show how much data is buffered from the current location (forward buffer)   distanceFromLiveEdge        The distance from the live edge              ms        Used to know currently buffered duration ahead                          droppedFrames           The number of dropped frames till now                    Used to calculate the total number of dropped frames                         videoSize.width videoSize.height  The width and height of the video             px        Used to know the resolution being played                             watchDuration           Total duration watched                  ms        used to know the overall watch duration (not the stream length)                 "
    },
    {
        "title": "Stats For Nerds",
        "link": "/javascript/v2/how--to-guides/measure-network-quality-and-performance/stats",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/measure-network-quality-and-performance/stats",
        "keywords": [],
        "content": "  <Note type=\"warning\">   This feature is still in Beta. To know more or report any issues, feel free to reach out to us   over <a href= 'https://discord.com/invite/kGdmszyzq2' >Discord</a>. </Note> Sometimes you need a way to capture certain metrics related to a call. This may be helpful if you want to tailor the experience to your users or debug issues. Typical metrics of interest are audio/video bitrate, round trip time, total consumed bandwidth and packet loss. <APILink type=\"class\" name=\"HMSReactiveStore get-stats\"> getStats </APILink> has got your back. With <APILink type=\"class\" name=\"HMSReactiveStore get-stats\"> getStats </APILink> you could get statistics for the whole session and individual tracks of all the peers in the room.   Stats On Tile (/docs/v2/stats-tile.png)   Initialization  s id=\"stats-init\" items=  'Javascript', 'React'       id=\"stats-init-0\">   jsx import   HMSReactiveStore   from '@100mslive/hms-video-store'; const hms = new HMSReactiveStore(); export const hmsStats = hms.getStats();   </Tab>   id=\"stats-init-1\">   jsx import   HMSRoomProvider   from '@100mslive/react-sdk'; export function App()     return (     <HMSRoomProvider isHMSStatsOn= true >       <MyApp       </HMSRoomProvider>   );     </Tab> This initializes the SDK to collect stats from WebRTC periodically and store it in a reactive Zustand store called <APILink> HMSStatsStore </APILink>. This is similar to <APILink> HMSStore </APILink> in terms of retrieving data from the store. If you have  Redux Devtools (https://chrome.google.com/webstore/detail/redux-devtools/lmhkpmbekcpmknklioeibfkpmmfibljd?hl=en) installed(which is highly recommended), you could open the HMSStatsStore to get a quick sense of the quality of the call by looking at the bitrate, packets lost, jitter, etc...   Stats Redux Store (/docs/v2/stats-redux-store.png)   Retrieving data from HMSStatsStore Retrieving data is similar to how you do it with HMSStore. You have getState and subscribe methods, if you're using our JS SDK and useHMSStatsStore , if you're using our react SDK and a bunch of useful selectors under <APILink type=\"selector\"> selectHMSStats </APILink>.   Global Level Stats for Local Peer Use the selectHMSStats.localPeerStats selector to get the combined stats for the room for local peer.  s id=\"local-peer-stats\" items=  'Javascript', 'React'       id=\"local-peer-stats-0\">   jsx import   selectHMSStats   from '@100mslive/hms-video-store'; const renderLocalPeerStats = (localPeerStats) =>     if ( localPeerStats) return;   console.log('Publish Bitrate', localPeerStats.publish?.bitrate);   console.log('Subscribe Bitrate', localPeerStats.subscribe?.bitrate);   console.log('Total Bytes Sent', localPeerStats.publish?.bytesSent);   console.log('Total Bytes Received', localPeerStats.subscribe?.bytesReceived);   console.log('Total Packets Lost', localPeerStats.subscribe?.packetsLost);   console.log('Total Jitter', localPeerStats.subscribe?.jitter);  ; hmsStats.subscribe(renderLocalPeerStats, selectHMSStats.localPeerStats);   </Tab>   id=\"local-peer-stats-1\">   jsx import   selectHMSStats, useHMSStatsStore   from '@100mslive/react-sdk'; const renderLocalPeerStats = (localPeerStats) =>     if ( localPeerStats) return;   console.log('Publish Bitrate', localPeerStats.publish?.bitrate);   console.log('Subscribe Bitrate', localPeerStats.subscribe?.bitrate);   console.log('Total Bytes Sent', localPeerStats.publish?.bytesSent);   console.log('Total Bytes Received', localPeerStats.subscribe?.bytesReceived);   console.log('Total Packets Lost', localPeerStats.subscribe?.packetsLost);   console.log('Total Jitter', localPeerStats.subscribe?.jitter);  ; const LocalPeerStats = () =>     const localPeerStats = useHMSStatsStore(selectHMSStats.localPeerStats);   useEffect(() =>       renderLocalPeerStats(localPeerStats);    ,  localPeerStats );  ;   </Tab>   Individual Track Stats Use the selectHMSStats.trackStatsByID selector to get an individual track's stats.  s id=\"track-stats\" items=  'Javascript', 'React'       id=\"track-stats-0\">   jsx import   selectHMSStats   from '@100mslive/hms-video-store'; const renderTrackStats = (trackStats) =>     if ( trackStats) return;   console.log('Stat Type', trackStats.type); // 'outbound-rtp' for local tracks, 'inbound-rtp' for remote tracks   console.log('Track Type', trackStats.kind); // 'video'  'audio'   console.log('Bitrate', trackStats.bitrate);   console.log('Packets Lost', trackStats.packetsLost);   console.log('Packets Lost Rate', trackStats.packetsLostRate);   console.log('Jitter', trackStats.jitter);   const isLocal = trackStats.type.includes('outbound');   if (isLocal)       console.log('Bytes Sent', trackStats.bytesSent);     else       console.log('Bytes Received', trackStats.bytesReceived);       if (trackStats.kind === 'video')       console.log('Frame Width', trackStats.frameWidth);     console.log('Frame Height', trackStats.frameHeight);     console.log('Framerate', trackStats.framesPerSecond);     if (isLocal)         console.log('Quality Limitation Reason', trackStats.qualityLimitationReason);            ; hmsStats.subscribe(renderTrackStats, selectHMSStats.trackStatsByID('some-track-id'));   </Tab>   id=\"track-stats-1\">   jsx import   selectHMSStats, useHMSStatsStore   from '@100mslive/react-sdk'; const renderTrackStats = (trackStats) =>     if ( trackStats) return;   console.log('Stat Type', trackStats.type); // 'outbound-rtp' for local tracks, 'inbound-rtp' for remote tracks   console.log('Track Type', trackStats.kind); // 'video'  'audio'   console.log('Bitrate', trackStats.bitrate);   console.log('Packets Lost', trackStats.packetsLost);   console.log('Packets Lost Rate', trackStats.packetsLostRate);   console.log('Jitter', trackStats.jitter);   const isLocal = trackStats.type.includes('outbound');   if (isLocal)       console.log('Bytes Sent', trackStats.bytesSent);     else       console.log('Bytes Received', trackStats.bytesReceived);       if (trackStats.kind === 'video')       console.log('Frame Width', trackStats.frameWidth);     console.log('Frame Height', trackStats.frameHeight);     console.log('Framerate', trackStats.framesPerSecond);     if (isLocal)         console.log('Quality Limitation Reason', trackStats.qualityLimitationReason);            ; const TrackStats = (trackID) =>     const trackStats = useHMSStatsStore(selectHMSStats.trackStatsByID(trackID));   React.useEffect(() =>       renderTrackStats(trackStats);    ,  trackStats );  ;   </Tab> You could also use selectHMSStats.localAudioTrackStats and selectHMSStats.localVideoTrackStats to get stats of local audio track and local video track respectively. For list of other stats-related selectors, look at <APILink type=\"selector\"> selectHMSStats </APILink>. "
    },
    {
        "title": "HLS Streaming",
        "link": "/javascript/v2/how--to-guides/record-and-live-stream/hls",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/record-and-live-stream/hls",
        "keywords": [],
        "content": "   HLS Streaming (./../foundation/live-streaming) allows for scaling to millions of viewers in near real time. You can give a link of your web-app which will be converted to a HLS feed by our server and can be played across devices for consumption. Behind the scenes, this will be achieved by having a bot join your room and stream what it sees and hears. Once the feed is ready, the server will give a URL which can be played using any HLS Player. > Note that the media server serving the content in this case is owned by 100ms. If you're looking for a way to stream > on YouTube, Twitch etc., please have a look at our RTMP streaming docs  here (./rtmp-recording).   Starting HLS To start HLS, you'll need to pass in a meeting URL or configure the HLS settings from dashboard in the template in destinations tab. The 100ms bot will open this URL to join your room, so it must allow access without any user level interaction. In the future, it'll be possible to start HLS for multiple such URLs for the same room. For this purpose, the API supports taking in an array, although currently only the first element of the array will be respected. To distinguish between multiple URLs, an additional field metadata can be optionally passed. The meetingURL and metadata are clubbed together to form what we'll call a variant . If everything is configured in the dashboard, you can directly call hmsActions.startHLSStreaming once you join the room. Otherwise, You can also call hmsActions.startHLSStreaming with a hlsConfig having an array of such variants. Additionally you can also start recording by passing in a recording config with below fields  1. singleFilePerLayer : if the desired end result is a mp4 file per HLS layer, false by default 2. hlsVod : if the desired end result is a zip of m3u8 and all the chunks, false by default   js async startHLSWithoutPrams()     try       // For this to work, destinations in your template needs to be configured in dashboard     await hmsActions.startHLSStreaming();     catch (err)       console.error(\"failed to start hls\", err);       async startHLS()     const params =       variants:   meetingURL: \"\", metadata: \"landscape\"      ;   params.recording =  singleFilePerLayer: true, hlsVod: false ; // to enable recording   try       await hmsActions.startHLSStreaming(params);     catch (err)       console.error(\"failed to start hls\", err);           Stopping HLS You can call hmsActions.stopHLSStreaming to stop HLS Streaming which will stop all the variants.   js async stopHLS()     try       await hmsActions.stopHLSStreaming();     catch (err)       console.error(\"failed to stop hls\", err);           Current room status The selectHLSState selector can be used to know if HLS is currently running, and the url where it can be viewed.  s id=\"hls\" items=  'JavaScript', 'React'     ' '    id='hls-0'>   js function updateHLSState(hlsState)     console.log('is hls streaming going on  ', hlsState.running);   if (hlsState.running)       console.log('hls url  ', hlsState.variants 0 ?.url);     else if (hlsState.error)       console.error('error in hls streaming  ', hlsState.error);       hmsStore.subscribe(updateHLSState, selectHLSState);   </Tab>   id='hls-1'>   jsx function HLSState()     const hlsState = useHMSStore(selectHLSState);   console.log('is hls streaming going on  ', hlsState.running);   if (hlsState.running)       console.log('hls url  ', hlsState.variants 0 ?.url);     else if (hlsState.error)       console.error('error in hls streaming  ', hlsState.error);       return null;     </Tab>   Displaying HLS Stream Not all browsers support HLS  natively (https://caniuse.com/?search=hls), however you can use players like  hls.js (https://github.com/video-dev/hls.js/) or  Shaka Player (https://github.com/google/shaka-player). In case you need more UI side customisations you can go with something like  videojs (https://videojs.com/) which internally uses hls.js for the hls piece. Here is a simple example of using hls.js to play the HLS URL given by the SDK (using hmsStore selector). hmsStore selector hook will help you with selector functions to fetch information from the state at any point in time. Check  here (./../guides/useful-selectors) for more examples. Make sure you've installed the library( npm i hls.js ) before using this code.  s id=\"hlsview\" items=  'JavaScript', 'React'     ' '    id='hlsview-0'>   js import Hls from 'hls.js'; import   hmsActions, hmsStore   from './hms'; function renderHLS(  hlsUrl  )     const hlsState = hmsStore.getState(selectHLSState);   const hlsUrl = hlsState.variants 0 ?.url;   const video = document.getElementById('video');   const browserHasNativeHLSSupport = video.canPlayType(     'application/vnd.apple.mpegurl'   );   if (Hls.isSupported())       let hls = new Hls();     hls.loadSource(hlsUrl);     hls.attachMedia(video);       // hls.js is not supported on iOS Safari, but as the browser has native support for playing HLS,   // we can use the video element directly.   else if (browserHasNativeHLSSupport)       video.src = hlsUrl;         </Tab>   id='hlsview-1'>   jsx import Hls from 'hls.js'; import   useHMSStore, selectHLSState   from '@100mslive/react-sdk'; function HLSView(  hlsUrl  )     const videoRef = useRef(null);   const hlsState = useHMSStore(selectHLSState);   const hlsUrl = hlsState.variants 0 ?.url;   useEffect(() =>       if (videoRef.current && hlsUrl)         const browserHasNativeHLSSupport = videoRef.current.canPlayType(         'application/vnd.apple.mpegurl'       );       if (Hls.isSupported())           let hls = new Hls();         hls.loadSource(hlsUrl);         hls.attachMedia(videoRef.current);               // hls.js is not supported on iOS Safari, but as the browser has native support for playing HLS,       // we can use the video element directly.       else if (browserHasNativeHLSSupport)           videoRef.current.src = hlsUrl;                  ,  hlsUrl );   return <video ref= videoRef  autoPlay controls></video>;     </Tab> >   Note:   If you wish to customize the stream, please check  this guide (./../foundation/live-streaming step-4-customize-the-stream) for more information.   Changing the quality of Stream(Hls.js) You may want to change the quality layer of the stream based on user input or some other criteria. Each library handles this differently. Below we mention an example snippet on how to do it if you choose to use hls.js. Please refer to the respective documentations of the library you use.  s id=\"hlsChangeQuality\" items=  'JavaScript', 'React'     ' '    id='hlsChangeQuality-0'>   js import Hls from 'hls.js'; let hls = null; const video = document.getElementById('video'); const browserHasNativeHLSSupport = video.canPlayType('application/vnd.apple.mpegurl'); if (Hls.isSupported())     hls = new Hls();   hls.loadSource(hlsUrl);   hls.attachMedia(video);   console.log('Available quality levels in current stream', hls.levels);   setCurrentLevel(hls.levels 0 ); //set the first level.   function setCurrentLevel(currentLevel)     // make sure we are choosing the right level by checking the   // height and width.   const newLevel = hls.levels.findIndex(     (level) => level.height === currentLevel.height && level.width === currentLevel.width   );   // set it.   hls.currentLevel = newLevel;     </Tab>   id='hlsChangeQuality-1'>   jsx import Hls from 'hls.js'; let hls = null; function HLSView(  hlsUrl  )     const videoRef = useRef(null);   useEffect(() =>       if (videoRef.current && hlsUrl)         if (Hls.isSupported())           hls = new Hls();         console.log('Available quality levels in current stream', hls.levels);         setCurrentLevel(hls.levels 0 ); //set the first level.                  ,  hlsUrl );   function setCurrentLevel(currentLevel)       // make sure we are choosing the right level by checking the     // height and width.     const newLevel = hls.levels.findIndex(       (level) => level.height === currentLevel.height && level.width === currentLevel.width     );     // set it.     hls.currentLevel = newLevel;       return <video ref= videoRef  autoPlay controls></video>;     </Tab>   Testing with the Dashboard web-app You may want to try out the end to end flow on our dashboard web-app before moving ahead with building in your app. To do this, you can create an additional role named hls-viewer which will be presented with the HLS stream when they join the room.   Go live (/docs/v2/live-streaming-go-live.gif) To start the HLS stream:   Click on the Go Live button on the top right of the page.   Choose the Live Stream with HLS option and click Go Live to start streaming.   Once HLS has started, you can join from the hls-viewer role to see the HLS stream.   Tips   If you're using the dashboard web-app from 100ms, please make sure to use a role which doesn't have publish permissions for beam tile to not show up.   The meeting url can be edited in the streaming/recording popup to join with a recording specific role configured as such.   If using your own web-app, do put in place retries for API calls like tokens etc. just in case any call fails. As human users we're   used to reloading the page in these scenarios which is difficult to achieve in the automated case.   Make sure to not disable the logs for the passed in meeting URL. This will allow for us to have more visibility into the room, refreshing the page   if join doesn't happen within a time interval. "
    },
    {
        "title": "RTMP Streaming / Recording",
        "link": "/javascript/v2/how--to-guides/record-and-live-stream/rtmp-recording",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/record-and-live-stream/rtmp-recording",
        "keywords": [],
        "content": "  Want to preserve your video call for posterity in a recording? Or live stream it out to millions of viewers on YouTube, Twitch, Facebook, MUX or whatever gives you an RTMP ingest URL? Turn on RTMP streaming or recording. In 100ms, recording and streaming is usually achieved by having a bot join your room and stream what it sees and hears to a file (recording) or to an RTMP ingest URL (streaming). It's also possible to start streaming and recording from  server side APIs (/server-side/v2/Destinations/rtmp-streaming-and-browser-recording). > There is another type > of SFU based recording as well which allows for all the tracks to be recorded individually, you can find more details about it >  here (/server-side/v2/Destinations/recording).   Start and stop streaming / recording rtmpUrls  These are the options which need to be passed to start RTMP or recording  1.   meetingURL  : _string_. The URL the 100ms bot user will open to join your room. It must allow access without any user level interaction. 2.   rtmpURLs  : _string  _. Optional, if streaming is required, this has to be one or more RTMP Ingest URLs with max limit of 3 URLs where the stream should go.    Format: rtmp://server.com/app/STREAM_KEY    Example: rtmp://a.rtmp.youtube.com/live2/k0jv-329m-1y7f-ktth-ck48      \"rtmp://a.rtmp.youtube.com/live2/\"  RTMP stream URL.      \"k0jv-329m-1y7f-ktth-ck48\"  RTMP stream key.    Please refer to the platform specific documentation for details on how to obtain the stream URL and stream key. Here are some examples:      YouTube (https://support.google.com/youtube/answer/2907883?hl=en&ref_topic=9257892)      Facebook (https://www.facebook.com/help/587160588142067)      Instagram (https://about.instagram.com/blog/tips-and-tricks/instagram-live-producer)      Twitch (https://help.twitch.tv/s/article/twitch-stream-key-faq?language=en_US)      LinkedIn (https://www.linkedin.com/help/linkedin/answer/a564446/go-live-using-a-custom-stream-rtmp) 3.   record  : _boolean_. Optional, If recording is required this can be set as true. This value has no effect on streaming. 4.   resolution  : _  width: number, height: number  _. Optional, if passed the recording or RTMP will be done in the passed in resolution.   If both rtmpURLs and record = true are provided, both streaming and recording will begin.   If only rtmpURLs are provided, only streaming will begin.   If only record true is provided, only recording will begin. If either one is started, the other can't be started without first stopping whatever is running. Eg: Only streaming is started. Recording can't be started unless streaming is stopped first. If both are required, they have to be started together by providing both RTMP Ingest URLs and recording = true.   js async start()     const params =       meetingURL: \"\",     rtmpURLs:  \"\" ,     record: true    ;   try       await hmsActions.startRTMPOrRecording(params);     catch(err)       console.error(\"failed to start RTMP/recording\", err);       async stop()     try       await hmsActions.stopRTMPAndRecording();     catch (err)       console.error(\"failed to stop RTMP/recording\", err);           Current Recording/RTMP State As it is with everything else in the 100ms web world, we've selectors to tell us about the current state of the room, whether recording or streaming is already going on and if recording is happening, which type of recording is currently running.   js const recordingState = hmsStore.getState(selectRecordingState); const rtmpState = hmsStore.getState(selectRTMPState); console.log('is sfu recording going on  ', recordingState.server.running); if (recordingState.browser.running)     console.log('time when browser recording was started  ', recordingState.browser.startedAt);   else if (recordingState.browser.error)     console.error('error in browser recording  ', recordingState.browser.error);   if (rtmpState.running)     console.log('time when RTMP was started  ', rtmpState.startedAt);   else if (rtmpState.error)     console.error('error in RTMP streaming  ', rtmpState.error);       Tips   If you're using the dashboard web-app from 100ms, please make sure to use a role which doesn't have publish permissions for beam tile to not show up.   The meeting URL can be edited in the streaming/recording popup to join with a recording specific role configured as such.   If using your own web-app, do put in place retries for API calls like tokens etc. just in case any call fails. As human users we're   used to reloading the page in these scenarios which is difficult to achieve in the automated case.   Make sure to not disable the logs for the passed in meeting URL. This will allow for us to have more visibility into the room, refreshing the page   if join doesn't happen within a time interval. "
    },
    {
        "title": "Show Audio Level",
        "link": "/javascript/v2/how--to-guides/set-up-video-conferencing/render-video/audio-level",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/set-up-video-conferencing/render-video/audio-level",
        "keywords": [],
        "content": "    Show Audio Level (/guides/audio-levels.png) You can also show audio levels in your UI corresponding to every peer so it's easier to see who is speaking currently. You can retrieve all the speaker object which will contain peerID, trackID and their audioLevel using selectSpeakers selector. You can fetch audio level for a particular peer using selectPeerAudioByID and for a particular track using selectTrackAudioByID selector. You can also fetch the current active speaker using the selectDominantSpeaker selector.  s id=\"audio-level\" items=  'JavaScript', 'React'     ' '    id='audio-level-0'>   js import     selectPeerAudioByID,   selectDominantSpeaker,   selectSpeakers   from '@100mslive/hms-video-store'; const peerId = '..'; function updateAudioLevel(audioLevel)     console.log( audio level for peer  $ peerID  is $ audioLevel  );   hmsStore.subscribe(updateAudioLevel, selectPeerAudioByID(peerId)); console.log('all speakers', hmsStore.getState(selectSpeakers)); function activeSpeaker(peer, prevPeer)     console.log( previous active speaker  $ prevPeer.name  with role  $ prevPeer.roleName  );   console.log( current active speaker  $ peer.name  with role  $ peer.roleName  );   hmsStore.subscribe(activeSpeaker, selectDominantSpeaker);   </Tab>   id='audio-level-1'>   jsx import     useHMSStore,   useHMSActions,   selectLocalPeer,   selectPeerAudioByID,   selectSpeakers,   selectDominantSpeaker   from '@100mslive/react-sdk'; const AudioLevelInfo = () =>     /   get localpeer from store  /   const localpeer = useHMSStore(selectLocalPeer);   /   get a given peer's audio level.  /   const peerAudioLevel = useHMSStore(selectPeerAudioByID(localpeer.id));   console.log( audio level for peer  $ localpeer.id  is $ peerAudioLevel  );   /   get all speakers. Gives back a list of all peers who are not muted.  /   const allSpeakers = useHMSStore(selectSpeakers);   console.log('all speakers', allSpeakers);   /   gets the active speaker  /   const dominantSpeaker = useHMSStore(selectDominantSpeaker);   console.log(     current active speaker  $ dominantSpeaker.name  with role  $ dominantSpeaker.roleName    );   return (     <>       <h1>Active Speaker</h1>       <span>          dominantSpeaker.name :  dominantSpeaker.id        </span>     <    );  ;   </Tab> "
    },
    {
        "title": "Overview",
        "link": "/javascript/v2/how--to-guides/set-up-video-conferencing/render-video/overview",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/set-up-video-conferencing/render-video/overview",
        "keywords": [],
        "content": "  It all comes down to this. All the setup so far has been done so that we can show live streaming video in our beautiful apps. 1. First, you need to get all the peers in the room. You can use the selectPeers selector for this. 2. Next, you'll need to have a reference to a video element. You must add the attributes as present below.   html <video autoplay muted playsinline     3. You can then iterate over all the peers, and call the attachVideo function in hmsActions to render the video.   js hmsActions.attachVideo(videoTrack.id, videoElement);   The videos will now be rendered & start playing on your screen. 🥳 > In case you want to render local peer and remote peers separately, you can use the selectors  selectLocalPeer and selectRemotePeers .   Detach video to conserve bandwidth and cleanup elements You can call detachVideo on the hmsActions to unsubscribe a track and not fetch it's data. This can be done for example when a video goes out of view. This should also be called when the component is going to be unmounted for proper cleanups of the video elements.   js hmsActions.detachVideo(videoTrack.id, videoElement);     When to attach/detach videos We need to re-attach video when it's in view after every: 1. Unmute(when track.enable is true) 2. Plugin is added/removed(for example, enable/disable virtual background) 3. Camera/device change(track.deviceId is changed) You can achieve all this by using the selectVideoTrackByID selector and the aforementioned hmsActions.attachVideo and hmsActions.detachVideo functions.   js // assuming you have peer object and video element // complete code snippet below hmsStore.subscribe((track) =>     if ( track)       return;       if (track?.enabled)       hmsActions.attachVideo(track.id, videoElement);     else       hmsActions.detachVideo(track.id, videoElement);      , selectVideoTrackByID(peer.videoTrack));   > Note that if you're using the useVideo hook from react-sdk this is already being taken care of.   Example Snippet  s id=\"render\" items=  'JavaScript', 'React'     ' '    id='render-0'>   js import   hmsActions   from './hms'; const peersContainer = document.getElementById('peers-container'); // store peer IDs already rendered to avoid re-render on mute/unmute const renderedPeerIDs = new Set(); // render a single peer video tile function renderPeer(peer)     const peerTileDiv = document.createElement('div');   // you can either get an existing video element or create a new one.   const videoElement = document.createElement('video');   const peerTileName = document.createElement('div');   videoElement.autoplay = true;   videoElement.muted = true;   videoElement.playsinline = true;   peerTileName.textContent = peer.name;   hmsStore.subscribe((track) =>       if ( track)         return;           if (track.enabled)         hmsActions.attachVideo(track.id, videoElement);       else         hmsActions.detachVideo(track.id, videoElement);          , selectVideoTrackByID(peer.videoTrack));   peerTileDiv.append(videoElement);   peerTileDiv.append(peerTileName);   renderedPeerIDs.add(peer.id);   return peerTileDiv;   // display a tile for each peer in the peer list function renderPeers(peers)     peersContainer = document.getElementById('peers-container');   peers.forEach((peer) =>       if ( renderedPeerIDs.has(peer.id) && peer.videoTrack)         console.log(         rendering video for peer  $ peer.name , roleName  $ peer.roleName , isLocal $ peer.isLocal        );       peersContainer.append(renderVideo(peer));          );   // subscribe to the peers, so render is called whenever there is a change like peer join and leave hmsStore.subscribe(renderPeers, selectPeers);   </Tab>   id='render-1'>   jsx import   useRef, useEffect   from 'react'; import   useHMSStore, useVideo, selectPeers   from '@100mslive/react-sdk'; function VideoTile(  peer  )     const   videoRef   = useVideo(      trackId: peer.videoTrack    );   return <video ref= videoRef  autoPlay muted playsInline></video>;   function Conference()     const peers = useHMSStore(selectPeers);   return (     <>        peers.map((peer) => (         <VideoTile key= peer.id  peer= peer          ))      <    );     </Tab> "
    },
    {
        "title": "PIP (Picture-in-Picture) Mode",
        "link": "/javascript/v2/how--to-guides/set-up-video-conferencing/render-video/pip-mode",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/set-up-video-conferencing/render-video/pip-mode",
        "keywords": [],
        "content": "    Overview PiP Mode lets the user watch the room video in a small window pinned to a corner of the screen while navigating between apps or browsing content on the main screen. 100ms PiP selects top 4 peers with video enabled and render them in the PiP window. > NOTE: Local Peer is not shown in PiP by default   Picture-in-Picture (/guides/pip-mode.png)   Implementation   React projects For implementation copy this  folder (https://github.com/100mslive/100ms-web/tree/main/src/components/PIP) to your react project. Add worker-timers as a dependency.   jsx import   PIP   from '../path-to-above-folder'; function SomeComponent()     return <PIP  ;     Our  sample app implementation (https://github.com/100mslive/100ms-web/blob/main/src/components/PIP/index.jsx) prioritizes the pinned tiles in PiP mode, you can remove this if not required.   Non-React projects For non React projects, you can use PictureInPicture from PIPManager in the above folder and use it to start/stop PiP, check whether it is supported or not.   js import   PictureInPicture   from '../path-to-above-folder/PIPManager'; import   MediaSession   from '../path-to-above-folder/SetupMediaSession'; PictureInPicture.isSupported(); // to check if PiP is supported. This depends on browser implementation. For example, it is not supported in firefox PictureInPicture.isOn(); // to check whether PiP is enabled. // To start pip, pass hmsActions and a callback function which receives the current state of pip PictureInPicture.start(hmsActions, setIsPipOn).catch((err) =>   console.error('error in starting pip', err) ); // Call this to show controls in PiP window.(Audio toggle, Video toggle and end call are the options supported here) MediaSession.setup(hmsActions, store); // To stop pip PictureInPicture.stop().catch((err) => console.error('error in stopping pip', err));     Customization(in React) If you want to render custom peers in PiP from your end, you can pass those peers in the PIPComponent in the above folder. For example, if you want to show peers of particular role only in PiP, the following can be done.   jsx import   useHMSStore, selectPeersByRole   from '@100mslive/react-sdk'; function PIP()     const peers = useHMSStore(selectPeersByRole('some-role'));   return <PIPComponent peers= peers   ;     To include local peer in PiP, just pass showLocalPeer to PIPComponent.   jsx function PIP()     return <PIPComponent showLocalPeer  ;       Limitations:   PiP is not supported in Firefox   PiP is flaky in safari due to it's canvas rendering issues. check  known issues (../debugging/known-issues) for more information "
    },
    {
        "title": "Select Video Quality",
        "link": "/javascript/v2/how--to-guides/set-up-video-conferencing/render-video/simulcast",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/set-up-video-conferencing/render-video/simulcast",
        "keywords": [],
        "content": "  Simulcast enables  Adaptive Bitrate (../foundation/adaptive-bitrate) (ABR) in video conferencing scenarios. This means 100ms SDKs can upgrade or downgrade video quality for remote tracks based on user preferences or network conditions.   Minimum requirements   SDK versions @100mslive/hms-video-store@0.8.0 , @100mslive/react-sdk@0.6.0 or later   Simulcast to be enabled from dashboard template   Simulcast Layer 100ms API's Available simulcast layers are:   HMSSimulcastLayer.HIGH   HMSSimulcastLayer.MEDIUM   HMSSimulcastLayer.LOW  > Note: The number of layers available will be dependent on the video publish resolution for the role and the available simulcast resolutions, in template settings in the dashboard.   Action to update the layer:    s id=\"simulcast\" items=  'JavaScript', 'React'       id='simulcast-0'>   js import   HMSSimulcastLayer   from '@100mslive/hms-video-store'; /    layer can be one of:    HMSSimulcastLayer.HIGH    HMSSimulcastLayer.MEDIUM    HMSSimulcastLayer.LOW     Setting preferred layer does not guarantee that the layer will be selected.   The server tries to give you that preferred layer when possible.   Otherwise, it will pick the next closest layer. Some of the reasons for   not getting the preferred layer could be publisher is not publishing that   layer or your download bandwidth is not good enough to handle that layer.  / hmsActions.setPreferredLayer(<videoTrackID>, layer);   </Tab>   id='simulcast-1'>   jsx import   HMSSimulcastLayer, selectTrackByID, useHMSStore   from '@100mslive/react-sdk'; const SimulcastLayers = (  trackId  ) =>     const track = useHMSStore(selectTrackByID(trackId));   const handleLayerChange = (layer) =>       /        layer can be one of:       HMSSimulcastLayer.HIGH       HMSSimulcastLayer.MEDIUM       HMSSimulcastLayer.LOW       Setting preferred layer does not guarantee that the layer will be selected. The server tries to give you that preferred layer when possible. Otheriwse, it will pick the next closest layer.       Some of the reasons for not getting the preferred layer could be publisher is not publishing that layer or your download bandwidth is not good enough to handle that layer.      /     hmsActions.setPreferredLayer(<videoTrackID>, layer);       return (     <div>      track.layerDefinitions.map(trackLayer =>         return (<div onClick= () =>           handleLayerChange(trackLayer.layer);         >          trackLayer.layer        </div>     )      </div>   )     </Tab>   Simulcast fields on the track   The  HMSVideoTrack (/api-reference/javascript/v2/interfaces/HMSVideoTrack) has a few fields related to simulcast.   layer  is the current layer you are receiving from the server   layerDefinitions  are the available layers for the role based on the template. These will contain layer name and resolution(widthxheight).   preferredLayer  this will contain the layer that is either manually set or auto selected based on resolution.   Auto Layer selection: When using useVideo hook from our  React SDK (https://www.npmjs.com/package/@100mslive/react-sdk) the relevant simulcast layer is automatically subscribed to, based on the video element dimensions. "
    },
    {
        "title": "Handle Network Degradation",
        "link": "/javascript/v2/how--to-guides/set-up-video-conferencing/render-video/sub-degradation",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/set-up-video-conferencing/render-video/sub-degradation",
        "keywords": [],
        "content": "  Sometimes people have bad internet connections but everyone deserves a good meeting. When the network is too slow to support audio and video conversations together, the 100ms SDK can automatically turn off downloading other peer's videos, which may improve the audio quality and avoid disconnections. If the network quality improves, the videos will be restored automatically as well. To turn on subscribe degradation in your room, open the  templates (https://dashboard.100ms.live/templates) in the dashboard and enable it for roles there.   Dashboard Degradation Option (/docs/v2/degradation.png)   Responding in the app The  HMSVideoTrack (/api-reference/javascript/v2/interfaces/HMSVideoTrack degraded) object has a boolean degraded field which will become true if the track gets degraded. If degraded is true, treat it as if the video is turned off. If the UI is not changed, the video tile will appear black or frozen. So for eg. if your UI shows an avatar when the track is not enabled, it should also do so on degradation. "
    },
    {
        "title": "Chat",
        "link": "/javascript/v2/how--to-guides/set-up-video-conferencing/chat",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/set-up-video-conferencing/chat",
        "keywords": [],
        "content": "  What's a video call without being able to send messages to each other too? 100ms supports chat for every video/audio room you create.   Chat (/guides/chat.png) > Note: Chat messages are not persistent. For example, if you joined a call, received some messages and got disconnected and rejoin the call, you won't be able to see the previous messages   Sending Chat Messages   Broadcast Message: This will be received by everyone in the room.   js hmsActions.sendBroadcastMessage('hello everyone '); // yes it's that simple 😉     Group Message: This will be received by every peer who is part of the passed-in roles.   js hmsActions.sendGroupMessage('hi folks ',  'moderator', 'host' );     Direct Message: This will only be received by the peer to whom the message was sent to.   js hmsActions.sendDirectMessage('keep this message a secret ', peer.id);     Showing the messages The selector selectHMSMessages can be used to get all the messages. There are few other selectors to help with selecting more specific messages.  s id=\"chat\" items=  'JavaScript', 'React'       id='chat-0'>   js import     selectHMSMessages,   selectBroadcastMessages,   selectMessagesByRole,   selectMessagesByPeerID   from '@100mslive/hms-video-store'; function renderMessages(messages)     console.log('messages  ', messages);   hmsStore.subscribe(renderMessages, selectHMSMessages); // get all messages hmsStore.subscribe(renderMessages, selectBroadcastMessages); // get all broadcasted messages hmsStore.subscribe(renderMessages, selectMessagesByRole('host')); // get conversation with the host role hmsStore.subscribe(renderMessages, selectMessagesByPeerID(peer.id)); // get private conversation with peer   </Tab>   id='chat-1'>   jsx import   Message   from './components'; import     selectHMSMessages,   selectBroadcastMessages,   selectMessagesByRole,   selectMessagesByPeerID   from '@100mslive/react-sdk'; function Chat()     // use only the selectors that are required for the corresponding UI   const allMessages = useHMSStore(selectHMSMessages); // get all messages   const broadcastMessages = useHMSStore(selectBroadcastMessages); // get all broadcasted messages   const groupMessagesByRole = useHMSStore(selectMessagesByRole('host')); // get conversation with the host role   const directMessages = useHMSStore(selectMessagesByPeerID(peer.id)); // get private conversation with peer   return (     <>        messages.map((msg) => (         <Message key= msg.id  message= msg          ))      <    );     </Tab> Refer  API Reference for   HMSMessage Interface   (/api-reference/javascript/v2/interfaces/HMSMessage).   Marking a message as read We also have a boolean read field with every message which can be optionally used to track whether the user has seen the message. You can set the read status for a message with the following interface.   js const readStatus = true; // true/false hmsActions.setMessageRead(readStatus); // set status for all messages hmsActions.setMessageRead(readStatus, msg.id); // set status for a specific message     Message Notification When a peer receives a message they'll get a  notification (./notifications) with type HMSNotificationTypes.NEW_MESSAGE . The HMSMessage object will be present in notification.data .   Custom Events It's also possible to utilise the above system for sending custom messages across the room. All the above methods take type as an optional last param. The type is chat by default but can be changed to any valid string. This can be used to differentiate between different types of messages. For example.   js // broadcast a message to the whole room hmsActions.sendBroadcastMessage('START', 'GAME_EVENT'); hmsActions.sendBroadcastMessage('🚀', 'EMOJI_REACTION'); // Send a message to role groups hmsActions.sendGroupMessage('🔥',  'stage', 'backstage' , 'EMOJI_REACTION'); // Send a message to a specific peer hmsActions.sendDirectMessage('❤️', peer.id, 'EMOJI_REACTION'); hmsActions.sendDirectMessage('STOP_SCREENSHARE', peer.id, 'MODERATOR_EVENT');   Suppose you have a custom event for sending \"EMOJI_REACTION\" and you don't want these messages to be stored in the store. In such cases ignoreMessageTypes action methods comes in handy. Notifications for the ignored messages will still be sent, it'll only not be put in the store.   js hmsActions.ignoreMessageTypes( 'EMOJI_REACTION' );   Do note that the messages are not persistent and won't be available to a person joining the room after the event was sent. So even though the above is good for emoji reactions, if you're looking for implementing something like raise hand do check out our documentation for  peer metadata (../advanced-features/peer-metadata). > In case you want to send information in a more structured way than a simple string, you can send a stringified JSON object.   useCustomEvent This is a react hook that eases firing & listening to custom events. It takes in a config of type which is the type of your event and onEvent a handler function that is fired when an event comes. It returns an object containing sendEvent which helps to send the event data to others in the room who will receive it in the onEvent handler. Let's understand the hook with an example where we're showing a button on UI to send emoji reactions.   jsx import   useCallback   from 'react'; import   useCustomEvent, useHMSActions   from '@100mslive/react-sdk'; const Confetti = () =>     const actions = useHMSActions();   // onEvent should use useCallback   const onEvent = useCallback((msg) =>       console.log(msg); //  emoji: \"🚀\"      // show emoji reactions on UI    ,   );   const   sendEvent   = useCustomEvent(      type: 'EMOJI_REACTION',     onEvent: onEvent    );   return <button onClick= () => sendEvent(  emoji: '🚀'  ) >Rockets</button>;  ; export default Confetti;   "
    },
    {
        "title": "Device Selector",
        "link": "/javascript/v2/how--to-guides/set-up-video-conferencing/device-change",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/set-up-video-conferencing/device-change",
        "keywords": [],
        "content": "  Device changes include changes in audio/video input and audio output devices. Plugging a headset, or disconnecting your airpods mid call are some of the example of device changes. These are all taken care of by the SDK without requiring any code.   Manual Device Change   Device Change Settings (/guides/settings.png) You may still want to allow users to see the currently selected devices and change them if required. We'll do this in three steps  1. Fetching all attached devices list from hmsStore . 2. Fetching the currently selected devices from hmsStore . 3. Calling hmsActions to change the device if required. > Note that changing audio output device is available only in chromium based browsers.  s id=\"devicechange\" items=  'JavaScript', 'React'       id='devicechange-0'>   js //  audioInput, audioOutput, videoInput  const devices = hmsStore.getState(selectDevices); //  audioInputDeviceId, audioOutputDeviceId, videoInputDeviceId  const selected = hmsStore.getState(selectLocalMediaSettings); // show user a settings component to manually choose device // The following selected devices can be obtained when changed from your UI. // Update AudioInput device by calling hmsActions.setAudioSettings(  deviceId: selectedAudioDeviceID  ); // Update VideoInput device by calling hmsActions.setVideoSettings(  deviceId: selecedVideoDeviceID  ); // Update AudioOutput device by calling hmsActions.setAudioOutputDevice(selectedAudioOutputDeviceID);   </Tab>   id='devicechange-1'>   jsx import   useDevices, DeviceType   from '@100mslive/react-sdk'; const DeviceSettings = () =>     const   allDevices, selectedDeviceIDs, updateDevice   = useDevices();   // Call the updateDevice function like below to update the videoInput device. Similarly for audio input and output pass corresponding deviceId and DeviceType   updateDevice(      deviceId,     deviceType: DeviceType.videoInput    );   // render devices and selection from allDevices and selectedDeviceIDs   return <div  ;  ;   </Tab>   Remember manually selected devices We support an additional boolean in join config which can be passed in to let sdk use the localstorage to remember manual device selections for future room  join (./join).  s id=\"rememberdeviceselection\" items=  'JavaScript', 'React'       id='rememberdeviceselection-0'>   js await hmsActions.join(  ...joinConfig, rememberDeviceSelection: true  );   </Tab>   id='rememberdeviceselection-1'>   jsx const JoinCall = () =>     const hmsActions = useHMSActions();   const join = useCallback(async () =>       await hmsActions.join(  ...joinConfig, rememberDeviceSelection: true  );    ,  hmsActions );   return <button onClick= join >Join</button>;  ;   </Tab> > Note that hmsActions.attachVideo should be called when there's a change in camera. Refer  Render Video (./render-video when-to-attach-detach-videos) section to see how to handle it. "
    },
    {
        "title": "Join Room",
        "link": "/javascript/v2/how--to-guides/set-up-video-conferencing/join",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/set-up-video-conferencing/join",
        "keywords": [],
        "content": "  To join and interact with others in audio or video call, the user needs to join a room . When user indicates that they want to _join_ the room, your app should have  1. User Name  the name which should be displayed to other peers in the room. 2.  Authentication Token (../guides/token)  the client side authentication token generated by the Token Service. You can also optionally pass these fields  1. settings:  isAudioMuted, isVideoMuted   you can use this to set the initial audio/video state at time of join. 2. User metadata  this can be used to pass any additional metadata associated with the user. We'll call the join method on hmsActions object with a config containing above fields to join the room. > Note: join is async from this  version (../changelog/release-notes 2022-09-13)  s id=\"join\" items=  'JavaScript', 'React'     ' '    id='join-0'>   js:join.js import   hmsActions   from './hms'; const config =     userName: 'Jon Snow',   authToken: '<Auth token>', // client-side token generated from your token service   settings:       isAudioMuted: true,     isVideoMuted: false    ,   metaData: JSON.stringify( city: 'Winterfell', knowledge: 'nothing' ),   rememberDeviceSelection: true, // remember manual device change  ; await hmsActions.join(config);    </Tab>   id='join-1'>   jsx:join.jsx import   useHMSActions   from '@100mslive/react-sdk'; function Join()    const hmsActions = useHMSActions();  const config =     userName: 'Jon Snow',   authToken: '<Auth token>', // client-side token generated from your token service   settings:       isAudioMuted: true,     isVideoMuted: false,    ,   metaData: JSON.stringify( city: 'Winterfell', knowledge: 'nothing' ),   rememberDeviceSelection: true, // remember manual device change   ;  async function onJoinClick()     await hmsActions.join(config)     return <     // form with input and join button   ;     </Tab> That's it. You have joined a room successfully 🥳. You should now be able to have an audio only call with this.   Getting Current Room State You might want to hide your join form from the UI or navigate to another page when the join has completed. We provide  some selectors (/api-reference/javascript/v2/home/content functions) which operate upon the room state to give the relevant information. One such selector is   selectIsConnectedToRoom  (/api-reference/javascript/v2/home/content select-is-connected-to-room). Here's how to use it   s id=\"connected\" items=  'JavaScript', 'React'     ' '    id='connected-0'>   js import   hmsStore   from './hms'; import   selectIsConnectedToRoom   from '@100mslive/hms-video-store'; // use getState to get the state at any time console.log('isConnected  ', hmsStore.getState(selectIsConnectedToRoom)); function onRoomStateChange(connected)     console.log('isConnected  ', connected);   // you can also subscribe to the state and get your function called whenever the state changes hmsStore.subscribe(onRoomStateChange, selectIsConnectedToRoom);   </Tab>   id='connected-1'>   jsx import   useHMSStore, selectIsConnectedToRoom   from '@100mslive/react-sdk'; // if the connection state changes, the store hook will take care of rerendering the component function ConnectionState()     const isConnected = useHMSStore(selectIsConnectedToRoom);   return <div> isConnected ? 'connected' : 'not connected, please join.' </div>;     </Tab> > For more granular > room states, use   selectRoomState  (/api-reference/javascript/v2/home/content select-room-state). "
    },
    {
        "title": "Leave Room",
        "link": "/javascript/v2/how--to-guides/set-up-video-conferencing/leave",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/set-up-video-conferencing/leave",
        "keywords": [],
        "content": "  Leaving a room should be quick. Once a user wishes to end their interaction in the room, they can choose to leave the meeting. 👋 We provide a simple, no fuss API to do exactly that. Invoke the leave method using hmsActions . You can use the current  room state (./join getting-current-room-state) to render the UI accordingly post leave.   jsx hmsActions.leave()     Leave on Navigating Away If the user closes the tab without calling leave you might seem them as stuck for a few seconds for remote peers. This is because our server assumes that there might be a network issue and keeps a buffer waiting for the peer to rejoin. For this reason, it is important to call the leave api if we detect the user navigating away from the page. This includes, refreshing or closing the tab. We can set up leave to be called on window unload event.   js window.addEventListener('beforeunload', () => hmsActions.leave()); window.addEventListener('onunload', () => hmsActions.leave());   "
    },
    {
        "title": "Mute / Unmute",
        "link": "/javascript/v2/how--to-guides/set-up-video-conferencing/mute",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/set-up-video-conferencing/mute",
        "keywords": [],
        "content": "  Mute is something that applies to both audio and video. When you mute audio, you can't be heard by other people. When you mute video, you will not be broadcasting your video to other people.   js await hmsActions.setLocalVideoEnabled(false); await hmsActions.setLocalAudioEnabled(true);     Toggling current mute state 1. Get current state 2. Invert the state and set as new value  s id=\"mute\" items=  'JavaScript', 'React'       id='mute-0'>   js async function toggleAudio()     const enabled = hmsStore.getState(selectIsLocalAudioEnabled);   await hmsActions.setLocalAudioEnabled( enabled);   async function toggleVideo()     const enabled = hmsStore.getState(selectIsLocalVideoEnabled);   await hmsActions.setLocalVideoEnabled( enabled);     </Tab>   id='mute-1'>   jsx function MyComponent()    const audioEnabled = useHMSStore(selectIsLocalAudioEnabled);  const videoEnabled = useHMSStore(selectIsLocalVideoEnabled);  const hmsActions = useHMSActions();  async function toggleAudio()     await hmsActions.setLocalAudioEnabled( audioEnabled);     async function toggleVideo()     await hmsActions.setLocalVideoEnabled( videoEnabled);     return <     // toggle buttons for user   ;     </Tab> "
    },
    {
        "title": "Change Peer Name",
        "link": "/javascript/v2/how--to-guides/set-up-video-conferencing/peer-name",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/set-up-video-conferencing/peer-name",
        "keywords": [],
        "content": "  Any peer can change their own name before or after joining a room. Before joining, the name would have to be specified in HMSConfig that is passed to the join method. After joining, name can be changed with this call to hmsActions    jsx await hmsActions.changeName(newName);   The store will update on the remote side, and as a result corresponding UI will also be updated automatically. A notification of type HMSNotificationTypes.NAME_UPDATED will also be sent in case a toast needs to be presented. "
    },
    {
        "title": "Preview Screen",
        "link": "/javascript/v2/how--to-guides/set-up-video-conferencing/preview",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/set-up-video-conferencing/preview",
        "keywords": [],
        "content": "  Preview screen is a frequently used UX element which allows users to check if their input devices are working properly and set the initial state (mute/unmute) of their audio and video tracks before joining. 100ms SDKs provide an easy-to-use API to back this feature. Additionally, the SDK will try to establish a connection to 100ms server to verify there are no network issues and that the auth credentials are valid so that if everything is in order the subsequent room join is instant. To invoke this API call   js const config =     userName: 'Jon Snow',   authToken: '<Auth token>', // client-side token generated from your token service   settings:       // initial states     isAudioMuted: true,     isVideoMuted: false    ,   metaData: JSON.stringify(  city: 'Winterfell', knowledge: 'nothing'  ),   rememberDeviceSelection: true, // remember manual device change   captureNetworkQualityInPreview: false // whether to measure network score in preview  ; await hmsActions.preview(config);   You would need the same config object that you would pass to  join API (./join). The interface to render video, mute/unmute etc. remains the same. > Note that if you use preview, there is no need of maintaining any state on your side for settings(device id, audio, video on). Selections > during preview will be moved across to the join call.   Post Preview call Once you have made the preview call, these things will happen    SDK will connect with the 100ms backend to ensure that the token is correct and fetch the permissions for the role.   Network speed will be measured and a connection score will be notified to the UI if  turned on (../advanced-features/connection-quality).   One peer for the local peer will be added to the store, accessible via selectLocalPeer selector.   Permission prompt will be given to the user for camera/mic if permissions are not there   Audio, Video tracks will be created as per the role permissions.   The video can be rendered  similarly as post join (./render-video).    Audio Level (../advanced-features/audio-level) will start reflecting for the microphone, this can be used to verify that   the microphone is working properly.   User will be able to see and modify input/output  devices (./device-change).   When you do join When join is done after the preview call, changes done by the user during preview such as changing device or muting their tracks will be carry forwarded when user enters the room.   Limitations   You can't preview and join with two different roles, in case you do need to do this, you can call leave and wait on the promise   to resolve before calling join.   Know Others in the room It's also possible to know about the room state and other peers in the room, if the permission to receive room state is given for the role in dashboard. The peers present in the room can be known by using the selectPeers selector as given  here (../guides/useful-selectors who-all-are-in-my-room).  s id=\"peers\" items=  'Javascript', 'React'       id='peers-0'>   js const peers = hmsStore.getState(selectRemotePeers);   </Tab>   id='peers-1'>   jsx const peers = useHMSStore(selectRemotePeers);   </Tab>   Network Quality in preview Follow  this section (../advanced-features/connection-quality connection-quality-score-in-preview) to get a sense of user's network quality in preview. "
    },
    {
        "title": "Screen Share",
        "link": "/javascript/v2/how--to-guides/set-up-video-conferencing/screen-share",
        "platformName": "JavaScript",
        "objectID": "/javascript/v2/how--to-guides/set-up-video-conferencing/screen-share",
        "keywords": [],
        "content": "  Screenshare involves sharing either the complete screen, a specific window or, a browser tab. For a peer to share their screen, their role must have screenshare enabled in the dashboard.   ScreenshareDashboard (/docs/v2/screenshare-dashboard.png) To start a screenshare:   js try     await hmsActions.setScreenShareEnabled(true);   catch (error)     // an error will be thrown if user didn't give access to share screen       AudioOnly Screenshare To start audio only screenshare, you can pass a config as second argument.   js try     await hmsActions.setScreenShareEnabled(true,       audioOnly: true    );   catch (error)     // an error will be thrown if user didn't give access to share screen       VideoOnly Screenshare To start video only screenshare, you can pass a config as second argument.   js try     await hmsActions.setScreenShareEnabled(true,       videoOnly: true    );   catch (error)     // an error will be thrown if user didn't give access to share screen     To stop the screenshare:   js hmsActions.setScreenShareEnabled(false);     Useful Selectors   js // to know if someone is screensharing const screenshareOn = hmsStore.getState(selectIsSomeoneScreenSharing); // to get the HMSPeer object of the peer screensharing, will select first if multiple screenshares const presenter = hmsStore.getState(selectPeerScreenSharing); // to get the HMSPeer object of all the peers screensharing const presenters = hmsStore.getState(selectPeersScreenSharing); // a boolean to know if the local peer is the one who is screensharing const amIScreenSharing = hmsStore.getState(selectIsLocalScreenShared); // to get the screenshare video track, this can be used to call attachVideo for rendering const screenshareVideoTrack = hmsStore.getState(selectScreenShareByPeerID(presenter.id)); // Get the peer who is sharing audio only screenshare const peer = hmsStore.getState(selectPeerSharingAudio); // Get the audio track of audio Only screenshare const audioTrack = hmsStore.getState(selectScreenShareAudioByPeerID(peer?.id));   Find more  here (/api-reference/javascript/v2/home/content functions).   Screenshare with audio: We also give support of adding screenshare with audio in chromium based browsers. This only applies if checkbox at bottom left shown below is checked while sharing a browser tab.   ScreenshareAudio (/docs/v2/screenshare-audio.png) "
    },
    {
        "title": "Capture HmsView",
        "link": "/react-native/v2/advanced-features/capture-hmsView",
        "platformName": "React Native",
        "objectID": "/react-native/v2/advanced-features/capture-hmsView",
        "keywords": [],
        "content": "  > This is an Android only feature. The 100ms React Native SDK provides HmsView component that renders the video on screen. Check out  Render Video docs (../features/render-video) to know more about HmsView component. You render videos of yourself (Local Peer) and remote peers on screen using this component. Sometimes, you may want to take a screenshot (or capture frame) only from video of a single remote peer instead of whole screen. You can use capture method available on the ref( React Refs docs (https://reactjs.org/docs/refs-and-the-dom.html)) of HmsView component to capture frame of video currently rendered by this component. capture method call resolves with base64 string which is the captured frame of the video stream. > You also need  \"External Storage Permission (WRITE_EXTERNAL_STORAGE)\" (https://developer.android.com/reference/android/Manifest.permission WRITE_EXTERNAL_STORAGE) to capture frame of video stream. This is how you can use capture method to capture frame of peers' video stream    js import React from 'react'; import   TouchableOpacity, Text, View   from 'react-native'; // This Component handles rendering of HmsView export const PeerTile = () =>    // this ref will be reference to HmsView component  const hmsViewRef = React.useRef(null);  // This function handles capturing frame of the video stream  const captureVideoFrame = async () =>     try      // First -> request for WRITE_EXTERNAL_STORAGE permission before calling capture method on ref    const base64Data = await hmsViewRef.current?.capture();    //  base64Data  is the captured frame of the video stream     catch (error)      console.log(error);       ;  // hmsInstance acquired from build method  // HmsView is available on HMSSDK instance.  const HmsView = hmsInstance.HmsView;  return (   <View>     /  HmsView rendering video of a Peer  /     <HmsView     ref= hmsViewRef  // this ref can be used to capture frame of video stream with current tracKId     key= trackId      trackId= trackId      style=   height: '100%', width: '100%'             /  Button to press when you want to take screenshot of peers' video  /     <TouchableOpacity onPress= captureVideoFrame >     <Text>Take Screenshot</Text>    </TouchableOpacity>   </View>  );     "
    },
    {
        "title": "Persistent Participant States (Peer Metadata)",
        "link": "/react-native/v2/advanced-features/change-metadata",
        "platformName": "React Native",
        "objectID": "/react-native/v2/advanced-features/change-metadata",
        "keywords": [],
        "content": "  Looking for a persistent state that can be set on a peer and updated anytime, for everyone in the room? Peer metadata it is. Metadata can be set initially in the HMSConfig object that's passed into the join method. This can be used to display profile pictures as the user's avatar in meetings. You can imagine metadata as a persistent object attached to the peer which has more details about them. Peer Metadata can be changed after joining the Room as well.  To set metadata before joining the room, pass the metadata property to HMSConfig used to Join the Room.   js const config = new HMSConfig(           authToken: 'eyJH5c...',          username: 'John Appleseed',          metadata: ' \"avatar\": \"location/on/amazon/storage\" '          );   This section will show you how to: 1.  Get Peer Metadata ( get-peer-metadata). 2.  How to respond when a remote peer changes its metadata ( how-to-listen-to-updates-when-metadata-of-any-peer-is-updated). 3.  How to set a peer's metadata ( set-peer-metadata).   Get Peer Metadata Peer Metadata is available on HMSPeer object of that peer. You can read the metadata property on any HMSPeer instance to get metadata of that peer.   js // Getting Local Peer object const localPeer = await hmsInstance.getLocalPeer(); // Getting \"metadata\" of local peer const localPeerMetaData = localPeer.metadata;     How to listen to updates when metadata of any peer is updated Whenever a peers' metadata is updated, HMSUpdateListenerActions.ON_PEER_UPDATE event is emitted with HMSPeerUpdate.METADATA_CHANGED update type. You can subscribe to this event and update the UI for updated peer.   js const onPeerListener = (data:   peer: HMSPeer; type: HMSPeerUpdate;  ) =>    const   peer, type   = data;  if (type === HMSPeerUpdate.METADATA_CHANGED)     // Metadata for the  peer  is changed   // Update UI for the  peer    // Parsing the updated Metadata   const metadata = JSON.parse(peer.metadata);     ; hmsInstance.addEventListener(HMSUpdateListenerActions.ON_PEER_UPDATE, onPeerListener);     Set Peer Metadata You can use changeMetadata method available on HMSSDK instance to change the metadata of yourself (Local Peer).   js try    const metadata =   zodiacSign: \"virgo\"  ;  const metadataString = JSON.stringify(metadata);  hmsInstance.changeMetadata(metadataString);  console.log('Change Metadata Success');   catch (error)    console.log('Change Metadata Error: ', error);       Use cases    Hand Raise This is a very common requirement where the people attending a webinar or online class wish to ask a question. They can let the speaker know by raising their hands in the application similar to what they would have done in an offline event. Let's understand this with a diagram:   hand-raise (/docs/v2/flutter-raise-hand-flowchart.png) Let's check the implementation step-by-step: <div className=\"steps-container\">    PeerA changes Metadata Peer A calls changeMetadata with metadata as    js try    const metadata =   isHandRaised: true  ;  const metadataString = JSON.stringify(metadata);  hmsInstance.changeMetadata(metadataString);  console.log('Change Metadata Success');   catch (error)    console.log('Change Metadata Error: ', error);        All the peers receives Metadata update As soon as changeMetadata method call in above step is successful, All the peers in the room will get HMSUpdateListenerActions.ON_PEER_UPDATE event with HMSPeerUpdate.METADATA_CHANGED update type. Let's see how we can update the UI using this:   js const onPeerListener = (data:   peer: HMSPeer; type: HMSPeerUpdate;  ) =>    const   peer, type   = data;  if (type === HMSPeerUpdate.METADATA_CHANGED)     // We can get the metadata from HMSPeer object   // Parsing the updated Metadata   const metadata = JSON.parse(peer.metadata);   // Metadata for the  peer  is changed, Update Hand Raised UI for the  peer    console.log(metadata.isHandRaised);     ; hmsInstance.addEventListener(HMSUpdateListenerActions.ON_PEER_UPDATE, onPeerListener);   </div>   Polls We can use peers' metadata property to implement polls similar to what we did in above implementation.  There are many more use cases which can be implemented using peer metadata. Have any questions regarding the use cases please reach out to us  here (https://100ms.live/discord) "
    },
    {
        "title": "Echo Cancellation\r",
        "link": "/react-native/v2/advanced-features/echo-cancellation",
        "platformName": "React Native",
        "objectID": "/react-native/v2/advanced-features/echo-cancellation",
        "keywords": [],
        "content": " -\r \r The 100ms SDK automatically applies the best-known settings to cancel echoes from devices. It prefers to use the device's hardware for echo cancellation over software. Therefore, if a device has hardware for echo cancellation, the SDK will use it.\r \r We have found that some Android devices have issues with their hardware echo cancellation, even though they have the hardware, it may be faulty. In such cases, we must rely on software-based echo cancellation on such devices.\r Although we continuously update the SDK with the latest known devices that have this issue, you may encounter one that we haven't tested yet.\r \r If you find that an Android device has the aforementioned hardware echo cancellation issue, you may try using software-based echo cancellation instead of relying on the device's hardware to solve the problem.\r \r The SDK provides you with an option to disable 'hardware echo cancellation' and use software-based echo cancellation on the device. You can create an instance of the HMSAudioTrackSettings class with the useHardwareEchoCancellation property set to false . When the useHardwareEchoCancellation property is set to false , the SDK will not use hardware echo cancellation. Then, you can use this HMSAudioTrackSettings instance when building an instance of the HMSDK class using the static build method.\r \r  js 3 \r // Creating Audio Setting with 'hardware echo cancellation' set to false\r const audioSettings = new HMSAudioTrackSettings( \r  useHardwareEchoCancellation: false\r  );\r \r // Creating Track settings with Audio Settings for HMSSDK\r const trackSettings = HMSTrackSettings( \r  audio: audioSettings\r  );\r \r // Pass the Track Settings object to the build function\r // Now, the SDK will not use hardware echo cancellation\r c̶o̶n̶s̶t̶ ̶h̶m̶s̶I̶n̶s̶t̶a̶n̶c̶e̶ ̶=̶ ̶a̶w̶a̶i̶t̶ ̶H̶M̶S̶S̶D̶K̶.̶b̶u̶i̶l̶d̶(̶)̶;̶\r const hmsInstance = await HMSSDK.build(  trackSettings  );\r  \r \r > 💡 Note: It is important to note that hardware echo cancellation must be turned off based on specific device  models (https://developer.android.com/reference/android/os/Build MODEL). Disabling it for all devices can result in echoing on devices that did not experience it before. It is important to use this feature selectively only on devices that require it.\r "
    },
    {
        "title": "Getter Methods",
        "link": "/react-native/v2/advanced-features/get-methods",
        "platformName": "React Native",
        "objectID": "/react-native/v2/advanced-features/get-methods",
        "keywords": [],
        "content": "  The HMSSDK instance provides several 'Getter' methods for obtaining the latest data of a Room, which can be used to update stale data in the app and re-render the user interface. Let's look at the methods one by one:   getRoom The getRoom method is used to obtain information about the room you are currently in. This method returns a promise that resolves with an HMSRoom object.   js 3  try    // Latest HMSRoom object  const room: HMSRoom = await hmsInstance.getRoom();  console.log('getRoom Success: ', room);   catch(error)    console.log('getRoom Error: ', error);       getSessionMetaData The getSessionMetaData method is used to retrieve the currently set session metadata. If no metadata has been set on the session, this method returns null .   js 2  try    const sessionMetadata: string  null = await hmsInstance.getSessionMetaData();  console.log('getSessionMetaData Success: ', sessionMetadata);   catch (error)    console.log('getSessionMetaData Error: ', error);     The setSessionMetaData method can be used to set session metadata.   getRoles The getRoles method is used to retrieve all of the roles available in the current room. This method returns a promise that resolves with a list of HMSRole objects.   js 2  try    const roles: HMSRole   = await hmsInstance.getRoles();  console.log('getRoles Success: ', roles);   catch (error)    console.log('getRoles Error: ', error);       getLocalPeer The getLocalPeer method is used to retrieve the updated local peer object. This method returns a promise that resolves with a HMSLocalPeer object.   js 2  try    const localPeer: HMSLocalPeer = await hmsInstance.getLocalPeer();  console.log('getLocalPeer Success: ', localPeer);   catch (error)    console.log('getLocalPeer Error: ', error);       getRemotePeers The getRemotePeers method is used to retrieve all of the peers present in the current room. This method returns a promise that resolves with a list of HMSRemotePeer objects.   js 2  try    const remotePeers: HMSRemotePeer   = await hmsInstance.getRemotePeers();  console.log('getRemotePeers Success: ', remotePeers);   catch (error)    console.log('getRemotePeers Error: ', error);       isScreenShared The isScreenShared method is used to retrieve the current 'screen sharing' status of the local peer. This method returns a promise that resolves to true if screen is being shared and false otherwise. > You can learn more about the screen sharing feature in the  Screenshare (../features/screenshare) documentation.   js 2  try    const screenShareStatus: boolean = await hmsInstance.isScreenShared();  console.log('isScreenShared Success: ', screenShareStatus);   catch (error)    console.log('isScreenShared Error: ', error);       isPipModeSupported  Android Only  The isPipModeSupported method is used to determine if the Android device supports Picture-In-Picture mode. This method returns a promise that resolves to true if PIP mode is supported and false otherwise. > You can refer to the  Picture-In-Picture mode (./pip-mode) documentation to learn more about this feature.   js 2  try    const isPIPSupported = await hmsInstance.isPipModeSupported();  console.log('isPipModeSupported Success: ', isPIPSupported);   catch (error)    console.log('isPipModeSupported Error: ', error);       getAudioDevicesList  Android Only  The getAudioDevicesList method is used to retrieve a list of available audio output devices on the device. This method returns a promise that resolves with a list of HMSAudioDevice enum values. Note that this method is only available on the Android platform. > You can refer to the  Audio Output Routing (../features/audio-output-routing) documentation to learn more about this feature.   js 12  export enum HMSAudioDevice    SPEAKER_PHONE = 'SPEAKER_PHONE',  WIRED_HEADSET = 'WIRED_HEADSET',  EARPIECE = 'EARPIECE',  BLUETOOTH = 'BLUETOOTH',  AUTOMATIC = 'AUTOMATIC',   ... try    const audioDevicesList: HMSAudioDevice   = await hmsInstance.getAudioDevicesList();  console.log('getAudioDevicesList Success: ', audioDevicesList);   catch (error)    console.log('getAudioDevicesList Error: ', error);       getAudioOutputRouteType  Android Only  The getAudioOutputRouteType method is used to retrieve the 'current audio output route type' on the device. This method returns a promise that resolves with HMSAudioDevice enum value. Note that this method is only available on the Android platform. > You can refer to the  Audio Output Routing (../features/audio-output-routing) documentation to learn more about this feature.   js 12  export enum HMSAudioDevice    SPEAKER_PHONE = 'SPEAKER_PHONE',  WIRED_HEADSET = 'WIRED_HEADSET',  EARPIECE = 'EARPIECE',  BLUETOOTH = 'BLUETOOTH',  AUTOMATIC = 'AUTOMATIC',   ... try    const currentAudioOutputDevice: HMSAudioDevice = await hmsInstance.getAudioOutputRouteType();  console.log('getAudioOutputRouteType Success: ', currentAudioOutputDevice);   catch (error)    console.log('getAudioOutputRouteType Error: ', error);       isAudioShared  Android Only  The isAudioShared method is used to retrieve the current 'audio sharing' status of the local peer. This method returns a promise that resolves to true if audio is being shared and false otherwise. > You can learn more about the audio sharing feature on both iOS and Android platforms in the  Audio Share (../features/audio-share) documentation.   js 2  try    const audioShareStatus: boolean = await hmsInstance.isAudioShared();  console.log('isAudioShared Success: ', audioShareStatus);   catch (error)    console.log('isAudioShared Error: ', error);       getAudioMixingMode  Android Only  The getAudioMixingMode method is used to retrieve the current 'Audio Streaming mode' of the local peer. This method returns a promise that resolves with a value of the HMSAudioMixingMode enum. Audio Streaming mode of the local peer can be of following types: 1. HMSAudioMixingMode.TALK_ONLY : Only data captured by the microphone will be streamed in the room. 2. HMSAudioMixingMode.TALK_AND_MUSIC : Data captured by the microphone and the audio currently being played back on the device will be streamed in the room. 3. HMSAudioMixingMode.MUSIC_ONLY : Only the audio currently being played back on the device will be streamed in the room. Note that this method is only available on the Android platform. > You can learn more about Audio Mixing Mode in  Audio Share (../features/audio-share android) docs of Android platform.   js 10  export enum HMSAudioMixingMode    TALK_ONLY = 'TALK_ONLY',  TALK_AND_MUSIC = 'TALK_AND_MUSIC',  MUSIC_ONLY = 'MUSIC_ONLY',   ... try    const currentAudioMixingMode: HMSAudioMixingMode = await hmsInstance.getAudioMixingMode();  console.log('getAudioMixingMode Success: ', currentAudioMixingMode);   catch (error)    console.log('getAudioMixingMode Error: ', error);     "
    },
    {
        "title": "Mini View",
        "link": "/react-native/v2/advanced-features/mini-view",
        "platformName": "React Native",
        "objectID": "/react-native/v2/advanced-features/mini-view",
        "keywords": [],
        "content": "  The 100ms React Native SDK provides the HmsView component, which renders video on the screen. You can refer to the  Render Video docs (../features/render-video) to learn more about the HmsView component. Typically, all HmsView s should be rendered side by side in a list or grid layout, which is also the default layout behavior of the HmsView component. However, if you desire a mini view / main view layout (like Picture-in-Picture within the app) where one or more HmsView components are not in the normal flow of the layout and are overlapping other HmsView s, then this special layout can be achieved by using the setZOrderMediaOverlay prop on the HmsView component. The setZOrderMediaOverlay prop enables the 'Picture-in-picture' feature for the HmsView component, where one HmsView is rendered on top of another HmsView by the SDK. By setting the setZOrderMediaOverlay prop to true on a smaller HmsView, you can place it on top of another larger HmsView. This creates a layout similar to a picture-in-picture layout shown  here ( example-of-mini-view-layout).   js 8  // hms instance acquired from build method const HmsView = hmsInstance.HmsView; <HmsView  key= trackId   trackId= trackId   mirror= true   setZOrderMediaOverlay= true  // should be passed true only for the mini view  style=   height: '100%', width: '100%'          Example of mini view layout   Mini View (/docs/v2/mini-view.png) "
    },
    {
        "title": "Connection Quality",
        "link": "/react-native/v2/advanced-features/network-quality",
        "platformName": "React Native",
        "objectID": "/react-native/v2/advanced-features/network-quality",
        "keywords": [],
        "content": "    Network Icons (/docs/v2/network-icons.png) Video/Audio conferencing is by nature a data intensive operation. The 100ms SDK attempts to stabilize connections especially if subscribe degradation is turned on in the template but it's possible for really bad connections that users will still have problems. It can be helpful to measure a user's connection speed before joining a room in order to set expectations or decide to have them join with video off etc. Letting the participants know of each other's connection status is also a great value addition. Once you have joined the room, you can get a network quality score for all the peers including yourself. You can also get current user's connection quality score when in preview. The score ranges from -1 to 5, 5 being the hightest(Good Network) and 0 being the lowest(No Network). -1 indicates an undefined state that is either the score hasn't yet been determined or it couldn't be determined. > ⚠️ The downlink speed is measured by having the user download a file (1mb as of this writing) after the websocket connection is established during a preview. The download will be continued for at most a fixed number of seconds (eg: 10 seconds) and the speed during that interval is calculated. The entire file may not be downloaded if it exceeds the timeout. DNS time is not counted in this, only throughput is measured.   Requesting/Reading Network Quality Updates Network Quality information for every peer is available on networkQuality property of HMSPeer object. networkQuality property value is an instance of HMSNetworkQuality class    js interface HMSNetworkQuality    downlinkQuality: number;     On networkQuality property value, we have downlinkQuality property which represents the peers' network quality. This is a number ranging from -1 to 5. To know about the meaning of each value refer to  Interpreting the Values ( interpreting-the-values) section. Whenever network quality of a peer changes, HMSUpdateListenerActions.ON_PEER_UPDATE event is emitted with HMSPeerUpdate.NETWORK_QUALITY_UPDATED update type. Then inside the function subscribed to this event, you can can check networkQuality property on the updated HMSPeer object and update the UI accordingly.   js const onPeerListener = (data:   peer: HMSPeer; type: HMSPeerUpdate;  ) =>    const   peer, type   = data;  // Network quality changed for  peer   if (type === HMSPeerUpdate.NETWORK_QUALITY_UPDATED)     console.log( Network Quality of $ peer.name : $ peer.networkQuality?.downlinkQuality  );     ; hmsInstance.addEventListener(HMSUpdateListenerActions.ON_PEER_UPDATE, onPeerListener);     Requesting/Reading Network Quality Updates in Preview When creating a HMSConfig object to request a preview, You have to set the captureNetworkQualityInPreview property to true to get the users' downlink network quality in preview room state. When information is available, HMSUpdateListenerActions.ON_PEER_UPDATE event is emitted with HMSPeerUpdate.NETWORK_QUALITY_UPDATED update type. Rest is same as mentioned in  above ( requesting-reading-network-quality-updates) section.   js import   HMSConfig   from '@100mslive/react-native-hms'; const config = new HMSConfig(   authToken: '...jgvc...',  username: 'John Applesead',  captureNetworkQualityInPreview: true,  ); // hms instance acquired by build method hmsInstance.preview(config);   Refer to  Preview Guide (../features/preview) to learn more about room preview   Interpreting the Values  Peer.networkQuality?.downlinkQuality will be a value in range -1 to 5.   -1 -> Undefined  yet to be determined or not enough data to determine   0 -> Disconnected or error in measuring score(in preview)   1 -> Very Bad Connection   2 -> Bad Connection   3 -> Moderate Connection   4 -> Good Connection   5 -> Excellent Connection > 0 score in preview could also mean a failure in measuring the network due to firewall block if the internet otherwise looks good.   Showing in the UI You can show this as a network icon on every peer tile or show only a list of peers not having good connection. Feel free to checkout how the code from our dashboard app's implementation  here (https://github.com/100mslive/react-native-hms/tree/main/example) where we show a network bar on each peer's tile as well in the participant list with a tooltip describing the connection state. "
    },
    {
        "title": "PIP (Picture-In-Picture) Mode",
        "link": "/react-native/v2/advanced-features/pip-mode",
        "platformName": "React Native",
        "objectID": "/react-native/v2/advanced-features/pip-mode",
        "keywords": [],
        "content": "  100ms React Native SDK provides support for creating Picture in Picture mode experience for video calls. PIP Mode lets the user watch the room video in a small window pinned to a corner of the screen while navigating between apps or browsing content on the main screen. Currently this functionality is only available on Android.   Minimum Requirements   Minimum version required to support PiP is Android 8.0 (API level 26)   Minimum react-native-hms SDK version required is 1.0.0   How to add PiP support 1. You need to update the activity tag in the AndroidManifest.xml   xml  <activity   ....   android:supportsPictureInPicture=\"true\"   android:configChanges=\"keyboard keyboardHidden orientation screenSize smallestScreenSize screenLayout uiMode\"   ...     2. Call enablePipMode method on \"HMS Instance\" when you want to start PIP mode.   js  // You can also check if PIP is supported on device or not.  const isPIPSupported = await hmsInstance.isPipModeSupported();  const isEnabled = await hmsInstance.enablePipMode();  if (isEnabled)     // App has entered into PIP Mode   setPIPModeActive(true);      You can also add aspectRatio for Picture in Picture window.   js  ...  const isEnabled = await hmsInstance.enablePipMode(  aspectRatio:  16, 9   );  ...   Pip mode resizes your whole activity to a small container. So, you should hide the content that you don't want to show. 3. Detect when App goes to \"fullscreen\" from PIP mode   js  import   AppState, AppStateStatus   from 'react-native';  ...  const appStateRef = useRef(AppState.currentState);  useEffect(() =>     // We will register an AppState listener when App is in PIP Mode   if (pipModeActive)      appStateRef.current = AppState.currentState;    const appStateListener = (nextAppState: AppStateStatus) =>       if (      appStateRef.current.match(/inactive background/) &&      nextAppState === \"active\"     )        // Now, App is not in PIP mode      setPIPModeActive(false);           appStateRef.current = nextAppState;     ;    AppState.addEventListener('change', appStateListener);    return () =>       AppState.removeEventListener('change', appStateListener);            ,  pipModeActive );     Example of showing content when App is in PIP Mode   js  // Getting trackId to show in PIP mode  // getPreferredTrackId function returns trackId of preferred video track  const pipTrackId = getPreferredTrackId()  ...  // If PIP mode is active, showing only single peer  if (isPipModeActive)     return (    <HmsView     ...     trackId= pipTrackId      ...        )      // Showing tiles for all the peers  return <MultiplePeers    ...     Checkout Video of PIP in Action <video width=\"295\" height=\"640\" controls>  <source src=\"https://user-images.githubusercontent.com/45194090/205296059-6b9b399d-9a51-4d35-99cf-4413f6368169.mp4\" type=\"video/mp4\"    Your browser does not support the video tag. </video> 👀 To see an example of Picture in Picture implementation in our example app, checkout  our example project (https://github.com/100mslive/react-native-hms/tree/main/example). "
    },
    {
        "title": "Set Volume",
        "link": "/react-native/v2/advanced-features/set-volume",
        "platformName": "React Native",
        "objectID": "/react-native/v2/advanced-features/set-volume",
        "keywords": [],
        "content": "  Sometimes during calls, we need to deafen or turn up a particular peer's volume. For example, suppose a remote peer is too loud compared to others, you may want reduce their volume level and vice-versa. 100ms SDK gives you option to adjust volume of a remote peer's audio. The change in volume level is only effective locally, remote peers remains unaffected. That means only you will hear the lower/higher volume of that Peer. Other peers in Room will continue to listen at the default volume level as before. You can use setVolume method available on HMSSDK instance to change volume levels of a remote peer. This method accepts 2 parameters as follows  1. track : HMSTrack object of the   audio track   of   remote peer  . 2. volume : New volume level in number . It can be in range of 0 (min) to 10 (max). The default value is 1.0 . An example of using setVolume method when track is added    js const onTrackListener = (   peer,  track,  type,  :    peer: HMSPeer;  track: HMSTrack;  type: HMSTrackUpdate;  ) =>    if(   type === HMSTrackUpdate.TRACK_ADDED && // When track is added   track.type === HMSTrackType.AUDIO && // Making sure track is audio track   peer.isLocal === false // Making sure peer is remote peer  )     // Increasing volume level of newly added audio track of remote peer   hmsInstance.setVolume(track, 10.0);     ; hmsInstance.addEventListener(HMSUpdateListenerActions.ON_TRACK_UPDATE, onTrackListener);   > Volume level can vary from 0(min) to 10(max). The default value for volume would be 1.0 "
    },
    {
        "title": "Show Audio Levels",
        "link": "/react-native/v2/advanced-features/show-audio-level",
        "platformName": "React Native",
        "objectID": "/react-native/v2/advanced-features/show-audio-level",
        "keywords": [],
        "content": "    Getting Audio Levels for all speaking peers We can find the currently speaking peers by subscribing to HMSUpdateListenerActions.ON_SPEAKER event as follows    js import    HMSUpdateListenerActions,  HMSSpeaker,   from '@100mslive/react-native-hms'; // onSpeaker function will be invoked whenever active speaker changes in room const onSpeaker = (data: HMSSpeaker  ) =>    // data is the list of HMSSpeaker objects  data.forEach((speaker: HMSSpeaker) => console.log('speaker audio level: ', speaker.level));   ; // hmsInstance obtained by build method hmsInstance.addEventListener(HMSUpdateListenerActions.ON_SPEAKER, onSpeaker);   Let's look at the HMSSpeaker class:   js interface HMSSpeaker    level: Number;  peer: HMSPeer;  track: HMSTrack;     Let's understand the properties of HMSSpeaker class: 1. level : Number -> The level of the audio. It may vary from 0-100. A higher value indicates a higher speaking volume. 2. peer : HMSPeer -> The peer who was speaking. This may be null if the peer has left before the update information has reached the other person. 3. track : HMSTrack -> The audio track corresponding to this speaker. It may be null when the speaker who was speaking loudly, leaves.   Where can we use Active Speakers Updates   Active Speaker Views To maintain an active speaker view, such as the default view in the  sample app (https://github.com/100mslive/react-native-hms/tree/develop/sample-apps/rn-quickstart-app), you need to keep track of active speakers over time. We'll be using the Active Speakers list which we get in function subscribed to HMSUpdateListenerActions.ON_SPEAKER event as mentioned above and then building something that attempts to show all the active speakers while minimizing re-ordering the list.   Dominant Speaker  the loudest speaker The Active Speakers list which we get in function subscribed to HMSUpdateListenerActions.ON_SPEAKER event is already in descending order based on the audio level. So, the first element of the list will be loudest speaker. "
    },
    {
        "title": "Strict Privacy Applications",
        "link": "/react-native/v2/advanced-features/strict-privacy-applications",
        "platformName": "React Native",
        "objectID": "/react-native/v2/advanced-features/strict-privacy-applications",
        "keywords": [],
        "content": "  Strict Privacy Applications are those that request permissions only as needed, following the principle of least privilege. This means that the application only requests the minimal set of permissions necessary for its intended functionality, rather than requesting all possible permissions in advance.   Steps to create a Strict Privacy Application    Join in a role without Video and/or Audio permissions  Create three roles (../foundation/templates-and-roles) with permissions as:  Host     host (/docs/v2/flutter-host.png)  Guest    guest (/docs/v2/flutter-guest.png)  Viewer    viewer (/docs/v2/flutter-viewer.png)  When you join a Room with the Viewer role, your app does not need to request microphone and camera permissions from the user because users joining with the Viewer role do not have video and audio publishing permissions. However, when you join with the Host or Guest role, you will need to request permissions before joining the room in advance. > 💡 Note: The roles mentioned above are just for demonstration purposes, you should create your own roles based on your use cases and requirements.    Request Permissions on Role Change If you are requested to change from a non-publishing role to a role that has audio-video publishing permission, the app should request permissions before accepting the role change request, if the permissions were not requested during the initial room join. It is important to note that in case of a   'force'   role change, the app is required to have audio and video permissions in advance. > 💡 Note: We recommend using the  react-native-permissions (https://www.npmjs.com/package/react-native-permissions) package for requesting permissions from user.   js // HMSRoleChangeRequest is a class that contains info about the role change request. interface HMSRoleChangeRequest    requestedBy: HMSPeer; // This is an HMSPeer object which contains info about the peer who performed the role change  suggestedRole: HMSRole; // This is an HMSRole object which contains info about the destination role   ... import   PERMISSIONS, request   from 'react-native-permissions'; const onRoleChangeRequest = async (   requestedBy,  suggestedRole  : HMSRoleChangeRequest) =>    try     // Requesting Camera Permission if new role can publish video   if(suggestedRole.publishSettings?.allowed.contains(\"video\") ?? false)      const cameraResult = await request(PERMISSIONS.ANDROID.CAMERA);       // Requesting Microphone and Bluetooth Permission if new role can publish audio   if(suggestedRole.publishSettings?.allowed.contains(\"audio\") ?? false)      const audioResult = await request(PERMISSIONS.ANDROID.RECORD_AUDIO);    const blConnectResult = await request(PERMISSIONS.ANDROID.BLUETOOTH_CONNECT);       // Open a prompt to Accept or Reject the role change request, OR straightaway accept the request   await hmsInstance.acceptRoleChange();    catch (error)     console.log('onRoleChangeRequest Error: ', error);     ; hmsInstance.addEventListener(  HMSUpdateListenerActions.ON_ROLE_CHANGE_REQUEST,  onRoleChangeRequest );   You can refer to the  Change Role documentation (../features/change-role) to learn more role changes. "
    },
    {
        "title": "Set Track Settings",
        "link": "/react-native/v2/advanced-features/track-settings",
        "platformName": "React Native",
        "objectID": "/react-native/v2/advanced-features/track-settings",
        "keywords": [],
        "content": "  You can customize local peers' Audio & Video track settings before Joining the Room.  These settings are a optional parameter and meant to be passed in the build function of HMSSDK as trackSettings parameter which is a HMSTrackSettings object.   Set Audio Track settings Audio Track settings are defined by HMSAudioTrackSettings class. We have following settings for the Audio Track:  initialState   Property to set the initial state of the Audio track (microphone) i.e. Mute/Unmute before joining the room. By default Audio track is Unmuted meaning microphone is ON.   You can use HMSTrackSettingsInitState enum to set this value.    js  enum HMSTrackSettingsInitState     // If the track needs to be kept mute while joining   MUTED,     // If the track needs to be kept unmute while joining    UNMUTED     ...  // Creating \"Audio Track settings\" config object  const audioSettings = new HMSAudioTrackSettings(    initialState: HMSTrackSettingsInitState.MUTED,   );  // Creating \"Track settings\" config object  const trackSettings = new HMSTrackSettings(    audio: audioSettings,   );  // Pass the Track Settings object to the build function  c̶o̶n̶s̶t̶ ̶h̶m̶s̶I̶n̶s̶t̶a̶n̶c̶e̶ ̶=̶ ̶a̶w̶a̶i̶t̶ ̶H̶M̶S̶S̶D̶K̶.̶b̶u̶i̶l̶d̶(̶)̶;̶  const hmsInstance = await HMSSDK.build(  trackSettings  );  ...  // add Event Listeners to subscribe to Join Success or Failure updates  hmsInstance.addEventListener(HMSUpdateListenerActions.ON_ERROR, onError);  hmsInstance.addEventListener(HMSUpdateListenerActions.ON_JOIN, onJoin);  // Join Room with above created config  hmsInstance.join(config);      More info about this can be found in  Join with Muted Audio (../features/join join-with-muted-audio-video) docs.  For more info about Muting and Unmuting Audio Track, check out  Mute and Unmute (../features/mute) docs.  useHardwareEchoCancellation  Android Only   Property to enable Hardware echo cancellation. By default, it's set to true if the device supports it. Please note that on some devices the hardware wrongly reports the HW echo canceler to be present whereas it does not work. In such cases, the application needs to set this to false, so that SW echo canceler is picked up. This setting uses the phone's Acoustic echo Cancellation instead of relying on the SDK's software-based implementation.  audioSource  iOS Only   Property to configure audio nodes for audio sharing. More info about this can be found in  Audio Share (../features/audio-share) docs.  We can create HMSAudioTrackSettings with above mentioned properties as follows    js const audioTrackSettings = new HMSAudioTrackSettings(   initialState: HMSTrackSettingsInitState.UNMUTED,  useHardwareEchoCancellation: true,  audioSource:  'audio_file_player_node' ,  );     Set Video Track settings Video Track settings are defined by HMSVideoTrackSettings class. We have following settings for the Video Track:  initialState   Property to set the initial state of the Video track (camera) i.e Mute/Unmute before joining the room. By default Video track is Unmuted meaning camera is ON.   You can use HMSTrackSettingsInitState enum to set this value.    js  enum HMSTrackSettingsInitState     // If the track needs to be kept mute while joining   MUTED,     // If the track needs to be kept unmute while joining    UNMUTED     ...  // Creating \"Video Track settings\" config object  const videoSettings = new HMSVideoTrackSettings(    initialState: HMSTrackSettingsInitState.MUTED,   );  // Creating \"Track settings\" config object  const trackSettings = new HMSTrackSettings(    video: videoSettings,   );  // Pass the Track Settings object to the build function  c̶o̶n̶s̶t̶ ̶h̶m̶s̶I̶n̶s̶t̶a̶n̶c̶e̶ ̶=̶ ̶a̶w̶a̶i̶t̶ ̶H̶M̶S̶S̶D̶K̶.̶b̶u̶i̶l̶d̶(̶)̶;̶  const hmsInstance = await HMSSDK.build(  trackSettings  );  ...  // add Event Listeners to subscribe to Join Success or Failure updates  hmsInstance.addEventListener(HMSUpdateListenerActions.ON_ERROR, onError);  hmsInstance.addEventListener(HMSUpdateListenerActions.ON_JOIN, onJoin);  // Join Room with above created config  hmsInstance.join(config);     More info about this can be found in  Join with Muted Video (../features/join join-with-muted-audio-video) docs.   For more info about Muting and Unmuting Video Track, check out  Mute and Unmute (../features/mute) docs.  cameraFacing   Property specifies which camera to open while joining. It can be toggled later on. You can use HMSCameraFacing enum to set this value. The default value is HMSCameraFacing.FRONT .  forceSoftwareDecoder  Android Only   This can be used when a lot of videos are rendered at a single time. It is known that the hardware decoder on certain phones doesn't tend to work well with large grids. This may cause an adverse effect like the phone heating up, use this flag only when required. The default value is set as false .  disableAutoResize  Android Only   The SDK intelligently downscales the resolution when publisher's bandwidth is flaky or is CPU bound. This results in a low resolution for the viewers. But if the viewers are persistent, they want the highest resolution at all times, then this setting comes in handy. The default value is set as false .  We can create HMSVideoTrackSettings with above mentioned properties as follows    js const videoTrackSettings = new HMSVideoTrackSettings(   initialState: HMSTrackSettingsInitState.MUTED,  cameraFacing: HMSCameraFacing.BACK,  forceSoftwareDecoder: false,  disableAutoResize: false,  );     Using Track Settings in HMSSDK constructor Here's a sample implementation of adding track settings while initializing 100ms SDK    js import    HMSSDK,  HMSAudioTrackSettings,  HMSVideoTrackSettings,  HMSTrackSettings,  HMSTrackSettingsInitState,  HMSCameraFacing,   from '@100mslive/react-native-hms'; const getTrackSettings = () =>    const audioTrackSettings = new HMSAudioTrackSettings(    initialState: HMSTrackSettingsInitState.UNMUTED,   useHardwareEchoCancellation: true,   audioSource:  'audio_file_player_node' ,   );   const videoTrackSettings = new HMSVideoTrackSettings(    initialState: HMSTrackSettingsInitState.MUTED,   cameraFacing: HMSCameraFacing.BACK,   forceSoftwareDecoder: false,   disableAutoResize: false,   );   return new HMSTrackSettings(    video: videoTrackSettings,   audio: audioTrackSettings,   );   const trackSettings = getTrackSettings(); const build = await HMSSDK.build(  trackSettings  );   "
    },
    {
        "title": "Release Notes",
        "link": "/react-native/v2/changelog/release-notes",
        "platformName": "React Native",
        "objectID": "/react-native/v2/changelog/release-notes",
        "keywords": [],
        "content": "    1.1.1  2022-12-22   Fixed  Corrected the path of SDK Versions JSON file used in Gradle & Podspec files    1.1.0  2022-12-16   Added  Added support for Bulk Role Change  Bulk Role Change is used when you want to convert all Roles from a list of Roles, to another Role.  For example, if peers join a room with a _Waiting_ Role and now you want to change all these peers to _Viewer_ Role then use the changeRoleOfPeersWithRoles API.    js  // fetch all available Roles in the room  const roles = await hmsInstance.getRoles();  // get the Host Role object  const hostRole = roles.find(role => role.name === 'host');  // get list of Roles to be updated  in this case \"Waiting\" and \"Guest\" Roles  const rolesToChange = roles.filter(role => role.name === 'waiting'  role.name === 'guest');  // now perform Role Change of all peers in \"Waiting\" and \"Guest\" Roles to the \"Host\" Role  await hmsInstance.changeRoleOfPeersWithRoles(rolesToChange, hostRole);     For More Information, refer: https://www.100ms.live/docs/react-native/v2/features/change-role   Added HMSPIPConfig to enable buttons on PIP Window on Android  Actionable buttons like Mute/Unmute Audio/Video, Leave Room for local peers can now be configured while enabling PIP Mode.   js  await hmsInstance?.enablePipMode(       aspectRatio:  16, 9 ,  // for 16:9 aspect ratio      endButton: true,       videoButton: true,       audioButton: true,      );     For More Information, refer: https://www.100ms.live/docs/react-native/v2/advanced-features/pip-mode    Deprecated  Deprecated changeRole API in favour of changeRoleOfPeer   No change in functionality or method signature.    Fixed  Corrected Speaker updates by avoiding sending of update event if a peer is not found in room on Android  Microphone not getting captured on Role Change from a non-publishing to publishing Role on iOS  Corrected an issue where on iOS a default Audio Mixer was getting created if Track Settings was passed while building the HMSSDK instance  Updated to Native Android SDK 2.5.4 & Native iOS SDK 0.5.3 Full Changelog:  1.0.0...1.1.0 (https://github.com/100mslive/react-native-hms/compare/1.0.0...1.1.0)    1.0.0  2022-12-01    Added  Added support for Picture in Picture mode on Android.   PIP Mode lets the user watch the room video in a small window pinned to a corner of the screen while navigating between apps or browsing content on the main screen.   Example implementation for checking device support & enabling PIP mode:       js   const isPIPSupported = await hmsInstance.isPipModeSupported();   const isEnabled = await hmsInstance.enablePipMode();        Added Session Metadata APIs.   Session metadata is a shared state that is accessible by everyone in the room.      Session metadata can be used to achieve features like pinned text, spotlight(bringing a particular peer into a larger stage), etc.     Example usage of setting & getting Room's Session Metadata:       js   // set session metadata for the room   hmsInstance.setSessionMetaData(value).then(() =>      // on success, you can send a message to all the peers to fetch metadata    hmsInstance.sendBroadcastMessage('refresh', HMSMessageType.METADATA);    );   // peers in the room can fetch current Session Metadata any time using getSessionMetaData method   const metadata = await hmsInstance.getSessionMetaData();      Added accessing of the local peer from the HMSRoom instance    js   // you can now directly access local peer from HMSRoom object returned in ON_JOIN event listener   const onJoinSuccess = (data:  room: HMSRoom ) =>       console.log('local peer: ', data.room.localPeer);    ;      Added HMSMessageType enum to Message     Fixed  Fixed memory leak on changing props of HMSView  Importing Native Android SDK dependency from Maven Central instead of Jitpack Updated to Native Android SDK 2.5.1 & Native iOS SDK 0.4.7 Full Changelog:  0.9.94...1.0.0 (https://github.com/100mslive/react-native-hms/compare/0.9.94...1.0.0)    0.9.94  2022-10-31   Breaking Change  Removed sending of the list of all peers in onPeerUpdate & onTrackUpdate event listeners. Now, only the specific peer & track for which the change occurred will be sent to the listeners. All peers in the room can be fetched using the getRemotePeers & getLocalPeer functions.    Added  Added option to use Software Decoder for Video rendering on Android devices  Added support for Joining with Muted Audio & Video for local peers  Added better Telemetrics for analytics  Added support for receiving Server-side HMSMessage  Added Maven Central repository to look for Android dependencies  Added HMSLogSettings to configure Native Android SDK logs  Added Utility functions to get all peers  Remote or Local peers & all Roles in the room  Added toggle functions to enable/disable network quality updates    Fixed  Corrected RTMP Streaming & Recording running status  Corrected HMSSubscribeSettings 's roles which are subscribed  Fixed LetterBoxing (Black borders on top and bottom) observed when sharing the screen from Android devices  Corrected Preview callback track types  Removed unused setters for Local Audio & Video Track Settings Updated to Native Android SDK 2.5.1 & Native iOS SDK 0.4.6 Full Changelog:  0.9.93...0.9.94 (https://github.com/100mslive/react-native-hms/compare/0.9.93...0.9.94)   0.9.93  2022-09-27  Corrected params required while building HMSSDK instance  Added the ability to play local audio files from iOS devices  Added ability to share audio playing on iOS devices  Updated HMSException class. Added canRetry attribute  Updated to Native Android SDK 2.4.8 & Native iOS SDK 0.4.2   Full Changelog  :  0.9.92...0.9.93 (https://github.com/100mslive/react-native-hms/compare/0.9.92...0.9.93)   0.9.92  2022-09-13  Added ability to set track settings while building the SDK  Added capability to start screen share from iOS devices  Added more descriptive error messages   Full Changelog  :  0.9.91...0.9.92 (https://github.com/100mslive/react-native-hms/compare/0.9.91...0.9.92)   0.9.91  2022-08-18  Starting HLS streaming can now be done without HLSConfig object  Added ability to do custom audio routing on Android  Added the ability to share audio from local files on Android  Updated permissions available to roles  Corrected properties available in HMSException class  Fixed an issue where exiting from Preview without joining the room was not releasing camera access  Correct RTC Stats updates on Android  minifyEnabled is set to true for Android release builds  Updated to Native Android SDK 2.4.8 & Native iOS SDK 0.3.3   Full Changelog  :  0.9.9...0.9.91 (https://github.com/100mslive/react-native-hms/compare/0.9.9...0.9.91)   0.9.9  2022-07-11  leave functions fails when invoked in Reconnecting (No Internet) state  Added ability to remove logger after attaching  changeMetadata & remoteMuteAllAudio functions is now async   Full Changelog  :  0.9.8...0.9.9 (https://github.com/100mslive/react-native-hms/compare/0.9.8...0.9.9)   0.9.8  2022-07-08  Added resolution option in HMSRTMPConfig  Added granular error information when incorrect/missing params found in API invocations  Corrected current audio playback status when new tracks are added  Updated to Native Android SDK 2.4.4 & Native iOS SDK 0.3.2   Full Changelog  :  0.9.7...0.9.8 (https://github.com/100mslive/react-native-hms/compare/0.9.7...0.9.8)   0.9.7  2022-06-16  Corrected parsing of time in HMSMessage objects  Made sender peer nullable in HMSMessage. The sender will be null when messages are triggered from Server.  Updated to Native Android SDK 2.4.1 & Native iOS SDK 0.3.1   Full Changelog  :  0.9.6...0.9.7 (https://github.com/100mslive/react-native-hms/compare/0.9.6...0.9.7)   0.9.6  2022-06-09  Corrected HLSRecordingState updates received via room update events  Corrected return types of Browser/Server Recording & RTMP Streaming APIs  Fixed incorrect source type being passed for some tracks  Added function & missing param details on receiving REQUIRED_KEYS_NOT_FOUND errors  Added destroy API which will de-initialize all HMSSDK instances, even for multiple instances scenarios  Added sessionId to HMSRoom class  Updated to Native Android SDK 2.3.9 & Native iOS SDK 0.3.1   0.9.5  2022-05-11  Added the ability to take capture video frame (screenshot) of an individual video track  Resolved circular dependency warnings  Updated HMSView params  removed sink & id keys, made style optional  Corrected serverReceiveTime param type in HMSMessage  Updated to Native Android SDK 2.3.7 & Native iOS SDK 0.2.11   0.9.4  2022-04-20  Added ability to overlay HMS Views in z-index via setZOrderMediaOverlay on HmsView  to enable Mini-view type of layouts (https://www.100ms.live/docs/react-native/v2/advanced-features/mini-view)  Added the ability to take screenshots of  individual video tiles using screenshot on HmsView  (https://www.100ms.live/docs/react-native/v2/advanced-features/capture-hmsView)  Sending error callback when a user denies permission to start screen-share from an Android device  Updated to Native Android SDK 2.3.5 & Native iOS SDK 0.2.9   0.9.3  2022-04-05  Added Network Quality Reports measuring a user's connection speed  Added the ability to render local auxiliary tracks on Android  Updated to Native Android SDK 2.3.4 & Native iOS SDK 0.2.9   0.9.2  2022-03-28  Added hlsRecordingConfig?: HMSHLSRecordingConfig to allow HLS Recording  Added hlsRecordingState?: HMSHLSRecordingState to HMSRoom for fetching the HLS Recording state of the room  Added sender: HMSPeer & recipient: HMSMessageRecipient properties on HMSMessage  Added promise to startScreenshare API so that clients can perform an action when screen share from an android mobile device  Fixed an issue where muting audio/video of a remote peer does not work  Updated to Native Android SDK 2.3.4 & Native iOS SDK 0.2.8    0.9.1  2022-03-02  Added room update & peer update callbacks on Preview. So before joining the room, you can know the peers already in the room & current room state like recording/streaming, etc  Added RTC Call Stats to show info about packet loss, bitrate, jitter, fps, resolution, etc for video tracks  Added start & stop timestamps in Server / Browser Recording & RTMP / HLS Streaming  Added streaming allowed attribute to peer's role permission  Added peer count attribute on HMSRoom object  Updated Native iOS SDK to 0.2.7, Native Android SDK to 2.3.1   0.9.0  2022-01-28  Added support for HLS Streaming  You can now start / stop / view HLS from the package  Added support for Android Screenshare  From an android device, you can do a screen share in the meeting room  Updated API parameter types  Updated to Native iOS SDK version 0.2.6 & Native Android SDK version 2.2.8   0.8.4  2022-01-07  Added check for Preview in progress before invoking join  Added support React Native version 0.63   0.8.3  2021-12-29  Made HMSTrackSettings as an optional parameter while building the HMS SDK instance   0.8.2  2021-12-29  Resolved a bug in Self Role Change which prevented the operation to succeed  Updated data passed in onPeerUpdate & onTrackUpdate callbacks  Made metadata as an optional field on peer object  Added configuration to enable/disable the usage of Hardware Echo Cancellation on Android  Added promise callbacks to Messaging & Change Track/Role state APIs  Corrected error messages sent by SDK   0.8.1  2021-12-10  Added peer metadata APIs using which you can build Raise Hand like features  Added Recording APIs to start / stop recording / streaming meeting room  Added set / get volume APIs  Corrected emitting of errors on failures  Added support for custom video sources   0.8.0  2021-11-19    Breaking  build function of HMSSDK will not create a singleton now.  requestedBy field has been made optional for HMSChangeTrackStateRequest, HMSRemovedFromRoomNotification, HMSRoleChangeRequest. It will be nil if the request originated from the REST API.  leave function is now asynchronous. Join another room only when the previous leave call has been completed.    Changed  Corrected a bug where userId was not being passed.  Sending full error description when any error/exceptions occur.  isMute function of HMSTrack now returns correct status.  HMSView is now exported from HMSSDK class.    Added  Added isPlaybackAllowed & setPlaybackAllowed for Remote Audio/Video tracks.  Added getRoom function on HMSSDK which returns the currently joined room. Returns null if the peer is not in a room.  Added changeTrackStateForRole API with which you can mute/unmute audio/video of all peers of a particular Role.   0.7.4  2021-11-03  Catching exception on join function  Cleared iOS global variables on leave  Removed listeners on leave  Exposed type param in message functions   0.7.3  2021-10-29  Clearing local cache data when leaving a room  Catching exceptions thrown by join function  Added Role-based permissions checks   0.7.2  2021-10-28  Added isPlaybackAllowed to remote audio & video tracks to mute/unmute remote tracks locally  Added changeTrackState API to mute/unmute remote peers audio/video tracks for the entire room   0.7.1  2021-10-27  Added mirror prop to HMSView to horizontally flip video tracks   0.7.0  2021-10-22  Added audio level, peer & track object to HMSSpeaker  Made package backwards compatible upto React Native version 0.63.0  Removed Xcode 13 requirement  Updated Sample app   0.6.0  2021-10-15  Added aspect ratio prop to HMSView  Asking for Camera & Microphone permissions only when required  Updated video tiles aspect ratio  Added horizontal pagination in sample app to improve performance   0.5.0  2021-09-29  Added Role-based permissions  You can now Change the Role of Remote Peers  Change Role of yourself  If you have permission then you can remove a peer from the room  Multiple improvements for network & device usage optimization   0.3.0  2021-08-31  Updated to \"react-native\": \"0.65.1\" to mitigate security vulnerabilities  Example App updated  Multiple bug fixes & performance improvements   0.2.0  2021-08-18   Added Support for Android  🥳   0.1.0  2021-08-09 The first version of 100ms React Native SDK comes power-packed with support for multiple features like    Join/Leave Rooms   Mute / Unmute Audio / Video   Switch Camera   Chat   Preview Screen   Network Switch Support   Subscribe Degradation in bad network scenarios   Error Handling and much more. Take it for a spin  🥳 "
    },
    {
        "title": "Audio Mode Change",
        "link": "/react-native/v2/features/audio-mode-change",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/audio-mode-change",
        "keywords": [],
        "content": "  This is android only API to change Audio Mode manually. The most common use-case would be toggle between (in-call volume/media volume) for in call volume  AudioManager.MODE_IN_COMMUNICATION  media volume  AudioManager.MODE_NORMAL    Switch audio mode   js fun setAudioMode(audioMode: HMSAudioMode)     Types of audio mode.  Audio Modes in Android (https://developer.android.com/reference/android/media/AudioManager MODE_IN_COMMUNICATION)   MODE_NORMAL  Normal audio mode: not ringing and no call established. (common one )   MODE_RINGTONE  Ringing audio mode. An incoming is being signaled.   MODE_IN_CALL  In call audio mode. A telephony call is established.(common one )   MODE_IN_COMMUNICATION  In communication audio mode. An audio/video chat or VoIP call is established.   MODE_CALL_SCREENING  Call screening in progress. Call is connected and audio is accessible to call screening applications but other audio use cases are still possible invoke this method with appropriate option, to switch audio mode manually   js import   HMSAudioMode   from '@100mslive/react-native-hms'; hmsInstance?.setAudioMode(HMSAudioMode.MODE_IN_COMMUNICATION);   "
    },
    {
        "title": "Audio Output Routing",
        "link": "/react-native/v2/features/audio-output-routing",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/audio-output-routing",
        "keywords": [],
        "content": "    \"These are Android only APIs.\"   Audio Output Routing is helpful when users want to switch the audio output to a connected device other than the default one. For example, you can implement an in-call speaker button that toggles audio output between the speaker, earpiece or any other connected earphones  wired or wireless, etc. Let's see how we can use HMSSDK to implement this: <div className=\"steps-container\">   Fetch Available Audio Devices You can use getAudioDevicesList method available on HMSSDK instance to fetch the list of available audio devices.   js enum HMSAudioDevice    SPEAKER_PHONE = 'SPEAKER_PHONE',  WIRED_HEADSET = 'WIRED_HEADSET',  EARPIECE = 'EARPIECE',  BLUETOOTH = 'BLUETOOTH',  AUTOMATIC = 'AUTOMATIC',   ... try    // List of available devices  const audioDevices: HMSAudioDevice   = await hmsInstance.getAudioDevicesList();  console.log('Get Audio Device List Success: ', audioDevices);   catch (error)    console.log('Get Audio Device List Error: ', error);      getAudioDevicesList method returns list of HMSAudioDevice enum. HMSAudioDevice has following values   SPEAKER_PHONE  device speaker  EARPIECE  device earpiece  WIRED_HEADSET  connected wired device  BLUETOOTH  connected Bluetooth device  AUTOMATIC  other automatic device    Switch Audio Focus to Another Device You can use switchAudioOutput method available on HMSSDK instance to switch the device audio output to different device.  switchAudioOutput method takes a HMSAudioDevice as a parameter. You can pass any device from the device list you got from previous step ( getAudioDevicesList method call).   js try    // List of available devices  const audioDevices: HMSAudioDevice   = await hmsInstance.getAudioDevicesList();  // EARPIECE device  const earpieceAudioDevice = audioDevices.find(audioDevice => audioDevice === HMSAudioDevice.EARPIECE);  // Switching Audio Output from default to EARPIECE device  hmsInstance.switchAudioOutput(earpieceAudioDevice);  console.log('Switch Audio Output Success');   catch (error)    console.log('Switch Audio Output Error: ', error);        Receive the device change updates You can add a listener to get updates whenever a audio device is removed or attached to the device such as Bluetooth, Wired Headset etc. You can use setAudioDeviceChangeListener method available on HMSSDK instance to add device change listener.  setAudioDeviceChangeListener method takes a callback as a parameter, which will be called _when output device has changed_ or a _device is removed or attached_.   js const deviceChangeListener = (data:   device: HMSAudioDevice; audioDevicesList: HMSAudioDevice    ) =>    // device param  Focused Audio Device  // audioDevicesList param  List of available Audio Devices  console.log( Audio Device Output changed to $ data.device  );  ; hmsInstance.setAudioDeviceChangeListener(deviceChangeListener);   You can remove this listener as follows    js hmsInstance.removeEventListener(HMSUpdateListenerActions.ON_AUDIO_DEVICE_CHANGED);   </div>    Get Current Focussed Device You can use getAudioOutputRouteType method available on HMSSDK instance to get the current focused audio output device.  getAudioOutputRouteType method returns audio device of type HMSAudioDevice .   js try    const focusedAudioDevice: HMSAudioDevice = await hmsInstance.getAudioOutputRouteType();  console.log('Get Audio Output Route Type Success: ', focusedAudioDevice);   catch (error)    console.log('Get Audio Output Route Type Error: ', error);     "
    },
    {
        "title": "Audio Share (Beta)",
        "link": "/react-native/v2/features/audio-share",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/audio-share",
        "keywords": [],
        "content": "  >  This feature is still in Beta. To know more or report any issues, feel free to reach out to us >  over  Discord (https://discord.com/invite/kGdmszyzq2).  This feature is the analog of screen capture, but for audio. There may be cases where the application needs to stream music which is either stored in the device locally or from some other app present on the device in the room where the peer is joined. Examples of such use cases can be a FM like application where the host would want to stream music while also interacting with others in the room or a host in a gaming app who would want to stream music from their device in the room along with their regular audio track.   Android > Note: The Audio share option currently available in Android 10 and above.   How does Audio Share works on Android 100ms SDK uses the  MediaProjection (https://developer.android.com/guide/topics/media/av-capture capture_audio_playback) APIs of Android to capture the device audio and stream it along with the user's regular audio track. To achieve this SDK starts a foreground service and starts capturing the device audio and mixes the bytes with the data collected from mic, so that the stream contains both system music and mic data. This API gives apps the ability to copy the audio being played by other apps which have set its usage to USAGE_MEDIA, USAGE_GAME, or USAGE_UNKNOWN (Audio from apps like YouTube, Music, etc can be captured)   Start device Audio Streaming from the App Follow below steps to start audio share on android devices  <div className=\"steps-container\">    Native file change Add FOREGROUND_SERVICE permission and HMSAudioshareActivity activity in the android/app/src/main/AndroidManifest.xml file    xml 2, 8-10  <manifest  ...  >  <uses-permission android:name=\"android.permission.FOREGROUND_SERVICE\"    ...  <application  ...  >   ...   <activity    android:name=\"com.reactnativehmssdk.HMSAudioshareActivity\"    android:label=\"@string/app_name\"     ...  </application> </manifest>      Start Audio Share You can call startAudioshare method available on HMSSDK instance with the mode in which you want to start the audio stream. Audio stream mode is of type HMSAudioMixingMode enum. Lets take a look at values of HMSAudioMixingMode enum  1. HMSAudioMixingMode.TALK_ONLY : Only data captured by the mic will be streamed in the room. 2. HMSAudioMixingMode.TALK_AND_MUSIC : Data captured by the mic and playback audio being captured from the device will be streamed in the room. 3. HMSAudioMixingMode.MUSIC_ONLY : Only the playback audio being captured from the device will be streamed in the room. Following is the code snippet of using startAudioshare method    js import   HMSAudioMixingMode   from '@100mslive/react-native-hms'; try    await hmsInstance.startAudioshare(HMSAudioMixingMode.MUSIC_ONLY);  console.log('Start Audio Share Success');   catch (error)    console.log('Start Audio Share Error: ', error);     </div>   Stop device Audio Streaming You can call stopAudioshare method available on HMSSDK instance to stop the audio stream    js try    await hmsInstance.stopAudioshare();  console.log('Stop Audio Share Success');   catch (error)    console.log('Stop Audio Share Error: ', error);       Change device Audio Streaming mode To change the Audio Streaming mode while the user is streaming audio, You can call setAudioMixingMode method available on HMSSDK instance.  setAudioMixingMode method accepts a value of HMSAudioMixingMode enum as a parameter. Lets take a look at values of HMSAudioMixingMode enum  1. HMSAudioMixingMode.TALK_ONLY : Only data captured by the mic will be streamed in the room. 2. HMSAudioMixingMode.TALK_AND_MUSIC : Data captured by the mic and playback audio being captured from the device will be streamed in the room. 3. HMSAudioMixingMode.MUSIC_ONLY : Only the playback audio being captured from the device will be streamed in the room. > Note that HMSAudioMixingMode.TALK_ONLY mode is equivalent to regular mode, that is without starting this API. Following is the code snippet of using setAudioMixingMode method    js import   HMSAudioMixingMode   from '@100mslive/react-native-hms'; try    await hmsInstance.setAudioMixingMode(HMSAudioMixingMode.MUSIC_ONLY);  console.log('Set Audio Mixing Mode Success');   catch (error)    console.log('Set Audio Mixing Mode Error: ', error);       Current Audio Share Status You can call isAudioShared method available on HMSSDK instance to get the current Audio Share status.  isAudioShared method returns true incase Audioshare is currently active and being used, and false for inactive state.   js try    const shared = await hmsInstance.isAudioShared();  console.log('Is Audio Shared Success: ', shared);   catch (error)    console.log('Is Audio Shared Error: ', error);       iOS > Note: Minimum iOS version required to support Audio Share is iOS 13   How does Audio Share works on iOS The audio that device shares goes to other peers through the mic channel. To be able to share audio, you need to setup the sdk to use a custom audio source instead of default mic.   Share Audio Follow below steps to   share audio playing on your iOS device   or   play and stream audio from a local file   on iOS device  <div className=\"steps-container\">    File Player and Mic Nodes First you need to create instances of HMSAudioFilePlayerNode and HMSMicNode classes.  HMSAudioFilePlayerNode requires a parameter of type string which will be used at control music player in room.   js import   HMSAudioFilePlayerNode, HMSMicNode   from '@100mslive/react-native-hms'; // creating File Player Node, which will be used as an audio source const audioFilePlayerNode = new HMSAudioFilePlayerNode('audio_file_player_node'); // creating Mic Node, which will be used as an audio source const micNode = new HMSMicNode();      Audio Mixer Sources Next, you create an instance of HMSAudioMixerSource class using list of nodes that you created in above step    js import   HMSAudioMixerSource   from '@100mslive/react-native-hms'; const audioMixerSource = new HMSAudioMixerSource(   node:  audioFilePlayerNode, micNode ,  );      SDK Initialization with Audio Mixer Sources Now you have custom audio sources to setup audio share in your app. Next, you set \"custom audio source\" on audioSource property while creating an instance of HMSAudioTrackSettings class. Lets take a look at the SDK Initialization with custom audio sources    js import   HMSSDK, HMSAudioTrackSettings, HMSTrackSettings   from '@100mslive/react-native-hms'; // creating \"Custom Audio Sources\" from the \"Audio Mixer Source\" we created in above step const customAudioSources = audioMixerSource.toList(); // Creating Audio Track Settings with \"Custom Audio Sources\" const audioSettings = new HMSAudioTrackSettings(   audioSource: customAudioSources,  ); // Creating Track Settings const trackSettings = new HMSTrackSettings(   audio: audioSettings,  ); // Creating instance of HMSSDK with custom track settings const hmsInstance = await HMSSDK.build(  trackSettings  );      Play and Stream Audio File(s) You can use play method available on HMSAudioFilePlayerNode instance (that we created in  1st step ( file-player-and-mic-nodes)) to play a file on device in meeting room.  play method accepts three parameters: 1. fileUrl : string local URL of the file on device.  You can get the local URL of a file on device using library such as  react-native-document-picker (https://www.npmjs.com/package/react-native-document-picker). 2. loops : boolean optional  whether to play file in a loop. 3. interrupts : boolean optional  whether to interrupt the already playing file. If this is set to false then current file will be played after already playing file. > Note: default value of loops and interrupts parameter is false   js try    const audioFilePlayerNode = ... // HMSAudioFilePlayerNode instance that we created in 1st step  await audioFilePlayerNode.play('...file_url...');  console.log('Start Audioshare Success');   catch(error)    console.log('Start Audioshare Error: ', error);        Play multiple files \"back-to-back\" You can use interrupts property (3rd parameter) of play method available on HMSAudioFilePlayerNode instance (that we created in  1st step ( file-player-and-mic-nodes)) to play multiple files back-to-back. When you set interrupts property to false while calling play method, the SDK does not interrupts the already playing audio file, if any is playing. Current Audio file will be played after the already playing file. The default value of interrupts property is false . So, You don't have to explicitly pass this value to play method. You can simply call play method multiple times with different files URLS without passing interrupts property. This way each audio file will be played back-to-back.   js try    const audioFilePlayerNode = ... // HMSAudioFilePlayerNode instance that we created in 1st step  audioFilePlayerNode.play('...1st_file_url...');  // interrupts property (3rd parameter) usage example  // Below statement is same as above  audioFilePlayerNode.play('...2nd_file_url...', undefined, false); // 2nd file will be played after 1st one  audioFilePlayerNode.play('...3rd_file_url...'); // 3rd file will be played after 2nd one  console.log('Start Audioshare Success');   catch(error)    console.log('Start Audioshare Error: ', error);        Play multiple files \"concurrently\" You can create instance of HMSAudioMixerSource class (refer to  step 2 ( audio-mixer-sources) for more info) _with multiple instances of HMSAudioFilePlayerNode class_ to play multiple files concurrently.   js import   HMSAudioFilePlayerNode, HMSMicNode, HMSAudioMixerSource   from '@100mslive/react-native-hms'; // Multiple HMSAudioFilePlayerNode class instances to play multiple files concurrently const backgroundMusicNode = new HMSAudioFilePlayerNode('background_music_node'); const audioFilePlayerNode = new HMSAudioFilePlayerNode('audio_file_player_node'); // Mic node const micNode = new HMSMicNode(); // Creating Audio Mixer Source with \"Mic\" node and multiple \"Audio File Player\" node const audioMixerSource = new HMSAudioMixerSource(   node:  backgroundMusicNode, audioFilePlayerNode, micNode ,  ); // Initialize SDK with audioMixerSource  refer to step 3    After SDK initialization with multiple file player nodes, you can play a looping background music at low volume and an audio file at the same time by passing second parameter loops in play function as true    js // Playing background music file in loop and setting value for interrupts to true // so that it can be played with other files backgroundMusicNode.play(fileUrl, true, true); // Playing normal music file and setting value for interrupts to true // so that it can be played with background and other files audioFilePlayerNode.play(fileUrl, false, true);   > Note: default value of loops and interrupts parameter is false     Share Audio playing on your iPhone Note: iOS allows to get access to audio playing on iOS device (for example, from another app like spotify) only while broadcating your entire iPhone screen. So, you should implement screen sharing in your app for this to work. You should follow  Screen Share setup (./screenshare ios-setup) docs for iOS to set it up. After you have completed the screenshare setup for iOS, You can follow below steps to enable device audio broadcasting while sharing your screen: 1. Create instance of HMSAudioMixerSource class (refer to  step 2 ( audio-mixer-sources) for more info) with instance of HMSScreenBroadcastAudioReceiverNode class.    js  import   HMSScreenBroadcastAudioReceiverNode, HMSAudioFilePlayerNode, HMSMicNode, HMSAudioMixerSource   from '@100mslive/react-native-hms';  // creating Node to share audio playing on iOS  const screenBroadcastAudioReceiverNode = new HMSScreenBroadcastAudioReceiverNode();  // creating File Player Node, which will be used as an audio source  const audioFilePlayerNode = new HMSAudioFilePlayerNode('audio_file_player_node');  // creating Mic Node, which will be used as an audio source  const micNode = new HMSMicNode();  // Creating Audio Mixer Source with \"Mic\" node and multiple \"Audio File Player\" node  const audioMixerSource = new HMSAudioMixerSource(    node:  audioFilePlayerNode, micNode, screenBroadcastAudioReceiverNode ,   );  // Initialize SDK with audioMixerSource  refer to step 3       Now your audio mixer source is set to receive audio from your broadcast extension.  > Note: You can only pass single instances of HMSMicNode and HMSScreenBroadcastAudioReceiverNode to HMSAudioMixerSource . You will get error if you pass multiple instances of these classes.  2. Next, you need to setup broadcast extension to send audio to the main app  Broadcast extension receives audio that's playing on your iOS device in processSampleBuffer function in your RPBroadcastSampleHandler class. To send audio from broadcast extension to main app, you call process (audioSampleBuffer) function on HMSScreenRenderer :    swift  override func processSampleBuffer(_ sampleBuffer: CMSampleBuffer, with sampleBufferType: RPSampleBufferType)      ...    case RPSampleBufferType.audioApp:      _ = self.screenRenderer.process(audioSampleBuffer: sampleBuffer)      break    ...        Now your broadcast extension is set to send audio to the main app.  And that's it. Now your custom mixer source in the main app can receive the audio from broadcast extension as well. </div>    Change volume of different nodes You can use setVolume method on nodes to control the volume level    js // setting volume of file player node audioFilePlayerNode.setVolume(0.5); // setting volume of peer mic micNode.setVolume(0.9);   Note: Volume value ranges from 0.0 to 1.0    Pause, Resume and Stop Audio File Node playback and more You can use following methods on HMSAudioFilePlayerNode to pause, resume or stop playback and more    js audioFilePlayerNode.pause(); // this pauses the audio file playback audioFilePlayerNode.resume(); // this resumes the audio file playback audioFilePlayerNode.stop(); // this stops the audio file playback const isPlaying = await audioFilePlayerNode.isPlaying(); // returns playing status of Audio File Player node const totalPlaybackDuration = await audioFilePlayerNode.duration(); // returns duration of Audio File const currentPlaybackTime = await audioFilePlayerNode.currentDuration(); // returns current duration of Audio File   "
    },
    {
        "title": "Auto Video Degrade/Restore",
        "link": "/react-native/v2/features/auto-video-degrade",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/auto-video-degrade",
        "keywords": [],
        "content": "  Sometimes people have bad internet connections but everyone deserves a good meeting. When the network is too slow to support audio and video conversations together, the 100ms SDK can automatically turn off downloading other peer's videos, which may improve the audio quality and avoid disconnections. If the network quality improves, the videos will be restored automatically as well. To turn on subscribe degradation in your room, open the  templates (https://dashboard.100ms.live/templates) in the dashboard and enable it for roles there. Here's more information about  templates (../foundation/templates-and-roles).   Dashboard Degradation Option (/docs/v2/degradation.png)   Get Video Degrade Status You can check if the SDK has stopped downloading a peers' video (video quality has been degraded) using isDegraded property available on all instances of HMSVideoTrack class. If isDegraded is true, treat it as if the video of the peer is turned off. Note: If the UI is not changed, when isDegraded property of a video track becomes true ,   the video tile will appear black  .   js const isDegraded: boolean = peer.videoTrack.isDegraded;     Video Degrade and Restore Updates When a peers' video is restored and degraded, the SDK emits HMSUpdateListenerActions.ON_TRACK_UPDATE event with HMSTrackUpdate.TRACK_RESTORED and HMSTrackUpdate.TRACK_DEGRADED update types respectively. You can listen to HMSUpdateListenerActions.ON_TRACK_UPDATE event for above mentioned update types and update your layout accordingly    js const onTrackListener = (   peer,  track,  type,  :    peer: HMSPeer;  track: HMSTrack;  type: HMSTrackUpdate;  ) =>    if (type === HMSTrackUpdate.TRACK_RESTORED)     //  track  of  peer  is restored   // Now, update your UI to show video of peer again   console.log( TRACK_RESTORED: $ track  );     if (type === HMSTrackUpdate.TRACK_DEGRADED)     //  track  of  peer  is degraded   // Now, update your UI to hide video of peer, otherwise video tile will appear black   console.log( TRACK_DEGRADED: $ track  );      hmsInstance.addEventListener(HMSUpdateListenerActions.ON_TRACK_UPDATE, onTrackListener);   "
    },
    {
        "title": "Background Handling",
        "link": "/react-native/v2/features/background-handling",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/background-handling",
        "keywords": [],
        "content": "  While a user has joined a 100ms Room they can put the app in Background Mode & usually expect a certain subset of app functionality continues to work.  Background Mode implies that the application is not currently visible to the user, not responding to user input, and running in the Background.  Background state in React Native apps is represented by AppState.currentState === 'background' as described  here (https://reactnative.dev/docs/appstate app-states). Android & iOS have different mechanisms of handling app behaviour which includes limiting access to Camera & Microphone, time allowed to keep running before getting completely stopped, Aggressive Battery Optimisations on Android OEMs, etc. There are 2 prominent behaviours that are mainly affected when apps transition to Background State  Publishing Audio/Video, and Playing incoming Audio/Video. Let's breakdown how your apps can implement efficient handling of Background states on iOS & Android Platforms.   iOS Background Handling When your app goes into the background, by default it can no longer access camera or microphone and publish it to other peers in the room. Also, you cannot hear audio of other participants in the call if your app is in the background without enabling Background Modes. This is the default iOS platform behaviour whereby it limits access to Microphone & Camera for capturing Audio & Video. By default, iOS also stops playing audio of the Room when your app is in Background. By Enabling Background Modes you can ensure that your iOS app has access to Microphone & can play incoming Audio from the Room. Following steps show how to add Background Modes in iOS: <div className=\"steps-container\">   Open project in Xcode Open project in Xcode by right clicking on ios folder in project and select Open in Xcode as shown in image below.   OpenXcode (/docs/v2/flutter-background-service-1.png)   Add Capability Click on your Main App Project in Xcode Navigator then select TARGETS and click on Signing & Capabilities and add Capability . In the Example below, the Main App Target is RNHMSExample    addCapability (/guides/rn-background-service-2.png)   Background Mode Search Background Mode and add it to Project by clicking on it.   backgroundMode (/guides/rn-background-service-3.png)   Enable Background Mode Enable the checkbox under Background Modes named Audio, AirPlay, and Picture in Picture .   backgroundProcessing (/guides/rn-background-service-4.png) </div> Let's look at different scenarios on iOS with & without Enabling Background Modes.    Without Background Modes Enabled (Default iOS Behaviour) The table lists down iOS app behaviours when the app transitions to background when a 100ms Room is ongoing without enabling background processing. The \"  Scenario  \" on the left of the Table below implies the activity which is ongoing in the App when user has joined a 100ms Room. The \"  Behaviours  \" on the right of the Table shows what happens when the App transitions to Background.    Scenario       Behaviours     :     :                Mic is Unmuted  Mic will get   Muted     Camera is unmuted  Camera will get   Muted     Remote Peers are publishing audio  Incoming Audio from Room   Stops   Playing    Background Modes Enabled (Recommended) The table lists down iOS app behaviours when the app transitions to background when a 100ms Room is ongoing with Background Modes   Enabled  . The \"  Scenario  \" on the left of the Table below implies the activity which is ongoing in the App when user has joined a 100ms Room. The \"  Behaviours  \" on the right of the Table shows what happens when the App transitions to Background.    Scenario       Behaviours                     :     :                Mic is unmuted Mic will remain   Unmuted   and the user will able to publish audio without any restriction   Camera is unmuted  Camera will get   Muted     Remote Peers are publishing audio  Incoming Audio from Room   Continues   to Play    Android Background Handling On Android devices, by default Capturing Video & Audio from Camera & Microphone is   allowed   for sometime  usually 60 seconds. This time limit depends on different Android OEMs, Battery Optimisations Mode (Aggressive/Doze), etc. This leads to inconsistent behaviours of your apps in Background Mode on different Android devices. As per your use-case, apps can choose to implement an  Android Foreground Service (https://developer.android.com/guide/components/foreground-services) to ensure consistent behaviour. With a Foreground Service, you can continuously access Microphone & Camera & publish the captured Audio & Video to other peers in the Room.  Foreground Services show a status bar notification, so that users are actively aware that your app is performing a task and is consuming system resources.  > Note: Playing Audio continuously while app is in Background is allowed by default on Android devices.   Without Android Foreground Service (Default Android Behaviour) The table below lists down Android app behaviours when the app transitions to background when a 100ms Room is ongoing without using any Android Foreground Service. The \"  Scenario  \" on the left of the Table below implies the activity which is ongoing in the App when user has joined a 100ms Room. The \"  Behaviours  \" on the right of the Table shows what happens when the App transitions to Background.     Scenario       Behaviours                     :     :                Mic is unmuted  Mic will remain   Unmuted   and the user will able to publish audio for a limited period (time depending on OEMs)   Camera is Unmuted  Camera will remain   Unmuted   and the user will able to publish video for a limited period (time depending on OS)   Remote Peers are publishing audio  Able to play incoming audio from the Room    With Android Foreground Service When your app implements a Foreground Service it continues access to capturing Audio & Video even when the app is in Background Mode.    Scenario       Behaviours                     :     :                Mic is unmuted  Mic will remain unmuted and the user will able to publish audio without any restriction   Camera is unmuted  Camera will remain unmuted and the user will able to publish video without any restriction   Remote Peers are publishing audio  Able to play incoming audio from the Room   For implementing a Foreground Service we recommend using the  @voximplant/react-native-foreground-service (https://www.npmjs.com/package/@voximplant/react-native-foreground-service) package which allows the app to run in the background with a Persistent Status Bar Notification. "
    },
    {
        "title": "Change User Name",
        "link": "/react-native/v2/features/change-name",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/change-name",
        "keywords": [],
        "content": "  Any peer can change their own name before or after joining a room. Before joining, the name would have to be specified in HMSConfig that is passed to the join method. This document shows how the name can be changed after joining.   Changing the name The peer who wants to change their name should call the changeName method on HMSSDK instance with the new name.   js try    const newName = \"new name\";  await hmsInstance.changeName(newName);  console.log('Change Name Success');   catch (error)    console.log('Change Name Error: ', error);       Responding to name changes Once the name change is successful, all the peers receive HMSUpdateListenerActions.ON_PEER_UPDATE event with HMSPeerUpdate.NAME_CHANGED as update type. When this event is received, the UI for that peer should be updated.   js const onPeerListener = (   peer,  type,  :    peer: HMSPeer;  type: HMSPeerUpdate;  ) =>    if (type === HMSPeerUpdate.NAME_CHANGED)     // Name Changed for Peer   console.log( Update UI of $ peer.name  );     ; hmsInstance.addEventListener(HMSUpdateListenerActions.ON_PEER_UPDATE, onPeerListener);   "
    },
    {
        "title": "Change Role",
        "link": "/react-native/v2/features/change-role",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/change-role",
        "keywords": [],
        "content": "   Role is a powerful concept that takes a lot of complexity away in handling permissions and supporting features like breakout rooms.  Each HMSPeer instance has a role property which returns an HMSRole instance. You can use this property to do the following: 1. Check what this Role is allowed to publish i.e. can it publish a video (and at what resolution)? Can it publish audio? Can it share a screen? Who can this role subscribe to?   For example, a Student Role can only see the Teacher's video. This is can be discovered by checking publishSettings and subscribeSettings properties.  2. Check what actions this Role can perform. i.e can it change someone else's current Role, End the Meeting, or remove someone from the room. This is can be discovered by checking the permissions property.  In certain scenarios, you may want to change someone's role. Imagine an audio room with 2 roles speaker and listener . Only someone with a speaker role can publish audio to the room while a listener can only subscribe. Now at some point, the speaker may decide to nominate some listener to become a speaker .  This is where Change Role capabilities come in play.  You may choose to do either: 1.  Single Peer Role Change ( single-peer-role-change): Change the role of a single peer to a specified one using the changeRoleOfPeer API  2.  Bulk Role Change ( bulk-role-change): Change the role of all peers with a certain role, to a specified one using the changeRoleOfPeersWithRoles API   Single Peer Role Change To invoke the API you will need 3 parameters    forPeer : An instance of HMSPeer of the peer who's role you want to change.   toRole : The HMSRole instance for the target role.   force : Whether you want to change their role without asking them or give them a chance to accept/reject.  All the peers that are in the current room are accessible via the peers property of the HMSRoom instance.  A list of all available roles in the current room can be accessed via the getRoles method of HMSSDK instance. Now with all the parameters you can invoke the changeRoleOfPeer API:   js import   HMSRole, HMSRemotePeer   from '@100mslive/react-native-hms'; try    const force = false;  // instance acquired from build() method  const result = await hmsInstance.changeRoleOfPeer(peer, newRole, force) // request role change, not forced  console.log('Change Role Success: ', result);   catch (error)    console.log('Change Role Error: ', error);     > Note: changeRoleOfPeer success doesn't mean that the role was changed, it just means that the server accepted the request as valid. > Note: 🔑 changeRoleOfPeer is the same as changeRole but we have deprecated changeRole and it will be removed in future releases. So, Please use changeRoleOfPeer . If the changing role succeeds, you will get an update in the HMSUpdateListenerActions.ON_PEER_UPDATE event with HMSPeerUpdate.ROLE_CHANGED update type:   js const onPeerListener = (   peer,  type,  :    peer: HMSPeer;  type: HMSPeerUpdate;  ) =>    if (type === HMSPeerUpdate.ROLE_CHANGED)     // Role was changed of  peer , New role is  peer.role    // Update your app UI here     ; hmsInstance.addEventListener(HMSUpdateListenerActions.ON_PEER_UPDATE, onPeerListener);   Registered HMSUpdateListenerActions.ON_PEER_UPDATE event listener will receive the same peer, you passed as targetPeer and an HMSPeerUpdate.ROLE_CHANGED update type.   Accept Single Peer Role Change request When a peer wishes to change the role of another peer it calls changeRoleOfPeer with a parameter force .  When force is set to true  role is directly changed and the peers receives an HMSUpdateListenerActions.ON_PEER_UPDATE event with HMSPeerUpdate.ROLE_CHANGED update type.  When force is set to false  This is a polite request e.g. \"Would you like to change your role from listener to speaker?\". Which can be ignored by the peer. The way it works is that the target peer will first receive a \"Role Change Request\" by HMSUpdateListenerActions.ON_ROLE_CHANGE_REQUEST event. If the request is accepted, the peers receives an HMSUpdateListenerActions.ON_PEER_UPDATE event with HMSPeerUpdate.ROLE_CHANGED update type otherwise nothing happens.  Let's understand this with a diagram:   accept-change-role (/docs/v2/flutter-accept-role-change.png) Now let's do it step-by-step: <div className=\"steps-container\">    PeerA calls changeRoleOfPeer with force as false   js try    const force = false;  // instance acquired from build() method  const result = await hmsInstance.changeRoleOfPeer(peer, newRole, force) // request role change, not forced  console.log('Change Role Success: ', result);   catch (error)    console.log('Change Role Error: ', error);        PeerB receives Role Change Request PeerB receive a \"Role Change Request\" by HMSUpdateListenerActions.ON_ROLE_CHANGE_REQUEST event    js // HMSRoleChangeRequest is a class that contains info about the role change request. interface HMSRoleChangeRequest    requestedBy: HMSPeer; // This is an HMSPeer object which contains info about the peer who performed the role change  suggestedRole: HMSRole; // This is an HMSRole object which contains info about the destination role   ... const onRoleChangeRequest = (   requestedBy,  suggestedRole  : HMSRoleChangeRequest) =>    // open a prompt to accept or reject the request  ; hmsInstance.addEventListener(HMSUpdateListenerActions.ON_ROLE_CHANGE_REQUEST, onRoleChangeRequest);   At this point, the app should show a prompt to the user asking for permission to accept or deny role change request.    PeerB accepts Role Change request Role Change request can be accepted by calling acceptRoleChange method on HMSSDK instance    js try    // call acceptRoleChange if you want to accept role change else close the prompt  await hmsInstance.acceptRoleChange();  console.log('Accept change role Success');   catch (error)    console.log('Accept change role Error: ', error);     If the user wants to ignore the request, he/she can just dismiss the prompt.    All the peers receive Peer Role Change update Now, all peers in the room will receive an HMSUpdateListenerActions.ON_PEER_UPDATE event with HMSPeerUpdate.ROLE_CHANGED update type so that they can do the necessary UI updates. PeerB will have all the permissions of new role.   js const onPeerListener = (   peer,  type,  :    peer: HMSPeer;  type: HMSPeerUpdate;  ) =>    if (type === HMSPeerUpdate.ROLE_CHANGED)     // Role was changed of  peer , New role is  peer.role    // Update your app UI here     ; hmsInstance.addEventListener(HMSUpdateListenerActions.ON_PEER_UPDATE, onPeerListener);   </div>    Force Role Change Request case Now let's imagine the newly nominated speaker is not behaving nicely and we want to move him back to the listener without a prompt. This is where the force parameter comes in. When force is set to true the other party will not receive a confirmation HMSUpdateListenerActions.ON_ROLE_CHANGE_REQUEST event, but instead will straight away receive a new set of updated permissions and will stop publishing audio. Note that HMSUpdateListenerActions.ON_PEER_UPDATE event with HMSPeerUpdate.ROLE_CHANGED update type will still be emitted so that the app can update the user's UI state.   Bulk Role Change Bulk Role Change is used when we want to change the role of peers with a specific role to another role. For example, if peers join a room with a waiting role and now you want to change all these peers to viewer role then use the changeRoleOfPeersWithRoles method. It takes fewer parameters than for a single peer. Here is the method signature:   js changeRoleOfPeersWithRoles = async (ofRoles: HMSRole  , toRole: HMSRole) =>   ...  ;   1. toRole is the HMSRole they should be changed to. 2. ofRoles is a list of HMSRole whose role should be changed.  Here's how the method could be called to change all waiting and guest roles to host :   js // fetch all available Roles in the room const roles = await hmsInstance.getRoles(); // get the Host Role object const hostRole = roles.find(role => role.name === 'host'); // get list of Roles to be updated  in this case \"Waiting\" and \"Guest\" Roles const rolesToChange = roles.filter(role => role.name === 'waiting'  role.name === 'guest'); // now perform Role Change of all peers in \"Waiting\" and \"Guest\" Roles to the \"Host\" Role await hmsInstance.changeRoleOfPeersWithRoles(rolesToChange, hostRole);      Edge cases with bulk role change 1. Note that if an empty list is sent to ofRoles , no roles will be changed. This is to avoid accidentally changing roles you may not have intended such as the bots that provide recording and streaming with the roles beam . 2. Also, Bulk Role Changes are always forced i.e. no option will be given for the peer to accept it, they will just be changed immediately.    Bulk Role Change Errors You may get the following errors for bulk role change:  Message                 Meaning                                                      invalid role           A role in the list of roles to change does not exist in this room.        target role clash with requested roles  the 'toRole' is also listed as one to change to 'toRole'   role does not have required permission  Peer does not have role change permission.          peer left                The peer who's role was to be changed has left.       role invalid               The 'toRole' is invalid.                  "
    },
    {
        "title": "Change Track State For Roles",
        "link": "/react-native/v2/features/change-track-state-roles",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/change-track-state-roles",
        "keywords": [],
        "content": "  You're running a video call room and decide that someone who's currently talking shouldn't be talking. You'd prefer they'd stay mute. Or perhaps you want their video turned off as well as their audio. You're looking for a remote mute. Muting can apply to both audio and video.   Unmuting You may also decide that you want to let someone else speak who was currently muted. Or may want to ask someone to turn on their video. You can request people to unmute themselves as well.   Check if user has permissions Can't let just anyone mute others. First you need to create a role with the permissions to mute others and also to ask them to unmute. The permission to mute others is within PermissionsParams mute and you should check for that within the HMSRole of the peer to see if they have it. Similarly the permission to unmute other peers is within PermissionsParams unmute. Here's how to check whether the local peer has the permission to mute or unmute others:   js const mute: boolean = peer.role.permissions.mute; const unmute: boolean = peer.role.permissions.unmute;     Example Imagine a room with 10 people having 7 listeners & 3 speakers. You want to unmute audio/video of all the speakers and mute audio/video of all the listeners.   Muting/Unmuting There are two APIs for muting/unmuting.   One which only works on a single track (audio/video) for a single peer at a time.  Learn more about API here. (./change-track-state)   One which can mute multiple tracks for multiple peers. Either by their role, or track source or track type or any combination of the above.   Multiple Peers or Tracks Once you have checked that the caller has permissions to mute another peer's audio or video, call hmsInstance.changeTrackStateForRoles . To mute audio for a multiple peers or tracks:  hmsInstance.changeTrackStateForRoles takes four parameters   Mute Boolean true if the track needs to be muted, false otherwise   Type Optional, the HMSTrackType which should be affected. If this and source are specified, it is considered an AND operation . If null, all track sources are affected.   Source Optional, the HMSTrackSource which should be affected. If this and type are specified, it is considered an AND operation . If null, all track sources are affected.   Roles Array of HMSRole optional, may have a single item in a array, whose tracks should be affected. If null, all roles are affected. Here's an example of how you would check if the caller was allowed to mute peers and then call for a mute/unmute on all peers in the chat.   js // instance acquired from build() method await hmsInstance?.changeTrackStateForRoles(true, undefined, undefined,    hmsInstance?.knownRoles,  ).then(d => console.log('Change Track State Roles Success: ', d))  .catch(e => console.log('Change Track State Roles Error: ', e));   This can be further narrowed by specifying only those tracks of type Audio. Note that HMSTrackSource.REGULAR is the peer's own audio and video as opposed to one provided by screenshare or a plugin.  js import   HMSTrackType, HMSTrackSource   from '@100mslive/react-native-hms'; // instance acquired from build() method await hmsInstance?.changeTrackStateForRoles(true, HMSTrackType.AUDIO, HMSTrackSource.REGULAR,    hmsInstance?.knownRoles,  ).then(d => console.log('Change Track State Roles Success: ', d))  .catch(e => console.log('Change Track State Roles Error: ', e));   "
    },
    {
        "title": "Remote Mute",
        "link": "/react-native/v2/features/change-track-state",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/change-track-state",
        "keywords": [],
        "content": "  You're running a video call room and decide that someone who's currently talking shouldn't be talking. You'd prefer they'd stay mute or perhaps you want their video turned off as well. You're looking for a Remote Mute feature. You may also decide that you want to let someone else speak who was currently muted or may want to ask someone to turn on their video as well. You're looking for a Request Unmute feature. Muting/Unmuting can apply to both audio and video. 100ms provides these functionalities using \"Change Track State\" APIs. These APIs allows users to mute other peers's audio / video and request unmute other peer's audio / video if they have required permissions or not according to their role.   Permissions Can't let just anyone mute others. First, you need to create a  role (../foundation/templates-and-roles) with the permissions to mute others and also to ask them to Unmute.   Permissions  Remote mute/unmute (/guides/permissions.png) Only the peers which has role with above permissions allowed will be able to Mute peers and/or Request Unmute from peers. You can check if local peer has permission to Mute peers or Request Unmute from peers as follows    js const localPeer = await hmsInstance.getLocalPeer(); // Permissions are available on HMSRole object of Local Peer const localPeerPermissions: HMSPermissions = localPeer.role?.permissions; // Check if Local Peer has Mute Permission const canMute: boolean = localPeerPermissions?.mute; // Check if Local Peer has Request Unmute Permission const canRequestUnmute: boolean = localPeerPermissions?.unmute;     Example Imagine a room with 10 people having 3 speakers and they have to speak one by one. The first speaker can mute other 2 speakers and start. After some point when the first speaker is finished they can mute himself and request other speakers to unmute.   Mute / Unmute Remote Peers You can use changeTrackState method available on HMSSDK instance to mute and unmute remote peers.  changeTrackState method accepts two parameters: 1. _track_: HMSTrack that you want to mute or unmute. 2. _mute_: boolean value,    pass true to mute the track. track is muted immediately without any requests from track owner(peer). Peer is notified about the change by HMSUpdateActionListeners.ON_CHANGE_TRACK_STATE_REQUEST event.    pass false to request unmute for the track. track owner(peer) will receive a unmute request. If the request is accepted by the remote peer HMSUpdateActionListeners.ON_TRACK_UPDATE event will be emitted. Make sure you have above mentioned permissions for mute/unmute to work.   js try    await hmsInstance.changeTrackState(audioTrack as HMSTrack, true); // Passing true to mute  console.log('Audio Track Mute Success');  await hmsInstance.changeTrackState(videoTrack as HMSTrack, false); // Passing false to request unmute  console.log('Video Track Unmute Request Sent');   catch (error)    console.log('Change Track State Error: ', error);       Mute / Unmute Remote Peers with Specific Roles You can use changeTrackStateForRoles method available on HMSSDK instance to mute and unmute   multiple peers at once by specifying specific roles  . This is not possible with changeTrackState method, it only mutes and unmutes single track for single peer at once.  changeTrackStateForRoles method accepts four parameters: 1. mute  true to mute the track(s) and false to request unmute for the track(s) 2. type  Optional   type is the HMSTrackType that should be affected. If type and source are specified, it is considered an AND operation. If null / undefined , all track sources are affected. 3. source  Optional   source is the HMSTrackSource that should be affected. If type and source are specified, it is considered an AND operation. If null / undefined , all track types are affected. 4. roles  Optional   list of HMSRole whose tracks should be affected. If null / undefined , all roles are affected. Also make sure you have above mentioned permissions for mute/unmute to work.   js try    const roles = await hmsInstance.getRoles();  const  role1, role2  = roles;  await hmsInstance?.changeTrackStateForRoles(   true, // mute, set false to request unmute   HMSTrackType.AUDIO, // only mute audio tracks   HMSTrackSource.REGULAR, // only mute regular tracks    role1, role2 , // roles to target  );  console.log('Change Track State Roles Success');   catch (error)    console.log('Change Track State for Roles Error: ', error);       Handle Track Mute Mute is automatically applied to the target peer, No action is required. However, Peer will still receives HMSUpdateListenerActions.ON_CHANGE_TRACK_STATE_REQUEST event, notifying peer about the track got muted.   Handle Track Unmute request Once the peer with adequate permissions requests unmute of a track, the track owner ( HMSPeer ) receives HMSUpdateListenerActions.ON_CHANGE_TRACK_STATE_REQUEST event. In this events' listener, you can show a prompt to the peer to accept or reject the unmute request. then Peer can accept the unmute request by Unmuting the audio/video track or reject the request by dismissing the prompt.   js // let's look at what the HMSChangeTrackStateRequest looks like  interface HMSChangeTrackStateRequest    requestedBy: HMSPeer; // Peer who Muted or requested Unmute  trackType: string; // 'audio' or 'video' track type  mute: boolean; // true  Peer Muted your track; false  Peer Requested Unmute of your track   ... const onChangeTrackStateRequest = (data: HMSChangeTrackStateRequest) =>    const requestedBy = data.requestedBy?.name; // Peer Name who Muted or requested Unmute  if (data.trackType === 'audio')     // show Prompt to peer to accept or reject \"Audio\" track unmute request   // In Prompt, Peer can accept request by  localPeer.localAudioTrack()?.setMute(false)     else if (data.trackType === 'video')     // show Prompt to peer to accept or reject \"Video\" track unmute request   // In Prompt, Peer can accept request by  localPeer.localVideoTrack()?.setMute(false)     ; hmsInstance.addEventListener(HMSUpdateListenerActions.ON_CHANGE_TRACK_STATE_REQUEST, onChangeTrackStateRequest);     Mute Audio of all Remote Peers You can use remoteMuteAllAudio method available on HMSSDK instance to mute audio track of all remote peers at once. Make sure you have above mentioned permissions for mute to work.   js try    await hmsInstance.remoteMuteAllAudio();  console.log('Remote Mute All Audio Success');   catch (error)    console.log('Remote Mute All Audio Error: ', error);     "
    },
    {
        "title": "Chat",
        "link": "/react-native/v2/features/chat",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/chat",
        "keywords": [],
        "content": "  What's a video call without being able to send messages to each other too? 100ms supports chat for every video/audio room you create.    Message Types    Broadcast messages ( broadcast-messages) are sent to \"Everyone\" in the chat, using  hmsInstance.sendBroadcastMessage .    Direct messages ( direct-messages) are sent to a \"specific person\", using  hmsInstance.sendDirectMessage .    Group messages ( group-messages) are sent to \"everyone with a particular HMSRole \". Such as all hosts or all teachers or all students , using  hmsInstance.sendGroupMessage .     You can learn more about  Templates and Roles here (../foundation/templates-and-roles).   Sending Chat Messages On the HMSSDK instance call different send message functions     Broadcast Messages Want to let everyone in the chat know something? Call sendBroadcastMessage method on the HMSSDK instance to send a broadcast message.  sendBroadcastMessage method accepts following parameters: 1. _message_: The text of the message. 2. _type_ optional : The type ( HMSMessageType ) of the message, default value is HMSMessageType.CHAT , other available value is HMSMessageType.METADATA .  > 💡 Note: Promise returned from sendBroadcastMessage method only lets you know if the server has received your request for the message or if there was some error. It does not convey whether the message was delivered to or read by the recipient.   js // instance acquired from HMSSDk.build() method try    const result = await hmsInstance.sendBroadcastMessage('hello everyone '); // yes it's that simple 😉  console.log('Broadcast Message Success: ', result);   catch (error)    console.log('Broadcast Message Error: ', error);        Direct Messages Got secrets to share with someone? Call sendDirectMessage method on the HMSSDK instance to send message directly to a single person.  sendDirectMessage method accepts following parameters: 1. _message_: The text of the message. 2. _peer_: The HMSPeer instance that should receive the message. 3. _type_ optional : The type ( HMSMessageType ) of the message, default value is HMSMessageType.CHAT , other available value is HMSMessageType.METADATA .  > 💡 Note: Promise returned from sendDirectMessage method only lets you know if the server has received your request for the message or if there was some error. It does not convey whether the message was delivered to or read by the recipient.   js // instance acquired from HMSSDk.build() method try    const result = await hmsInstance.sendDirectMessage('keep this message a secret ', peer);  console.log('Direct Message Success: ', result);   catch (error)    console.log('Direct Message Error: ', error);        Group Messages Need to call attention to All the hosts or All the teachers or All the developers? Call sendGroupMessage method on the HMSSDK instance to send message to peers with specific roles.  sendGroupMessage method accepts following parameters: 1. _message_: The text of the message. 2. _roles_: The list of HMSRole i.e the roles to whom the message needs to be sent. 3. _type_ optional : The type ( HMSMessageType ) of the message, default value is HMSMessageType.CHAT , other available value is HMSMessageType.METADATA .  > 💡 Note: Promise returned from sendGroupMessage method only lets you know if the server has received your request for the message or if there was some error. It does not convey whether the message was delivered to or read by the recipient.   js // all available Roles can be obtained getRoles method on HMSSDK instance // it will return an array of HMSRole from which required roles can be selected for sendGroupMessage method. const roles = await hmsInstance.getRoles(); const  role1, role2  = roles; // getting first 2 roles from roles list try    const result = await hmsInstance.sendGroupMessage('hi folks ',  role1, role2 );  console.log('Group Message Success: ', result);   catch (error)    console.log('Group Message Error: ', error);       Receiving Chat Messages After  Joining the Room (./join), you can add a listener for HMSUpdateListenerActions.ON_MESSAGE event to receive new messages as HMSMessage object during the call.   js // instance acquired from build() method hmsInstance.addEventListener(HMSUpdateListenerActions.ON_MESSAGE, onMessageReceived); const onMessageReceived = (message: HMSMessage) =>    // new message received, show this message in chat UI  ;   Let's look at what the HMSMessage looks like    js interface HMSMessage    message: string;  type: HMSMessageType;  time: Date;  sender?: HMSPeer;  recipient: HMSMessageRecipient;       _message_: The actual text message string that was sent.   _type_: Type of message sent. Default value is HMSMessageType.CHAT , other available value is HMSMessageType.METADATA .   _time_: DateTime of when the server received the message. This can be used for accurate ordering of your messages.   _sender_: The HMSPeer who sent this message.   _recipient_: The intended recipient(s) of this message as a HMSMessageRecipient object.  Now let's look at what the HMSMessageRecipient looks like    js interface HMSMessageRecipient    recipientPeer?: HMSPeer;  recipientRoles?: HMSRole  ;  recipientType?: HMSMessageRecipientType;       _recipientPeer_: Peer that should receive this message. This is available when a specific single peer is being direct messaged.   _recipientRoles_: Peers with these Roles whom should receive this message. This is available when a group message is being sent to many roles.   _recipientType_: HMSMessageRecipientType Enum Values  BROADCAST , PEER or ROLES .   HMSMessageRecipientType.BROADCAST will be set when it 's a message being sent to everyone.   HMSMessageRecipientType.PEER will be set when it 's a direct message.   HMSMessageRecipientType.ROLES will be set when it 's a message to one or many roles.    Chat Messages UI HMSSDK doesn't provide UI for chat, The UI is completely up to you to decide    You'll also need to hold onto all the received messages if you want to display history  .   Advanced Use-Cases Sometimes the app requires to show messages in different styles. Something similar to this:   Chat (/docs/v2/flutter-chat.gif)  HMSSDK provides the type parameter on the HMSMessage object and methods used to send different types of messages to take care of such use cases.  You can send messages as:   js await hmsInstance.sendBroadcastMessage(\"😁\", \"emoji\"); // OR await hmsInstance.sendDirectMessage(\"😁\", peer, \"emoji\"); // OR await hmsInstance.sendGroupMessage(\"😁\",  role1, role2 , \"emoji\");   When you receive messages, you can filter out the messages based on the type parameter on HMSMessage object and handle the UI accordingly.   js // instance acquired from build() method hmsInstance.addEventListener(HMSUpdateListenerActions.ON_MESSAGE, onMessageReceived); const onMessageReceived = (message: HMSMessage) =>    if(message.type === \"emoji\")     // Show as emoticons on UI    else    // Handle other cases     ;   "
    },
    {
        "title": "End Room",
        "link": "/react-native/v2/features/end-room",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/end-room",
        "keywords": [],
        "content": "  Done with talking and it's time to end the video call room for everyone not just yourself? You may be looking to end the room.   How to End Room  <div className=\"steps-container\">   Permissions Can't let just anyone end the video call room. First you need to create a  Role (https://www.100ms.live/docs/react-native/v2/foundation/templates-and-roles) with the permissions to end a room. The permission to end a room is called permissions.endRoom and you should check for that within the HMSRole of the peer to see if they have it. Here's how to check whether the local peer has the permission to end the room:   js const localPeer = await hmsInstance.getLocalPeer(); const haveEndRoomPermission = localPeer.role?.permissions?.endRoom;    await hmsInstance.getLocalPeer() will not return null/undefined as long as you're in a preview or in a room. Since you likely won't need to check for permissions if you're not in one it would be safe to omit null check.   Ending the Room Once you're sure the peer has the permissions to end the room, we can end the room by calling:   js const reason = 'Host ended the room'; // Reason to end the room const lock = false; // whether to Lock room to prevent future joins try    // instance acquired from build() method  await hmsInstance.endRoom(reason, lock);  console.log('End Room Success');  // Navigate away from the Meeting screen, when end room is successful  navigation.goBack();   catch(error)    console.log('End Room Error: ', error);      endRoom takes the following parameters  1.  lock : A boolean for whether you want to prevent anyone from rejoining the room. If false, they will be allowed to enter the room again if the client called join . If this is true, they will NOT able to join this room again. 2.  reason : reason string is the text that is passed describing why the room is being ended. > 💡 After calling endRoom, the video calling UI should be disposed of as well since only other peers will get the HMSUpdateListenerActions.ON_REMOVED_FROM_ROOM event. The caller can terminate the meeting room UI locally when the promise returned from endRoom is resolved. </div>   Handle App UI When Someone else Ends Room This section explains how to handle the app UI when someone else ends the room. Once the peer with adequate permissions calls endRoom , all other peers in the room will receive a HMSUpdateListenerActions.ON_REMOVED_FROM_ROOM event. You can update your apps' UI and  Free Resources (https://www.100ms.live/docs/react-native/v2/features/release-resources) on receive of HMSUpdateListenerActions.ON_REMOVED_FROM_ROOM event.   js const onRemovedFromRoom = (data:   requestedBy: HMSPeer  null, reason: string; roomEnded: boolean;  ) =>    // Free App resources and do cleanup  destroy();  // Redirect to home screen or Navigate away from Meeting screen  navigation.navigate('Home');  ; hmsInstance.addEventListener(HMSUpdateListenerActions.ON_REMOVED_FROM_ROOM, onRemovedFromRoom);   > 💡 This is the same callback that will be triggered if a peer is  removed from a room (https://www.100ms.live/docs/react-native/v2/features/remove-peer) as well. Except that roomEnded will be true when the entire room is ended. Description of keys of data received from HMSUpdateListenerActions.ON_REMOVED_FROM_ROOM event:   _reason_: The string message detailing why the room was ended.   _requestedBy_: The details of the peer who called endRoom .   _roomEnded_: true if the entire room was ended. false if only the receiving peer(local peer) was removed. "
    },
    {
        "title": "Error Handling",
        "link": "/react-native/v2/features/error-handling",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/error-handling",
        "keywords": [],
        "content": "  When interacting with the 100ms SDK using the HMSSDK instance, or joining and previewing the Room, it may encounter issues that it cannot automatically recover from, and app or user intervention may be required. In such cases, the 100ms SDK emits HMSUpdateListenerActions.ON_ERROR event with error encounter by it. You can subscribe to HMSUpdateListenerActions.ON_ERROR event to get notified about the errors encountered by the SDK    js // this function will be called with error as first parameter const onError = (error: HMSException) =>    // Logging error  console.log( $ error.code  $ error.description  );  // You should handle error as per its' code  // Each code represents different error. Read below section to get more details  if (error.code === 4005  error.code === 1003)     leaveMeeting();     ; // Subscribe to get SDK errors hmsInstance.addEventListener(HMSUpdateListenerActions.ON_ERROR, onError);     HMSException Function subscribed to HMSUpdateListenerActions.ON_ERROR event is called with error object, which is an instance of HMSException class. Lets take a look at HMSException class interface    js interface HMSException    code: number;  description: string;  isTerminal: boolean;  canRetry?: boolean; // Android Only  message?: string; // Android Only  name?: string; // Android Only  action?: string; // Android Only     Here are the details of each property of HMSException object:   code The code property denotes error code which a unique identifier for the error.   description The description property denotes what is the source of the error.   isTerminal The isTerminal property denotes wether error has caused the current session to terminate and the app will need to call join again to reconnect. Terminal error example:   Terminal Error (/docs/v2/flutter-errorCode-1003.png) The error code 1003 (error in above image) occurs when the reconnection attempt fails. The SDK returns this error after attempting to reconnect for 60 seconds. In such cases, the isTerminal property can be used to handle updates to the UI. > 🔑 More info on reconnection handling can be found  here (./reconnecting-reconnected)   canRetry It is an android only property. The canRetry property denotes wether app can call join again with the same configuration it has used before. The value be false in cases like token expiring or room getting locked. You can use this property while implementing infinite retry in your app.   message It is an android only property. The message property denotes error information.   name It is an android only property. The name property denotes error name.   action It is an android only property. The action property tells what to do when a error occurs. For example SEND_ALL_REQUIRED_KEYS means you have not sent some required key in the API call.   Different Error Types Following are the different errors emitted by the SDK with their codes and action you should take to resolve the error     Error Code      Cause of the error                                                            Action to be taken                                                   :     :                                  :                                 1003       Websocket disconnected  Happens due to network issues                                        Mention user to check their network connection or try again after some time.                        2002       Invalid Endpoint URL                                                         Check the endpoint provided while calling join on HMSSDK .                                2003       Endpoint is not reachable                                                       Mention user to check their network connection or try again after some time.                        2004       Token is not in proper JWT format                                                   The token passed while calling join is not in correct format. Retry getting a new token.                 3001       Cant Access Capture Device                                                      Ask user to check permission granted to audio/video capture devices.                            3002       Capture Device is not Available                                                    Ask user to check if the audio/video capture device is connected or not.                          3003       Capture device is in use by some other application.                                          Show notification to user mentioning that the capturing device is used by some other application currently.         3008       Browser has thrown an auto-play exception.                                              Show notification to user mentioning that the browser blocked auto-play.                          3012        BLUETOOTH_CONNECT (https://developer.android.com/reference/android/Manifest.permission BLUETOOTH_CONNECT) permission is not given.  Add the BLUETOOTH_CONNECT permission to your app and request it at runtime since it's a dangerous level permission.     3013        BLUETOOTH (https://developer.android.com/reference/android/Manifest.permission BLUETOOTH) permission is not given.          Add the BLUETOOTH permission to your app.                                          4001       WebRTC error                                                             Some webRTC error has occurred. Need more logs to debug.                                  4002       WebRTC error                                                             Some webRTC error has occurred. Need more logs to debug.                                  4003       WebRTC error                                                             Some webRTC error has occurred. Need more logs to debug.                                  4004       WebRTC error                                                             Some webRTC error has occurred. Need more logs to debug.                                  4005       ICE Connection Failed due to network issue                                              Mention user to check their network connection or try again after some time.                        5001       Trying to join a room which is already joined                                             Trying to join an already joined room.                                           6002       webRTC Error: Error while renegotiating                                                Please try again.                                                      40101      Token Error: Invalid Access Key                                                    Access Key provided in the token is wrong.                                         40102      Token Error: Invalid Room Id                                                     RoomID provided in the token is wrong.                                           40103      Token Error: Invalid Auth Id                                                     AuthID provided in the token is wrong.                                           40104      Token Error: Invalid App Id                                                      App ID provided in the token is wrong.                                           40105      Token Error: Invalid Customer Id                                                   Customer Id provided in the token is wrong.                                         40107      Token Error: Invalid User Id                                                     User ID provided in the token is wrong.                                           40108      Token Error: Invalid Role                                                       The role provided in the token is wrong.                                          40109      Token Error: Bad JWT Token                                                      Bad JWT Token.                                                       40100      Generic Error                                                             Need to debug further with logs.                                              40001      Invalid Room                                                             Room ID provided while fetching the token is an invalid room.                                40002      Room Mismatched with Token                                                      Room ID provided while fetching the token does not match.                                  40004      Peer already joined                                                          Peer who is trying to join has already joined the room.                                   41001      Peer is gone                                                             The peer is no more present in the room.                                          7001       Platform Not Supported                                                        The platform is not supported for plugin                                          7002       Plugin Init Failed                                                          Plugin initialization has failed                                              7003       Plugin Processing Failed                                                       Plugin processing failed                                                  7004       Plugin Add Already Going on                                                      Plugin add is already in progress                                             Setting log levels in SDK (Android Only) Logs can be used to diagnose the performance of room sessions or debug various issues. To enable logging, create an HMSLogSettings object and pass it to the build method when constructing the HMSSDK instance. The SDK also provides an option to save these logs to disk on   Android devices   using the isLogStorageEnabled property of the HMSLogSettings object. You can set the isLogStorageEnabled property to true to enable the SDK to write logs to the disk.   js // Creating the Log Settings object with disk storage turned on const logSettings = new HMSLogSettings(   level: HMSLogLevel.VERBOSE,  isLogStorageEnabled: true,  maxDirSizeInBytes: HMSLogAlarmManager.DEFAULT_DIR_SIZE, // Maximum size of the directory where log will be saved  ); // Pass the Log Settings object to the build function c̶o̶n̶s̶t̶ ̶h̶m̶s̶I̶n̶s̶t̶a̶n̶c̶e̶ ̶=̶ ̶a̶w̶a̶i̶t̶ ̶H̶M̶S̶S̶D̶K̶.̶b̶u̶i̶l̶d̶(̶)̶;̶ const hmsInstance = await HMSSDK.build(  logSettings  );    HMSLogLevel is an enum with values    js export enum HMSLogLevel    // To receive all the logs  VERBOSE = 'VERBOSE',  // To receive warnings  WARNING = 'WARNING',  // To receive errors  ERROR = 'ERROR',     "
    },
    {
        "title": "Event Listeners Enums",
        "link": "/react-native/v2/features/event-listeners-enums",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/event-listeners-enums",
        "keywords": [],
        "content": "    HMSPeerUpdate   js import   HMSPeerUpdate, HMSUpdateListenerActions, HMSRoom, HMSLocalPeer, HMSRemotePeer   from '@100mslive/react-native-hms'; // hms instance acquired from build method hmsInstance?.addEventListener(  HMSUpdateListenerActions.ON_PEER_UPDATE,  onPeerListener, ); const onPeerListener = (    peer,   type,  :     peer: HMSPeer;   type: HMSPeerUpdate;  ) =>    if(type === HMSPeerUpdate.PEER_JOINED)    // when a peer joins     if(type === HMSPeerUpdate.PEER_LEFT)    // when a peer leaves     if(type === HMSPeerUpdate.ROLE_CHANGED)    // when a peer's role is changed     if(type === HMSPeerUpdate.METADATA_CHANGED)    // when a peer's metadata is changed     if(type === HMSPeerUpdate.NAME_CHANGED)    // when a peer's name is changed     if(type === HMSPeerUpdate.NETWORK_QUALITY_UPDATED)    // when a peer's network quality is changed     ;     HMSTrackUpdate   js import   HMSTrackUpdate, HMSUpdateListenerActions, HMSRoom, HMSLocalPeer, HMSRemotePeer   from '@100mslive/react-native-hms'; // hms instance acquired from build method hmsInstance?.addEventListener(  HMSUpdateListenerActions.ON_TRACK_UPDATE,  onTrackListener, ); const onTrackListener = (    track,   peer,   type,  :     track: HMSTrack;   peer: HMSPeer;   type: HMSTrackUpdate;  ) =>    if(type === HMSTrackUpdate.TRACK_ADDED)    // when track is added     if(type === HMSTrackUpdate.TRACK_REMOVED)    // when track is removed     if(type === HMSTrackUpdate.TRACK_MUTED)    // when track is muted     if(type === HMSTrackUpdate.TRACK_UNMUTED)    // when track is unmuted     if(type === HMSTrackUpdate.TRACK_DESCRIPTION_CHANGED)    // when track's description is changed     if(type === HMSTrackUpdate.TRACK_DEGRADED)    // when track is degraded     if(type === HMSTrackUpdate.TRACK_RESTORED)    // when track is restored     ;     HMSRoomUpdate   js import   HMSRoomUpdate, HMSUpdateListenerActions, HMSRoom, HMSLocalPeer, HMSRemotePeer   from '@100mslive/react-native-hms'; // hms instance acquired from build method hmsInstance?.addEventListener(  HMSUpdateListenerActions.ON_ROOM_UPDATE,  onRoomListener, ); const onRoomListener = (    room,   type,   :     room: HMSRoom;   type: HMSRoomUpdate;   ) =>    if(type === HMSRoomUpdate.ROOM_MUTED)    // when room is muted     if(type === HMSRoomUpdate.ROOM_UNMUTED)    // when room is unmuted     if(type === HMSRoomUpdate.SERVER_RECORDING_STATE_UPDATED)    // when server recording state is updated     if(type === HMSRoomUpdate.RTMP_STREAMING_STATE_UPDATED)    // when rtmp streaming state is updated     if(type === HMSRoomUpdate.BROWSER_RECORDING_STATE_UPDATED)    // when browser recording state is updated     if(type === HMSRoomUpdate.HLS_RECORDING_STATE_UPDATED)    // when hls recording state is updated     if(type === HMSRoomUpdate.HLS_STREAMING_STATE_UPDATED)    // when hls streaming state is updated     ;   "
    },
    {
        "title": "Event Listeners",
        "link": "/react-native/v2/features/event-listeners",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/event-listeners",
        "keywords": [],
        "content": "    Common Events Implementation Lifecycle The below diagram shows an overview of which Events are emitted and at what time when you Join or Preview a Room. You will start receiving event updates after you call preview or join method on HMSSDK instance.   Common Events Implementation Lifecycle Diagram (/guides/rn-event-lifecycle-diagram.png)   All Available Event Listeners You can easily add or remove an event listener by using the addEventListener() or removeEventListener() method. HMSUpdateListenerActions is an enum class which has all the types of event listeners.   js import    HMSSDK,  HMSUpdateListenerActions,  HMSException,  HMSMessage,  HMSLeaveRoomRequest,  HMSSpeaker,  HMSPeer,  HMSTrack   from '@100mslive/react-native-hms'; const hmsInstance = await HMSSDK.build(); hmsInstance.addEventListener(  HMSUpdateListenerActions.ON_PREVIEW,  onPreviewListener ); const onPreviewListener = (  previewTracks  ) =>    const videoTrack = previewTracks.videoTrack;  const videoTrackId = videoTrack.trackId;  // gets triggered when we call preview function. You can set your camera and mic on/off while joining.  // perform action  ; hmsInstance.addEventListener(HMSUpdateListenerActions.ON_JOIN, onJoinListener); const onJoinListener = (  localPeer, remotePeers  ) =>    // gets triggered when join is successful. You can navigate to other screens.  // use these objects to update your local and remote peers.  ; hmsInstance.addEventListener(  HMSUpdateListenerActions.ON_PEER_UPDATE,  onPeerListener ); const onPeerListener = (    peer,   type,  :     peer: HMSPeer;   type: HMSPeerUpdate;  ) =>    // gets triggered when peer leaves, joins, peer's audio or video is muted, starts or stops speaking, role is changed or becomes dominant speaker.  // use these objects to update your local and remote peers.  ; hmsInstance.addEventListener(  HMSUpdateListenerActions.ON_TRACK_UPDATE,  onTrackListener ); const onTrackListener = (    track,   peer,   type,  :     track: HMSTrack;   peer: HMSPeer;   type: HMSTrackUpdate;  ) =>    // gets triggered when track is added, removed, muted, unmuted, degraded and restored back.  // use these objects to update your local and remote peers.  ; hmsInstance.addEventListener(  HMSUpdateListenerActions.ON_ERROR,  onErrorListener ); const onErrorListener = (data: HMSException) =>    // gets triggered whenever some error occours with a error description. You can either log it or navigate to some error screen.  // data contains a error code and message due to which error occoured.  ; hmsInstance.addEventListener(  HMSUpdateListenerActions.ON_MESSAGE,  onMessageListener ); const onMessageListener = (data: HMSMessage) =>    // gets triggered whenever you receive a direct message, broadcasted message or role-based message.  // whenever local peer receives a message this is triggered. Add the message to reducer.  ; hmsInstance.addEventListener(  HMSUpdateListenerActions.ON_SPEAKER,  onSpeakerListener ); const onSpeakerListener = (data: HMSSpeaker  ) =>    // gets triggered whenever someone speaks  // an array of speakers is received. Use it to highlight the speakers.  ; hmsInstance.addEventListener(  HMSUpdateListenerActions.RECONNECTING,  onReconnectingListener ); hmsInstance.addEventListener(  HMSUpdateListenerActions.ON_ROOM_UPDATE,  onRoomListener ); const onRoomListener = (    room,   type,   :     room: HMSRoom;   type: HMSRoomUpdate;   ) =>    // gets triggered when room is muted or unmuted.  ; const onReconnectingListener = (data) =>    // triggered whenever local peer is trying to reconnect to room i.e. bad network.  ; hmsInstance.addEventListener(  HMSUpdateListenerActions.RECONNECTED,  onReconnectedListener ); const onReconnectedListener = (data) =>    // triggered when local peer is reconnected to the room.  ; hmsInstance.addEventListener(  HMSUpdateListenerActions.ON_ROLE_CHANGE_REQUEST,  onRoleChangeRequestListener ); const onRoleChangeRequestListener = (data: HMSRoleChangeRequest) =>    // triggered when someone requests a role change for local peer. We can get data.requestedBy.name, data.suggestedRole.name  // You can show a modal allowing user to accept or decline the role change request whenever this is triggered.  ; hmsInstance.addEventListener(  HMSUpdateListenerActions.ON_REMOVED_FROM_ROOM,  onRemovedFromRoomListener ); const onRemovedFromRoomListener = (data: HMSLeaveRoomRequest) =>    // triggered whenever someone removes local peer from the room or the room is ended.  // You can navigate to home screen, clear all reducers and reset all the states whenever this is triggered  ; hmsInstance.addEventListener(  HMSUpdateListenerActions.ON_CHANGE_TRACK_STATE_REQUEST,  onChangeTrackStateRequest ); const onChangeTrackStateRequest = (data: HMSChangeTrackStateRequest) =>    // triggered when someone requests a unmute for local peer. We can get data.requestedBy.name, data.trackType  // You can show a modal allowing user to accept or decline the role change request whenever this is triggered.  ; hmsInstance?.addEventListener(  HMSUpdateListenerActions.ON_LOCAL_AUDIO_STATS,  onChangeLocalAudioStats, ); const onChangeLocalAudioStats = (data:    localAudioStats: HMSLocalAudioStats;  track: HMSLocalAudioTrack;  peer: HMSPeer;  ) =>    // This contains stats related to local audio track.  ; hmsInstance?.addEventListener(  HMSUpdateListenerActions.ON_LOCAL_VIDEO_STATS,  onChangeLocalVideoStats, ); const onChangeLocalVideoStats = (data:    localVideoStats: HMSLocalVideoStats;  track: HMSLocalVideoTrack;  peer: HMSPeer;  ) =>    // This contains stats related to local video track.  ; hmsInstance?.addEventListener(  HMSUpdateListenerActions.ON_RTC_STATS,  onChangeRtcStats, ); const onChangeRtcStats = (data:  rtcStats: HMSRTCStatsReport ) =>    // This will contain the combined stats for the room.  ;  hmsInstance?.addEventListener(  HMSUpdateListenerActions.ON_REMOTE_AUDIO_STATS,  onChangeRemoteAudioStats, ); const onChangeRemoteAudioStats = (data:    remoteAudioStats: HMSRemoteAudioStats;  track: HMSRemoteAudioTrack;  peer: HMSPeer;  ) =>    // This contains stats related to remote audio track.  ; hmsInstance?.addEventListener(  HMSUpdateListenerActions.ON_REMOTE_VIDEO_STATS,  onChangeRemoteVideoStats, ); const onChangeRemoteVideoStats = (data:    remoteVideoStats: HMSRemoteVideoStats;  track: HMSRemoteVideoTrack;  peer: HMSPeer;  ) =>    // This contains stats related to remote video track.  ;   Check out the  Event Listener Enums (../event-listeners-enums) docs to understand the Update Types emitted by the SDK for events like ON_PEER_UPDATE , ON_TRACK_UPDATE , ON_ROOM_UPDATE , etc. "
    },
    {
        "title": "HLS Streaming & Recording",
        "link": "/react-native/v2/features/hls",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/hls",
        "keywords": [],
        "content": "   HLS Streaming (./../foundation/live-streaming) allows for scaling to millions of viewers in near real time. You can give a link of your web-app which will be converted to a HLS feed by our server and can be played across devices for consumption. Behind the scenes, this will be achieved by having a bot join your room and stream what it sees and hears. Once the feed is ready, the server will give a URL which can be played using any HLS Player. > Note that the media server serving the content in this case is owned by 100ms. If you're looking for a way to stream on YouTube, Twitch, etc. then please have a look at our RTMP streaming docs  here (recording).   Permissions Can't let just anyone stream and record using HLS. First, you need to create a  role (../foundation/templates-and-roles) with the \"HLS livestream\" and \"Beam Recording\" permissions. You can check if you (local peer) have permission to HLS stream as follows    ts const localPeer = await hmsInstance.getLocalPeer(); // Permissions are available on HMSRole object of Local Peer const localPeerPermissions: HMSPermissions = localPeer.role?.permissions; // Check if Local Peer has HLS Live stream permission const canHLSStream: boolean = localPeerPermissions?.hlsStreaming;     Starting HLS Stream HLS can be started in two ways depending on the level of customization you need  1. Default View: The simplest view to just begin a stream with default UI and parameters. 2. Custom Views: To use your own UI for HLS streaming, you need to provide your own web-app URL for our bot to join and stream. Also, we can record the stream.   Default View Begins a stream with default parameters. You can use startHLSStreaming method available on HMSSDK instance to start the HLS Stream with default parameters as follows    js try    await hmsInstance.startHLSStreaming();  console.log('HLS Stream Start Success');   catch (error)    console.log('HLS Stream Start Error: ', error);       Custom View To use our own browser UI for HLS, you'll need to pass in a meeting URL. The 100ms bot will open this URL to join your room, so it must allow access without any user level interaction. Let's understand this by following steps: <div className=\"steps-container\">    Create an HMSHLSConfig instance  HMSHLSConfig object will be used to start the HLS Stream with customizations.   js import   HMSHLSConfig, HMSHLSRecordingConfig, HMSHLSMeetingURLVariant   from '@100mslive/react-native-hms'; const hlsURLVariant = new HMSHLSMeetingURLVariant(   meetingUrl: 'https://...app.100ms.live/streaming/meeting...',  metadata: '',  ); const hlsRecordingConfig = new HMSHLSRecordingConfig(   singleFilePerLayer: false,  videoOnDemand: false  ); const hlsConfig = new HMSHLSConfig(   hlsRecordingConfig: hlsRecordingConfig,  meetingURLVariants:  hlsURLVariant   );   Let's look at how HMSHLSConfig object looks like    js interface HMSHLSConfig    hlsRecordingConfig?: HMSHLSRecordingConfig;  meetingURLVariants?: HMSHLSMeetingURLVariant  ;     Now, Let's look at its properties one by one:    hlsRecordingConfig    To record the HLS stream you may specify an HMSHLSRecordingConfig object within the HMSHLSConfig config object.   Here's what the HMSHLSRecordingConfig looks like     js  interface HMSHLSRecordingConfig     singleFilePerLayer: boolean;   videoOnDemand: boolean;        1. singleFilePerLayer if the desired end result is a mp4 file per HLS layer. false by default.  2. videoOnDemand if the desired end result is a zip of m3u8 and all the chunks, false by default.    meetingURLVariants    This is a list of HMSHLSMeetingURLVariant objects. In the future, it'll be possible to start HLS Stream for multiple such URLs for the same room. So it's a list but currently only the first element of the list is used. To distinguish between multiple URLs an additional field metadata can be passed.  Here's what the HMSHLSMeetingURLVariant looks like     js  interface HMSHLSMeetingURLVariant     meetingUrl: string;   metadata: string;        1. meetingUrl URL of the meeting you wish to record  2. metadata meta-data (any extra info to identify the URL) you wish to pass with the URL.    Call startHLSStreaming method on HMSSDK instance After creating config ( HMSHLSConfig ) object, You can use startHLSStreaming method available on HMSSDK instance with HMSHLSConfig object to start the HLS Stream as follows  Make sure you have above mentioned permissions for starting HLS Stream.   js try    // Calling startHLSStreaming with config created in above step  await hmsInstance.startHLSStreaming(hlsConfig);  console.log('Start Hls Success');   catch (error)    console.log('Start Hls Error: ', error);     </div>   Stopping HLS Stream You can use stopHLSStreaming method available on HMSSDK instance to stop HLS Streaming. This will stop all the streaming variants.   js try    await hmsInstance.stopHLSStreaming();  console.log('Stop Hls Success');   catch (error)    console.log('Stop Hls Error: ', error);     Want to see how this works in a live project? Take a look at our  advanced sample app (https://github.com/100mslive/react-native-hms/tree/main/example).   Current Room Status The current status for the room is always reflected in the HMSRoom object. This can be used to show the live stream status on UI something similar to this:   stream-state (/docs/v2/flutter-stream-state.jpg)  hlsStreamingState is the property inside the HMSRoom object which you can read to get the current HLS streaming status of the room. This object contains a boolean running which lets you know if HLS is running in the room right now. Also, It contains a list of m3u8 URLs variants , which you can use along with  react-native-video (https://www.npmjs.com/package/react-native-video) library to display HLS Stream on the screen.   hlsStreamingState   is an instance of HMSHLSStreamingState , which looks like:   js interface HMSHLSStreamingState    running: boolean;  variants: HMSHLSVariant  ;     1. running  true indicates that HLS streaming is running 2. variants  This represents a live stream to one or more HLS URLs in the container of HMSHLSVariant . Which looks like:   js interface HMSHLSVariant    hlsStreamUrl?: string; // It contains m3u8 url which we will use to show the stream  meetingUrl?: string; // URL of the room which is getting streamed  metadata?: string; // Extra info about the room corresponding to meetingUrl which is passed in HMSHLSMeetingURLVariant while starting HLS  startedAt?: Date; // time at which HLS Stream was started       When to check for room status The room status should be checked in following two places  1. Inside onJoinListener function which is subscribed to HMSUpdateListenerActions.ON_JOIN event 2. Inside onRoomListener function which is subscribed to HMSUpdateListenerActions.ON_ROOM_UPDATE event. The type property will be HMSRoomUpdate.HLS_STREAMING_STATE_UPDATED .    Key Tips   If you're using the dashboard web app from 100ms, please make sure to use a role which doesn't have publish permissions, for beam tile to not show up.   If using your own web app, do put in place retries for API calls like tokens etc. just in case any call fails. As human users, we're used to reloading the page in these scenarios which is difficult to achieve in the automated case.   Make sure to not disable the logs for the passed-in meeting URL. This will allow for us to have more visibility into the room, refreshing the page if join doesn't happen within a time interval. "
    },
    {
        "title": "Integrating The SDK",
        "link": "/react-native/v2/features/integration",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/integration",
        "keywords": [],
        "content": "    Installing the dependencies   bash npm install save @100mslive/react-native-hms     Permissions   For iOS Add following lines in Info.plist file   xml <key>NSCameraUsageDescription</key> <string>Please allow access to Camera to enable video conferencing</string> <key>NSMicrophoneUsageDescription</key> <string>Please allow access to Microphone to enable video conferencing</string> <key>NSLocalNetworkUsageDescription</key> <string>Please allow access to network usage to enable video conferencing</string>     For Android Add following permissions in AndroidManifest.xml   xml <uses-feature android:name=\"android.hardware.camera\"  <uses-feature android:name=\"android.hardware.camera.autofocus\"  <uses-permission android:name=\"android.permission.CAMERA\"  <uses-permission android:name=\"android.permission.CHANGE_NETWORK_STATE\"  <uses-permission android:name=\"android.permission.MODIFY_AUDIO_SETTINGS\"  <uses-permission android:name=\"android.permission.RECORD_AUDIO\"  <uses-permission android:name=\"android.permission.INTERNET\"  <uses-permission android:name=\"android.permission.ACCESS_NETWORK_STATE\"  <uses-permission android:name=\"android.permission.FOREGROUND_SERVICE\"   <uses-permission android:name=\"android.permission.BLUETOOTH\" android:maxSdkVersion=\"30\"   <uses-permission android:name=\"android.permission.BLUETOOTH_CONNECT\"     You will also need to request Camera and Record Audio permissions at runtime before you join a call or display a preview. Please follow  Android Documentation (https://developer.android.com/training/permissions/requesting request-permission) for runtime permissions. We suggest using  react-native-permission (https://www.npmjs.com/package/react-native-permissions) to acquire permissions from both platforms. > Note: Compiling with Kotlin for compileSdkVersion 30 requires JDK 11 or above   Proguard configuration    100ms Android SDK 2.1.2 or higher If using 100ms Android SDK version 2.1.2 and higher proguard configuration is bundled inside it. No additional code is required.    Prior to Android SDK version 2.1.2 Prior to Android SDK version 2.1.2, the following configuration was needed in release builds. Add it to your proguard-rules.pro   js  100ms proguard rules -keep class org.webrtc.      ;   -keep class live.hms.video.      ;       Expo Setup 100ms React Native SDK supports development with  Expo (https://expo.dev/). Following is a guide to set it up  1. Install node modules  bash npm install   2. Install expo-cli globally to run the Expo commands  bash npm install global expo-cli   3. Install Expo Dev Client  bash expo install expo-dev-client   4. Now we can run on iOS and Android  bash expo run: ios -d    bash expo run: android -d   Alternatively, you can run it from IDE of your choice (VS Code, Android Studio, Xcode, etc) and start the dev server from terminal (it will help running on devices). To start the server use the following command   bash expo start dev-client       Github Repo (https://github.com/100mslive/react-native-hms/) You can checkout the 100ms React Native SDK Github repo which also contains a fully fledged  Example app implementation here (https://github.com/100mslive/react-native-hms/)      Example App (https://github.com/100mslive/react-native-hms/tree/develop/example) In the  100ms Example App (https://github.com/100mslive/react-native-hms/tree/main/example) we have shown how to setup the various listeners, what data to store in the redux and what all features you can implement. We have also implemented multiple views which are commonly used. Checkout the  videos & relevant code in the Example app (https://github.com/100mslive/react-native-hms/tree/main/example additional-features). "
    },
    {
        "title": "Join Room",
        "link": "/react-native/v2/features/join",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/join",
        "keywords": [],
        "content": "    Overview To join and interact with others in audio or video call, the user needs to join a room . When user indicates that they want to _join_ the room, your app should have  1. User Name  The name which should be displayed to other peers in the room. 2.  Authentication Token (../guides/token)  The Client side Authentication Token generated by the Token Service. You can also optionally pass these fields  1. Track Settings  Such as joining a Room with Muted Audio or Video using the HMSTrackSettings object. More information available  here ( join-with-muted-audio-video) 2. User Metadata  This can be used to pass any additional metadata associated with the user using metadata of  HMSConfig object (https://www.100ms.live/docs/api-reference/react-native/v2/classes/HMSConfig.html). For Eg: user-id mapping at the application side. More information is available  here (../advanced-features/change-metadata).   Join a Room We'll call the join method on HMSSDK object with a config containing above fields to join the room. <div className=\"steps-container\">   Create HMSSDK instance First, create an instance of HMSSDK using the build function.   js import   HMSSDK   from '@100mslive/react-native-hms'; // create HMSSDK instance using the build function const hmsInstance = await HMSSDK.build();     Attach Event Listeners You'll need to add Event Listeners for HMSUpdateListenerActions , which are invoked to notify about updates happening in the room like a peer joins/leaves, a track got muted/unmuted, etc.   js // add Event Listeners to subscribe to Join Success or Failure updates hmsInstance.addEventListener(HMSUpdateListenerActions.ON_ERROR, onError); hmsInstance.addEventListener(HMSUpdateListenerActions.ON_JOIN, onJoin);     Create HMSConfig object Next, create an object of  HMSConfig class (https://www.100ms.live/docs/api-reference/react-native/v2/classes/HMSConfig.html) using the available joining configurations.   js let config = new HMSConfig(    authToken: 'eyJH5c...', // client-side token generated from your token service   username: 'John Appleseed'  );     Invoke Join Now, we are primed to join the room. All you have to do is calling join by passing the config object.   js hmsInstance.join(config);   </div> After calling join your app will be provided an update from the 100ms SDK. ✅ If successful, the onJoin function which is listening for HMSUpdateListenerActions.ON_JOIN event will be invoked with information about the room encapsulated in the HMSRoom object. ❌ If failure, the onError function which is listening for HMSUpdateListenerActions.ON_ERROR event will be invoked with exact failure reason. Once you get onJoin callback, You have joined a room successfully 🥳   Join with Muted Audio / Video Joining with Muted Audio / Video is a customization that sets the microphone and camera state before joining the room. By default 100ms SDK keeps the microphone and camera ON but by using this feature you can decide their state according to your use case. HMSSDK provides this capability using initialState property of Audio & Video Track Settings. Here, initialState property of HMSAudioTrackSettings and HMSVideoTrackSettings is an Enum of type HMSTrackSettingsInitState which has only two possible values as    js enum HMSTrackSettingsInitState     // If the track needs to be kept mute while joining   MUTED,   // If the track needs to be kept unmute while joining   UNMUTED     Let's see how this can be achieved in the following steps. <div className=\"steps-container\">   Create Track Settings object First, create an instance of HMSTrackSettings , it will be used when we initialize our SDK. For joining a Room with Muted Audio and/or Video, We set initialState property of HMSAudioTrackSettings and/or HMSVideoTrackSettings with HMSTrackSettingsInitState.MUTED as follows    js // Customize Audio & Video initial states as per user discretion const getTrackSettings = () =>     let audioSettings = new HMSAudioTrackSettings(      initialState: HMSTrackSettingsInitState.MUTED    );   let videoSettings = new HMSVideoTrackSettings(      initialState: HMSTrackSettingsInitState.MUTED    );   return new HMSTrackSettings(      video: videoSettings,     audio: audioSettings    );  ; // Get the Track Settings object const trackSettings = getTrackSettings();     Create HMSSDK instance Now, create an instance of HMSSDK by passing the HMSTrackSettings object created above to the build function    js import   HMSSDK   from '@100mslive/react-native-hms'; // Pass the Track Settings object to the build function c̶o̶n̶s̶t̶ ̶h̶m̶s̶I̶n̶s̶t̶a̶n̶c̶e̶ ̶=̶ ̶a̶w̶a̶i̶t̶ ̶H̶M̶S̶S̶D̶K̶.̶b̶u̶i̶l̶d̶(̶)̶;̶ const hmsInstance = await HMSSDK.build(  trackSettings  );   Rest all the steps are same as above. Now, the user joins the room with the microphone and the camera turned off. </div>   Update Listeners 100ms SDK provides multiple Event Listeners  HMSUpdateListenerActions which notifies about the updates happening in the room like a peer joins/leaves, a track got muted/unmutes, etc. After calling join your app will be provided an update from the 100ms SDK. ✅ If successful, the HMSUpdateListenerActions.ON_JOIN event will be triggered with information about the room encapsulated in the HMSRoom object. ❌ If failure, the HMSUpdateListenerActions.ON_ERROR event will be triggered with exact failure reason in HMSException object. Depending on your use case, you'll need to implement a selective set of the Update Listener actions. The most common ones are ON_JOIN, ON_PEER_UPDATE, ON_TRACK_UPDATE & ON_ERROR. You'll need to implement these listeners to perform UI Actions, update App State, etc. These updates can be used to render the video on screen or to display other info regarding the room.   js const hmsInstance = await HMSSDK.build(); hmsInstance.addEventListener(HMSUpdateListenerActions.ON_JOIN, onJoinListener); // gets triggered when join is successful const onJoinListener = (data:   room: HMSRoom  ) =>     handleJoin(data.room);  ; hmsInstance.addEventListener(HMSUpdateListenerActions.ON_PEER_UPDATE, onPeerListener); const onPeerListener = (  peer, type  :   peer: HMSPeer, type: HMSPeerUpdate  ) =>     // gets triggered when peer leaves, joins, peer's audio or video is muted, starts or stops speaking, role is changed or becomes dominant speaker.   // use these objects to update your local and remote peers.  ; hmsInstance.addEventListener(HMSUpdateListenerActions.ON_TRACK_UPDATE, onTrackListener); const onTrackListener = (    track,   peer,   type  :     track: HMSTrack,   peer: HMSPeer,   type: HMSTrackUpdate  ) =>     // gets triggered when track is added, removed, muted, unmuted, degraded and restored back.   // use these objects to update your local and remote peers.  ; hmsInstance.addEventListener(HMSUpdateListenerActions.ON_ERROR, onErrorListener); const onErrorListener = (data: HMSException) =>     // gets triggered whenever some error occours with a error description. You can either log it or navigate to some error screen.   // data contains a error code and message due to which error occoured.  ; hmsInstance.addEventListener(HMSUpdateListenerActions.ON_MESSAGE, onMessageListener); const onMessageListener = (data: HMSMessage) =>     // gets triggered whenever you receive a direct message, broadcasted message or role-based message.   // whenever local peer receives a message this is triggered. Add the message to reducer.  ; hmsInstance.addEventListener(HMSUpdateListenerActions.ON_SPEAKER, onSpeakerListener); const onSpeakerListener = (data: HMSSpeaker  ) =>     // gets triggered whenever someone speaks   // an array of speakers is received. Use it to highlight the speakers.  ; hmsInstance.addEventListener(HMSUpdateListenerActions.ON_ROOM_UPDATE, onRoomListener); const onRoomListener = (  room, type  :   room?: HMSRoom, type?: HMSRoomUpdate  ) =>     // gets triggered when room is muted or unmuted.   // use these objects to update your local and remote peers.  ; hmsInstance.addEventListener(HMSUpdateListenerActions.ON_PREVIEW, onPreviewListener); // gets triggered when we call preview function before joining a room const onPreviewListener = (  previewTracks  ) =>     const videoTrack = previewTracks.videoTrack;   const videoTrackId = videoTrack.trackId;  ; hmsInstance.addEventListener(HMSUpdateListenerActions.RECONNECTING, onReconnectingListener); const onReconnectingListener = (data) =>     // triggered whenever local peer is trying to reconnect to room in a bad network  ; hmsInstance.addEventListener(HMSUpdateListenerActions.RECONNECTED, onReconnectedListener); const onReconnectedListener = (data) =>     // triggered when local peer is reconnected to the room.  ; hmsInstance.addEventListener(   HMSUpdateListenerActions.ON_ROLE_CHANGE_REQUEST,   onRoleChangeRequestListener ); const onRoleChangeRequestListener = (data: HMSRoleChangeRequest) =>     // triggered when someone requests a role change for local peer. We can get data.requestedBy.name, data.suggestedRole.name   // You can show a modal allowing user to accept or decline the role change request whenever this is triggered.  ; hmsInstance.addEventListener(   HMSUpdateListenerActions.ON_REMOVED_FROM_ROOM,   onRemovedFromRoomListener ); const onRemovedFromRoomListener = (data: HMSLeaveRoomRequest) =>     // triggered whenever someone removes local peer from the room or the room is ended.   // You can navigate to home screen, clear all reducers and reset all the states whenever this is triggered  ; hmsInstance.addEventListener(   HMSUpdateListenerActions.ON_CHANGE_TRACK_STATE_REQUEST,   onChangeTrackStateRequest ); const onChangeTrackStateRequest = (data: HMSChangeTrackStateRequest) =>     // triggered when someone requests a unmute for local peer. We can get data.requestedBy.name, data.trackType   // You can show a modal allowing user to accept or decline the role change request whenever this is triggered.  ;   "
    },
    {
        "title": "Leave Room",
        "link": "/react-native/v2/features/leave",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/leave",
        "keywords": [],
        "content": "  Once you're done with the meeting and want to exit, call leave on the HMSSDK instance you created to join the room. Before calling leave, remove the previously added Event Listeners (HMSUpdateListenerActions) as    js // Removing all registered event listeners hmsInstance.removeAllListeners();   To leave the meeting, call the leave method of HMSSDK instance. leave method returns promise which is resolved upon completion of leave process.   js try    await hmsInstance.leave();  console.log('Leave Success');   catch (error)    console.log('Leave Error: ', error);     After leaving the meeting, you can free your apps' resources if you don't need the HMSSDK instance anymore, check out  Release Resources Docs (https://www.100ms.live/docs/react-native/v2/features/release-resources) "
    },
    {
        "title": "Logging",
        "link": "/react-native/v2/features/logger",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/logger",
        "keywords": [],
        "content": "  HMSLogger is a logging api that lets you see all the other api logs as they are called. Instead of logging the values on your app in each and every function you can easily monitor and debug via these logs. By default all the logs are disabled. You need to call updateLogLevel to enable logs.   js import   HMSLogger, HMSLogLevel   from '@100mslive/react-native-hms'; // create HMSLogger instance and set log level const logger = new HMSLogger(); logger.updateLogLevel(HMSLogLevel.VERBOSE, true); // instance acquired from build method build.setLogger(logger);     Update Log level  js logger.updateLogLevel(HMSLogLevel.VERBOSE, false); logger.updateLogLevel(HMSLogLevel.WARNING, true);     HMSLogLevel You can subscribe to different log levels by passing HMSLogLevel enums to HMSLogger  HMSLogLevel.VERBOSE Verbose will display basic logs from the sdk in the console  HMSLogLevel.WARNING Warnings will be displayed on screen with a yellow background. These alerts are known as YellowBoxes. Click on the alerts to show more information or to dismiss them.  HMSLogLevel.ERROR In-app errors are displayed in a full screen alert with a red background inside your app. This screen is known as a RedBox.   Example   HMSLogger Example (/docs/v2/hmsLoggerExample.png)"
    },
    {
        "title": "Mute & Unmute",
        "link": "/react-native/v2/features/mute",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/mute",
        "keywords": [],
        "content": "  Mute is something that applies to both audio and video. When you mute audio, you can't be heard by other people i.e your audio is OFF. When you mute a video, you will not be broadcasting your video to other people i.e your video is OFF.   Check Mute Status Current Mute Status of any HMSTrack can be checked by calling isMute method on it. In most of the cases, You would need current mute status to toggle mute and render correct mic and camera icons.   js // You can get HMSTrack object from HMSPeer object // or HMSUpdateListenerActions.ON_TRACK_UPDATE event const audioMuted = peer.audioTrack?.isMute(); // audioTrack property can be undefined const videoMuted = peer.videoTrack?.isMute(); // videoTrack property can be undefined     Change Your(Local Peer) Mute Status To switch your \"video\" or \"audio\" on/off, you can call setMute method available on HMSLocalVideoTrack or HMSLocalAudioTrack objects respectively with true or false values. If you want to toggle mute status of your \"video\" or \"audio\", then you can first get the current mute status and then pass opposite value to setMute method.   js // instance acquired from build() method const localPeer = await hmsInstance.getLocalPeer(); // Muting Example   localPeer.localAudioTrack().setMute(true); // Muting Local Audio localPeer.localVideoTrack().setMute(true); // Muting Local Video  // Toggling Mute Example  const audioMuted = localPeer.audioTrack?.isMute(); // Toggling Local Audio, audio will be unmuted localPeer.localAudioTrack().setMute( audioMuted); // passing opposite value of audioMuted const videoMuted = localPeer.videoTrack?.isMute(); // Toggling Local Video, video will be unmuted localPeer.localVideoTrack().setMute( videoMuted); // passing opposite value of videoMuted    For muting remote peer refer to  Change Track State (./change-track-state) docs and for muting remote peers locally refer to  Playback Allowed (./playback-allowed) docs. "
    },
    {
        "title": "Playback Allowed",
        "link": "/react-native/v2/features/playback-allowed",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/playback-allowed",
        "keywords": [],
        "content": "  Mute is something that applies to both audio and video and its possible to   mute remote peers only for yourself  . When you will mute audio or video track of remote peer, you won't be able to hear or see the remote person but it will be audible and visible to others. You can use \"Playback Allowed\" API to \"mute Audio and/or Video\" or \"check mute status\" of Remote Peers only for yourself (locally). If you set \"Playback Allowed\" to false , then it means remote peers' tracks can't be played, meaning \"Mute Status\" is set to true and vice-versa.   Check Mute Status of Remote Peers locally To check playback allowed status, you can call isPlaybackAllowed method on HMSRemoteAudioTrack and HMSRemoteVideoTrack objects available on HMSRemotePeer object. You can get list of HMSRemotePeer objects from getRemotePeers method available on HMSSDK instance created using build method.   js // getting list of Remote Peers const remotePeers = await hmsInstance.getRemotePeers(); // Make sure you have atleast one remote peer, otherwise list will be empty, and firstRemotePeer will be undefined const firstRemotePeer = remotePeers 0 ; // Checking mute status of first remote peer const isAudioPlaybackAllowed = firstRemotePeer.remoteAudioTrack().isPlaybackAllowed(); const isVideoPlaybackAllowed = firstRemotePeer.remoteVideoTrack().isPlaybackAllowed();     Muting Remote Peers for yourself (locally) To change playback allowed status, you can call setPlaybackAllowed method on HMSRemoteAudioTrack and HMSRemoteVideoTrack objects available on HMSRemotePeer object with true or false values. You can get list of HMSRemotePeer objects from getRemotePeers method available on HMSSDK instance created using build method. You can set playback for a certain remote peer's audio or video. If you set it to false it will turn off the audio or video only for you. You can revert it back by setting it true. > Note: If you want to Mute/Unmute a remote peer   for the whole room  , check out  Mute Remote Peer Docs (./change-track-state)   js // getting list of Remote Peers const remotePeers = await hmsInstance.getRemotePeers(); // Make sure you have atleast one remote peer, otherwise list will be empty, and firstRemotePeer will be undefined const firstRemotePeer = remotePeers 0 ; // Changing mute status of first remote peer for yourself only firstRemotePeer.remoteAudioTrack().setPlaybackAllowed(false); firstRemotePeer.remoteVideoTrack().setPlaybackAllowed(true);     Muting \"Audio\" of All Remote Peers for yourself (locally) You can mute \"audio\" of all remote peers at once for yourself. Use setPlaybackForAllAudio method available on HMSSDK instance created using build method with true or false values.   js hmsInstance.setPlaybackForAllAudio(true) // Mute Locally only hmsInstance.setPlaybackForAllAudio(false) // Unmute Locally only   "
    },
    {
        "title": "Preview",
        "link": "/react-native/v2/features/preview",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/preview",
        "keywords": [],
        "content": "    Overview Preview screen is a frequently used UX element which allows users to check if their input devices are working properly and set the initial state (mute/unmute) of their audio and video tracks before joining. 100ms SDKs provide an easy-to-use API to back this feature. Additionally, the SDK will try to establish a connection to 100ms server to verify there are no network issues and that the auth credentials are valid so that if everything is in order the subsequent room join will be much faster.   Start Preview We'll call the preview method on HMSSDK object with a HMSConfig object. <div className=\"steps-container\">   Create HMSSDK instance First, create an instance of HMSSDK using the build function.   js import   HMSSDK   from '@100mslive/react-native-hms'; // create HMSSDK instance using the build function const hmsInstance = await HMSSDK.build();     Attach Event Listeners It is advised to attach Event Listeners for HMSUpdateListenerActions.ON_ERROR and HMSUpdateListenerActions.ON_PREVIEW before calling the preview function to get updates if preview got executed successfully or if it failed.   js const onPreview = (data:   room: HMSRoom; previewTracks: HMSTrack  ;  ) =>    // You can use previewTracks to render preview for the local peer  console.log(data.previewTracks);   // add Event Listeners to subscribe to Preview Success or Failure updates hmsInstance.addEventListener(HMSUpdateListenerActions.ON_ERROR, onError); hmsInstance.addEventListener(HMSUpdateListenerActions.ON_PREVIEW, onPreview);     Create HMSConfig object  Next, create an object of HMSConfig class using the available joining configurations.   js const config = new HMSConfig(            authToken: 'eyJH5c...', // client-side token generated from your token service           username: 'John Appleseed',          );     Invoke Preview function Now, you are primed to preview the room and local tracks. All you have to do is call the preview method on HMSSDK object with a HMSConfig object.   js hmsInstance.preview(config);   After calling preview your app will be provided an update from the 100ms SDK. ✅ If successful, the onPreview function which is listening for HMSUpdateListenerActions.ON_PREVIEW event will be invoked with information about the Room and Local Audio & Video tracks. ❌ If failure, the onError function which is listening for HMSUpdateListenerActions.ON_ERROR event will be invoked with exact failure reason.    Render Preview Tracks In case of preview success, onPreview provides an array of local tracks in the parameter previewTracks . You can use local audio track to change and show audio track mute status. You can use trackId from local video track to  Render Video (https://www.100ms.live/docs/react-native/v2/features/render-video) of local peer, change and show video track mute status. Refer to  Mute and Unmute Guide (https://www.100ms.live/docs/react-native/v2/features/mute) to know how to check and change mute status.   js // Get Local Audio Track from preview tracks const regularAudioTrack = previewTracks.find(previewTrack =>    return previewTrack.source === HMSTrackSource.REGULAR && previewTrack.type === HMSTrackType.AUDIO;  ); // Get Local Video Track from preview tracks const regularVideoTrack = previewTracks.find(previewTrack =>    return previewTrack.source === HMSTrackSource.REGULAR && previewTrack.type === HMSTrackType.VIDEO;  ); // If we have regular video track, then we can render it if (regularVideoTrack)    return (   <HmsView    trackId= regularVideoTrack.trackId  // Render Video track by using its' trackId    scaleType= HMSVideoViewMode.ASPECT_FILL     style= styles.hmsView     mirror= true       );       Receive Room Updates To receive the Room updates like number of peers in the room or when new peer joins the room, You can also add listener for HMSUpdateListenerActions.ON_ROOM_UPDATE as follows  > Note: To receive Room State in preview i.e. before joining the room, You can enable this feature in  Template (https://www.100ms.live/docs/react-native/v2/foundation/templates-and-roles) of the room. Refer  here ( get-peer-updates-and-room-state-in-preview) for more info   js const onRoomUpdate = (data:   room: HMSRoom; type: HMSRoomUpdate  ) =>    console.log(data.room.peerCount); // Number of peers inside Room  console.log(data.room.peers); // list of peers inside Room   // This listener is called when there is a change in any property of the Room hmsInstance.addEventListener(HMSUpdateListenerActions.ON_ROOM_UPDATE, onRoomUpdate);     Remove Preview event listener When you are ready to  Join Room (https://www.100ms.live/docs/react-native/v2/features/join) or navigating away from preview, you don't need to keep listening to preview updates. To stop receiving preview updates, we can remove the preview event listener as follows    js hmsInstance.removeEventListener(HMSUpdateListenerActions.ON_PREVIEW);   </div>   Get Peer Updates and Room State in Preview To enable Room State and Peer Updates in the preview, we need to enable room state from the dashboard. This can be enabled by selecting a template and then navigating to advanced settings.   Advanced settings (/docs/v2/flutter-advanced-settings.png) These options are available in advanced settings:   Room State in preview (/docs/v2/flutter-room-state-in-preview.png) "
    },
    {
        "title": "Reconnection Handling",
        "link": "/react-native/v2/features/reconnecting-reconnected",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/reconnecting-reconnected",
        "keywords": [],
        "content": "  Real-world apps operate in varying network conditions and thus require handling for network bandwidth issues. The SDK provides callbacks when a user gets disconnected due to network issues or network switch and also when the user gets reconnected again. > 💡 Note: The SDK tries to reconnect automatically for 60 seconds after that the connection is terminated, the peer is removed from the room and the room has to be rejoined.   Reconnecting & Reconnected Events  HMSUpdateListenerActions.RECONNECTING event is emitted when SDK detects a network issue and it's trying to Reconnect with servers.  HMSUpdateListenerActions.RECONNECTED event is emitted when SDK has successfully recovered from a network drop, switch or a network issue etc. You should subscribe to these events to receive Reconnecting & Reconnected updates as follows    js const onReconnectingListener = (  error  :   error: HMSException  ) =>    console.log('SDK is trying to Reconnect to servers due to error: ', error);   // Subscribing to Reconnecting updates hmsInstance.addEventListener(HMSUpdateListenerActions.RECONNECTING, onReconnectingListener);  const onReconnectedListener = () =>    console.log('SDK has successfully Reconnected to servers ');   // Subscribing to Reconnected updates hmsInstance.addEventListener(HMSUpdateListenerActions.RECONNECTED, onReconnectedListener);   > 💡 Note: On getting reconnected, the SDK will send all the updates(Track and Peer updates) again.   Handle case when user fails to reconnect SDK tries to reconnect the user automatically for 60 seconds, after that the connection gets terminated and the peer is removed from the room by SDK. If reconnection fails the SDK emits HMSUpdateListenerActions.ON_ERROR event with error.   js const onErrorListener = (error: HMSException) =>    if (error.code === 1003  error.code === 4005)     // Free Resources and do cleanup   doCleanup();   // navigate user to the HomeScreen, showing a toast or errorDialog with error description.   navigation.navigate('HomeScreen');     ; // Subscribing to Error updates hmsInstance.addEventListener(HMSUpdateListenerActions.ON_ERROR, onErrorListener);   > To learn more about cleanup and freeing resources refer to  Release Resources (./release-resources) docs. The user will receive the error object which has isTerminal property set to true and errorCode property as:   Error Code: 1003, Cause: Websocket disconnected   Error Code:1003 (/docs/v2/flutter-errorCode-1003.png)   Error Code: 4005, Cause: ICE Connection Failed due to network issue PUBLISH    Error Code:1003 (/docs/v2/flutter-errorCode-4005-PUBLISH.png)   Error Code: 4005, Cause: ICE Connection Failed due to network issue SUBSCRIBE    Error Code:1003 (/docs/v2/flutter-errorCode-4005-SUBSCRIBE.png)   Best Practices The SDK emits:  HMSUpdateListenerActions.RECONNECTING event, when the user disconnects  HMSUpdateListenerActions.RECONNECTED event, when the user reconnects, and  HMSUpdateListenerActions.ON_ERROR event, when the user   fails   to reconnect These callbacks can be handled as follows:   In \"Reconnecting\" Mode: Do not allow the user to interact with UI and show loaders or Reconnecting indicator.   When \"Reconnected\" back: Switch back to room UI allowing interactions as usual.   When user fails to reconnect then SDK emits HMSUpdateListenerActions.ON_ERROR event, In the function subscribed to this event, look for code property and if it is something(generally 1003, 4005 etc.) such that the user can never reconnect then navigate user to the HomeScreen, showing a toast or errorDialog with error description. > 🗝️ You can find the errorCodes, descriptions and the suggested actions to be taken  here (./error-handling)  You can also measure a users' connection speed. Learn more about connection quality API  here (../advanced-features/network-quality) "
    },
    {
        "title": "RTMP Streaming & Recording",
        "link": "/react-native/v2/features/recording",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/recording",
        "keywords": [],
        "content": "  Want to preserve your video call for posterity in a recording? Or live stream it out to millions of viewers on Twitch or YouTube or whatever gives you an RTMP ingest URL? Turn on RTMP Streaming or Recording  In 100ms, recording and streaming is usually achieved by having a bot join your room and stream what it sees and hears to a file (recording) or an RTMP ingest URL (streaming).   Types of Recordings  Apart from the RTMP stream and the browser recording, there is also a Server Recording, which can be turned on for the room.    Server Recording  This is used for archival purposes and cannot be stopped by method calls from SDK. This can only be enabled/disabled from the dashboard. This represents that the room was set to be recorded when it was created and all sessions within it will always be recorded for archival by the server.    Browser Recording  This is a normal recording that can be enabled/disabled using HMSSDK instance method startRTMPOrRecording with record property set to true.   RTMP Streaming & Recording The topics covered in this doc are: 1.  How to start streaming / recording. ( start-streaming-recording) 2.  How to stop streaming / recording. ( stop-streaming-recording) 3.  How to check the current status for streaming / recording. ( current-room-status) 4.  When to check the current status ( when-to-check-for-room-status)   Permissions Can't let just anyone record and stream room audio and video. First, you need to create a  role (../foundation/templates-and-roles) with the permissions to record and stream room. You can check if you (local peer) have permission to record and stream room as follows    ts const localPeer = await hmsInstance.getLocalPeer(); // Permissions are available on HMSRole object of Local Peer const localPeerPermissions: HMSPermissions = localPeer.role?.permissions; // Check if Local Peer has Room Recording Permission const canRecord: boolean = localPeerPermissions?.browserRecording; // Check if Local Peer has Room Streaming Permission const canStream: boolean = localPeerPermissions?.rtmpStreaming;     Start Streaming / Recording Let's understand the process by following steps: <div className=\"steps-container\">   Create an HMSRTMPConfig instance  HMSRTMPConfig object will be used to start the Room recording and streaming.   js import   HMSRTMPConfig   from '@100mslive/react-native-hms'; const roomJoinLink = 'https://...app.100ms.live/streaming/meeting...'; const recordingConfig = new HMSRTMPConfig(   meetingURL: $ roomJoinLink /viewer?token=beam_recording ,  rtmpURLs:   ,  record: true,  );   Let's look at how HMSRTMPConfig object looks like    js interface HMSRTMPConfig    meetingURL: string;  rtmpURLs?: string  ;  record: boolean;  resolution?: HMSRtmpVideoResolution;     1. _meetingURL_: string , The URL that the 100ms bot user will open to join your room. It must allow access without any user-level interaction. 2. _rtmpURLs_: string   , If streaming is required, this has to be one or more RTMP Ingest URLs with a max limit of 3 URLs where the stream should go. If only recording, this can be an empty list.    Format: rtmp://server.com/app/STREAM_KEY    Example: rtmp://a.rtmp.youtube.com/live2/k0jv-329m-1y7f-ktth-ck48      \"rtmp://a.rtmp.youtube.com/live2/\"  RTMP stream URL.      \"k0jv-329m-1y7f-ktth-ck48\"  RTMP stream key.    Please refer to the platform specific documentation for details on how to obtain the stream URL and stream key. Here are some examples:      YouTube (https://support.google.com/youtube/answer/2907883?hl=en&ref_topic=9257892)      Facebook (https://www.facebook.com/help/587160588142067)      Instagram (https://about.instagram.com/blog/tips-and-tricks/instagram-live-producer)      Twitch (https://help.twitch.tv/s/article/twitch-stream-key-faq?language=en_US)      LinkedIn (https://www.linkedin.com/help/linkedin/answer/a564446/go-live-using-a-custom-stream-rtmp) 3. _record_: boolean , If the recording is required, set it true . If the recording is not required, set it to false . This value does not affect streaming. 4. _resolution_: HMSRtmpVideoResolution , An optional value for the output resolution of the stream. For instance, the default is a landscape at 1280x720 but this could be set for a portrait mode to 720x1280 or smaller values like 480x80 . The HMSResolution class takes width and height .    Key Notes  If both rtmpURLs and record = true are provided, both streaming and recording will begin.  If only rtmpURLs are provided, only streaming will begin.  If only record = true is provided, only recording will begin. If either one is started, the other can't be started without first stopping whatever is running. > Eg: If only streaming is started then recording can't be started unless streaming is stopped first. If both are required, they have to be started together by providing both rtmpURLs and record = true .   Call method on HMSSDK instance After creating config ( HMSRTMPConfig ) object, You can use startRTMPOrRecording method available on HMSSDK instance to start the RTMP Streaming and Recording as follows  Make sure you have above mentioned permissions for starting RTMP streaming and recording.   js try    // Calling startRTMPOrRecording with config created in above step  await hmsInstance.startRTMPOrRecording(recordingConfig);  console.log('Start Recording Success');   catch (error)    console.log('Start Recording Error: ', error);     </div>   Stop Streaming / Recording You can use stopRtmpAndRecording method available on HMSSDK instance to stop the RTMP Streaming and Recording. Make sure you have above mentioned permissions for stopping RTMP streaming and recording.   js try    await hmsInstance.stopRtmpAndRecording();  console.log('Stop Recording Success');   catch (error)    console.log('Stop Recording Error: ', error);       Current Room Status The current status for the room is always reflected in the HMSRoom object which we can get from getRoom method available on HMSSDK instance or HMSUpdateListenerActions.ON_ROOM_UPDATE event. This can be used to show the stream or recording status on UI something similar to this:   stream-state (/docs/v2/flutter-stream-state.jpg) Here are the relevant properties inside the HMSRoom object which we can read to get the current recording/streaming status of the room:   rtmpHMSRtmpStreamingState  Contains info about RTMP Streaming, running attribute if true indicates streaming is ON currently  browserRecordingState  Contains info about Browser Recording, running attribute if true indicates browser recording is ON currently  serverRecordingState  Contains info about Server Recording, running attribute if true indicates server recording is ON currently Each of them is an object which contains a boolean running which lets you know if it's active in the room right now and error which lets you know if there was an error. 1.   rtmpHMSRtmpStreamingState   is an instance of HMSRtmpStreamingState , which looks like:   js interface HMSRtmpStreamingState    running: boolean; // true indicates that RTMP streaming is running  error?: HMSException; // Gets populated if there is some error in starting the stream or recording  startedAt: Date; // time at which RTMP streaming was started  stoppedAt: Date; // time at which RTMP streaming was stopped     2.   browserRecordingState   is an instance of HMSBrowserRecordingState , which looks like:   js interface HMSBrowserRecordingState    running: boolean; // true indicates that browser recording is running  error?: HMSException; // Gets populated if there is some error in starting the browser recording  startedAt: Date; // time at which browser recording was started  stoppedAt: Date; // time at which browser recording was stopped     3.   serverRecordingState   is an instance of HMSServerRecordingState , which looks like:  This represents that the room was set to be recorded when it was created and all sessions within it will always be recorded for archival by the server.   js interface HMSServerRecordingState    running: boolean; // true indicates that server recording is running  error?: HMSException; // Gets populated if there is some error in starting the server recording  startedAt: Date; // time at which server recording was started       When to check for room status The room status should be checked in any of the following ways  1. Inside onJoinListener function which is subscribed to HMSUpdateListenerActions.ON_JOIN event 2. Inside onRoomListener function which is subscribed to HMSUpdateListenerActions.ON_ROOM_UPDATE event 3. When hmsInstance.startRTMPOrRecording(recordingConfig) is called 4. When hmsInstance.stopRtmpAndRecording() is called   In the above mentioned onJoinListener & onRoomListener functions, The properties mentioned above will be on the HMSRoom object.   Whenever either of the startRTMPOrRecording or stopRtmpAndRecording functions are called, the values of the streaming and recording will be updated on the room object available in onRoomListener function. So, You should update the instance of HMSRoom in your application at that time. "
    },
    {
        "title": "Release Resources ",
        "link": "/react-native/v2/features/release-resources",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/release-resources",
        "keywords": [],
        "content": "  Releasing resources when they are not required anymore is an important part of the app. If the resources are not released and acquired again time to time, it might hamper app performance.  When the hms instance is not required any more or when a new hms instance has to be created instead of using the previous one, \"destroy\" the previous instance. You can call the destroy method available on HMSSDK instance to free resources. This method removes native and JS event listeners and destroys SDK instance on native side.   js try    await hmsInstance.destroy();  console.log('Destroy Success');  // Removing HMSSDK instance saved in redux or any other store  clearHmsReference();   catch (error)    console.log('Destroy Error: ', error);     "
    },
    {
        "title": "Remove Peer",
        "link": "/react-native/v2/features/remove-peer",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/remove-peer",
        "keywords": [],
        "content": "  Someone's overstayed their welcome and now you need to remove a peer from the video call room. removePeer method of HMSSDK instance comes to rescue.   Permissions Can't let just anyone remove others from the video call room. First, you need to create a  role (../foundation/templates-and-roles) with the permissions to remove others. Only the peers who has role with remove permissions allowed will be able to Remove peers from the Room. You can check if you (local peer) have permission to remove peers as follows    ts const localPeer = await hmsInstance.getLocalPeer(); // Permissions are available on HMSRole object of Local Peer const localPeerPermissions: HMSPermissions = localPeer.role?.permissions; // Check if Local Peer has Remove Peer Permission const canRemoveOthers: boolean = localPeerPermissions?.removeOthers;     Remove Peers You can use removePeer method available on HMSSDK instance to remove peers from the room.  removePeer method accepts two parameters: 1. _peer_: HMSPeer object of the peer that you want to remove 2. _reason_: string reason for removal. Target Peer will receive this reason in HMSUpdateListenerActions.ON_REMOVED_FROM_ROOM event Make sure you have above mentioned permission for removePeer to work.   js try    const reason = 'removed from room';  await hmsInstance.removePeer(peer, reason); // peer is HMSPeer object of the peer to remove  console.log('Remove Peer Success');   catch (error)    console.log('Remove Peer Error: ', error);       Remove Peer Notification This section explains how to handle the app UI when you (local peer) are removed from the room by someone else. Once the peer with adequate permissions calls removePeer , the removed peer will receive a HMSUpdateListenerActions.ON_REMOVED_FROM_ROOM event. You can update your apps' UI and  Free Resources (./release-resources) on receive of HMSUpdateListenerActions.ON_REMOVED_FROM_ROOM event.   js const onRemovedFromRoom = (data:   requestedBy: HMSPeer  null, reason: string; roomEnded: boolean;  ) =>    // Free App resources and do cleanup  destroy();  // Redirect to home screen or Navigate away from Meeting screen  navigation.navigate('Home');  ; hmsInstance.addEventListener(HMSUpdateListenerActions.ON_REMOVED_FROM_ROOM, onRemovedFromRoom);   > 💡 Note: This is the same event that will be emitted if a peer  ends the room (./end-room). Except that roomEnded will be false when a peer is removed from room. Description of keys of data received from HMSUpdateListenerActions.ON_REMOVED_FROM_ROOM event:   _reason_: The string message detailing why the peer was removed.   _requestedBy_: HMSPeer object of the peer who called removePeer .   _roomEnded_: false if only the receiving peer(local peer) was removed. true if the entire room was ended. "
    },
    {
        "title": "Render Video",
        "link": "/react-native/v2/features/render-video",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/render-video",
        "keywords": [],
        "content": "  It all comes down to this. All the setup so far has been done so that we can show Live Streaming Video in our beautiful apps. 100ms React Native Package provides HmsView component that renders the video on the screen. You can access HmsView from the HMSSDK instance. We simply have to pass a Video Track's trackId to the HmsView to begin automatic rendering of Live Video Stream. We can also optionally pass props like key , scaleType , mirror to customize the HmsView component.   js // use the save HMSSDK instance acquired from build method const HmsView = hmsInstance.HmsView; <HmsView  trackId= videoTrackId   key= videoTrackId   style= styles.hmsView    const styles = StyleSheet.create(   hmsView:     height: '100%',   width: '100%',   ,  );     What is Track Update Listener? Let's first understand what is the Track Update Listener.  Once you have successfully joined a Room, all the available Audio & Video Tracks will be received via the ON_TRACK_UPDATE Event Listener. The Peer, Track & the Update Type data is available to be used for rendering Video on the App UI.  Before joining the Room, you should attach the ON_TRACK_UPDATE Event Listener.  The following snippet shows how to attach the listener    js // create HMSSDK instance using the build function const hmsInstance = await HMSSDK.build(); hmsInstance.addEventListener(HMSUpdateListenerActions.ON_JOIN, onJoinSuccess); hmsInstance.addEventListener(HMSUpdateListenerActions.ON_PEER_UPDATE, onPeerUpdate); // attach the on Track Update Listener before Joining the Room hmsInstance.addEventListener(HMSUpdateListenerActions.ON_TRACK_UPDATE, onTrackUpdate); hmsInstance.addEventListener(HMSUpdateListenerActions.ON_ERROR, onError); // Next, create an object of HMSConfig class using the available joining configurations. let config = new HMSConfig(            authToken: 'eyJH5c...', // client-side token generated from your token service           username: 'John Appleseed',          ); // Now, we are primed to join the room. All you have to do is calling join by passing the config object hmsInstance.join(config);   The Track Update Event Listener is fired whenever there is any change in Audio, Video, Screenshare, etc types of Tracks in the Room. These changes can be of many types, most common ones being as follows    Track Added or Removed: This usually happens when a peer who is publishing audio or video joins or leave the Room  Track Muted or Unmuted: When a peer mutes or unmutes their Audio or Video Track  Track Degraded or Restored: Due to bad network conditions, a peer stops publishing Video Track until network conditions get restored. The following snippet shows method signature of onTrackUpdate    js const onTrackListener = (data:   peer: HMSPeer; track: HMSTrack; type: HMSTrackUpdate  ) =>    const   peer, track, type   = data;  // We will only consider Video tracks events to render videos  if (track.type === HMSTrackType.VIDEO)     // If Video track is added, you can use trackId to render video   if (type === HMSTrackUpdate.TRACK_ADDED)      console.log( $ peer.name s' video track Added: $ track.trackId  );    console.log( Render HMSView with trackId: $ track.trackId  );       // If Video track is removed, remove HMSView which is using this trackId   if (type === HMSTrackUpdate.TRACK_REMOVED)      console.log( $ peer.name s' video track Removed: $ track.trackId  );    console.log( Remove HMSView rendering trackId: $ track.trackId  );       if (    type === HMSTrackUpdate.TRACK_MUTED     type === HMSTrackUpdate.TRACK_UNMUTED     type === HMSTrackUpdate.TRACK_RESTORED     type === HMSTrackUpdate.TRACK_DEGRADED   )      console.log(     Update UI to show Muted/Unmuted/Degraded/Restored updates: $ track.trackId     );         ; hmsInstance.addEventListener(  HMSUpdateListenerActions.ON_TRACK_UPDATE,  onTrackListener );     Using Track Update Listener The ON_PEER_UPDATE and ON_TRACK_UPDATE listeners are triggered a peer joins/leaves room, peer's track gets added, removed, muted, etc. It's recommended to use both these Listeners to correctly update your App UI. For simplicity, the following snippet shows usage of only onTrackUpdate to start rendering Videos in a FlatList    js // In this example code snippet, We are keeping things very simple. // You will get an overview of how to render HMSView s for list of trackId s and how to keep that list up to date. // We don't need ON_PEER_UPDATE event listener for keeping track of only trackId s. // So, we have registered only ON_TRACK_UPDATE event listener here const  trackIds, setTrackIds  = useState<string  >(  ); const onTrackListener = (data:   peer: HMSPeer; track: HMSTrack; type: HMSTrackUpdate  ) =>    // We will only consider Video tracks for this example  if (data.track.type  == HMSTrackType.VIDEO) return;  // If Video track is added, add trackId to our list  if (data.type === HMSTrackUpdate.TRACK_ADDED) setTrackIds(prevTrackIds =>  ...prevTrackIds, data.track.trackId );  // If Video track is removed, remove trackId from our list  if (data.type === HMSTrackUpdate.TRACK_REMOVED) setTrackIds(prevTrackIds => prevTrackIds.filter(prevTrackId => prevTrackId  == data.track.trackId));  ; hmsInstance.addEventListener(  HMSUpdateListenerActions.ON_TRACK_UPDATE,  onTrackListener ); // Render multiple HMSView for trackIds inside FlatList // Note: HMSView will render blank if video track of peer is muted, Make sure video of peers is not muted. <FlatList  data= trackIds  // trackIds is an array of trackIds of video tracks  keyExtractor= (trackId) => trackId   renderItem= (  item  ) => <HMSView key= item  trackId= item  style=   width: '100%', height: '50%'     ...       ...      To see a more detailed implementation of these events, you can refer our  Quickstart App Implementation (https://github.com/100mslive/react-native-hms/blob/21d7871691ec6c8ffd125a0592a3b16c5e79e1cd/sample-apps/rn-quickstart-app/src/screens/MeetingScreen/hooks.ts L158). In the above example, we haven't considered showing Muted Status, Name of Peers, Avatars, Network Quality, etc. All of this data & much more are available which can used according to your App's UI design. To show all these visual elements both Peer and Track object are required. You can use PeerTrackNode interface mentioned in below code snippet & build on top of it    js interface PeerTrackNode    id: string;  peer: HMSPeer;  track: HMSVideoTrack;  isDegraded: boolean;     Then, you can use list of PeerTrackNode instead of trackIds in FlatList to render videos & show other available data in your Apps. We have used PeerTrackNode object type in our  Quickstart Sample app (https://github.com/100mslive/react-native-hms/tree/develop/sample-apps/) and  Example app (https://github.com/100mslive/react-native-hms/tree/main/example). A detailed explanation of our Example App & mutliple UI components are  available here (https://github.com/100mslive/react-native-hms/tree/main/example peertracknodes).   Properties of HmsView The following listing shows all the available props of the HmsView component.   TrackId The trackId is a required prop in the HmsView component. Passing of correct Video TrackId is required to start rendering Videos of different peers in the Room. There are multiple ways to access Video TrackIds. The recommended way is to access Video Tracks is by attaching the HMSUpdateListenerActions.ON_TRACK_UPDATE Event Listener. A Video Track object can also be accessed from the HMSPeer object as peer.videoTrack .   js <HmsView  trackId= videoTrackId   key= videoTrackId   style= styles.hmsView         Scale Type The scaleType prop defines how the Video stream should fit the given dimension (width and height) of HmsView. Video capturing & rendering them on your App UI can have different Aspect Ratios (Resolution). So selecting a correct Scale Type prop is necessary so video does not get clipped from edges, or you don't have blank padding spaces in your UI, etc. It is an optional prop in the HmsView component. By default value of scale type is set to HMSVideoViewMode.ASPECT_FILL . There are 3 scale types you can pass to render a video.   js import   HMSVideoViewMode   from '@100mslive/react-native-hms'; // Video occupies all the available space and may get cropped const ASPECT_FILL = HMSVideoViewMode.ASPECT_FILL // Video maintains the aspect ratio so it only occupies space based on the aspect ratio const ASPECT_FIT = HMSVideoViewMode.ASPECT_FIT // Video aspect ratio is balanced to show the central part of video const ASPECT_BALANCED = HMSVideoViewMode.ASPECT_BALANCED  // example usage showing setting of Aspect_Fit scaleType prop <HmsView  trackId= videoTrackId   key= videoTrackId   scaleType= HMSVideoViewMode.ASPECT_FIT   style= styles.hmsView        Mirror The mirror is an optional prop in the HmsView component. Whenever the video is rendered on the HmsView we can flip the video frame on the horizontally to create a mirror frame. By default, the mirror prop is set to false.   js <HmsView  trackId= videoTrackId   key= videoTrackId   mirror= true   style= styles.hmsView         Front Camera The default HmsView has no mirroring. If you were facing the front camera, here's what that would look like on a display on the phone. Actual position is how you sit, display position is how it will look in the HmsView      Actual Position            Display Position ┌─────────────────────────┐      ┌─────────────────────────┐ │             │      │             │ │             │      │             │ │  Phone Front Camera  │      │  Phone Front Camera  │ │             │      │             │ │             │      │             │ │             │      │             │ │          You  │      │  You          │ │             │      │             │ │             │      │             │ └─────────────────────────┘      └─────────────────────────┘    To change this, turn on mirroring by setting the mirror= true       Actual Position            Display Position ┌─────────────────────────┐      ┌─────────────────────────┐ │             │      │             │ │             │      │             │ │  Phone Front Camera  │      │  Phone Front Camera  │ │             │      │             │ │             │      │             │ │             │      │             │ │          You  │      │          You  │ │             │      │             │ │             │      │             │ └─────────────────────────┘      └─────────────────────────┘      Back Camera Since the left and right for the back camera vs the display are the same, a non mirrored view of the back camera is the same as a mirrored view for the front camera. The Actual and Display positions are reverse for the back camera.    Camera Flip Effects If you set the mirror prop then flip the camera from front to back or vice versa, the value persists. A given surface once mirrored will remain mirrored. It should be noted however that left for the front facing the camera and left for the back facing camera are opposites. So you may want to toggle back the mirror value.    Points to Remember  One HmsView component can only be connected with one video trackId . To display multiple videos you have to create multiple instances of HmsView component.  It's recommended to always pass the key property while creating HmsView   If a null or undefined trackId is passed in HmsView you will have to unmount and remount with the new trackId . Using the key prop and passing trackId to it automatically achieves this.  Once the usage of that HmsView is finshed it should ALWAYS be disposed. You cannot reuse the same HmsView component for multiple Video TrackIds.  Every HmsView should be unique, which should be done by passing a key property and value as video trackId .  Recommended practice is to show maximum of 2 to 4 HmsView on a single page/screen of the app. This avoids network data consumption & video decoding resources of the device.   For listening to the audio you have to do absolutely nothing. Audio is automatically played once you join the room. To mute & unmute audio/video refer  here (./mute). To set Audio Volume levels refer  here (../advanced-features/set-volume).  TrackId should always be a valid Video TrackId of a Peer in the current ongoing Room.  Ensure that you are not passing a Video TrackId of some previously joined Room.  You can ensure the trackId is valid by listening to ON_TRACK_UPDATE event for Track updates.   When you receive TRACK_ADDED update type for \"Video\" track, you will have track object with valid trackId for each peer .    Handling Video Track Degradation Sometimes people have bad internet connections but everyone deserves a good meeting. When the network is too slow to support audio and video conversations together, the 100ms SDK can automatically turns off downloading other peers' videos, which may improve the audio quality and avoid disconnections.  If the network quality improves, the videos will be restored automatically as well. These updates are available via the onTrackUpdate event as TRACK_DEGRADED & TRACK_RESTORED Update Types. You can show an Avatar overlay with the peer's name's initials, push the video tile off-screen, etc when Video Track gets Degraded.   js const isDegraded: boolean = videoTrack.isDegraded   > Pro Tip: Always render 2 to 4 videos on a screen and dispose of other HmsView components. This will stop downloading video data of tiles which are not visible on the screen.    Troubleshooting Guide  If a video renders for the first time and then it doesn't appear at all this can be due to the usage of multiple instances.  It's possible to create multiple instances of SDK which is required in some niche requirements.  In most cases, prefer to use a single instance of SDK. That means once you call the build method and get an hms instance, save the instance in a global state.  Do not call the build method again, as it will return a new hms instance every time.   So in this way you can ensure that only a single instance of the SDK is created.  If peer is null or undefined, refer to solving  peerId undefined issue (https://www.100ms.live/docs/react-native/v2/guides/faq track-id-is-undefined-hms-view-is-rendering-blank-view) for more details  It's possible a black frame is first visible for sometime before actual video is seen. This is usually due to very bad network conditions or the device's capacity to render video is limited. Ensure only videos visible on screen are being rendered & minimize the number of videos shown on lower end devices or bad network regions.  Checkout our Example App UI implementation  available here (https://github.com/100mslive/react-native-hms/tree/main/example). "
    },
    {
        "title": "RTC Call Stats",
        "link": "/react-native/v2/features/rtc-stats",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/rtc-stats",
        "keywords": [],
        "content": "  Sometimes you need a way to capture certain metrics related to a call. This may be helpful if you want to tailor the experience to your users or debug issues. Typical metrics of interest are audio/video bitrate, round trip time, total consumed bandwidth and packet loss. 100ms SDK provides this data via certain events. These events are emitted continuously within a fixed interval of one second after a room has been joined. You can get stats:  as an overall summary by subscribing to HMSUpdateListenerActions.ON_RTC_STATS event, or  on a per track basis by subscribing to   HMSUpdateListenerActions.ON_LOCAL_AUDIO_STATS event for local audio stats,   HMSUpdateListenerActions.ON_LOCAL_VIDEO_STATS event for local video stats,   HMSUpdateListenerActions.ON_REMOTE_AUDIO_STATS event for remote peers' audio stats,   HMSUpdateListenerActions.ON_REMOTE_VIDEO_STATS event for remote peers' video stats > To get a list of all events emitted by the 100ms React Native SDK, check out  Event Listener guide (./event-listeners) Let's see how we can get Stats by subscribing to above events  <div className=\"steps-container\">   Enable Receiving Stats Events By default, Stats Events are not sent from the native platform to React Native side. To enable receiving these events on React Native side, the enableRTCStats method must be called on HMSSDK instance.   js hmsInstance.enableRTCStats();     Subscribe to Events Now, You can subscribe to stats events as per your use case:  HMSUpdateListenerActions.ON_RTC_STATS : This event provides combined stats for the Room session.  HMSUpdateListenerActions.ON_LOCAL_AUDIO_STATS : This event provides stats for local audio track.  HMSUpdateListenerActions.ON_LOCAL_VIDEO_STATS : This event provides stats for local video track with all the available layers.  HMSUpdateListenerActions.ON_REMOTE_AUDIO_STATS : This event provides stats for remote peers' audio tracks.  HMSUpdateListenerActions.ON_REMOTE_VIDEO_STATS : This event provides stats for remote peers' video tracks.   js const onChangeRtcStats = (data:    rtcStats: HMSRTCStatsReport; // HMSRTCStatsReport is explained in 'Supplementary bytes' section  ) =>    console.log(data.rtcStats);   hmsInstance.addEventListener(  HMSUpdateListenerActions.ON_RTC_STATS,  onChangeRtcStats, ); // This function is subscribed to ON_LOCAL_AUDIO_STATS event const onChangeLocalAudioStats = (data:    localAudioStats: HMSLocalAudioStats; // HMSLocalAudioStats is explained in 'Supplementary bytes' section  track: HMSLocalAudioTrack;  peer: HMSPeer;  ) =>    console.log(data.localAudioStats);   hmsInstance.addEventListener(  HMSUpdateListenerActions.ON_LOCAL_AUDIO_STATS,  onChangeLocalAudioStats, ); // This function is subscribed to ON_LOCAL_VIDEO_STATS event const onChangeLocalVideoStats = (data:    localVideoStats: HMSLocalVideoStats; // HMSLocalVideoStats is explained in 'Supplementary bytes' section  track: HMSLocalVideoTrack;  peer: HMSPeer;  ) =>    console.log(data.localVideoStats);   hmsInstance.addEventListener(  HMSUpdateListenerActions.ON_LOCAL_VIDEO_STATS,  onChangeLocalVideoStats, ); // This function is subscribed to ON_REMOTE_AUDIO_STATS event const onChangeRemoteAudioStats = (data:    remoteAudioStats: HMSRemoteAudioStats; // HMSRemoteAudioStats is explained in 'Supplementary bytes' section  track: HMSRemoteAudioTrack;  peer: HMSPeer;  ) =>    console.log(data.remoteAudioStats);   hmsInstance.addEventListener(  HMSUpdateListenerActions.ON_REMOTE_AUDIO_STATS,  onChangeRemoteAudioStats, ); // This function is subscribed to ON_REMOTE_VIDEO_STATS event const onChangeRemoteVideoStats = (data:    remoteVideoStats: HMSRemoteVideoStats; // HMSRemoteVideoStats is explained in 'Supplementary bytes' section  track: HMSRemoteVideoTrack;  peer: HMSPeer;  ) =>    console.log(data.remoteVideoStats);   hmsInstance.addEventListener(  HMSUpdateListenerActions.ON_REMOTE_VIDEO_STATS,  onChangeRemoteVideoStats, );     Disable Receiving Stats Events After you are done with these stats and don't want to receive these event updates anymore. You can stop receiving these events on React Native side by calling disableRTCStats method on HMSSDK instance.   js hmsInstance.disableRTCStats();     Unsubscribe from Events Now you can also remove your event subscriptions because subscribed functions will not receive any events until you again  enable receiving these events ( enable-receiving-stats-events)    js hmsInstance.removeEventListener(  HMSUpdateListenerActions.ON_LOCAL_AUDIO_STATS, ); hmsInstance.removeEventListener(  HMSUpdateListenerActions.ON_LOCAL_VIDEO_STATS, ); hmsInstance.removeEventListener(  HMSUpdateListenerActions.ON_REMOTE_AUDIO_STATS, ); hmsInstance.removeEventListener(  HMSUpdateListenerActions.ON_REMOTE_VIDEO_STATS, );   </div>   Supplementary bytes   HMSRTCStatsReport This class will contain the combined stats for the Room. Interface for HMSRTCStatsReport class looks like    js interface HMSRTCStats    // Total bytes sent in the current session.  bytesSent?: number;  // Total bytes received in the current session.  bytesReceived?: number;  // Total packets received in the current session.  packetsReceived?: number;  // Total packets lost in the current session.  packetsLost?: number;  // Total outgoing bitrate observed since previous report.  bitrateSent?: number;  // Total incoming bitrate observed since previous report in Kb/s.  bitrateReceived?: number;  // Average round trip time observed since previous report in seconds.  roundTripTime?: number;   interface HMSRTCStatsReport    // Summary of all video tracks  video?: HMSRTCStats;  // Summary of all audio tracks  audio?: HMSRTCStats;  // Combined audio + video values  combined?: HMSRTCStats;       HMSLocalAudioStats This class contains stats related to local audio track. Interface for HMSLocalAudioStats class looks like    js interface HMSLocalAudioStats    // Round trip time observed since previous report.  roundTripTime?: number;  // Total bytes sent by this track in the current session.  bytesSent?: number;  // Outgoing bitrate of this track observed since previous report in Kb/s.  bitrate?: number;       HMSLocalVideoStats This class contains stats related to local video track. Interface for HMSLocalVideoStats class looks like    js interface HMSVideoResolution    height?: number;  width?: number;   interface HMSLocalVideoStats    // Round trip time observed since previous report.  roundTripTime?: number;  // Total bytes sent by this track in the current session.  bytesSent?: number;  // Outgoing bitrate of this track observed since previous report in Kb/s.  bitrate?: number;  // Resolution of video frames being sent.  resolution?: HMSVideoResolution;  // Frame rate of video frames being sent (FPS).  frameRate?: number;       HMSRemoteAudioStats This class contains stats related to remote audio track. Interface for HMSRemoteAudioStats class looks like    js class HMSRemoteAudioStats    // Packet Jitter measured in seconds for this track. Calculated as defined in section 6.4.1. of RFC3550.  jitter?: number;  // Total bytes received by this track in the current session.  bytesReceived?: number;  // Incoming bitrate of this track observed since previous report in Kb/s.  bitrate?: number;  // Total packets received by this track in the current session.  packetsReceived?: number;  // Total packets lost by this track in the current session.  packetsLost?: number;       HMSRemoteVideoStats This class contains stats related to remote video track. Interface for HMSRemoteVideoStats class looks like    js interface HMSVideoResolution    height?: number;  width?: number;   class HMSRemoteVideoStats    // Packet Jitter measured in seconds for this track. Calculated as defined in section 6.4.1. of RFC3550.  jitter?: number;  // Total bytes received by this track in the current session.  bytesReceived?: number;  // Incoming bitrate of this track observed since previous report in Kb/s.  bitrate?: number;  // Total packets received by this track in the current session.  packetsReceived?: number;  // Total packets lost by this track in the current session.  packetsLost?: number;  // Resolution of video frames being received.  resolution?: HMSVideoResolution;  // Frame rate of video frames being received (FPS).  frameRate?: number;     "
    },
    {
        "title": "Screen Share",
        "link": "/react-native/v2/features/screenshare",
        "platformName": "React Native",
        "objectID": "/react-native/v2/features/screenshare",
        "keywords": [],
        "content": "  100ms React Native Package provides support for sharing the entire screen of the device to the room. Please note that for a peer to share their screen, their role must have screenshare enabled in the dashboard. Also select the appropriate resolution for the Screen share quality. 1080p is recommended for better text readability.   ScreenshareDashboard (/docs/v2/screenshare-dashboard.png)   Android Setup Add HmsScreenshareActivity to manifest located at android/app/src/main/AndroidManifest.xml as follows    xml <application  android:name=\".MainApplication\"  ...  android:label=\"@string/app_name\">  ...  <  Add below activity in your \"application\" tag >  <activity   android:name=\"com.reactnativehmssdk.HmsScreenshareActivity\"   android:label=\"@string/app_name\"    ... </application>     iOS Setup You need to create an iOS broadcast upload extension. It uses Apple's ReplayKit framework to record the device screen and delivers frame samples to your broadcast extension. You can share not only your own app but also the entire device sceeen including other apps on the device. <div className=\"steps-container\">   Step 1  Open Xcode project at the /ios path of your app Open your iOS Xcode project xed ios .   Step 2  Add Broadcast Upload Extension Click on your project in the Project Navigator to show the project settings. Press + at the bottom of the target list to add a new target.   AddExtension (/docs/v2/RNAddExtension.png) Select the Broadcast Upload Extension type for your new target.   SelectExtension (/docs/v2/RNSelectExtension.png) Enter your new target detail in the dialog. Uncheck Include UI Extension option.   DetailExtension (/docs/v2/RNDetailExtension.png) In the following dialog, activate the new scheme for the new target.   ActivateExtension (/docs/v2/RNActivateExtension.png)   Step 3  Add App Group Click + icon in Signing & Capabilities section. Select App Group from the list of Capabilities.   AddAppgroup (/docs/v2/RNSelectAppGroup.png) New section should be added under Signing & Capabilities named App Groups. Click + icon under that.   Appgroup (/docs/v2/RNAddAppGroup.png) Enter App Aroup name. This must be unique to your Apple Development Account. The standard practice is to use your domain name like group.your.domain.name .   AppgroupDetail (/docs/v2/RNAddAppGroupName.png)   Step 4  Edit Podfile In ios folder of your react project and open Podfile . Paste the following code and replace the extension name you just created:   ruby  ENSURE TO SET THE CORRECT SCREENSHARE EXTENSION TARGET NAME BELOW target 'RNHMSExampleBroadcastUpload' do  use_modular_headers   pod 'HMSBroadcastExtensionSDK' end   A sample of complete Podfile snippet after adding the Screenshare Target is as follows    ruby require_relative '../node_modules/react-native/scripts/react_native_pods' require_relative '../node_modules/@react-native-community/cli-platform-ios/native_modules' platform :ios, '13.0'  ENSURE TO SET THE CORRECT EXTENSION TARGET NAME BELOW.  Not required if starting Screenshare from iOS devices is not a feature of the app. Then Remove this Target snippet target 'RNHMSExampleBroadcastUpload' do  use_modular_headers   pod 'HMSBroadcastExtensionSDK' end  ENSURE TO SET THE CORRECT MAIN APP TARGET NAME BELOW target 'RNHMSExample' do  config = use_native_modules   permissions_path = '../node_modules/react-native-permissions/ios'  use_react_native (   :path => config :reactNativePath ,    to enable hermes on iOS, change false to true and then install pods    :hermes_enabled => false   :hermes_enabled => true  )  pod 'Permission-Camera', :path => \"  permissions_path /Camera\"  pod 'Permission-Microphone', :path => \"  permissions_path /Microphone\"  target 'RNHMSExampleTests' do   inherit  :complete    Pods for testing  end   Enables Flipper.     Note that if you have use_frameworks  enabled, Flipper will not work and   you should disable the next line.  use_flipper ()   POST INSTALL SCRIPT TO DISABLE BITCODE FROM ALL PODS & EXCLUDE arm64 FROM SIMULATORS  post_install do installer   react_native_post_install(installer)   installer.pods_project.build_configurations.each do config    config.build_settings \"EXCLUDED_ARCHS sdk=iphonesimulator  \"  = \"arm64\"    config.build_settings 'ENABLE_BITCODE'  = 'NO'   end  end end      Podfile (/docs/v2/RNPodfile.png) In Terminal, change the directory to ios and run pod install command.   Step 5  Edit SampleHandler Open the SampleHandler.swift file. It would be available in your TARGET_NAME > ExtensionName Xcode folder. For example, RNHMSExample > ExtensionName then SampleHandler.swift file.   SampleHandler (/docs/v2/RNSampleHandler.png) Replace the default SampleHandler.swift file with the code below and pset the correct App Group created for your Apple Developer Account.   swift import ReplayKit import HMSBroadcastExtensionSDK class SampleHandler: RPBroadcastSampleHandler     // ENSURE TO SET CORRECT APP GROUP LINKED TO YOUR APPLE DEVELOPER ACCOUNT HERE  let screenRenderer = HMSScreenRenderer(appGroup: \"group.reactnativehms\")   override func broadcastStarted(withSetupInfo setupInfo:  String: NSObject ?)        override func broadcastPaused()        override func broadcastResumed()        override func broadcastFinished()     screenRenderer.invalidate()      override func processSampleBuffer(_ sampleBuffer: CMSampleBuffer, with sampleBufferType: RPSampleBufferType)     switch sampleBufferType     case RPSampleBufferType.video:    // Handle video sample buffer    if let error = screenRenderer.process(sampleBuffer)       if error.code == .noActiveMeeting        finishBroadcastWithError(NSError(domain: \"ScreenShare\",                      code: error.code.rawValue,                      userInfo:  NSLocalizedFailureReasonErrorKey: \"You are not in a meeting.\" ))               break   case RPSampleBufferType.audioApp:    _ = self.screenRenderer.process(audioSampleBuffer: sampleBuffer)    break   case RPSampleBufferType.audioMic:    // Handle audio sample buffer for mic audio    break   @unknown default:    // Handle other sample buffer types    fatalError(\"Unknown type of sample buffer\")            </div>   Start Screenshare To start screen share, app needs to call the startScreenshare method of HMSSDK instance. To start screen share in iOS you need to pass App Group and Preferred Extension name while creating HMSSDK instance as follows:   js // Create HMSSDK instance with appGroup and preferredExtension properties const hmsInstance = await HMSSDK.build(    appGroup: 'group.reactnativehms', // ensure to pass the correct App Group linked to your Apple Developer Account   preferredExtension: 'RHHMSExampleBroadcastUpload', // ensure to pass the correct Extension Target name created in Xcode  ); ... // Start Screenshare try    await hmsInstance.startScreenshare();  console.log('Start startScreenshare Success');   catch(error)    console.log('Start startScreenshare Error: ', error);     You can find app group and extension name in Xcode under Signing and Capabilities section under target > yourExtensionName .   Parameter (/docs/v2/RNParameter.png)   Stop Screenshare To stop screen share, application needs to call the stopScreenshare method of HMSSDK instance.   js try    await hmsInstance.stopScreenshare();  console.log('Stop Screenshare Success');   catch(error)    console.log('Stop Screenshare Error: ', error);       How to get Screen Share Status Application needs to call the isScreenShared method of HMSSDK instance. This method returns true inscase Screenshare is currently active and being used, and false for inactive state.   js try    await hmsInstance.isScreenShared();  console.log('Is Screenshare Success');   catch(error)    console.log('Is Screenshare Error: ', error);       Render Screenshare Track You can get screenshare track in two ways:  from HMSUpdateListenerActions.ON_TRACK_UPDATE event, when the track is added  from peers' auxiliaryTracks property > Note: Screenshare track can be differentiated from normal track using tracks' source property  track.source === \"SCREEN\" for screenshare tracks Code snippet to get screenshare track from HMSUpdateListenerActions.ON_TRACK_UPDATE event    js const  peerTrackNodes, setPeerTrackNodes  = useState(  ); // Get screenshare track from onTrackListener const onTrackListener = (   peer,  track,  type,  :    peer: HMSPeer;  track: HMSTrack;  type: HMSTrackUpdate;  ) =>    if(   type === HMSTrackUpdate.TRACK_ADDED && // track is added   track.source === HMSTrackSource.SCREEN && // track source is screenshare   track.type === HMSTrackType.VIDEO // track type is video  )     // Saving screenshare track and peer to render it   setPeerTrackNodes(prevNodes =>      return         id: peer.peerID + track.trackId, peer, track,  ,     ...prevNodes     ;    );     ; ... const HmsView = hmsInstance.HmsView; <HmsView trackId= peerTrackNodes 0 ?.trackId      Code snippet to get screenshare track from peers' auxiliaryTracks property    js const remotePeers = await hmsInstance.getRemotePeers(); const firstPeer = remotePeers 0 ; // Get screenshare track from peer const firstPeerScreenshareTrack = firstPeer.auxiliaryTracks.find(auxiliaryTrack =>    return (   auxiliaryTrack.source === HMSTrackSource.SCREEN && // track source is screenshare    auxiliaryTrack.type === HMSTrackType.VIDEO // track type is video  );  ); ... const HmsView = hmsInstance?.HmsView; <HmsView trackId= firstPeerScreenshareTrack?.trackId      Refer to  Render Video (./render-video) docs to know more about HmsView component and help in rendering video.   Troubleshooting Guide For starting Screenshare from iOS devices (iPhones or iPads) following are some common setup you should already have within your Apps    Bitcode Disabled Bitcode has been disabled by Apple from Xcode 14 & iOS 16 and above. So 100ms packages also have Disabled Bitcode to ensure compatibility. Ensure that in your Xcode project Bitcode is Disabled for all Targets.   Disable Bitcode in Xcode (/docs/v2/RNDisableBitcode.png)    Podfile with Bitcode Disabled You can use the following Podfile which has post_install script to Disable Bitcode for all Pods . Ensure that you modify the target for your Main App and the Broadcast Upload Extension.  In this sample Podfile, the Target names are RNHMSExample and RNHMSExampleBroadcastUpload . Change these to the actual Target names defined in your Xcode project.   ruby require_relative '../node_modules/react-native/scripts/react_native_pods' require_relative '../node_modules/@react-native-community/cli-platform-ios/native_modules' platform :ios, '13.0'  ENSURE TO SET THE CORRECT SCREENSHARE EXTENSION TARGET NAME BELOW.  Not required if starting Screenshare from iOS devices is not a feature of the app. Then Remove this Target snippet target 'RNHMSExampleBroadcastUpload' do  use_modular_headers   pod 'HMSBroadcastExtensionSDK' end  ENSURE TO SET THE CORRECT MAIN APP TARGET NAME BELOW target 'RNHMSExample' do  config = use_native_modules   permissions_path = '../node_modules/react-native-permissions/ios'  use_react_native (   :path => config :reactNativePath ,    to enable hermes on iOS, change false to true and then install pods    :hermes_enabled => false   :hermes_enabled => true  )  pod 'Permission-Camera', :path => \"  permissions_path /Camera\"  pod 'Permission-Microphone', :path => \"  permissions_path /Microphone\"  target 'RNHMSExampleTests' do   inherit  :complete    Pods for testing  end   Enables Flipper.     Note that if you have use_frameworks  enabled, Flipper will not work and   you should disable the next line.  use_flipper ()   POST INSTALL SCRIPT TO DISABLE BITCODE FROM ALL PODS & EXCLUDE arm64 FROM SIMULATORS  post_install do installer   react_native_post_install(installer)   installer.pods_project.build_configurations.each do config    config.build_settings \"EXCLUDED_ARCHS sdk=iphonesimulator  \"  = \"arm64\"    config.build_settings 'ENABLE_BITCODE'  = 'NO'   end  end end      Background Modes Enabled In majority use-cases, playing audio from Room would be required when app is in Background Mode. Mostly, if users are starting Screenshare from their iPhones/iPads they would want to continue listening to audio from the Room. So, ensure that you have Background Modes Enabled in your Xcode project.   Enable Background Modes (/docs/v2/rn-background-modes.png)    App Groups Enabled Ensure that you have enabled App Groups for both your Main App Target & the newly created Broadcast Extension Target. If the same App Group is not enabled on both Targets then the App & the Screenshare Extension won't be able to communicate & starting Screenshare from your iOS device will fail. Also, ensure that there's no typo / spelling mis-matches between the App Group enabled on Main App Target & the Screenshare Broadcast Extension Target.   Same App Group for both Targets (/docs/v2/same-app-group.png)   Permission Denied EXC_BAD_ACCESS error 100ms Example Apps already have configurations for starting Screenshare on iOS devices. The values for App Group & Preferred Extension used in 100ms Example Apps cannot be reused by any other apps as they won't be linked to your Apple Developer Account. If the 100ms values for App Group which is \"group.flutterhms\" & Preferred Extension which is \"FlutterBroadcastUploadExtension\" are resued by your apps then you will have an Xcode exception containing Permission Denied EXC_BAD_ACCESS error message similar to the one shown below    CFMessagePort: bootstrap_register(): failed 1100 (0x44c) 'Permission denied', port = 0xbc03, name = 'group.flutterhms.88C5E70E-F40F-4C36-A48C-E65736E85CAC.audio.mach.port' See /usr/include/servers/bootstrap_defs.h for the error codes.   thread 1, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x8)   frame 0: 0x00000001c03ddc78 CoreFoundation_CFGetNonObjCTypeID + 92 ...   To resolve this issue ensure that you create a Unique App Group which is linked to your Apple Developer Account. If you do not have an Active Apple Developer Account or do not have permissions to create a New App Group, then you won't be able to this feature.    Running on Simulator Starting Screenshare from an Simulator is not supported by Apple. You can start Screenshare only from an actual iOS device like an iPhone or iPad.    iOS Deployment Target Version 100ms React Native Package is supported for iOS 12 and above versions. Ensure that the Minimum iOS Deployment Target Version is set to 12.0 or above in your Xcode project & the Podfile.    Role has Screenshare Permission Ensure that the  Role (../foundation/templates-and-roles) used to Join the Room has Screenshare permission Enabled from the  100ms Dashboard (https://dashboard.100ms.live/). If the Screenshare permission is not Enabled from the Dashboard, any users joining with this Role won't be able to start Screenshare. These users would still be able to see Screenshare performed by other Peers who have Screenshare permissions.   Screen Share Permission (/docs/v2/screenshare-permission.png) "
    },
    {
        "title": "Adaptive Bitrate",
        "link": "/react-native/v2/foundation/adaptive-bitrate",
        "platformName": "React Native",
        "objectID": "/react-native/v2/foundation/adaptive-bitrate",
        "keywords": [],
        "content": "  Adaptive bitrate (ABR) refers to features that enable dynamic adjustments to video quality—to optimise for end-user experience under diverse network conditions. ABR ensures that every participant is able to consume the highest possible quality video in conferencing or streaming use-cases, based on their bandwidth constraints.\r \r In addition to network, ABR can also optimise for the right video quality based on the size of the video element. For example, a video call running on active speaker layout has larger video tiles that require higher quality video track. These adjustments can be made dynamically with adaptive bitrate.\r \r Learn about how 100ms enables adaptive bitrate in:\r \r    Conferencing scenarios ( abr-in-conferencing)\r    Live streaming scenarios ( abr-in-live-streaming)\r \r  ABR in conferencing\r \r Peers in 100ms rooms can publish multiple video quality levels simultaneously. This is called “simulcast” in 100ms. Peers that consume these video tracks can upgrade or downgrade video quality.\r \r You can enable simulcast on the publishing role's template, and use manual or automatic layer changes on the subscriber's side.\r \r   Publisher-side configuration\r \r Simulcast configuration is opt-in and can be enabled on the role's configuration inside your template. The role's publish video quality determines video quality layers on simulcast. For example, a role configured to publish at 720p can simulcast 180p, 360p and 720p layers.\r \r  Video publish quality  Possible simulcast layers \r               \r  1080p          1080p, 540p, 270p     \r  720p          720p, 360p, 180p      \r  480p          480p, 240p         \r  360p          360p, 180p         \r  240p          240p            \r  180p          180p            \r \r   Enable via dashboard\r \r Enable \"can publish simulcast\" on the template page for a particular role. You can also specify how many video quality layers will be simultaneously published by peers of this role. The peer will publish these layers assuming network bandwidth permits.\r \r   Simulcast configuration (/docs/guides/simulcast-on-dashboard.png)\r \r   Enable via API\r \r Update role configuration using the  server-side API (/docs/server-side/v2/policy/create-update-role). The simulcast config payload can include 2 or 3 layers that scale down the selected publish layer.\r \r In the example below, the role is configured to publish 720p with 3 simulcast layers:\r \r   f for full with scale down factor of 1 (= 720p)\r   h for half with scale down factor of 2 (= 360p)\r   q for quarter with scale down factor of 4 (= 180p)   js     \"publishParams\":       ...     \"simulcast\":         \"video\":           \"layers\":                           \"rid\": \"f\",             \"scaleResolutionDownBy\": 1,             \"maxBitrate\": 700,             \"maxFramerate\": 30            ,                         \"rid\": \"h\",             \"scaleResolutionDownBy\": 2,             \"maxBitrate\": 250,             \"maxFramerate\": 30            ,                         \"rid\": \"q\",             \"scaleResolutionDownBy\": 4,             \"maxBitrate\": 100,             \"maxFramerate\": 30                              ,       \"screen\":                  \r   Subscribe-side behavior\r \r   Manual layer selection\r \r The 100ms client-side SDKs provide methods to set a preferred quality layer for a remote peer's video track. See docs for your preferred platform:\r \r    JavaScript (/docs/javascript/v2/advanced-features/simulcast)\r    iOS (/docs/ios/v2/advanced-features/simulcast)\r    Android (/docs/android/v2/advanced-features/simulcast)\r \r   Automatic layer selection\r \r <br \r <video loop=\"true\" controls=\"controls\" id=\"vid\" muted>\r   <source src=\"/docs/guides/simulcast-tile-size-1.mp4\" type=\"video/mp4\"  \r </video>\r <br \r \r  Based on video tile size: The SDK automatically ensures appropriate video layer is subscribed to, as demonstrated in the video above. For example, if the video element is 360 px in width, 360p or the closest layer will be selected.\r      JavaScript  : The useVideo hook in the 100ms React SDK auto-selects the appropriate video quality layer.\r      iOS  : HMSVideoView can auto-select video quality layer.\r      Android  : HMSVideoView ( see docs (/docs/android/v2/migrations/surfaceview-migration)) can auto-select video quality layer.\r  Based on network quality: ABR will work alongside subscribe degradation and auto-downgrade video quality for peers. This is _coming soon_.\r \r  ABR in live streaming\r \r 100ms uses the HTTP Live Streaming (HLS) protocol in live streaming scenarios. HLS supports adaptive bitrate out of the box, and HLS video players can automatically or manually pick appropriate video quality levels.\r \r Learn more on  how HLS works on our blog (https://www.100ms.live/blog/hls-101-beginners-guide).\r "
    },
    {
        "title": "Basic Concepts\r",
        "link": "/react-native/v2/foundation/basics",
        "platformName": "React Native",
        "objectID": "/react-native/v2/foundation/basics",
        "keywords": [],
        "content": " -\r \r  Architecture 100ms is a cloud platform that allows developers to add video and audio conferencing to Web, Android and iOS applications. The platform provides REST APIs, SDKs, and a dashboard that makes it simple to capture, distribute, record, and render live interactive audio, video. Any application built using 100ms' SDK has 2 components.     Client:   Use 100ms android, iOS, Web SDKs to manage connections, room states, render audio/video.     Server:   Use 100ms' APIs or dashboard to create rooms, setup room templates, trigger recording or RTMP streaming, access events.   Architecture (/docs/docs/v2/arch.png)   Basic Concepts   Room A room is the basic object that 100ms SDKs return on successful connection. This contains references to peers, tracks and everything you need to render a live a/v or live streaming app.   Peer A peer is the object returned by 100ms SDKs that contains all information about a user  name, role, video track etc.   Session A session depicts activity in a room. A session is created when one or more peers join a room to communicate with each other. A single room can have multiple sessions; a unique ID will be assigned to each session. The maximum allowed duration for a session on the 100ms platform is 12 hours.   Track A track is a segment of media (audio/video) captured from the peer's camera and microphone. Peers in a session publish local tracks and subscribe to remote tracks from other peers.   Role A role defines who can a peer see/hear, the quality at which they publish their video, whether they have permissions to publish video/screenshare, mute someone, change someone's role.   Template A template is a collection of roles, room settings, recording and RTMP settings (if used), that are used by the SDK to decide which geography to connect to, which tracks to return to the client, whether to turn on recording when a room is created, etc. Each room is associated with a template.   Destinations Destinations is used to save audio/video calls for offline viewing. 100ms supports 2 kinds of recording   SFU recording (/docs/javascript/v2/foundation/recordings sfu-recording-advanced) and  Browser recording (/docs/javascript/v2/foundation/recordings browser-recording-recommended). Also, HLS enabled configuration will allow you to live stream your room over HLS.   RTMP RTMP streaming is used to live stream your video conferencing apps to platforms like YouTube, Twitch, Facebook, MUX, etc.   Webhooks Webhook is an HTTP(S) endpoint used for pushing the notifications to your application. It will be invoked by 100ms servers to notify events of your room.   Workspace A workspace is an isolated environment which contains account data like templates, rooms, room and session history, etc. You can use workspaces to represent environments like “Production” and “Development” and invite team members to a workspace.   What are the steps to build a live app with 100ms? 1. Sign up on 100ms using the   Try For Free   button in the top navbar.   Signup for 100ms account (/docs/docs/v2/signup.png) 2. Once you're logged in to the dashboard, click on Create Your First App    Signup for 100ms account (/docs/docs/v2/create-your-first-app.png) 3.   Hover   on one of the Starter Kits. Deploy one of them. (We will use the   Video Conference Starter Kit   for this example)   Dashboard _ 100ms.png (/docs/docs/v2/select-starter-kit.png) 4. Select your account type and fill in the details   Dashboard _ 100ms.png (/docs/docs/v2/personal-details.png) 5. Choose a deployment option. This could be 100ms or Vercel (based on the Starter Kit you are deploying)   Video Conferencing Starter Kit (/docs/docs/v2/choose-your-deployment.png) 6. Enter a subdomain of your choice. Please avoid entering https/http/www or dots while entering the subdomain. Select a region closest to you and hit Continue.   choose subdomain (/docs/docs/v2/choose-subdomain.png) 7. Join or Invite someone to your deployed app with one of the roles:    join or invite (/docs/docs/v2/demo-your-app.png)   Where should I start? \r \r   Quickstart\r \r If you want to see 100ms' SDKs in action in under 5 minutes, run one of our quickstart  apps (../guides/quickstart)\r \r \r    Github Repo (https://github.com/100mslive/react-native-hms/)\r You can checkout the 100ms React Native SDK Github repo which also contains a fully fledged  Example app implementation here (https://github.com/100mslive/react-native-hms/) \r \r \r \r    Example App (https://github.com/100mslive/react-native-hms/tree/develop/example)\r \r In the  100ms Example App (https://github.com/100mslive/react-native-hms/tree/main/example) we have shown how to setup the various listeners, what data to store in the redux and what all features you can implement.\r \r We have also implemented multiple views which are commonly used. Checkout the  videos & relevant code in the Example app (https://github.com/100mslive/react-native-hms/tree/main/example additional-features).\r "
    },
    {
        "title": "Handling audio-video edge cases",
        "link": "/react-native/v2/foundation/handling-audio-video-edge-cases",
        "platformName": "React Native",
        "objectID": "/react-native/v2/foundation/handling-audio-video-edge-cases",
        "keywords": [],
        "content": "    Introduction 100ms handles a lot of standard audio/video issues internally without the developer needing to handle it explicitly. This page describes some common issues and how 100ms handles them. There are 3 major issues of issues that can occur in a audio/video conference  Device capture exceptions  Network disconnection/switching network exceptions  Network bandwidth limitation/large room exceptions   Device failure A common issue is a failure to capture mic/camera even though the user has all devices connected. Common causes include differences in OS/browser implementations of device capture APIs, permission not being granted by the user, or the device being in use by another program. The usual recourse in these exceptions is to prompt a user action  \"Grant permission\", \"Please close any other app using microphone\", \"Switch to Safari\" 100ms' SDKs come with a  preview method (../features/preview) that can be called before joining a room. This will test for device failures, network connectivity and throw errors with a recommended user action.   Network disconnection/Switching networks Another set of common issues are minor network blips. Common causes are when a user moves from one room to another, or switches from wifi to data. 100ms will send a notification within 10s of detecting a network disconnection and will automatically retry when connection is available upto 60s. After 60s, a terminal error is thrown to the client.   Network bandwidth limitation/large rooms A common occurrence in large rooms, or constrained networks is dropped frames. This results in robotic voices, frozen frames, pixelated screenshare or entire pieces of audio/video that are lost. 100ms will automatically prioritize connections if network limits are reached. This prioritization can be controlled by developers using the dashboard or 100ms APIs. eg. A developer can prioritize host's screenshare higher than guests' videos. In low bandwidth constraints, guests' videos will be turned off, while host's screenshare will remain.  "
    },
    {
        "title": "Interactive Live Streaming",
        "link": "/react-native/v2/foundation/live-streaming",
        "platformName": "React Native",
        "objectID": "/react-native/v2/foundation/live-streaming",
        "keywords": [],
        "content": "    Overview Live video interactions can span various levels of interactivity. For example, in a virtual event, some participants can be on stage talking to each other, whereas participants in the audience can be listening to them. At 100ms, we think of this as the   3 levels of interactivity  . 100ms enables you to build live video use-cases by mixing and matching these 3 levels to get to your ideal solution.     Level 1  : Full duplex audio/video in real-time   Level 1 participants publish their audio/video, and interact with others in sub-second latency. This is real-time video conferencing, similar to Zoom or Google Meet.     Level 2  : Webinar-style audience in real-time   Level 2 participants consume audio/video from level 1 participants with sub-second latency, without publishing their own audio/video. Level 2 participants can engage with level 1 through messaging (chat, emojis, custom events). This is similar to a Zoom webinar.   Levels 1 and 2 are enabled using WebRTC.     Level 3  : Live stream audience consuming in near real-time   Level 3 participants consume a composite live stream in near real-time (<10 secs of latency) without publishing their audio/video. They can interact with other participants via messaging. This is similar to viewers on Twitch or YouTube Live, and is enabled via   100ms Interactive Live Streaming  .   Live streaming uses  HLS (https://www.100ms.live/blog/hls-101-beginners-guide) to achieve near real-time latency at scale. The  roles primitive (/docs/javascript/v2/foundation/templates-and-roles) can be used to define capabilities of a participant and associate them to an interaction level. A participant can move between levels using a single API call to change roles.   Try Interactive Live Streaming Use our  Live Streaming Starter Kit (https://www.100ms.live/marketplace/live-streaming-starter-kit) to try out the experience before you write a line of code. <StepsToc  parentId=\"try-live-streaming\"  descriptions=  \"Use our Live streaming starter kit to try out the experience before you write a line of code.\", \"Understand the difference between a stream broadcaster and stream viewer.\", \"Use the demo app link to go live for the first time as a broadcaster and join the stream as viewer.\", \"Use the 100ms self-serve dashboard to update the layout, aspect ratio, etc of the stream.\"   > <StepsContainer id=\"try-live-streaming\">   Create a new app   Live streaming starter kit (/docs/docs/v2/live-streaming-starter-kit.png) 1. Make sure that you have  an account with 100ms (https://dashboard.100ms.live/register) and can access the  100ms dashboard (https://dashboard.100ms.live/) 1. On the dashboard, create a new app using the Live Streaming Starter Kit 1. Specify a subdomain and region to deploy the app   Understand roles   Live Streaming roles (/docs/docs/v2/live-streaming-roles.png) This starter kit configures your new app with  two roles (/docs/javascript/v2/foundation/templates-and-roles):   broadcaster : This role represents a streamer who publishes their audio/video. There can be multiple peers who join as broadcasters   hls-viewer : This role represents a circle 3 audience, who subscribes to the composite live stream and can interact using messaging   Go live   Go live (/docs/docs/v2/live-streaming-go-live.gif) 1. To go live for the first time, join the room as a broadcaster and start the live stream 2. Once the stream has started, join the room as an hls-viewer and you should be able to see the ongoing live stream 3. Use chat messages to interact between the viewer and the broadcaster   Customize the stream   Go live (/docs/docs/v2/live-streaming-customise.png) By default, the live stream is composed in landscape mode for desktop viewers (with an aspect ratio of 16:9). You can customise the live stream for viewers on mobile or to support multiple broadcaster tiles. 1. On the 100ms dashboard, click the gear icon on your app to open configuration settings 2. Go to \"destinations\" and scroll down to find live stream (HLS) configuration 3. Update the configuration based on your needs:    If your viewers are on mobile, change the video aspect ratio to 9:16    If you have multiple broadcasters joining in, choose grid or active speaker based on your needs    In case of grid layout, choose the tile size that fits your use-case. For example, a stream with 2 streamers looks better with 1:1 tiles. </StepsContainer>   Integrate in your app To integrate 100ms Interactive Live Streaming in your app, follow these steps: 1.  Enable live streaming destination ( enable-destination) 2.  Integrate the 100ms SDK in your app ( sdk-integration) 3.  Integrate live stream playback ( live-stream-playback) 4.  Video on demand (VOD) use cases ( video-on-demand-vod-use-cases)   Enable destination   Enable HLS (/docs/docs/v2/live-streaming-enable.gif) If your app is based on the Live Streaming Starter Kit (as shown above), the live streaming destination is enabled out-of-the-box. For custom apps, you can enable the live streaming destination manually: 1. Open configuration for your existing app using the 100ms dashboard 1. In the \"destinations\" tab, enable \"Live Streaming with HLS\" 1. Ensure that you have roles for the broadcaster (who can publish their audio/video) and the viewer (who cannot publish audio/video)   SDK integration Use the 100ms client-side SDKs to integrate streaming in your application. See code snippets for the client-side SDK  here (/docs/javascript/v2/features/hls).   Live stream playback Using our client-side SDKs, you can enable live stream playback and add interactive experiences like chat, raise hand and other functionalities to your app using  peer metadata (/docs/javascript/v2/advanced-features/peer-metadata). The process is so simple: 1. Once you  start ( step-3-go-live) live streaming, you will get an HLS URL (M3U8 URL) which you can use for playback. 2. You can use the  client-side SDK (/docs/javascript/v2/features/hls) to get the HLS URL by checking the  current state (/docs/javascript/v2/features/hls current-room-status) of the room and start playback. If you need to only enable HLS playback and don't need interactivity, you can follow one of the below approaches to get the HLS URL:     Webhook:   You can listen to hls.started.success  webhook event (/docs/server-side/v2/introduction/webhook hls-started-success) and get the HLS URL from the url field. Please check the  webhooks guide (/docs/server-side/v2/introduction/webhook) to learn more about webhooks.     Static URL:   This configuration will help you get a static URL for playback. You can enable the Static playback URLs in your template from the  dashboard (https://dashboard.100ms.live/dashboard). You can go to Destination > enable \"Live streaming with HLS\" > under \"Customise stream video output\" section > enable \"Static playback URLs.\"     Enable Static URL (/docs/docs/v2/enable-static-url.png)     _Format_: https://cdn.100ms.live/beam/<customer_id>/<room_id>/master.m3u8     customer_id : replace this placeholder with your customer_id from  developer section (https://dashboard.100ms.live/developer) on your dashboard.     room_id : replace this placeholder with the room_id of the respective room from which the stream will be broadcasted.   Video on Demand (VOD) use cases If you wish to replay your HLS stream for Video on demand (VOD) use case, 100ms provides the capability to record the HLS stream which will be posted to your webhook as a ZIP file of M3U8 format (same playback format as HLS) with all the chunks once the stream ends. You can start recording a live stream using the  client-side SDK (/docs/javascript/v2/features/hls) or using the  server API (/docs/server-side/v2/Destinations/rtmp-streaming-and-browser-recording start-streaming-recording). Once the HLS recording is completed, you will get the details of recording as a callback to the webhook configured in your account. Check hls.recording.success  webhook event (/docs/server-side/v2/introduction/webhook hls-recording-success) for more information. "
    },
    {
        "title": "Recordings",
        "link": "/react-native/v2/foundation/recordings",
        "platformName": "React Native",
        "objectID": "/react-native/v2/foundation/recordings",
        "keywords": [],
        "content": "  Recordings are an important part of the live video stack as they convert live, ephemeral content into a long-term asset. But the use of this asset varies from business to business depending on their respective use case. For example, one of the common use cases for recording is for archival purposes versus, for some, its content to be publicized. Based on your end goal, you can choose one of the recording types and its implementation. You can understand some key differences using the comparison table below.   Recording types   Recording types ( recording-types)    Quick Comparison ( quick-comparison)    Browser Recording  Recommended   ( browser-recording-recommended)    SFU Recording  Advanced   ( sfu-recording-advanced)    Recordings for Live Streaming Use-cases ( recordings-for-live-streaming-use-cases)     Video-on-demand Recording ( video-on-demand-recording)     Multiresolution Recording ( multiresolution-recording)   Configure storage ( configure-storage)    How to configure recording storage? ( how-to-configure-recording-storage)   Quick Comparison  Recording Features           Browser Recording  Recommended   SFU Recording  Advanced                                      Resolution               Upto 1080p            Only 720p              Participant-level Audio/Video Tracks  Not Available           Available              Portrait/Landscape Mode        Available             Not Available            Start/Stop Recording          On-demand             Auto start/stop with the session   Custom Layout             Available             Not Available            Role-Specific Recording        Available             Not Available            Recording Output            MP4                MP4, WebM                Browser Recording  Recommended  Browser recording is built to give users a participant-first recording experience. When enabled, our browser-based bot Beam joins a room to record the viewport like any other participant. The output is an MP4 file that captures the room's published audio/video tracks together into one single file. This option removes the complexity of syncing various audio/video tracks and offers an intuitive, participant-first recording experience. An example use case is to record a sales meeting for later usage.   Resources     How to implement Browser Recording (https://www.100ms.live/docs/server-side/v2/Destinations/rtmp-streaming-and-browser-recording)   SFU Recording  Advanced  SFU recording is built for advanced use cases, which require individual audio and video tracks for each participant. This recording option allows you to isolate recording at a participant level. Track recording allows you to record audio and video streams separately, making it easier to edit, layer, or reuse each of them. An example use case is to record a live podcast and later edit it for publishing. You can get track recordings in two forms:   Individual: Media for each peer is provided as a separate mp4 file. This file will have both audio and video of the peer. These files can be used for offline review or in implementing custom composition.   Composite  currently in beta : Audio and video of all peers are composed as per their joining/leaving the meeting and provided as a single mp4. This file can be used for offline viewing of the meeting.   Resources      How to implement SFU Recording (https://www.100ms.live/docs/server-side/v2/Destinations/recording)   Recordings for Live Streaming Use-cases These are the types of live streaming recordings:    Video-on-demand Recording Video-on-demand recording is available for our Interactive Live Streaming capability. This recording will be a file with an M3U8 file (same playback format as HLS), which can be used for replaying your HLS stream. This option is more suitable for Video-on-Demand use cases. For the implementation of this type of recording, please  contact us (https://www.100ms.live/contact).    Multiresolution Recording A multi-resolution recording is available for Interactive Live Streaming capability. This type of recording will have a multi-file structure for all available resolutions of the stream. The output will be multiple MP4 files with these resolutions: 240p, 480p, 720p, and 1080p. For an implementation of this type of recording, please  contact us (https://www.100ms.live/contact).   Configure storage You can specify a cloud storage location for your recording files in your template. Our current offering allows you to store your recordings in Amazon S3 buckets. Once you configure the S3 config of your bucket in a template, all respective recordings of sessions created via those templates will be sent to your configured bucket. This holds true for all types of aforementioned recordings.   How to configure recording storage? 1. Generate your credentials; for this example, you can check out a  guide from AWS (https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html). You can skip this step if you already have credentials. Please note that if you are running a Browser recording, you need to give upload permission to your key, but if you are running an SFU recording, you need to give both upload and download permission. 2. Go to 100ms Dashboard and go to template   configuration by selecting the configure icon  .   Create your first app (/docs/docs/v2/recording-storage-settings-step2.png) 3. Head over to the   Destinations   tab.   Destinations (/docs/docs/v2/recording-storage-settings-step3.png) 1. Key in your credentials (using an example of an S3 bucket here):    Access Key: Access Key generated from AWS IAM Console    Secret Key: Secret Key generated from AWS IAM Console    Bucket: Name of the bucket in S3    Region: Name of the region, for example, ap-south1    Prefix for Upload Path: Define the directory name (optional)   Destinations (/docs/docs/v2/recording-storage-settings-step4.png) 5. Use the   Validate Config   button to test your storage setup.   Destinations (/docs/docs/v2/recording-storage-settings-step5.png) 6. You will see a message that the AWS   configuration was successfully validated  .   Destinations (/docs/docs/v2/recording-storage-settings-step6.png) The above message ensures that your configuration is successful now, and all your recordings will start collecting in your configured destination. "
    },
    {
        "title": "Authentication and Tokens",
        "link": "/react-native/v2/foundation/security-and-tokens",
        "platformName": "React Native",
        "objectID": "/react-native/v2/foundation/security-and-tokens",
        "keywords": [],
        "content": "    Introduction In 100ms, two types of tokens are used to authenticate requests coming from your Client apps and Backend application server into the 100ms platform.    App token ( app-token) : Used to authenticate and allow end-users (peers) to join 100ms rooms. An App Token controls Peer identity and Room permissions in your real-time or Interactive live-streaming video application.    Management token ( management-token) : Used to authenticate all the requests to 100ms REST API. You can set the expiry to both these tokens; if you follow the code samples from this guide, the expiry will be set as 24 hours. However, a best practice is to set the expiry as short as feasible for your application. You must host your server to generate these tokens while transitioning your app to production.   App Token 100ms _client-side SDKs_ use App Tokens to authenticate a peer (participant) while  joining a room (./../features/join). Generate this token on the server side and make it available for your client-side apps that use the 100ms SDKs. To create an App Token, you need to use app_access_key , app_secret , room_id , and user_id .   You can get the app_access_key and app_secret from the  developer section (https://dashboard.100ms.live/developer) in your 100ms dashboard. This key and secret differ across workspaces so please ensure you are in the intended workspace before copying these credentials.     room_id  : This is the unique identifier for your room. You can get it from the  rooms page (https://dashboard.100ms.live/rooms) in your dashboard or in the response payload of the  create room server-side API (/docs/server-side/v2/Rooms/create-via-api).     user_id  : This identifier can be used to map a 100ms peer to your own internal user object for business logic. Specify your internal user identifier as the peer's user_id. If not available, use any random string.   How to use? You can get App tokens using a couple of approaches based on your app's lifecycle stage. Please check the below sections for more information:    Set up your own authentication endpoint (./../foundation/security-and-tokens set-up-your-own-authentication-endpoint)    Get app tokens from the token endpoint (./../foundation/security-and-tokens get-app-tokens-from-the-token-endpoint)    Get app tokens from the dashboard (./../foundation/security-and-tokens get-app-tokens-from-the-dashboard)    Set up your own authentication endpoint When you have completed your integration with 100ms, and while transitioning your app to production, we recommend you create your backend service for app token generation; use the code snippet below and set up the token generation service as per your preferred programming language.    Code sample: Generate app token  s id=\"client-code-token\" items=  'Node.js', 'Python', 'Java', 'Ruby', 'PHP'       id='client-code-token-0'>   javascript var jwt = require('jsonwebtoken'); var uuid4 = require('uuid4'); var app_access_key = '<app_access_key>'; var app_secret = '<app_secret>'; var payload =     access_key: app_access_key,   room_id: '<room_id>',   user_id: '<user_id>',   role: '<role>',   type: 'app',   version: 2,   iat: Math.floor(Date.now() / 1000),   nbf: Math.floor(Date.now() / 1000)  ; jwt.sign(   payload,   app_secret,         algorithm: 'HS256',     expiresIn: '24h',     jwtid: uuid4()    ,   function (err, token)       console.log(token);     );   </Tab>   id='client-code-token-1'>   python  /usr/bin/env python3 import jwt import uuid import datetime import sys app_access_key = \"<app_access_key>\" app_secret = \"<app_secret>\" def generate(room_id, user_id, role):   expires = expires or 24   3600   now = datetime.datetime.utcnow()   exp = now+ datetime.timedelta(seconds=expires)   return jwt.encode(payload=          \"access_key\": app_access_key,         \"type\":\"app\",         \"version\":2,         \"room_id\": room_id,         \"user_id\": user_id,         \"role\":role,         \"jti\": str(uuid.uuid4()),         \"exp\": exp,         \"iat\": now,         \"nbf\": now,          , key=app_secret) if __name__ == \"__main__\":   if len(sys.argv) == 3:     room_id = sys.argv 0      user_id = sys.argv 1      role = sys.argv 2    print(generate(room_id=room_id, user_id=user_id, role=role))   </Tab>   id='client-code-token-2' >   java import java.time.Instant; import java.util.Date; import java.util.HashMap; import java.util.Map; import java.util.UUID; import io.jsonwebtoken.Jwts; import io.jsonwebtoken.SignatureAlgorithm; private void generateHmsClientToken()     Map<String, Object> payload = new HashMap<>();   payload.put(\"access_key\", \"<app_access_key>\");   payload.put(\"room_id\", \"<room_id>\");   payload.put(\"user_id\", \"<user_id>\");   payload.put(\"role\", \"<role>\");   payload.put(\"type\", \"app\");   payload.put(\"version\", 2);   String token = Jwts.builder().setClaims(payload).setId(UUID.randomUUID().toString())     .setExpiration(new Date(System.currentTimeMillis() + 86400   1000))     .setIssuedAt(Date.from(Instant.ofEpochMilli(System.currentTimeMillis()  60000)))     .setNotBefore(new Date(System.currentTimeMillis()))     .signWith(SignatureAlgorithm.HS256, \"<app_secret>\".getBytes()).compact();      </Tab>   id='client-code-token-3' >   ruby require 'jwt' require 'securerandom'  $app_access_key = \"<app_access_key>\" $app_secret = \"app_secret\" def generateAppToken(room_id, user_id, role)   now = Time.now   exp = now + 86400   payload =       access_key: $app_access_key,     room_id: room_id,     user_id: user_id,     role: role,     type: \"app\",     jti: SecureRandom.uuid,     version: 2,     iat: now.to_i,     nbf: now.to_i,     exp: exp.to_i       token = JWT.encode(payload, $app_secret, 'HS256') end puts generateAppToken \"<room_id>\", \"<user_id>\", \"<role>\"    </Tab>   id='client-code-token-4'>   php <?php use Firebase JWT JWT; use Ramsey Uuid Uuid; $issuedAt = new DateTimeImmutable(); $expire  = $issuedAt->modify('+24 hours')->getTimestamp(); $accessKey = \"<app_access_key>\"; $secret = \"<app_secret>\"; $version  = 2; $type   = \"app\"; $role   = \"<role>\"; $roomId  = \"<room_id>\"; $userId  = \"<user_id>\"; $payload =     'iat' => $issuedAt->getTimestamp(),   'nbf' => $issuedAt->getTimestamp(),   'exp' => $expire,   'access_key' => $accessKey,   'type' => \"app\",   'jti' => Uuid::uuid4()->toString()   'version' => 2,   'role' => $role,   'room_id' => $roomId,   'user_id' => $userId  ; $token = JWT::encode(   $payload,   $secret,   'HS256' );   </Tab> <Note type=\"warning\">   We strictly advise you not to post <i>app_access_key</i> and <i>app_secret</i> publicly; if you   need to store them in <strong>Git</strong>, please change the repository from public to private.   <br     <br   You cannot use an <strong>App token</strong> to trigger server API requests. </Note>    Get app tokens from the token endpoint If you are building integration with 100ms, you can get app tokens from the 100ms token endpoint without hosting a token generation backend service. Refer to  this guide (./../guides/token-endpoint get-an-app-token-using-token-endpoint) for more information.    Get app tokens from the dashboard If you are building your first app by following one of our  quickstart guides (/docs/javascript/v2/guides/javascript-quickstart), you can get the app token directly from 100ms dashboard to join a room for the first time. Refer to  this guide (./../guides/token get-a-temporary-token-from-100ms-dashboard) for more information.   Management Token 100ms uses management tokens to authenticate REST APIs.  If you're evaluating 100ms  server APIs (/docs/server-side/v2/introduction/basics), you can use our public  Postman collection (/docs/server-side/v2/introduction/postman-guide fork-the-collection), which doesn't require you to create a management token as we've managed it using a  pre-request script (/docs/server-side/v2/introduction/postman-guide simplified-token-generation) within the collection. If you're transitioning your app to production, we recommend you create your backend service for management token generation. You must use the app_access_key and app_secret from the  developer section (https://dashboard.100ms.live/developer) in your 100ms dashboard to create the management token.    Code sample: Generate management token  s id=\"test-code\" items=  'Node.js', 'Python', 'Java', 'Ruby', 'PHP'       id='test-code-0'>   js var jwt = require('jsonwebtoken'); var uuid4 = require('uuid4'); var app_access_key = '<app_access_key>'; var app_secret = '<app_secret>'; jwt.sign(         access_key: app_access_key,     type: 'management',     version: 2,     iat: Math.floor(Date.now() / 1000),     nbf: Math.floor(Date.now() / 1000)    ,   app_secret,         algorithm: 'HS256',     expiresIn: '24h',     jwtid: uuid4()    ,   function (err, token)       console.log(token);     );   </Tab>   id='test-code-1'>   py  /usr/bin/env python3 import jwt import uuid import datetime app_access_key = '<app_access_key>' app_secret = '<app_secret>'  def generateManagementToken():   expires = 24   3600   now = datetime.datetime.utcnow()   exp = now + datetime.timedelta(seconds=expires)   return jwt.encode(payload=      'access_key': app_access_key,     'type': 'management',     'version': 2,     'jti': str(uuid.uuid4()),     'iat': now,     'exp': exp,     'nbf': now      , key=app_secret) if __name__ == '__main__':   print(generateManagementToken())   </Tab>   id=\"test-code-2\">   java import java.time.Instant; import java.util.Date; import java.util.HashMap; import java.util.Map; import java.util.UUID; import io.jsonwebtoken.Jwts; import io.jsonwebtoken.SignatureAlgorithm; private void generateManagementToken()     Map<String, Object> payload = new HashMap<>();   payload.put(\"access_key\", \"<app_access_key>\");   payload.put(\"type\", \"management\");   payload.put(\"version\", 2);   String token = Jwts.builder().setClaims(payload).setId(UUID.randomUUID().toString())     .setExpiration(new Date(System.currentTimeMillis() + 86400   1000))     .setIssuedAt(Date.from(Instant.ofEpochMilli(System.currentTimeMillis()  60000)))     .setNotBefore(new Date(System.currentTimeMillis()))     .signWith(SignatureAlgorithm.HS256, \"<app_secret>\".getBytes()).compact();      </Tab>   id=\"test-code-3\">   ruby require 'jwt' require 'securerandom' $app_access_key = \"<app_access_key>\" $app_secret = \"<app_secret>\" def generateManagementToken()   now = Time.now   exp = now + 86400   payload =     access_key: $app_access_key,   type: \"management\",   version: 2,   jti: SecureRandom.uuid,   iat: now.to_i,   nbf: now.to_i,   exp: exp.to_i   token = JWT.encode(payload, $app_secret, 'HS256') return token end puts generateManagementToken   </Tab>   id=\"test-code-4\">   php <?php use Firebase JWT JWT; use Ramsey Uuid Uuid; $app_access_key = \"<app_access_key>\"; $app_secret = \"<app_secret>\"; $issuedAt  = new DateTimeImmutable(); $expire   = $issuedAt->modify('+24 hours')->getTimestamp(); $payload =     'access_key' => $app_access_key,   'type' => 'management',   'version' => 2,   'jti' => Uuid::uuid4()->toString(),   'iat' => $issuedAt->getTimestamp(),   'nbf' => $issuedAt->getTimestamp(),   'exp' => $expire,  ; $token = JWT::encode($payload, $app_secret, 'HS256'); ?>   </Tab> <Note type=\"warning\">   We strictly advise you not to post <i>app_access_key</i> and <i>app_secret</i> publicly; if you   need to store them in <strong>Git</strong>, please change the repository from public to private.   <br     <br     You cannot authenticate room join requests from your client-side apps with a <strong>     Management token   </strong>. </Note> "
    },
    {
        "title": "Templates and Roles",
        "link": "/react-native/v2/foundation/templates-and-roles",
        "platformName": "React Native",
        "objectID": "/react-native/v2/foundation/templates-and-roles",
        "keywords": [],
        "content": "    Introduction Template is the blueprint of the room. It defines the settings of the room along with the behavior of users who are part of it. Room will inherit the properties from a template that you have specified while creating it. If you have not specified any template then it will pick the default template. Each template will be identified by its id or name. For example default_videoconf_7e450ffc-8ef1-4572-ab28-b32474107b89  Users can see or modify the templates by visiting  Templates on Dashboard (https://dashboard.100ms.live/templates) or via API (see below). After updating a template or some part of its like permissions, you need to rejoin or restart the session for the template updates to take place.   Template (/docs/docs/v2/template.png)   Roles Role is a collection of permissions that allows you to perform certain set of operations while being part of the room. It has the following attributes:   Name Every role has a name that should be unique inside a template. This name will be used while generating app tokens and referencing inside a template.   Priority Priority will determine the order in which the roles will be degraded. A lower number represents a higher priority.   Publish Strategies Publish strategies will be used to determine the tracks and their quality which can be published by this role.  Strategy       Description                                                                                                                                                                                                                            Can share audio    Whether the role is allowed to publish the audio track or not.                                                                                                                  Can share video    Whether the role is allowed to publish the video track or not                                                                                                                   Can share screen   Whether the role is allowed to do screen share or not                                                                                                                       Video quality     Quality of the video track which is going to be published by the role. Currently, 6 video qualities 1080p , 720p , 480p , 360p , 240p and 120p are predefined and the user can select one out of these values. This option will be visible only if the   Can share video   is enabled.   Screenshare quality  Quality of the screen which is going to be shared by the role. Currently, 2 video qualities 720p and 1080p are predefined and the user can select one out of these values. This option will be visible only if the   Can share screen   is enabled.                       Subscribe Strategies Subscribe strategies will be used to determine what all roles, this role can subscribe to.  Strategy        Description                                                                                                                                                                                                      Subscribe to      You can select all the roles of the template which this role will subscribe                                                                                             Subscribe Degradation  When this flag is turned on, one or more remote video tracks will be muted automatically when the network condition worsens. Such tracks will be marked as degraded . When the network condition improves, the degraded tracks will automatically be unmuted.    Permissions Permissions will contain a list of additional privileges that this role will have.  Permission               Description                                                                                                                                              Can change any participant's role   With this permission, user will be able to change the role of the other participant's who are present in the room                                  Can mute any participant        With this permission, user will be able to mute any participant's audio and/or video.                                                Can ask participant to unmute     With this permission, user will be able to ask any participant to unmute their audio and/or video.                                         Can remove participant from the room  With this permission, user will be able to remove any participant from the current session of the room.                                       Can end current session of the room  With this permission, user will be able to end the current session of the room.                                                   Can receive room state         With this permission, user will be able to receive room state like peer-count and peer-list on the preview screen.                                 Can start/stop RTMP livestream     With this permission, user will be able to publish live audio/video livestream externally to social media and custom platforms (e.g Youtube/Facebook/Twitter).           Can start/stop HLS livestream     With this permission, user will be able to publish audio/video livestream in the HLS format.                                            Can start/stop Beam Recording     With this permission, user will be able to record meeting/livestream via the browser recording approach where a bot will join the room and record the meeting/livestream as is.    Advanced Settings As the name suggests, Advanced Settings section contains more settings and controls for the advanced user.   Template (/docs/docs/v2/advanced-settings.png)   Preview room state Preview room state enables you to build a \"preview\" screen which shows the state of the room before joining. This room state includes a list of peers, which can be used to show who is in the room. Preview room state also includes recording and streaming state. Preview room state settings define strategy of sending state updates to client SDKs.  Setting                  Description                                                                                                                                                                                                 Room-state Message Interval (in seconds)  Room-state data will be sent over a regular interval of these many seconds. Consequently, the room state displayed on the preview screen will refresh accordingly. This value must be a multiple of 5, between 5 and 3600 seconds, both inclusive.   Send Peer List in Room-state        Enabling this will send peer-list info of the room. If disabled, only the peer count is sent.                                                                             Enable Room-State             If enabled, room-state data will be sent to the preview screen. If disabled, no such room-state data will be sent.                                                                   Roles with room-state permission      This is the list of all the roles which will get the room-state data. You can also individually toggle these settings in the Roles tab under the Permissions section.                                           API reference Apart from the dashboard, a programmatic way to interact with templates is via  API (/server-side/v2/policy/template-object). "
    },
    {
        "title": "Could not invoke HMSSDK.build or HMSManager.build",
        "link": "/react-native/v2/guides/faq#could-not-invoke-hmssdkbuild-or-hmsmanagerbuild",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/faq#could-not-invoke-hmssdkbuild-or-hmsmanagerbuild-0",
        "keywords": [],
        "content": "---\ntitle: Frequently Asked Questions (FAQ)\nnav: 2.5\n---\n\nThis page lists down frequently asked questions. If you want to add a new question or edit an older one, feel free to send us a PR.\n\n\n## Could not invoke HMSSDK.build or HMSManager.build\n\n![error](https://user-images.githubusercontent.com/56931905/174788439-53a39a54-847e-46f7-9104-7524b1547992.jpg)\n\nThis error generally appears in development mode due to hot reloading. When the peer has joined the room and then the app is hot reloaded from the terminal, the peer is still in the room and when he tries to join back this error occurs. To make sure this error does not occur you have to remove your peer from the room.\n\n-   To avoid you can add `instance.leave()` function on the unmounting of the Home Screen, so whenever the app is hot reloaded which leads to unmounting of the Home screen the leave function is called.\n\n```js section=CouldNotInvokeHMSManagerBuild sectionIndex=1\nconst onLeavePress = async () => {\n    await instance\n        ?.leave()\n        .then((d) => console.log('Leave Success: ', d))\n        .catch((e) => console.log('Leave Error: ', e));\n};\n\nuseEffect(() => {\n    return () => {\n        onLeavePress();\n    };\n}, []);\n```\n\n-   If this error occurred you can either join through web app and remove the peer which is still present due to hot reloading or you can kill the app and rebuild it.\n\n\n## "
    },
    {
        "title": "\"trackId\" is undefined (HMSView is rendering blank view)",
        "link": "/react-native/v2/guides/faq#trackid-is-undefined-hmsview-is-rendering-blank-view",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/faq#trackid-is-undefined-hmsview-is-rendering-blank-view-1",
        "keywords": [],
        "content": "\"trackId\" is undefined (HMSView is rendering blank view)\n\n`Peer` objects can have undefined `trackId`. If you are trying to use `trackId` directly from `Peer` object, then you might endup with undefined `trackId` problem.\n\nWe recommend using `ON_TRACK_UPDATE` event for listening to track updates. When you receive `TRACK_ADDED` update type on this event, you can save received `track` and `peer` objects.\n\nThen you can use `trackId` from `track` object to show video in `HMSView`. This way your `trackId` will never be undefined.\n\n\n> Note: `ON_TRACK_UPDATE` event is emitted for both \"Audio\" and \"Video\" tracks. For rendering video usecase, you only need to consider events received for \"Video\" tracks.\n\nSome Useful links:\n - [Render Video of Peer](https://www.100ms.live/docs/react-native/v2/features/render-video)\n - [HMSTrackUpdate Event Listener](https://www.100ms.live/docs/react-native/v2/features/event-listeners-enums#hms-track-update)\n\n\n## "
    },
    {
        "title": "Unable to Join Meeting or getting error on join / preview functions of HMS Instance",
        "link": "/react-native/v2/guides/faq#unable-to-join-meeting-or-getting-error-on-join-preview-functions-of-hms-instance",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/faq#unable-to-join-meeting-or-getting-error-on-join-preview-functions-of-hms-instance-2",
        "keywords": [],
        "content": "Unable to Join Meeting or getting error on join / preview functions of HMS Instance\n\nThis problem can happen due to many reasons. To self-serve, We recommend you to check if:\n\n- you are using correct `authToken` and `username`.\n\n  > `authToken` must contain correct `roomId` and `role`. `role` should be lowercase.\n  \n  Also check out [Auth Token and Security Guide](https://www.100ms.live/docs/react-native/v2/foundation/security-and-tokens).\n\n- you are calling static `build` function on `HMSSDK` class correctly. <strong>Do not create an instance of `HMSSDK` class yourself with `new` keyword</strong>.\n  \n  `build` function returns instance of `HMSSDK` class and also sets up SDK on native side.\n\n  ```js\n  import { HMSSDK } from '@100mslive/react-native-hms';\n\n  const hmsInstance = await HMSSDK.build();\n  ```\n\n  Also check out [Join Room Guide](https://www.100ms.live/docs/react-native/v2/features/join)\n\n- Meeting Joining link is not disabled\n\n\n## "
    },
    {
        "title": "Run the Example app",
        "link": "/react-native/v2/guides/faq#run-the-example-app",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/faq#run-the-example-app-3",
        "keywords": [],
        "content": "Run the Example app\n\nTo run the Example app on your system, follow these steps -\n\n1. Clone [100ms React Native Package repository](https://github.com/100mslive/react-native-hms)\n\n2. In the project root, run `npm install`\n\n3. Go to the example folder, `cd example`\n\n4. In the example folder, run `npm install`\n\n5. To run on Android, run `npx react-native run-android`\n\n  You can run example app on Android Emulator using \"deviceId\" option, run `npx react-native run-android --deviceId <device_id_here>`\n\n6. To run on iOS -\n\n  a. First run Pod Install in iOS folder, `cd ios && pod install && cd ../`\n\n  b. Set the Correct Development Team in Signing & Capabilities in Xcode Build Settings to run on an actual iPhone or iPad. Apple Development Team Signing is not required for running the app on Simulators.\n\n  c. Run the command `npx react-native run-ios`\n\n  You can run example app on iOS Simulator using \"simulator\" option, run `npx react-native run-ios --simulator <simulator_name_here>`.\n\n\n## "
    },
    {
        "title": "UI components to test all the features and functionalities",
        "link": "/react-native/v2/guides/faq#ui-components-to-test-all-the-features-and-functionalities",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/faq#ui-components-to-test-all-the-features-and-functionalities-4",
        "keywords": [],
        "content": "UI components to test all the features and functionalities\n\n100ms React Native package does not provide UI components except `HMSView`. However, We have created UI for testing all features of React Native SDK in our [example app](https://github.com/100mslive/react-native-hms/tree/main/example) and [sample apps](https://github.com/100mslive/react-native-hms/tree/develop/sample-apps).\n\nYou can use UI from these apps to quickly test features and experiment in your apps.\n\n\n## "
    },
    {
        "title": "Customize Track Settings",
        "link": "/react-native/v2/guides/faq#customize-track-settings",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/faq#customize-track-settings-5",
        "keywords": [],
        "content": "Customize Track Settings\n\nYou can customize Video and Audio track settings of Local Peer with `HMSTrackSettings`, `HMSVideoTrackSettings` and `HMSAudioTrackSettings` classes.\n\nwhile setting up `HMSSDK` instance you can pass instance of `HMSTrackSettings` to `build` function available on `HMSSDK` class.\n\nRefer to [Track Settings Guide](https://www.100ms.live/docs/react-native/v2/advanced-features/track-settings) for more info.\n\n**To customize more settings (other than defined on above mentioned classes) like video quality, aspect ratio, screenshare quality.**\n\nYou can change all these and more in [100ms dashboard](https://dashboard.100ms.live/templates). Check out [Templates and Roles Guide](https://www.100ms.live/docs/react-native/v2/foundation/templates-and-roles)\n\n\n## "
    },
    {
        "title": "BLUETOOTH_CONNECT permission error/warning",
        "link": "/react-native/v2/guides/faq#bluetooth_connect-permission-errorwarning",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/faq#bluetooth_connect-permission-errorwarning-6",
        "keywords": [],
        "content": "BLUETOOTH_CONNECT permission error/warning\n\nOn Android 12 devices, new Bluetooth permissions have been added to interact with other nearby Bluetooth devices. To fix this error -\n\n1. Declare `BLUETOOTH_CONNECT` permission in AndroidManifest.xml file.\n\n```xml\n<uses-permission android:name=\"android.permission.BLUETOOTH\" android:maxSdkVersion=\"30\" />\n\n<uses-permission android:name=\"android.permission.BLUETOOTH_CONNECT\" />\n```\n\n2. The `BLUETOOTH_CONNECT` permission is runtime permission. Therefore, you must also request user approval at runtime before you join a call or display a preview in your app, like we do for Camera and Audio Permissions.\n  \n  We suggest using [react-native-permission](https://www.npmjs.com/package/react-native-permissions) to acquire permissions from both platforms.\n\n  Check out official [Android Bluetooth Permissions](https://developer.android.com/guide/topics/connectivity/bluetooth/permissions) page.\n\n  You can also check [Our Android Integration Guide](https://www.100ms.live/docs/react-native/v2/features/integration#for-android) and permission related code in our [Quickstart Sample App](https://github.com/100mslive/react-native-hms/tree/develop/sample-apps)\n\n\n## "
    },
    {
        "title": "Using HMS with another WebRTC library",
        "link": "/react-native/v2/guides/faq#using-hms-with-another-webrtc-library",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/faq#using-hms-with-another-webrtc-library-7",
        "keywords": [],
        "content": "Using HMS with another WebRTC library\n\nWebRTC is a core dependency of 100ms SDKs. While building your Real Time Audio Video apps, developers tend to utilize multiple libraries. So it can happen that some another package also has WebRTC as a dependency. In this scenario, your app might emit compile time errors, crash at run time or have unexpected behaviours. This usually happens due to multiple WebRTC instances within the app. To avoid these issues, it's recommended to only add 100ms package & remove any other packages that also depend on WebRTC. 100ms provides a rich set of features which you can easily customize to build your ideal Audio Video experiences.\n\n\n## "
    },
    {
        "title": "Expo Support",
        "link": "/react-native/v2/guides/faq#expo-support",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/faq#expo-support-8",
        "keywords": [],
        "content": "Expo Support\n\nYes, Expo is supported by 100ms React Native SDK. You can follow our [Expo Setup Guide](https://www.100ms.live/docs/react-native/v2/features/integration#expo-setup) to complete your setup.\n\n\n## "
    },
    {
        "title": "Change Streaming Video Aspect Ratio",
        "link": "/react-native/v2/guides/faq#change-streaming-video-aspect-ratio",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/faq#change-streaming-video-aspect-ratio-9",
        "keywords": [],
        "content": "Change Streaming Video Aspect Ratio\n\nDefault Aspect Ratio of Streaming Video is 16:9. This can show many Peer Tiles in your streaming video.\n\nYou can change Aspect Ratio of Streaming video from -\n\n[100ms Dashboard](https://dashboard.100ms.live/dashboard) > `Templates > Select a Template > Destinations tab > scroll down to Live Streaming > \"Customize stream video output\"`\n\nYou may want to change default ratio as per your use case. for example, If you have only one Tile in your stream video, then you can make video aspect ratio as same as Peer Tile acpect ratio.\n\nThis will give your Stream Viewers very nice watching experience.\n\n\n## "
    },
    {
        "title": "How to get HLS Stream URL?",
        "link": "/react-native/v2/guides/faq#how-to-get-hls-stream-url",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/faq#how-to-get-hls-stream-url-10",
        "keywords": [],
        "content": "How to get HLS Stream URL?\n\nHLS Stream URL is available in `Room` object. Check out below code snippet -\n\n```js\n// you can check if hls stream is running\nconst isStreaming = room.hlsStreamingState?.running;\n\n// you can access various variants of running hls stream\nconst hlsStreamingVariant = room.hlsStreamingState?.variants[0];\n\n// on stream variant, you have access to stream url\nconst sreamURL = hlsStreamingVariant.hlsStreamUrl;\n\n// Showing running stream in a video player\n<VideoPlayer url={hlsStreamingVariant.hlsStreamUrl} />\n```\n\nYou can get `Room` object by `hmsInstance.getRoom` method and `HMSUpdateListenerActions.ON_ROOM_UPDATE` event -\n\n```js\n// Initially get room object from `getRoom` method\nconst room = await hmsInstance.getRoom();\n\n// Add listener to receive Room Updates\nhmsInstance.addEventListener(\n  HMSUpdateListenerActions.ON_ROOM_UPDATE,\n  (data: { room: HMSRoom; type: HMSRoomUpdate; }) => {\n    // Updated Room object\n    const room = data.room;\n\n    if (data.type === HMSRoomUpdate.HLS_STREAMING_STATE_UPDATED) {\n      console.log('HLS Streaming state has changed')\n    }\n  }\n);\n```\n\nTo know how to start or stop HLS Streaming, check out [HLS Streaming Docs](https://www.100ms.live/docs/react-native/v2/features/hls)\n\n\n## "
    },
    {
        "title": "Video Player to play HLS Streams",
        "link": "/react-native/v2/guides/faq#video-player-to-play-hls-streams",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/faq#video-player-to-play-hls-streams-11",
        "keywords": [],
        "content": "Video Player to play HLS Streams\n\n[react-native-video](https://www.npmjs.com/package/react-native-video) is the most reliable package to play videos on React Native apps. We are also using this package in our example app to play HLS Streams.\n\n\n## "
    },
    {
        "title": "How to Mute/Unmute Specific or All Remote Peers?",
        "link": "/react-native/v2/guides/faq#how-to-muteunmute-specific-or-all-remote-peers",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/faq#how-to-muteunmute-specific-or-all-remote-peers-12",
        "keywords": [],
        "content": "How to Mute/Unmute Specific or All Remote Peers?\n\n100ms have `changeTrackState` APIs to request mute or unmute remote peers.\n\nRefer to [Change Track State](https://www.100ms.live/docs/react-native/v2/features/change-track-state) API docs to learn more.\n\nTo Mute all Remote Peers at once, you can refer [here](https://www.100ms.live/docs/react-native/v2/features/change-track-state#mute-all-remote-audio-tracks).\n\n\n## "
    },
    {
        "title": "How to Mute/Unmute Specific or All Remote Peers only locally?",
        "link": "/react-native/v2/guides/faq#how-to-muteunmute-specific-or-all-remote-peers-only-locally",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/faq#how-to-muteunmute-specific-or-all-remote-peers-only-locally-13",
        "keywords": [],
        "content": "How to Mute/Unmute Specific or All Remote Peers only locally?\n\nWhen you mute audio or video of remote peer locally, you won't be able to hear or see the remote peer but it will be still audible and visible to others.\n\n100ms have \"Playback Allowed\" APIs to mute or unmute remote peers locally.\n\nRefer to [Playback Allowed](https://www.100ms.live/docs/react-native/v2/features/playback-allowed) API docs to learn more.\n\nTo locally mute all Remote Peers at once, you can refer [here](https://www.100ms.live/docs/react-native/v2/features/playback-allowed#local-mute-all-remote-peers-audio).\n\n\n## "
    },
    {
        "title": "Restrict Remote Peer from Speaking after their Audio is Muted",
        "link": "/react-native/v2/guides/faq#restrict-remote-peer-from-speaking-after-their-audio-is-muted",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/faq#restrict-remote-peer-from-speaking-after-their-audio-is-muted-14",
        "keywords": [],
        "content": "Restrict Remote Peer from Speaking after their Audio is Muted\n\nOnce you [Mute a Peer](https://www.100ms.live/docs/react-native/v2/features/change-track-state), they can unmute themselves. To prevent peers from un-muting themselves, you should [Change their Role](https://www.100ms.live/docs/react-native/v2/features/change-role) to a `Role` which has less [publishing permissions](https://www.100ms.live/docs/react-native/v2/foundation/templates-and-roles#publish-strategies) as per your use case instead of muting the peer.\n\n\n## "
    },
    {
        "title": "Enable PIP Mode automatically when user leaves app",
        "link": "/react-native/v2/guides/faq#enable-pip-mode-automatically-when-user-leaves-app",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/faq#enable-pip-mode-automatically-when-user-leaves-app-15",
        "keywords": [],
        "content": "Enable PIP Mode automatically when user leaves app\n\nCurrently, Enabling [Picture in Picture (PIP) mode](https://www.100ms.live/docs/react-native/v2/advanced-features/pip-mode) automatically (that is without calling any function) is not supported.\n\nWe recommend enabling Picture in Picture (PIP) mode (by calling `enablePipMode` function) **while app is active**, on a button click or programmatically.\n\n\n## "
    },
    {
        "title": "Enable PIP Mode in iOS device.",
        "link": "/react-native/v2/guides/faq#enable-pip-mode-in-ios-device",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/faq#enable-pip-mode-in-ios-device-16",
        "keywords": [],
        "content": "Enable PIP Mode in iOS device.\n\n[Picture in Picture (PIP) mode](https://www.100ms.live/docs/react-native/v2/advanced-features/pip-mode) is not supported in iOS devices due to lack of permission of using [multitasking-camera-access](https://developer.apple.com/documentation/bundleresources/entitlements/com_apple_developer_avfoundation_multitasking-camera-access?language=objc#discussion) entitlement.\n\nWe are working on making this work. Thanks for your patience.\n\n\n## "
    },
    {
        "title": "Using HMSSDK Instance in nested components without passing as props",
        "link": "/react-native/v2/guides/faq#using-hmssdk-instance-in-nested-components-without-passing-as-props",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/faq#using-hmssdk-instance-in-nested-components-without-passing-as-props-17",
        "keywords": [],
        "content": "Using HMSSDK Instance in nested components without passing as props\n\n**Don't call `HMSSDK.build` function just to get the `hmsInstance` in nested components without prop drilling to use various APIs.**\n\n`HMSSDK.build` creates and returns new instances of SDK each time you call it. It is not returning the previously created SDK instance.\n\nWe recommend using State Management solutions like Redux or Context API to save your originally created `hmsInstance` into store and use this stored instance in your nested components.\n\n\n## "
    },
    {
        "title": "How many HMSView can be rendered?",
        "link": "/react-native/v2/guides/faq#how-many-hmsview-can-be-rendered",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/faq#how-many-hmsview-can-be-rendered-18",
        "keywords": [],
        "content": "How many HMSView can be rendered?\n\nWe recommend rendering separate HMSView for each `trackId`. It means If you have 50 peers with trackIds in a room, then you will render 50 HMSView for each peer.\n\nThis can have an impact on your apps' performance. Therefore, since ideally maximum 4-6 HMSView should be visible at a time because of small screen size of mobile devices.\n\nYou can use [FlatList](https://reactnative.dev/docs/flatlist) to render HMSView for large list of peers, this way HMSView that are not in visible area will never be mounted and HMSViews that goes out of visible area will be unmounted.\n\nBy using FlatList you are improving your apps' performance and rendering separate HMSView for each `trackId`.\n\n> Pro Tip: You can use `key` prop with `trackId` as value to bind `HMSView` with trackId. Example - `<HMSView key={trackId} {...} />`\n"
    },
    {
        "title": "Joining with Preview",
        "link": "/react-native/v2/guides/joining-with-preview",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/joining-with-preview",
        "keywords": [],
        "content": "    Attaching Preview and Error Listeners It is important to attach preview listener and error listener before calling the preview function in order to get updates if preview got executed successfully or if it failed.   js section=AttachingPreviewAndErrorListeners sectionIndex=1 import   HMSSDK, HMSUpdateListenerActions   from '@100mslive/react-native-hms'; const build = () =>     const hmsInstance = HMSSDK.build(); // save this hms instance   hmsInstance.addEventListener(     HMSUpdateListenerActions.ON_PREVIEW,     onPreviewListener // function that will be called on Preview success   );   hmsInstance.addEventListener(     HMSUpdateListenerActions.ON_JOIN,     onJoinListener // function that will be called on Join success   );   hmsInstance.addEventListener(     HMSUpdateListenerActions.ON_ERROR,     onErrorListener // function that will be called on Preview or Join failure   );  ;     Calling Preview and Join functions Preview screen is a frequently used UX element which allows users to check if their input devices are working properly and set the initial state (mute/unmute) of their audio and video tracks before joining. Once the listeners are attached we can call the preview function. After the preview function is called we have to wait for the preview or error callback before calling the join function.   js section=CallingPreviewAndJoinFunctions sectionIndex=1 import   HMSException   from '@100mslive/react-native-hms'; const preview = () =>     hmsInstance.preview(config);  ; const onErrorListener = (data: HMSException) =>     // this callback will be triggered if preview or join functions fails  ; const onPreviewListener = () =>     // this callback will be triggered if preview is successful and then only we should call join   hmsInstance.join(config);  ; const onJoinListener = () =>     // this callback will be triggered if join is successful  ;   "
    },
    {
        "title": "React Native Quickstart Guide",
        "link": "/react-native/v2/guides/quickstart",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/quickstart",
        "keywords": [],
        "content": "    Overview This guide will walk you through simple instructions to create a Video Conferencing app using the 100ms React Native SDK and test it using an Emulator or your Mobile Phone. Please check our  Basic Concepts Guide (../foundation/basics) to understand the concepts like Templates, Rooms, Peers, Roles, Tracks, etc. This Guide contains instructions for two approaches to get you started with 100ms React Native SDK: 1.  Create a Sample App ( create-a-sample-app) — Instructions to create a React Native app quickly with a complete Code Sample. 2.  Building Step-by-Step ( building-step-by-step) — Instructions to walk you through the implementation of the app in a step-by-step manner. You can also check our  Basic Sample App (https://github.com/100mslive/react-native-hms/tree/develop/sample-apps/rn-quickstart-app) on GitHub. Check out the full-featured Example app implementation in the  100ms React Native SDK GitHub repository (https://github.com/100mslive/react-native-hms/tree/main/example) showcasing multiple features provided by 100ms. You can download the fully featured Example app on your iOS & Android Devices. 📲 Download the Example iOS app here: https://testflight.apple.com/join/v4bSIPad 🤖 Download the Example Android app here: https://appdistribution.firebase.dev/i/7b7ab3b30e627c35   Create a sample app This section contains instructions to create a simple React Native Video Conferencing app. We will help you with instructions to understand the project setup and complete code sample to implement this quickly. <div className=\"steps-container\">   Prerequisites   A  100ms account (https://dashboard.100ms.live/register) if you don't have one already.   Working  React Native Development Environment (https://reactnative.dev/docs/environment-setup) for React Native CLI   Familiar with basics of  React Native (https://reactnative.dev/docs/getting-started).    VS code (https://code.visualstudio.com/) or any other IDE / code editor   Create a React Native app Once you have the prerequisites, follow the steps below to create a React Native app. This guide will use  VS code (https://code.visualstudio.com/) but you can use any code editor or IDE. 1. Open your Terminal and navigate to directory/folder where you would like to create your app. 2. Run the below command to create React Native app:  ​   bash section=CreateRnApp npx react-native init HMSSampleApp version 0.68.5 npm && cd ./HMSSampleApp   3. Once the app is created, open it in VS code. 4. Test run your app   a. Build the App   ​    s id=\"test-run-sample-app\" items=  'iOS', 'Android'         id=\"test-run-sample-app-0\">     bash section=BuildApp sectionIndex=1 tab=iOS   npx react-native run-ios simulator=\"iPhone 14\"       </Tab>     id=\"test-run-sample-app-1\">     bash section=BuildApp sectionIndex=1 tab=Android   npx react-native run-android       </Tab>   b. Start Metro Bundler if it is not already started   ​     bash section=BuildApp sectionIndex=2   npx react-native start       or follow instructions printed in Terminal to start the Metro Bundler or Run the Application.   Install the Dependencies After the Test run of your app is successful, you can install  100ms React Native package (https://www.npmjs.com/package/@100mslive/react-native-hms) in your app. Since the app requires Camera & Microphone permissions, a package for requesting these permissions from users should also be installed. We recommend using the  react-native-permissions (https://www.npmjs.com/package/react-native-permissions) package.   bash npm install save @100mslive/react-native-hms react-native-permissions     Complete code example Now that your project setup is complete, let's replace the code in the App.js file with the complete code sample below    js section=completeCodeExample import React,   useState, useRef, useEffect   from 'react'; import     SafeAreaView,   FlatList,   StatusBar,   Text,   View,   TouchableHighlight,   ActivityIndicator   from 'react-native'; import   PERMISSIONS, request, requestMultiple, RESULTS   from 'react-native-permissions'; import     HMSSDK,   HMSUpdateListenerActions,   HMSConfig,   HMSTrackType,   HMSTrackUpdate,   HMSPeerUpdate   from '@100mslive/react-native-hms'; /     Take Auth Token from Dashbaord for this sample app.   For more info, Check out  @link https://www.100ms.live/docs/react-native/v2/foundation/security-and-tokens  Token Concept   / const AUTH_TOKEN = 'PASTE_AUTH_TOKEN_FROM_DASHBOARD_HERE'; const USERNAME = 'Test User'; // region Screens const App = () =>     const  joinRoom, setJoinRoom  = useState(false);   return (     <SafeAreaView style=   flex: 1, backgroundColor: ' EFF7FF'   >       <StatusBar barStyle= 'dark-content'           joinRoom ? (         <RoomScreen navigate= () => setJoinRoom(false)          ) : (         <HomeScreen navigate= () => setJoinRoom(true)          )      </SafeAreaView>   );  ; export default App; const HomeScreen = (  navigate  ) =>     // Function to handle \"Join Room\" button press   const handleJoinPress = async () =>       // Checking Device Permissions     const permissionsGranted = await checkPermissions(        PERMISSIONS.ANDROID.CAMERA,       PERMISSIONS.ANDROID.RECORD_AUDIO,       PERMISSIONS.ANDROID.BLUETOOTH_CONNECT      );     if (permissionsGranted)         navigate('RoomScreen');       else         console.log('Permission Not Granted ');          ;   return (     <View style=   flex: 1, alignItems: 'center', justifyContent: 'center'   >       <TouchableHighlight         onPress= handleJoinPress          underlayColor=\" 143466\"         style=             paddingHorizontal: 20,           paddingVertical: 12,           backgroundColor: ' 2471ED',           borderRadius: 8           >         <Text style=   fontSize: 20, color: ' ffffff'   >Join Room</Text>       </TouchableHighlight>     </View>   );  ; const RoomScreen = (  navigate  ) =>     /       usePeerTrackNodes hook takes care of setting up  @link HMSSDK  HMSSDK  instance, joining room and adding all required event listeners.     It gives us:     1. peerTrackNodes  This is a list of  @link PeerTrackNode , we can use this list to render local and remote peer tiles.     2. loading  We can show loader while Room Room join is under process.     3. leaveRoom  This is a function that can be called on a button press to leave room and go back to Welcome screen.    /   const   peerTrackNodes, loading, leaveRoom, hmsInstanceRef   = usePeerTrackNodes();   const HmsView = hmsInstanceRef.current?.HmsView;   const _keyExtractor = (item) => item.id;   // _renderItem function returns a Tile UI for each item which is PeerTrackNode object   const _renderItem = (  item  ) =>       const   peer, track   = item;     return (       <View         style=             height: 300,           margin: 8,           borderRadius: 20,           overflow: 'hidden',           backgroundColor: ' A0C3D2'           >          /  Checking if we have \"HmsView\" component, valid trackId and \"track is not muted\"  /           HmsView && track && track.trackId &&  track.isMute() ? (           // To Render Peer Live Videos, We can use HMSView           // For more info about its props and usage, Check out  @link https://www.100ms.live/docs/react-native/v2/features/render-video  Render Video            <HmsView             trackId= track.trackId              mirror= peer.isLocal              style=   width: '100%', height: '100%'                        ) : (           <View style=   flex: 1, alignItems: 'center', justifyContent: 'center'   >             <View               style=                   width: 100,                 height: 100,                 borderRadius: 50,                 alignItems: 'center',                 justifyContent: 'center',                 backgroundColor: ' FD8A8A'                 >               <Text                 style=                     textAlign: 'center',                   fontSize: 28,                   fontWeight: 'bold',                   textTransform: 'uppercase'                   >                  peer.name                   .split(' ')                   .map((item) => item 0 )                   .join('')                </Text>             </View>           </View>         )        </View>     );    ;   const handleRoomEnd = () =>       leaveRoom();     navigate('HomeScreen');    ;   return (     <View style=   flex: 1   >        loading ? (         // Showing loader while Join is under process         <View style=   flex: 1, alignItems: 'center', justifyContent: 'center'   >           <ActivityIndicator size= 'large'  color=\" 2471ED\"           </View>       ) : (         <View style=   flex: 1, position: 'relative'   >            peerTrackNodes.length > 0 ? (             // Rendering list of Peers             <FlatList               centerContent= true                data= peerTrackNodes                showsVerticalScrollIndicator= false                keyExtractor= _keyExtractor                renderItem= _renderItem                contentContainerStyle=                   paddingBottom: 120,                 flexGrow: Platform.OS === 'android' ? 1 : undefined,                 justifyContent: Platform.OS === 'android' ? 'center' : undefined                                          ) : (             <View style=   flex: 1, alignItems: 'center', justifyContent: 'center'   >               <Text style=   fontSize: 28, marginBottom: 32   >Welcome </Text>               <Text style=   fontSize: 16   >You’re the first one here.</Text>               <Text style=   fontSize: 16   >                 Sit back and relax till the others join.               </Text>             </View>           )             /  Button to Leave Room  /            <TouchableHighlight             onPress= handleRoomEnd              underlayColor=\" 6e2028\"             style=                 position: 'absolute',               bottom: 40,               alignSelf: 'center',               backgroundColor: ' CC525F',               width: 60,               height: 60,               borderRadius: 30,               alignItems: 'center',               justifyContent: 'center'               >             <Text style=   textAlign: 'center', color: ' ffffff', fontWeight: 'bold'   >               Leave Room             </Text>           </TouchableHighlight>         </View>       )      </View>   );  ; // endregion Screens /     Sets up HMSSDK instance, Adds required Event Listeners   Checkout Quick Start guide to know things covered  @link https://www.100ms.live/docs/react-native/v2/guides/quickstart  Quick Start Guide   / export const usePeerTrackNodes = () =>     const hmsInstanceRef = useRef(null); // We will save hmsInstance in this ref   const  loading, setLoading  = useState(true);   const  peerTrackNodes, setPeerTrackNodes  = useState(  ); // Use this state to render Peer Tiles   /       Handles Room leave process    /   const handleRoomLeave = async () =>       try         const hmsInstance = hmsInstanceRef.current;       if ( hmsInstance)           return Promise.reject('HMSSDK instance is null');               // Removing all registered listeners       hmsInstance.removeAllListeners();       /           Leave Room. For more info, Check out  @link https://www.100ms.live/docs/react-native/v2/features/leave  Leave Room         /       const leaveResult = await hmsInstance.leave();       console.log('Leave Success: ', leaveResult);       /           Free/Release Resources. For more info, Check out  @link https://www.100ms.live/docs/react-native/v2/features/release-resources  Release Resources         /       const destroyResult = await hmsInstance.destroy();       console.log('Destroy Success: ', destroyResult);       // Removing HMSSDK instance       hmsInstanceRef.current = null;       catch (error)         console.log('Leave or Destroy Error: ', error);          ;   /       Handles Join Update received from  @link HMSUpdateListenerActions.ON_JOIN  event listener     Receiving This event means User (i.e. Local Peer) has successfully joined room     @param  Object  data  object which has room object     @param  Object  data.room  current  @link HMSRoom  room  object    /   const onJoinSuccess = (data) =>       /         Checkout  @link HMSLocalPeer  HMSLocalPeer  Class      /     const   localPeer   = data.room;     // Creating or Updating Local Peer Tile     // updateNode function updates \"Track and Peer objects\" in PeerTrackNodes and returns updated list.     // if none exist then we are \"creating a new PeerTrackNode with the received Track and Peer\"     setPeerTrackNodes((prevPeerTrackNodes) =>       updateNode(          nodes: prevPeerTrackNodes,         peer: localPeer,         track: localPeer.videoTrack,         createNew: true        )     );     // Turning off loading state on successful Room Room join     setLoading(false);    ;   /       Handles Peer Updates received from  @link HMSUpdateListenerActions.ON_PEER_UPDATE  event listener     @param  Object  data  This has updated peer and update type     @param  HMSPeer  data.peer  Updated Peer     @param  HMSPeerUpdate  data.type  Update Type    /   const onPeerListener = (  peer, type  ) =>       // We will create Tile for the Joined Peer when we receive HMSUpdateListenerActions.ON_TRACK_UPDATE event.     // Note: We are chosing to not create Tiles for Peers which does not have any tracks     if (type === HMSPeerUpdate.PEER_JOINED) return;     if (type === HMSPeerUpdate.PEER_LEFT)         // Remove all Tiles which has peer same as the peer which just left the room.       // removeNodeWithPeerId function removes peerTrackNodes which has given peerID and returns updated list.       setPeerTrackNodes((prevPeerTrackNodes) =>         removeNodeWithPeerId(prevPeerTrackNodes, peer.peerID)       );       return;           if (peer.isLocal)         // Updating the LocalPeer Tile.       // updateNodeWithPeer function updates Peer object in PeerTrackNodes and returns updated list.       // if none exist then we are \"creating a new PeerTrackNode for the updated Peer\".       setPeerTrackNodes((prevPeerTrackNodes) =>         updateNodeWithPeer(  nodes: prevPeerTrackNodes, peer, createNew: true  )       );       return;           if (       type === HMSPeerUpdate.ROLE_CHANGED        type === HMSPeerUpdate.METADATA_CHANGED        type === HMSPeerUpdate.NAME_CHANGED        type === HMSPeerUpdate.NETWORK_QUALITY_UPDATED     )         // Ignoring these update types because we want to keep this implementation simple.       return;          ;   /       Handles Track Updates received from  @link HMSUpdateListenerActions.ON_TRACK_UPDATE  event listener     @param  Object  data  This has updated track with peer and update type     @param  HMSPeer  data.peer  Peer     @param  HMSTrack  data.track  Peer Track     @param  HMSTrackUpdate  data.type  Update Type    /   const onTrackListener = (  peer, track, type  ) =>       // on TRACK_ADDED update     // We will update Tile with the track or     // create new Tile for with the track and peer     if (type === HMSTrackUpdate.TRACK_ADDED && track.type === HMSTrackType.VIDEO)         // We will only update or create Tile \"with updated track\" when track type is Video.       // Tiles without Video Track are already respresenting Peers with or without Audio.       // Updating the Tiles with Track and Peer.       // updateNode function updates \"Track and Peer objects\" in PeerTrackNodes and returns updated list.       // if none exist then we are \"creating a new PeerTrackNode with the received Track and Peer\".       setPeerTrackNodes((prevPeerTrackNodes) =>         updateNode(            nodes: prevPeerTrackNodes,           peer,           track,           createNew: true          )       );       return;           // on TRACK_MUTED or TRACK_UNMUTED updates, We will update Tiles (PeerTrackNodes)     if (type === HMSTrackUpdate.TRACK_MUTED  type === HMSTrackUpdate.TRACK_UNMUTED)         // We will only update Tile \"with updated track\" when track type is Video.       if (track.type === HMSTrackType.VIDEO)           // Updating the Tiles with Track and Peer.         // updateNode function updates \"Track and Peer objects\" in PeerTrackNodes and returns updated list.         // Note: We are not creating new PeerTrackNode object.         setPeerTrackNodes((prevPeerTrackNodes) =>           updateNode(              nodes: prevPeerTrackNodes,             peer,             track            )         );         else           // Updating the Tiles with Peer.         // updateNodeWithPeer function updates Peer object in PeerTrackNodes and returns updated list.         // Note: We are not creating new PeerTrackNode object.         setPeerTrackNodes((prevPeerTrackNodes) =>           updateNodeWithPeer(              nodes: prevPeerTrackNodes,             peer            )         );               return;           if (type === HMSTrackUpdate.TRACK_REMOVED)         // If non-regular track, or       // both regular video and audio tracks are removed       // Then we will remove Tiles (PeerTrackNodes) with removed track and received peer       return;           /         For more info about Degrade/Restore. check out  @link https://www.100ms.live/docs/react-native/v2/features/auto-video-degrade  Auto Video Degrade       /     if (type === HMSTrackUpdate.TRACK_RESTORED  type === HMSTrackUpdate.TRACK_DEGRADED)         return;          ;   /       Handles Errors received from  @link HMSUpdateListenerActions.ON_ERROR  event listener     @param  HMSException  error         For more info, Check out  @link https://www.100ms.live/docs/react-native/v2/features/error-handling  Error Handling     /   const onErrorListener = (error) =>       setLoading(false);     console.log( $ error?.code  $ error?.description  );    ;   // Effect to handle HMSSDK initialization and Listeners Setup   useEffect(() =>       const joinRoom = async () =>         setLoading(true);       /           creating  @link HMSSDK  instance to join room         For more info, Check out  @link https://www.100ms.live/docs/react-native/v2/features/join join-a-room  Join a Room         /       const hmsInstance = await HMSSDK.build();       // Saving hmsInstance in ref       hmsInstanceRef.current = hmsInstance;       /           Adding HMSSDK Event Listeners before calling Join method on HMSSDK instance         For more info, Check out           @link https://www.100ms.live/docs/react-native/v2/features/join update-listener  Adding Event Listeners before Join ,          @link https://www.100ms.live/docs/react-native/v2/features/event-listeners  Event Listeners ,          @link https://www.100ms.live/docs/react-native/v2/features/event-listeners-enums  Event Listeners Enums         /       hmsInstance.addEventListener(HMSUpdateListenerActions.ON_JOIN, onJoinSuccess);       hmsInstance.addEventListener(HMSUpdateListenerActions.ON_PEER_UPDATE, onPeerListener);       hmsInstance.addEventListener(HMSUpdateListenerActions.ON_TRACK_UPDATE, onTrackListener);       hmsInstance.addEventListener(HMSUpdateListenerActions.ON_ERROR, onErrorListener);       /           Joining Room. For more info, Check out  @link https://www.100ms.live/docs/react-native/v2/features/join join-a-room  Join a Room                  Please make sure you are passing correct AUTH_TOKEN to join method        /       hmsInstance.join(new HMSConfig(  authToken: AUTH_TOKEN, username: USERNAME  ));      ;     joinRoom();     // When effect unmounts for any reason, We are calling leave function     return () =>         handleRoomLeave();      ;    ,   );   return   loading, leaveRoom: handleRoomLeave, peerTrackNodes, hmsInstanceRef  ;  ; // region Utilities /     Function to check permissions   @param  string    permissions   @returns  boolean  all permissions granted or not  / export const checkPermissions = async (permissions) =>     try       if (Platform.OS === 'ios')         return true;           const requiredPermissions = permissions.filter(       (permission) => permission.toString()  == PERMISSIONS.ANDROID.BLUETOOTH_CONNECT     );     const results = await requestMultiple(requiredPermissions);     let allPermissionsGranted = true;     for (let permission in requiredPermissions)         if ( (results requiredPermissions permission   === RESULTS.GRANTED))           allPermissionsGranted = false;               console.log(         $ requiredPermissions permission   : $ results requiredPermissions permission          );           // Bluetooth Connect Permission handling     if (       permissions.findIndex(         (permission) => permission.toString() === PERMISSIONS.ANDROID.BLUETOOTH_CONNECT       ) >= 0     )         const bleConnectResult = await request(PERMISSIONS.ANDROID.BLUETOOTH_CONNECT);       console.log( $ PERMISSIONS.ANDROID.BLUETOOTH_CONNECT  : $ bleConnectResult  );           return allPermissionsGranted;     catch (error)       console.log(error);     return false;      ; /     returns uniqueId for a given peer and track combination  / export const getPeerTrackNodeId = (peer, track) =>     return peer.peerID + (track?.source ?? HMSTrackSource.REGULAR);  ; /     creates PeerTrackNode object for given peer and track combination  / export const createPeerTrackNode = (peer, track) =>     let isVideoTrack = false;   if (track && track?.type === HMSTrackType.VIDEO)       isVideoTrack = true;       const videoTrack = isVideoTrack ? track : undefined;   return       id: getPeerTrackNodeId(peer, track),     peer: peer,     track: videoTrack    ;  ; /     Removes all nodes which has peer with id same as the given peerID .  / export const removeNodeWithPeerId = (nodes, peerID) =>     return nodes.filter((node) => node.peer.peerID  == peerID);  ; /     Updates peer of PeerTrackNode objects which has peer with peerID same as the given peerID .     If createNew is passed as true and no PeerTrackNode exists with id same as uniqueId generated from given peer and track    then new PeerTrackNode object will be created.  / export const updateNodeWithPeer = (data) =>     const   nodes, peer, createNew = false   = data;   const peerExists = nodes.some((node) => node.peer.peerID === peer.peerID);   if (peerExists)       return nodes.map((node) =>         if (node.peer.peerID === peer.peerID)           return   ...node, peer  ;               return node;      );       if ( createNew) return nodes;   if (peer.isLocal)       return  createPeerTrackNode(peer), ...nodes ;       return  ...nodes, createPeerTrackNode(peer) ;  ; /     Removes all nodes which has id same as uniqueId generated from given peer and track .  / export const removeNode = (nodes, peer, track) =>     const uniqueId = getPeerTrackNodeId(peer, track);   return nodes.filter((node) => node.id  == uniqueId);  ; /     Updates track and peer of PeerTrackNode objects which has id same as uniqueId generated from given peer and track .     If createNew is passed as true and no PeerTrackNode exists with id same as uniqueId generated from given peer and track    then new PeerTrackNode object will be created  / export const updateNode = (data) =>     const   nodes, peer, track, createNew = false   = data;   const uniqueId = getPeerTrackNodeId(peer, track);   const nodeExists = nodes.some((node) => node.id === uniqueId);   if (nodeExists)       return nodes.map((node) =>         if (node.id === uniqueId)           return   ...node, peer, track  ;               return node;      );       if ( createNew) return nodes;   if (peer.isLocal)       return  createPeerTrackNode(peer, track), ...nodes ;       return  ...nodes, createPeerTrackNode(peer, track) ;  ; // endregion Utility     Fetch token to Join the Room To test audio/video functionality, you need to connect to a 100ms Room. Please check the following steps for the same  1. Navigate to your  100ms dashboard (https://dashboard.100ms.live/dashboard) or  create an account (https://dashboard.100ms.live/register) if you don't have one. 2. Use the Video Conferencing Starter Kit to create a room with a default template assigned to it to test this app quickly. 3. Go to the  Rooms page (https://dashboard.100ms.live/rooms) in your dashboard, click on the Room Id of the room you created above, and click on the Join Room button on the top right. 4. You will see 100ms demo URLs for the roles created when you deployed the starter kit; you can click on the 'key' icon to copy the token and update the AUTH_TOKEN variable in \"App.js\" file.   > Token from 100ms dashboard is for testing purposes only, For production applications you must generate tokens on your own server. Refer to the  Management Token section (../foundation/security-and-tokens management-token) in Authentication and Tokens guide for more information. <video loop=\"true\" autoplay=\"autoplay\" controls=\"controls\" id=\"vid\" muted>   <source src=\"/docs/guides/update-token-react-native.mov\" type=\"video/mp4\"   </video> <br     Test the app Follow the instructions in one of the tabs below based on the target platform you wish to test the app.  s id=\"run-sample-app\" items=  'iOS', 'Android'       id=\"run-sample-app-0\">    Native file changes 1. Allow camera, recording audio and internet permissions Add the below snippet in the info.plist file  ​   xml section=ForIOSPermissions sectionIndex=1 <key>NSCameraUsageDescription</key> <string>Please allow access to Camera to enable video conferencing</string> <key>NSMicrophoneUsageDescription</key> <string>Please allow access to Microphone to enable video conferencing</string> <key>NSLocalNetworkUsageDescription</key> <string>Please allow access to network usage to enable video conferencing</string>   Add the below snippet in the ios/Podfile file  ​   json 3-6  target 'HMSSampleApp' do ...  permissions_path = '../node_modules/react-native-permissions/ios'  pod 'Permission-Camera', :path => \"  permissions_path /Camera\"  pod 'Permission-Microphone', :path => \"  permissions_path /Microphone\" ... end   2. Change ios target platform version to '13.0' in the ios/Podfile file  ​   json 4  require_relative '../node_modules/react-native/scripts/react_native_pods' require_relative '../node_modules/@react-native-community/cli-platform-ios/native_modules' platform :ios, '13.0' install  'cocoapods', :deterministic_uuids => false      Build and run the app Once you've made the above changes, your app is ready for testing. You can build the app and run it in a simulator. 1. Install pods  ​   bash section=RunApp cd ios && pod install && cd ../   2. Build the App  ​   bash section=InstallPods npx react-native run-ios simulator=\"iPhone 14\"   3. Start Metro Bundler if it is not already started  ​   bash npx react-native start   > If you see any permission related error, then check out react-native-permissions library  setup guide (https://github.com/zoontek/react-native-permissions setup) Now, after you click join , you should be able to see yourself (iOS simulator doesn't support actual video, you can connect an actual device to see your video in real-time). You can join the room using a browser as the second peer to check audio/video transactions between two or more peers. </Tab>   id=\"run-sample-app-1\">    Native file changes 1. Allow camera, recording audio and internet permissions by adding the below snippet to the AndroidManifest.xml file (at the application tag level).  ​   xml section=androidPermissions <uses-feature android:name=\"android.hardware.camera\"  <uses-feature android:name=\"android.hardware.camera.autofocus\"  <uses-permission android:name=\"android.permission.CAMERA\"  <uses-permission android:name=\"android.permission.CHANGE_NETWORK_STATE\"  <uses-permission android:name=\"android.permission.MODIFY_AUDIO_SETTINGS\"  <uses-permission android:name=\"android.permission.RECORD_AUDIO\"  <uses-permission android:name=\"android.permission.INTERNET\"  <uses-permission android:name=\"android.permission.ACCESS_NETWORK_STATE\"  <uses-permission android:name=\"android.permission.FOREGROUND_SERVICE\"   <uses-permission android:name=\"android.permission.BLUETOOTH\" android:maxSdkVersion=\"30\"   <uses-permission android:name=\"android.permission.BLUETOOTH_CONNECT\"     2. Change minSdkVersion to 21 in the android/build.gradle file  ​   json 4  buildscript    ext     ...   minSdkVersion = 21   ...           Build and run the app Once you've made the above changes, your app is ready for testing. You can build the app and run it in an emulator or an actual android device. 1. Build the App  ​   bash section=RunApp sectionIndex=1 npx react-native run-android   2. Start Metro Bundler if it is not already started  ​   bash section=RunApp sectionIndex=2 npx react-native start   Now, after you click join , you should be able to see yourself (android emulator doesn't support actual video, you can connect an actual device to see your video in real-time). You can join the room using a browser as the second peer to check audio/video transactions between two or more peers. </Tab> </div> <div style=   marginBottom: 30   >   <video loop=\"true\" autoplay=\"autoplay\" controls=\"controls\" id=\"vid\" muted>     <source src=\"/docs/guides/join-room-react-native.mov\" type=\"video/mp4\"     </video> </div>   Building step-by-step In this section, We'll walk through what the above code does. Below is the diagram which shows the common SDK implementation lifecycle. We start with requesting the Camera & Microphone permissions from user and end with leaving the 100ms Room.   HMSSDK Integration Lifecycle (/guides/rn-hmssdk-integration-lifecycle.png) Now we will look at code and explaination for each step in implementation lifecycle. <div className=\"steps-container\">   Handle device permissions We need permission from the user to access the media from the user's device. We must urge the user to grant permission to access camera, microphone, and bluetooth devices. We use the  react-native-permissions (https://www.npmjs.com/package/react-native-permissions) library that provides a cross-platform (iOS, Android) API to request permissions and check their status. Please ensure to add permissions declarations in the AndroidManifest.xml file for Android and info.plist file for iOS. Check  test the app section ( test-the-app) for more information.   js section=HandleDevicePermissions sectionIndex=1 import  PERMISSIONS, requestMultiple, RESULTS  from 'react-native-permissions'; export const checkPermissions = async (permissions) =>    try     if (Platform.OS === 'ios')      return true;       const requiredPermissions = permissions.filter(permission => permission.toString()  == PERMISSIONS.ANDROID.BLUETOOTH_CONNECT);   const results = await requestMultiple(requiredPermissions);   let allPermissionsGranted = true;   for (let permission in requiredPermissions)      if ( (results requiredPermissions permission   === RESULTS.GRANTED))       allPermissionsGranted = false;         console.log( $ requiredPermissions permission   : $ results requiredPermissions permission    );       // Bluetooth Connect Permission handling   if (    permissions.findIndex(permission => permission.toString() === PERMISSIONS.ANDROID.BLUETOOTH_CONNECT) >= 0   )      const bleConnectResult = await request(PERMISSIONS.ANDROID.BLUETOOTH_CONNECT);    console.log( $ PERMISSIONS.ANDROID.BLUETOOTH_CONNECT  : $ bleConnectResult  );       return allPermissionsGranted;    catch (error)     console.log(error);   return false;     ; ... // Function to handle \"Join Room\" button press const handleJoinPress = async () =>    // Checking Device Permissions  const permissionsGranted = await checkPermissions(    PERMISSIONS.ANDROID.CAMERA,   PERMISSIONS.ANDROID.RECORD_AUDIO,   PERMISSIONS.ANDROID.BLUETOOTH_CONNECT,   );  if (permissionsGranted)     navigate('RoomScreen');    else     console.log('Permission Not Granted ');     ;   In the above code snippet, first, we have checkPermissions function which uses requestMultiple function from react-native-permissions library to request permissions from device. Then we have handleJoinPress function which calls checkPermissions function to request Camera, Microphone and Bluetooth permissions on \"Join Room\" button press.   Implement Join screen This section will help you create the join screen user interface. To keep it simple for the quickstart, we have not created many UI elements. You can refer to the  sample app implementation (https://github.com/100mslive/react-native-hms/blob/develop/sample-apps/rn-quickstart-app/src/screens/Welcome/index.tsx) for a complete Preview/Join user interface.   js section=ImplementJoinScreen sectionIndex=1 const HomeScreen = (  navigate  ) =>    // Function to handle \"Join Room\" button press  const handleJoinPress = async () =>     ...   ;  return (   <View style=   flex: 1, alignItems: 'center', justifyContent: 'center'   >    <TouchableHighlight     onPress= handleJoinPress      underlayColor=' 143466'     style=   paddingHorizontal: 20, paddingVertical: 12, backgroundColor: ' 2471ED', borderRadius: 8       >     <Text style=   fontSize: 20, color: ' ffffff'   >Join Room</Text>    </TouchableHighlight>   </View>  );  ;   In the above code snippet, We are implementing UI for \"Join Room\" button in Home screen.   Initializing the SDK 100ms SDK provides various events which the client apps can subscribe to. These events notify about updates happening in the room after a user has joined. After requesting required Audio & Video permissions from the user, we are ready to initialize the 100ms SDK to subscribe to updates.   js section=InitializingSdk sectionIndex=1 // Effect to handle HMSSDK initialization and Listeners Setup useEffect(() =>    const joinRoom = async () =>     setLoading(true);   /        creating  @link HMSSDK  instance to join room      For more info, Check out  @link https://www.100ms.live/docs/react-native/v2/features/join join-a-room  Join a Room      /   const hmsInstance = await HMSSDK.build();   // Saving hmsInstance in ref   hmsInstanceRef.current = hmsInstance;   ...   ;  joinRoom();  // When effect unmounts for any reason, we are calling leave function to exit the 100ms Room  return () =>     handleRoomLeave();   ;  ,   );   In above code snippet, we have a useEffect hook which initializes the HMSSDK by calling static build method.   Listen to Join and Peer Updates The 100ms SDK emits:   HMSUpdateListenerActions.ON_JOIN event when the user has joined the Room successfully, and   HMSUpdateListenerActions.ON_PEER_UPDATE event when any change happens for any Peer in the Room. Our application must subscribe to these events to get the updates. Check out the  Event Listeners (../features/event-listeners) docs to know more about events emitted by the SDK.   js section=JoinAndPeerUpdates sectionIndex=1 // Subscribing to Join updates hmsInstance.addEventListener(HMSUpdateListenerActions.ON_JOIN, onJoinSuccess); // Subscribing to Peer updates hmsInstance.addEventListener(HMSUpdateListenerActions.ON_PEER_UPDATE, onPeerListener); const onJoinSuccess = (data) =>     /       Checkout  @link HMSLocalPeer  HMSLocalPeer  Class    /   const   localPeer   = data.room;   // Creating or Updating Local Peer Tile   // updateNode function updates \"Track and Peer objects\" in PeerTrackNodes and returns updated list.   // if none exist then we are \"creating a new PeerTrackNode with the received Track and Peer\"   setPeerTrackNodes((prevPeerTrackNodes) =>     updateNode(        nodes: prevPeerTrackNodes,       peer: localPeer,       track: localPeer.videoTrack,       createNew: true      )   );   // Turning off loading state on successful Room Room join   setLoading(false);  ; const onPeerListener = (  peer, type  ) =>     // We will create Tile for the Joined Peer when we receive HMSUpdateListenerActions.ON_TRACK_UPDATE event.   // Note: We are chosing to not create Tiles for Peers which does not have any tracks   if (type === HMSPeerUpdate.PEER_JOINED) return;   if (type === HMSPeerUpdate.PEER_LEFT)       // Remove all Tiles which has peer same as the peer which just left the room.     // removeNodeWithPeerId function removes peerTrackNodes which has given peerID and returns updated list.     setPeerTrackNodes((prevPeerTrackNodes) =>       removeNodeWithPeerId(prevPeerTrackNodes, peer.peerID)     );     return;       if (peer.isLocal)       // Updating the LocalPeer Tile.     // updateNodeWithPeer function updates Peer object in PeerTrackNodes and returns updated list.     // if none exist then we are \"creating a new PeerTrackNode for the updated Peer\".     setPeerTrackNodes((prevPeerTrackNodes) =>       updateNodeWithPeer(  nodes: prevPeerTrackNodes, peer, createNew: true  )     );     return;       if (     type === HMSPeerUpdate.ROLE_CHANGED      type === HMSPeerUpdate.METADATA_CHANGED      type === HMSPeerUpdate.NAME_CHANGED      type === HMSPeerUpdate.NETWORK_QUALITY_UPDATED   )       // Ignoring these update types because we want to keep this implementation simple.     return;      ;     Listen to Track Updates The 100ms SDK emits HMSUpdateListenerActions.ON_TRACK_UPDATE event when any change happens for any Track in the Room. Our application must subscribe to this event to get the track updates. Check out the  Event Listeners (../features/event-listeners) docs to know more about events emitted by the SDK.   js section=TrackUpdates sectionIndex=1 // Subscribing to Track updates hmsInstance.addEventListener(HMSUpdateListenerActions.ON_TRACK_UPDATE, onTrackListener); const onTrackListener = (  peer, track, type  ) =>     // on TRACK_ADDED update   // We will update Tile with the track or   // create new Tile for with the track and peer   if (type === HMSTrackUpdate.TRACK_ADDED && track.type === HMSTrackType.VIDEO)       // We will only update or create Tile \"with updated track\" when track type is Video.     // Tiles without Video Track are already respresenting Peers with or without Audio.     // Updating the Tiles with Track and Peer.     // updateNode function updates \"Track and Peer objects\" in PeerTrackNodes and returns updated list.     // if none exist then we are \"creating a new PeerTrackNode with the received Track and Peer\".     setPeerTrackNodes((prevPeerTrackNodes) =>       updateNode(          nodes: prevPeerTrackNodes,         peer,         track,         createNew: true        )     );     return;       // on TRACK_MUTED or TRACK_UNMUTED updates, We will update Tiles (PeerTrackNodes)   if (type === HMSTrackUpdate.TRACK_MUTED  type === HMSTrackUpdate.TRACK_UNMUTED)       // We will only update Tile \"with updated track\" when track type is Video.     if (track.type === HMSTrackType.VIDEO)         // Updating the Tiles with Track and Peer.       // updateNode function updates \"Track and Peer objects\" in PeerTrackNodes and returns updated list.       // Note: We are not creating new PeerTrackNode object.       setPeerTrackNodes((prevPeerTrackNodes) =>         updateNode(            nodes: prevPeerTrackNodes,           peer,           track          )       );       else         // Updating the Tiles with Peer.       // updateNodeWithPeer function updates Peer object in PeerTrackNodes and returns updated list.       // Note: We are not creating new PeerTrackNode object.       setPeerTrackNodes((prevPeerTrackNodes) =>         updateNodeWithPeer(            nodes: prevPeerTrackNodes,           peer          )       );           return;       if (type === HMSTrackUpdate.TRACK_REMOVED)       // If non-regular track, or     // both regular video and audio tracks are removed     // Then we will remove Tiles (PeerTrackNodes) with removed track and received peer     return;       /       For more info about Degrade/Restore. check out  @link https://www.100ms.live/docs/react-native/v2/features/auto-video-degrade  Auto Video Degrade     /   if (type === HMSTrackUpdate.TRACK_RESTORED  type === HMSTrackUpdate.TRACK_DEGRADED)       return;      ;     Listen to Other Updates The 100ms SDK emits various other events to handle different scenarios in the app. For example, you can use HMSUpdateListenerActions.ON_ERROR event to get errors from HMSSDK . Check out the  Event Listeners (../features/event-listeners) docs to know more about events emitted by the SDK.   js section=OtherUpdates sectionIndex=1 // Subscribing to Error updates hmsInstance.addEventListener(HMSUpdateListenerActions.ON_ERROR, onErrorListener); const onErrorListener = (error) =>     setLoading(false);   console.log( $ error?.code  $ error?.description  );  ;     Join Room After adding all required event listeners, you need to create an HMSConfig instance and use that instance to call the join method of HMSSDK instance to join a Room. >   Note:   An App token is required to authenticate a Room join request from your client-side app. Please ensure to set the AUTH_TOKEN variable value with correct \"Auth Token\" by fetching it from your dashboard. Check  fetch token to join a room section ( fetch-token-to-join-the-room) for more information. <br  Read more about authentication and tokens in  this guide (../foundation/security-and-tokens). You can learn more about joining the Room from  Join Room (../features/join) docs.   js section=JoinRoom sectionIndex=1 // Effect to handle HMSSDK initialization and Listeners Setup useEffect(() =>    const joinRoom = async () =>     setLoading(true);   /       creating  @link HMSSDK  instance to join room     For more info, Check out  @link https://www.100ms.live/docs/react-native/v2/features/join join-a-room  Join a Room     /   const hmsInstance = await HMSSDK.build();   ...   /       Joining Room. For more info, Check out  @link https://www.100ms.live/docs/react-native/v2/features/join join-a-room  Join a Room          Please make sure you are passing correct AUTH_TOKEN to join method    /   hmsInstance.join(new HMSConfig(  authToken: AUTH_TOKEN, username: USERNAME  ));   ;  joinRoom();  // When effect unmounts for any reason, We are calling leave function  return () =>     handleRoomLeave();   ;  ,   );   In above code snippet, we have the same useEffect hook which initializes the HMSSDK . After initialization and adding listeners it creates an HMSConfig object with AUTH_TOKEN and USERNAME and passes the created config object to join method of HMSSDK instance.   Render Video in a Tile We are taking use of PeerTrackNode class to render video and avatar of peers. Let's take a look at interface for PeerTrackNode class    js section=RenderVideoTile sectionIndex=1 interface PeerTrackNode     id: string; // Unique ID for each peer and track combination   peer: HMSPeer; // HMSPeer object of peer   track?: HMSTrack; // HMSTrack object of video track of the peer     To display a video track, you can simply get the trackId of the Video Tracks in PeerTrackNode object and pass it to HmsView component. If track in PeerTrackNode object is undefined or null then you can render avatar or name initials of the peer from the peer.name property in PeerTrackNode . Check the  Render Video (../features/render-video) guide for more information.   js section=RenderVideoTile sectionIndex=2 // _renderItem function returns a Tile UI for each item which is PeerTrackNode object const _renderItem = (  item  ) =>     const   peer, track   = item;   return (     <View       style=           height: 300,         margin: 8,         borderRadius: 20,         overflow: 'hidden',         backgroundColor: ' A0C3D2'         >        /  Checking if we have \"HmsView\" component, valid trackId and \"track is not muted\"  /         HmsView && track && track.trackId &&  track.isMute() ? (         // To Render Peer Live Videos, We can use HMSView         // For more info about its props and usage, Check out  @link https://www.100ms.live/docs/react-native/v2/features/render-video  Render Video          <HmsView           trackId= track.trackId            mirror= peer.isLocal            style=   width: '100%', height: '100%'                    ) : (         // Render Peer name Initials         <View style=   flex: 1, alignItems: 'center', justifyContent: 'center'   >           <View             style=                 width: 100,               height: 100,               borderRadius: 50,               alignItems: 'center',               justifyContent: 'center',               backgroundColor: ' FD8A8A'               >             <Text               style=                   textAlign: 'center',                 fontSize: 28,                 fontWeight: 'bold',                 textTransform: 'uppercase'                 >                peer.name                 .split(' ')                 .map((item) => item 0 )                 .join('')              </Text>           </View>         </View>       )      </View>   );  ;   In the above code snippet, we have _renderItem function which returns Tile UI. Inside Tile UI, we are rendering HmsView if we have valid track otherwise an Avatar with Peer's Name Initials.   Render Video Tiles for Peers You can create a   list of PeerTrackNode objects   (mentioned in  Render Video in a Tile ( render-video-in-a-tile) step) for each Peer and Track combination and then use this list to render Tiles for all the PeerTrackNodes objects. We recommend using  FlatList (https://reactnative.dev/docs/flatlist) component to render multiple Tiles. This ensures uniques Tiles are created for each trackId .   js section=RenderMultipleTiles sectionIndex=1 const  peerTrackNodes, setPeerTrackNodes  = useState(  ); // Use this state to render Peer Tiles ... const _keyExtractor = (item) => item.id; // _renderItem function returns a Tile UI for each item which is PeerTrackNode object const _renderItem = (  item  ) =>   ...  ; ... // Rendering list of Peers <FlatList  centerContent= true   data= peerTrackNodes   showsVerticalScrollIndicator= false   keyExtractor= _keyExtractor   renderItem= _renderItem   contentContainerStyle=     paddingBottom: 120,   flexGrow: Platform.OS === 'android' ? 1 : undefined,   justifyContent: Platform.OS === 'android' ? 'center' : undefined,         In the above code snippet, we are rendering FlatList for a list of PeerTrackNode objects.   Leaving the Room You can leave the meeting once you are done with it. To leave the meeting you can call the leave method on HMSSDK instance. You can learn more about leave the room in  Leave Room (../features/leave) docs.   js section=LeaveRoom sectionIndex=1 const handleRoomLeave = async () =>    try     const hmsInstance = hmsInstanceRef.current;   if ( hmsInstance)      return Promise.reject('HMSSDK instance is null');       // Removing all registered listeners   hmsInstance.removeAllListeners();   /       Leave Room. For more info, Check out  @link https://www.100ms.live/docs/react-native/v2/features/leave  Leave Room     /   const leaveResult = await hmsInstance.leave();   console.log('Leave Success: ', leaveResult);   /       Free/Release Resources. For more info, Check out  @link https://www.100ms.live/docs/react-native/v2/features/release-resources  Release Resources     /   const destroyResult = await hmsInstance.destroy();   console.log('Destroy Success: ', destroyResult);   // Removing HMSSDK instance   hmsInstanceRef.current = null;    catch (error)     console.log('Leave or Destroy Error: ', error);     ; ... const handleRoomEnd = () =>    leaveRoom();  navigate('HomeScreen');  ; ... <TouchableHighlight  onPress= handleRoomEnd   underlayColor=' 6e2028'  style=     position: 'absolute', bottom: 40, alignSelf: 'center',   backgroundColor: ' CC525F',   width: 60, height: 60, borderRadius: 30,   alignItems: 'center', justifyContent: 'center',     >  <Text style=   textAlign: 'center', color: ' ffffff', fontWeight: 'bold'   >Leave Room</Text> </TouchableHighlight>   In the above code snippet, first we have handleRoomLeave function which calls leave and destroy methods. Then we have implementation of \"Leave Room\" button UI in Room meeting screen which eventually calls handleRoomLeave function.   Test App You can refer to the  Test the App section ( test-the-app) to test your app on Android or iOS Device or Simulators. </div>   Next Steps We have multiple example apps to get you started with 100ms React Native SDK    Basic Example For a Basic Sample Reference, check out the  React Native Quickstart Sample App (https://github.com/100mslive/react-native-hms/tree/develop/sample-apps/rn-quickstart-app) on GitHub.   Full-featured Example You can also check out the fully featured Example App implementation in the  100ms React Native SDK GitHub repository (https://github.com/100mslive/react-native-hms/tree/main/example) showcasing multiple features provided by 100ms.   Check Deployed Sample Apps You can download and check out the 100ms React Native deployed apps  🤖 Download the Sample Android App  here (https://appdistribution.firebase.dev/i/7b7ab3b30e627c35) 📲 Download the Sample iOS App  here (https://testflight.apple.com/join/v4bSIPad) "
    },
    {
        "title": "Auth Token Endpoint Guide",
        "link": "/react-native/v2/guides/token-endpoint",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/token-endpoint",
        "keywords": [],
        "content": "    Overview 100ms provides an option to get App Tokens without setting up a token generation backend service to simplify your integration journey while testing the  sample app (https://github.com/100mslive/100ms-web) or building integration with 100ms. You can find the token endpoint from the  developer page (https://dashboard.100ms.live/developer) in your 100ms dashboard.   Token endpoint (/guides/token-endpoint-dashboard.png) We recommend you move to your token generation service before you transition your app to production, as our token endpoint service will not scale in production. The \"Sample Apps\" built using 100ms client SDKs require an App Token to join a room to initiate a video conferencing or live streaming session. Please check the  Authentication and Tokens guide (./../foundation/security-and-tokens) Please note that you cannot use the token endpoint to create a Management Token for server APIs. Refer to the  Management Token section (./../foundation/security-and-tokens management-token) in Authentication and Tokens guide for more information.   Get an app token using token endpoint You can use the token endpoint from your 100ms dashboard while building integration with 100ms. This acts as a tool enabling front-end developers to complete the integration without depending on the backend developers to set up a token generation backend service.   URL format:   <YOUR_TOKEN_ENDPOINT>api/token  100ms token endpoint can generate an app token with the inputs passed, such as room_id, role, & user_id (optional  your internal user identifier as the peer's user_id). You can use  jwt.io (https://jwt.io/) to validate whether the app token contains the same input values. <PostRequest title=\"https://prod-in2.100ms.live/hmsapi/johndoe.app.100ms.live/api/token\"   <Request id=\"req-comp-0\">   bash curl location request POST 'https://prod-in2.100ms.live/hmsapi/johndoe.app.100ms.live/api/token'  header 'Content-Type: application/json'  data-raw '    \"room_id\":\"633fcdd84208780bf665346a\",   \"role\":\"host\",   \"user_id\":\"1234\"  '   </Request> <ResponseBox id=\"resp-0\" status=\"200 OK\">   json     \"token\": \"eyJ0eXAiOiJKV1QiLCJhbGciOi                           R3tT-Yk\",   \"msg\": \"token generated successfully\",   \"status\": 200,   \"success\": true,   \"api_version\": \"2.0.192\"     </ResponseBox>   Example client-side implementation You can directly add this to your client-side implementation, check our  sample app (https://github.com/100mslive/react-native-hms/blob/7bd6420ea49d520acd881de9ac5d76b36498bb67/example/src/services/index.ts L3) for reference.   Disable 100ms token endpoint Due to some security concerns, if you don't wish to use the token endpoint to generate app tokens, then you can disable it on the  Developers page (https://dashboard.100ms.live/developer) on your dashboard by disabling the option \"Disable &lt;room_id&gt;/&lt;role&gt; link format.\"   Disable Token endpoint (/guides/disable-token-endpoint.png)    Error Response Once you're disabled it on the dashboard, the requests to create an app token using the 100ms token endpoint will throw the below error:   json     \"success\": false,   \"msg\": \"Generating token using the room_id and role is disabled.\",   \"api_version\": \"2.0.192\"     "
    },
    {
        "title": "Auth Token Quickstart Guide",
        "link": "/react-native/v2/guides/token",
        "platformName": "React Native",
        "objectID": "/react-native/v2/guides/token",
        "keywords": [],
        "content": "    Create a 100ms account  Create an account at  Dashboard of 100ms (https://dashboard.100ms.live/)   Create Account (/docs/guides/token/create-account.png)  After you have created your account you have to Confirm your Email , check the promotions tab in your Mail Box if you can't find it.  Login to the Dashboard and you will see an option to Create your first app. Click on it.   Create your first app (/docs/guides/token/starter-kit-initialize-first-step.png)  Then you would see this popup with multiple starter kits, hover over one of the starter kits and click Deploy . We will choose \"Video Conferencing\" for now.   Initialize Started Kit (/docs/guides/token/starter-kit-initialize.png)  In the Choose your deployment step, select 100ms and enter the subdomain you wish in the Subdomain field.   Domain details (/docs/guides/token/domain-details.png)  After you're App is set click on \"Go to Dashboard\" or  Go Here (https://dashboard.100ms.live/dashboard)   Dasboard (/docs/guides/token/go-to-dashboard.png)   Create a room  Go over to  Room in Dashboard (https://dashboard.100ms.live/rooms) and click on \"Create Room\" , While creating a room you can specify it's name, roles or enable recording.   Create Room (/docs/guides/token/create-room.png)  You will now see \"Room Details\" section and we have a room_id created, copy it somewhere.   Room Id (/docs/guides/token/room-id.png)   Get a temporary token from 100ms dashboard Any client connecting calling 100ms' service needs to authenticate using an auth token. In production you would have your own servers generating the tokens (see more  here (/docs/javascript/v2/foundation/security-and-tokens)), but for a quick start you can use the dashboard to create a token for you. The token will expire in 24 hours and should not be hard-coded into a production app.  To get a temporary token click on \"Join room\" button.   Join Room (/docs/guides/token/join-room.png)  In the popup that shows up click on icon with a key shape next to the role you want to join as.   Copy Token (/docs/guides/token/copy-token.png) The token will be copied to your clipboard. Use this along with the room_id to proceed with the quickstart guide. "
    },
    {
        "title": "End an active room",
        "link": "/server-side/v2/active-rooms/end-active-room",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/active-rooms/end-active-room",
        "keywords": [],
        "content": "  Trigger this request to end an active room. <PostRequest title=\"https://api.100ms.live/v2/active-rooms/<room_id>/end-room\"   <Request id=\"req-0\">   bash curl location request POST 'https://api.100ms.live/v2/active-rooms/<room_id>/end-room'  header 'Authorization: Bearer <management_token>'  header 'Content-Type: application/json'  data-raw '    \"reason\": \"Class has ended\",   \"lock\": false  '   </Request> <ResponseBox id=\"res-0\" status=\"200 OK\">   json     \"message\": \"session is ending\"     </ResponseBox>   Arguments  Name             Type    Description                                                           Required   :        :    :                                 :     room_id <br  — path param  string   Unique identifier of the room you wish to disable. <br <br   Example  : 627cda81ab4f3b56a077dc33                Yes     reason <br  — body param   string   Description to indicate the reason for ending the active room.                                  No      lock <br  — body param    boolean  Status of the room to be set. <br  Allowed values : true, false. <br  Default : false (ends the current active session only)  No     >   Warning:   If you set the lock argument to true , it will end the active room and users will not be able to join the room later as well. You can use  enable a room API (/server-side/v2/Rooms/disable-or-enable) or  dashboard (https://dashboard.100ms.live/rooms) to enable the room again.   Why would you use this API?   To disconnect all connected peers at a pre-defined timestamp if you wish to add a hard stop for any sessions.   Use this to allow peers to join the room only once daily for a respective time slot.     Trigger this API at the end of the session with the lock argument as true ,     The next day, one minute before the schedule  use the  enable a room API (/server-side/v2/Rooms/disable-or-enable) to enable the room to allow the users to join the session. "
    },
    {
        "title": "Active room object",
        "link": "/server-side/v2/active-rooms/object",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/active-rooms/object",
        "keywords": [],
        "content": "  The active room object enables you to perform actions on the room object. The active room object is the existing room object with additional information like the session object (with an array of peer identifiers). This is not a new object. <EndpointRequest title=\"https://api.100ms.live/v2/active-rooms\"     Object  Argument   Description                                                                           id      Unique identifier for the room                                   name     Alias for the room                                         customer_id  Unique identifier for your account                                 session    Object of type session . This object contains an array of the unique identifier of the peers.    Session object  Argument   Description                                                      id      Unique identifier for the session                    created_at  Timestamp when the session started.                   peers    This object contains an array of the unique identifier of the peers    Postman collection You can use our Postman collection to start exploring 100ms APIs.    Run in Postman (https://run.pstmn.io/button.svg) (https://god.gw.postman.com/run-collection/22726679-47dcd974-29d5-4965-a35b-bf9b74a8b25a?action=collection%2Ffork&collection-url=entityId%3D22726679-47dcd974-29d5-4965-a35b-bf9b74a8b25a%26entityType%3Dcollection%26workspaceId%3Dd9145dd6-337b-4761-81d6-21a30b4147a2) Refer to the  Postman guide (/server-side/v2/introduction/postman-guide) to get started with 100ms API collection. "
    },
    {
        "title": "Overview",
        "link": "/server-side/v2/active-rooms/overview",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/active-rooms/overview",
        "keywords": [],
        "content": "    When does a room become active? A room becomes active when one or more peers join a room to start real-time video/audio interactions in a video conferencing or live streaming session.   Active room (/docs/v2/active-room.png)   Active room APIs Use Active Room APIs to perform server-to-server actions that automate your business logic in 100ms rooms. These actions can be used to control peer behavior and build interactivity scenarios   Control peers by  changing roles (/server-side/v2/active-rooms/update-a-peer),  removing peers (/server-side/v2/active-rooms/remove-peers), and  ending the room (/server-side/v2/active-rooms/end-active-room)   Build interactivity scenarios by  updating peer metadata (/server-side/v2/active-rooms/update-a-peer) and  sending broadcast/direct messages (/server-side/v2/active-rooms/send-message)    Retrieving the details of an active room (/server-side/v2/active-rooms/retrieve-active-room) and  a specific peer (/server-side/v2/active-rooms/retrieve-peer) will act as supporting pillars to implement the above mentioned actions.   When should I use room APIs and Active room APIs?     Room APIs  : You can use room APIs to create a room, link a template, and update room properties such as recording configuration and server region.     Active room APIs  : You can use these APIs to control peer behavior and build interactivity scenarios in an active room.   What can I build? We have drafted some examples to help you understand how active room APIs will enable you to control peer behavior and build interactivity scenarios:    Fetch the list of peers (/server-side/v2/active-rooms/retrieve-active-room) in an active room and perform actions like:     Nudge a specific peer if their  video is turned off (/server-side/v2/active-rooms/retrieve-peer why-would-you-use-this-api).     Send a broadcast message to all peers to inform the speaker's unavailability. For  example (/server-side/v2/active-rooms/send-message why-would-you-use-this-api), send a message to all the students if the teacher cannot join the session at the last minute.      Kick a peer (/server-side/v2/active-rooms/remove-peers why-would-you-use-this-api) in case of any misconduct(abusive chat).   To build real-time interactivity, like  updating a custom avatar (/server-side/v2/active-rooms/update-a-peer why-would-you-use-this-api) in the video tile,  virtual poker handler (/server-side/v2/active-rooms/send-message example2-poker-handler) for a multiplayer game, and more.   Postman collection You can use our Postman collection to start exploring 100ms APIs.    Run in Postman (https://run.pstmn.io/button.svg) (https://god.gw.postman.com/run-collection/22726679-47dcd974-29d5-4965-a35b-bf9b74a8b25a?action=collection%2Ffork&collection-url=entityId%3D22726679-47dcd974-29d5-4965-a35b-bf9b74a8b25a%26entityType%3Dcollection%26workspaceId%3Dd9145dd6-337b-4761-81d6-21a30b4147a2) Refer to the  Postman guide (/server-side/v2/introduction/postman-guide) to get started with 100ms API collection. "
    },
    {
        "title": "Remove peers",
        "link": "/server-side/v2/active-rooms/remove-peers",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/active-rooms/remove-peers",
        "keywords": [],
        "content": "  You can use this API to remove/disconnect a connected peer from an active room. 100ms provides options to disconnect a peer either with their peer_id or role. 1. If peer_id is specified  respective connected peer will be disconnected from the active room. 2. If the role is specified  all the connected peers assigned with that particular role will be disconnected from the active room. 3. If both peer_id and role are specified  preference will be given to the peer_id and the respective peer will be disconnected from the active room. 4. If both peer_id and role are not specified  you must specify peer_id or role for this request. <PostRequest title=\"https://api.100ms.live/v2/active-rooms/<room_id>/remove-peers\"   <Request id=\"req-0\">   bash curl location request POST 'https://api.100ms.live/v2/active-rooms/<room_id>/remove-peers'  header 'Authorization: Bearer <management_token>'  header 'Content-Type: application/json'  data-raw '    \"peer_id\": \"fd9e87d8-5c7b-4750-932f-d8d2ff18b70b\",   \"role\": \"host\",   \"reason\": \"\"  '   </Request> <ResponseBox id=\"res-0\" status=\"200 OK\">   json     \"message\": \"peer remove request submitted\"     </ResponseBox>   Arguments  Name             Type    Description                                                                        Required    :        :    :                                        :     room_id <br  — path param  string  Unique identifier of the room to which the peer is connected. <br <br   Example  : 627cda81ab4f3b56a077dc33                       Yes       peer_id <br  — body param  string  Unique identifier of the connected peer/participant you wish to remove from the active room. <br <br   Example  : fd9e87d8-5c7b-4750-932f-d8d2ff18b70b  Conditional   role <br  — body param    string  You can use this to remove a list of connected peers with the specified role type from the active room.                          Conditional   reason <br  — body param   string  You can specify the message to be passed to other connected peers in the active room.                                   No       >   Note: Conditional Arguments   <br <br  You should define either of these arguments (peer_id or role) in the payload for the request to be successful.   Why would you use this API?   Use this API to remove a peer from an active room for misconduct (e.g., abusive chat). This can be combined with custom logic not to generate app tokens for this user to block them from future joins. "
    },
    {
        "title": "Retrieve a specific active room",
        "link": "/server-side/v2/active-rooms/retrieve-active-room",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/active-rooms/retrieve-active-room",
        "keywords": [],
        "content": "  Retrieves the details of a specific active room. 100ms allows you to retrieve details of an active room with their ID.   You can use this API to check if a room is currently active or not.   You can also use this in conjunction with  retrieve peer API (/server-side/v2/active-rooms/retrieve-peer) and perform actions such as  updating peers (/server-side/v2/active-rooms/update-a-peer),  sending messages (/server-side/v2/active-rooms/send-message), and  removing peers (/server-side/v2/active-rooms/remove-peers) based on your use case requirements. If you need to get the list of all active sessions, you can use the  list sessions API (/server-side/v2/Sessions/list-sessions) with the filter active as true . <GetRequest title=\"https://api.100ms.live/v2/active-rooms/<room_id>\"   <Request id=\"req-comp-0\">   bash curl location request GET 'https://api.100ms.live/v2/active-rooms/<room_id>'  header 'Authorization: Bearer <management_token>'   </Request> <ResponseBox id=\"resp-0\" status=\"200 OK\">   json     \"id\": \"631a0f990e6ffae22efa610c\",   \"name\": \"5d971abb-8e51-4c86-9d35-8e619c3d8877\",   \"customer_id\": \"627cdddff2e4e30487862ad1\",   \"session\":       \"id\": \"633c22b420636a0cb361dea0\",     \"created_at\": \"2022-10-04T12:10:28.939011796Z\",     \"peers\":         \"c0bbae7e-fbc0-48c3-89aa-83f85427acbf\",       \"161d2478-1535-49a0-b48b-326aaa26dcd0\",       \"5e2c13af-dd3d-4a46-8265-f4648a0d68ef\"               </ResponseBox>   Arguments  Name             Type    Description                                                      Required   :        :    :                              :     room_id <br  — path param  string  Unique identifier of the active room you wish to fetch details for. <br <br   Example  : 631a0f990e6ffae22efa610c  Yes    "
    },
    {
        "title": "Retrieve a specific peer",
        "link": "/server-side/v2/active-rooms/retrieve-peer",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/active-rooms/retrieve-peer",
        "keywords": [],
        "content": "  Retrieves the details of a specific peer in an active room. You can use the peer_id to retrieve the details of the peer. Use  retrieve active room state API (/server-side/v2/active-rooms/retrieve-active-room) to get the list of connected peers in an active room. >   Note:   You can leverage this API to retrieve the details of connected peers in an active room only. To retrieve details of the peers that left the session already, use the  Retrieve a specific session API (/server-side/v2/Sessions/retrieve-a-session) <GetRequest title=\"https://api.100ms.live/v2/active-rooms/<room_id>/peers/<peer_id>\"   <Request id=\"req-0\">   bash curl location request GET 'https://api.100ms.live/v2/active-rooms/<room_id>/peers/<peer_id>'  header 'Authorization: Bearer <management_token>'   </Request> <ResponseBox id=\"res-0\" status=\"200 OK\">   json     \"id\": \"2e8fc83e-6aef-4ca8-9124-bedc253f3529\",   \"name\": \"nixon-test2\",   \"user_id\": \"4c661446-f6f2-49c2-a317-16892756a2fa\",   \"metadata\":       \"Lower_hand\": true,     \"business_user_id\": \"XYZ_1234\"    ,   \"role\": \"backstage\",   \"joined_at\": \"2022-10-06T05:22:41.635589462Z\",   \"tracks\":       \"780a28e0-f2df-450b-9954-70307a440530\":         \"id\": \"780a28e0-f2df-450b-9954-70307a440530\",       \"stream_id\": \"2cc4afed-20ba-4793-a223-d4c76c331456\",       \"mute\": false,       \"type\": \"video\",       \"source\": \"regular\",       \"started_at\": \"2022-10-06T05:22:41.752327705Z\",       \"description\": \"\"      ,     \"d6831044-3584-42aa-92be-efd051cae353\":         \"id\": \"d6831044-3584-42aa-92be-efd051cae353\",       \"stream_id\": \"2cc4afed-20ba-4793-a223-d4c76c331456\",       \"mute\": true,       \"type\": \"audio\",       \"source\": \"regular\",       \"started_at\": \"2022-10-06T05:22:41.752327705Z\",       \"description\": \"\"               </ResponseBox>   Arguments  Name             Type    Description                                                              Required   :        :    :                                   :     room_id <br  — path param   string  Unique identifier of the room where the peer is active. <br <br   Example  : 627cda81ab4f3b56a077dc33                Yes     peer_id <br  — path param  string  Unique identifier of the peer/participant you wish to fetch details for. <br <br   Example  : 1038fa72-9345-4651-beab-436f9242ebf4  Yes      Why would you use this API?   To check if a specific peer has enabled/disabled their audio/video/screen-share at a given time in an active room.   To check if a specific peer is on-stage and backstage at a particular time.   You can also use this API to store peer metadata in your database and retrieve it for an abruptly disconnected peer once they join the session again. For example, to enable the user to resume playing the multiplayer game. "
    },
    {
        "title": "Send message",
        "link": "/server-side/v2/active-rooms/send-message",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/active-rooms/send-message",
        "keywords": [],
        "content": "  You can use this API to send messages to a single peer, a set of peers (a particular role), or all connected peers in the active room. This API supports arguments such as peer_id and role to implement this. 1. If peer_id is specified  message will be sent only to the specified connected peer. 2. If the role is specified  message will be sent to all the connected peers assigned with that particular role. 3. If both peer_id and role are specified  preference will be given to the peer_id and the message will be sent only to the specified connected peer. 4. If both peer_id and role are not specified  message(broadcast) will be sent to all the connected peers. <PostRequest title=\"https://api.100ms.live/v2/active-rooms/<room_id>/send-message\"   <Request id=\"req-0\">   bash curl location request POST 'https://api.100ms.live/v2/active-rooms/<room_id>/send-message'  header 'Authorization: Bearer <management_token>'    header 'Content-Type: application/json'  data-raw '    \"peer_id\":\"018b84da-0786-48c5-a3a6-02961d9eac03\",   \"role\":\"\",   \"message\": \"hi\",   \"type\":\"chat\"  '   </Request> <ResponseBox id=\"res-0\" status=\"200 OK\">   json     \"message\": \"message sent\"     </ResponseBox>   Arguments  Name             Type    Description                                                                   Required   :        :    :                                     :     room_id <br  — path param  string  Unique identifier of the room where the peer is active. <br <br   Example  : 627cda81ab4f3b56a077dc33                     Yes     message <br  — body param  string  Actual message to be sent.                                                           Yes     peer_id <br  — body param  string  Unique identifier of the peer/participant for which you wish to send the message. <br <br   Example  : 1038fa72-9345-4651-beab-436f9242ebf4  No      role <br  — body param    string  You can use this to send the message to a set of peers (a particular role).                                   No      type <br  — body param    string  Indicates the type of message sent, for example, chat or emoji.                                         No       Why would you use this API?   Based on your requirements, you can use this API to send a broadcast message, private message, or reactions like emojis.   Example1  Inform unavailability of teacher   Suppose a teacher cannot attend a virtual classroom session at the last minute; you can inform all the connected peers (students) by sending a broadcast message to inform the teacher's unavailability.   Example2  Poker handler   You can build a handler for a multiplayer game like Poker using this API in conjunction with the \"peer.join.success\"  webhook event (/server-side/v2/introduction/webhook peer-join-success). Let's consider below scenario:   Four users join the game and start playing  your app server stores the user_id for each user.   Total bet amount  create a custom event to show the data only to the particular player.     Example payload  :    json         \"peer_id\": \"018b84da-0786-48c5-a3a6-02961d9eac03\", // will send the data only to peer_id specified.     \"message\": \"<TOTAL_BET_AMOUNT>\",     \"type\": \"custom_event\"           Showdown  create a custom event to show the data to all the players.     Example payload  :    json         \"message\": \"<ACTUAL_CARD_REVEAL_DATA>\", // will send the data to all the peers in the active room as peer_id or role is not specified.     \"type\": \"custom_event\"         "
    },
    {
        "title": "Update a peer",
        "link": "/server-side/v2/active-rooms/update-a-peer",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/active-rooms/update-a-peer",
        "keywords": [],
        "content": "  You can use this API to update the details of a connected peer, such as name, role, and metadata (custom/additional information to build real-time interactivity scenarios). <PostRequest title=\"https://api.100ms.live/v2/active-rooms/<room_id>/peers/<peer_id>\"   <Request id=\"req-0\">   bash curl location request POST 'https://api.100ms.live/v2/active-rooms/<room_id>/peers/<peer_id>'  header 'Authorization: Bearer <management_token>'  header 'Content-Type: application/json'  data-raw '    \"name\": \"nixon-test2\",   \"metadata\": \"  \"isHandRaised \":false, \"isBRBOn \":true \"  '   </Request> <ResponseBox id=\"res-0\" status=\"200 OK\">   json     \"id\": \"2b8bee10-5733-43af-a2f3-ecce623cbb90\",   \"name\": \"nixon-test2\",   \"user_id\": \"4f33d87f-e86b-4f58-97c6-d07fc476d936\",   \"metadata\": \"  \"isHandRaised \":false, \"isBRBOn \":true \",   \"role\": \"host\",   \"joined_at\": \"2022-10-12T12:56:17.759148635Z\",   \"tracks\":       \"a22befac-b77c-49bf-82ff-7b93c9f49fef\":         \"id\": \"a22befac-b77c-49bf-82ff-7b93c9f49fef\",       \"stream_id\": \"b90ef533-2af9-412c-9442-52cd70e758b6\",       \"mute\": true,       \"type\": \"audio\",       \"source\": \"regular\",       \"started_at\": \"2022-10-12T12:56:17.932404678Z\",       \"description\": \"\"      ,     \"a8cfe2d3-9fd8-47a0-b5ca-3203fd84bbd7\":         \"id\": \"a8cfe2d3-9fd8-47a0-b5ca-3203fd84bbd7\",       \"stream_id\": \"b90ef533-2af9-412c-9442-52cd70e758b6\",       \"mute\": true,       \"type\": \"video\",       \"source\": \"regular\",       \"started_at\": \"2022-10-12T12:56:17.932404678Z\",       \"description\": \"\"               </ResponseBox>   Arguments  Name             Type    Description                                                                        Required   :        :    :                                       :     room_id <br  — path param   string  Unique identifier of the room where the peer is active. <br <br   Example  : 627cda81ab4f3b56a077dc33                          Yes     peer_id <br  — path param   string  Unique identifier of the peer/participant for which you wish to update details. <br <br   Example  : 1038fa72-9345-4651-beab-436f9242ebf4        Yes     name <br  — body param    string  The name of the peer used in the active session.                                                     No      role <br  — body param    string  You can use this to update the role type of the peer. For example, to take the peer to a breakout room.                          No      metadata <br  — body param  object  You can use a stringified JSON to update various custom/additional information to a peer such as business_user_id, raise_hand/lower_hand info, and more  No       Why would you use this API?   You can use these APIs to update various properties of the connected peer, such as avatar, BRB status, hand-raise status, etc.     Custom Avatar:       A user updates their profile information on your app/ongoing session; the data is stored in your data store.     Use Update peer API and pass the profile picture data from your data store to your client app to display it as the respective connected peer's avatar (based on user_id mapping). "
    },
    {
        "title": "Release Notes",
        "link": "/server-side/v2/changelog/release-notes",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/changelog/release-notes",
        "keywords": [],
        "content": "  This Changelog highlights notable changes to the 100ms server-side API, such as API additions, improvements, and deprecations. Also, we've included developer experience improvements to this page to keep you on track with items that will enhance your integration journey.   January 13, 2023.    Additions   Added support to  Google cloud (https://cloud.google.com/storage)  gs and  Alibaba Cloud (https://www.alibabacloud.com/product/object-storage-service)  oss . Check recording config for  Create a room API (../Rooms/create-via-api recording_info-arguments) and  Create a template API (../policy/create-template-via-api recording-object) for more information.   January 11, 2023.    Additions   Added a new IP address for whitelisting 129.154.236.15/32 . Check  IP whitelisting (./../introduction/webhook ip-whitelisting) for more information.   January 04, 2023.    Additions   Added user_id field in Sessions object.    Deprecations   Deprecated user field in Sessions object. user_id can be used instead of user .   December 29, 2022.    Improvements   Standardised error_type , error_message fields in beam.failure and hls.failure webhooks.    Deprecations   Deprecated state_name , state_message , and state_timestamp fields from since they do not add any value. timestamp can be used instead of state_timestamp .   Deprecated error_code . This has always been 1 and does not have any significance.   November 23, 2022.    Additions   Added a new event  role.change.failure (./../introduction/webhook role-change-failure) to notify role change failures.   November 18, 2022.    Additions   Added new fields in HLS, beam, recording, role change, peer leave and session close webhook events.  Event                                    Fields                                                               :                    :                                   hls.recording.success (./../introduction/webhook hls-recording-success)   recording_path , <br  recording_presigned_url , <br  hls_vod_recording_path , <br  hls_vod_recording_presigned_url . <br     hls.failure (./../introduction/webhook hls-failure)             recording_single_file_per_layer_enabled , <br  recording_vod_playlist_enabled . <br                        hls.stopped.success (./../introduction/webhook hls-stopped-success)     recording_single_file_per_layer_enabled , <br  recording_vod_playlist_enabled . <br                        hls.started.success (./../introduction/webhook hls-started-success)     recording_single_file_per_layer_enabled , <br  recording_vod_playlist_enabled , <br  max_width , <br  max_height . <br      beam.recording.success (./../introduction/webhook beam-recording-success)  recording_path <br  recording_presigned_url . <br                                         beam.started.success (./../introduction/webhook beam-started-success)    max_width , <br  max_height . <br                                                 recording.failed (./../introduction/webhook recording-failed)        session_stopped_at .                                                        recording.success (./../introduction/webhook recording-success)       recording_path , <br  recording_presigned_url , <br  session_stopped_at . <br                           role.change.success (./../introduction/webhook role-change-success)     previous_role , <br  role_changed_at . <br                                             peer.leave.failure (./../introduction/webhook peer-leave-failure)      joined_at ,                                                             peer.leave.success (./../introduction/webhook peer-leave-success)      joined_at ,                                                             session.close.success (./../introduction/webhook session-close-success)   session_duration .                                                         November 03, 2022.    Additions   Added support for securing webhooks by whitelisting 100ms  NAT gateway IP addresses (/server-side/v2/introduction/webhook ip-whitelisting).   Added new fields in  Beam events (/server-side/v2/introduction/webhook rtmp-streaming-and-browser-recording-events): recording_enabled , max_width , and max_height   Added new fields in  HLS events (/server-side/v2/introduction/webhook hls-streaming-events): recording_single_file_per_layer_enabled and recording_vod_playlist_enabled   Added hlsDestination response object in  retrieve a specific template API (/server-side/v2/policy/retrieve-a-template)    Developer experience   Documentation improvements in  webhooks guide (/server-side/v2/introduction/webhook)  merged description and example columns to provide better readability.   October 06, 2022.    Additions   A new set of  Active room APIs (/server-side/v2/active-rooms/object) enables you to remotely control your active rooms by performing actions such as  retrieving details of an active session (/server-side/v2/active-rooms/retrieve-active-room),  retrieving (/server-side/v2/active-rooms/retrieve-peer)/ updating (/server-side/v2/active-rooms/update-a-peer) peer information,  sending messages (/server-side/v2/active-rooms/send-message),  removing peers (/server-side/v2/active-rooms/remove-peers), and  ending an active session (/server-side/v2/active-rooms/end-active-room).    Developer experience     Simplified generation of management token within Postman collection    we've added a pre-request script in our collection to help you generate the management token without creating a token management service at your end for testing. Refer to the  Postman guide (/server-side/v2/introduction/postman-guide add-management-token) for more details.   As environment variables created some confusion, we've removed the environment from our Postman collection. We recommend you use the collection variables going forward. Refer to the  Postman guide (/server-side/v2/introduction/postman-guide add-management-token) for more details.   Added the  API reference documentation (</server-side/v2/legacy-api%20(v1)/room>) for legacy APIs(V1).    Deprecations   Removed active field from  room object (/server-side/v2/Rooms/object) as enabled field was already available to indicate whether the room is in enabled or disabled state.   September 24, 2022.    Server update   The API is now available in the production environment, using the following base URL:   https://api.100ms.live/v2/    Additions   A new set of  Room APIs (/server-side/v2/Rooms/object)   A new set of  Session APIs (/server-side/v2/Sessions/object)   A new set of  Policy APIs (/server-side/v2/policy/template-object)    Destinations object (/server-side/v2/policy/create-template-via-api destinations-object) is added to the template object.    API Improvements     RESTful adherence    params are now URL params and not query params.     Extended pagination for all endpoints    We paginate the responses with a list of objects ranging from 10 to 100 to ensure they are easier to handle.    Developer experience     Virtual controlled Postman collection    you can fork our  Postman collection (/server-side/v2/introduction/postman-guide) to stay up to date with the latest API improvements. This will enable you to get familiar with and interact more seamlessly with our product.     Detailed API reference    We have improved our  documentation (/server-side/v2/introduction/basics), so it's easier to get started and stay up to date with the latest API improvements.    Deprecations   In the template object, the fields _id, and customer, supported with the  Legacy API (V1) (</server-side/v2/legacy-api%20(v1)/templates-and-roles>), is deprecated. You can continue to use them as id and customer_id with the new API. Refer to the  template object section (/server-side/v2/policy/template-object template-object) for more information.   In the room object, the field template supported with the template name in the  Legacy API (V1) (</server-side/v2/legacy-api%20(v1)/room>) will not be supported in the new API. Instead, you can use the  template_id (/server-side/v2/Rooms/create-via-api main-arguments) field. >   Note  : The Legacy API (V1) will remain fully functional. However, we recommend you use our new API to leverage the new functionalities and features and stay up to date with future releases; you can continue to check the docs for the Legacy API (V1)s  here (</server-side/v2/legacy-api%20(v1)/room>). "
    },
    {
        "title": "Interactive Live Streaming via dashboard",
        "link": "/server-side/v2/Destinations/live-streaming-via-dashboard",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/Destinations/live-streaming-via-dashboard",
        "keywords": [],
        "content": "    Overview Live video interactions can span various levels of interactivity. For example, in a virtual event, some participants can be on stage talking to each other, whereas participants in the audience can be listening to them. At 100ms, we think of this as the   3 levels of interactivity  . 100ms enables you to build live video use-cases by mixing and matching these 3 levels to get to your ideal solution.     Level 1  : Full duplex audio/video in real-time   Level 1 participants publish their audio/video, and interact with others in sub-second latency. This is real-time video conferencing, similar to Zoom or Google Meet.     Level 2  : Webinar-style audience in real-time   Level 2 participants consume audio/video from level 1 participants with sub-second latency, without publishing their own audio/video. Level 2 participants can engage with level 1 through messaging (chat, emojis, custom events). This is similar to a Zoom webinar.   Levels 1 and 2 are enabled using WebRTC.     Level 3  : Live stream audience consuming in near real-time   Level 3 participants consume a composite live stream in near real-time (<10 secs of latency) without publishing their audio/video. They can interact with other participants via messaging. This is similar to viewers on Twitch or YouTube Live, and is enabled via   100ms Interactive Live Streaming  .   Live streaming uses  HLS (https://www.100ms.live/blog/hls-101-beginners-guide) to achieve near real-time latency at scale. The  roles primitive (/docs/javascript/v2/foundation/templates-and-roles) can be used to define capabilities of a participant and associate them to an interaction level. A participant can move between levels using a single API call to change roles.   Try Interactive Live Streaming Use our  Live Streaming Starter Kit (https://www.100ms.live/marketplace/live-streaming-starter-kit) to try out the experience before you write a line of code. <StepsToc  parentId=\"try-live-streaming\"  descriptions=  \"Use our Live streaming starter kit to try out the experience before you write a line of code.\", \"Understand the difference between a stream broadcaster and stream viewer.\", \"Use the demo app link to go live for the first time as a broadcaster and join the stream as viewer.\", \"Use the 100ms self-serve dashboard to update the layout, aspect ratio, etc of the stream.\"   > <StepsContainer id=\"try-live-streaming\">   Create a new app   Live streaming starter kit (/docs/docs/v2/live-streaming-starter-kit.png) 1. Make sure that you have  an account with 100ms (https://dashboard.100ms.live/register) and can access the  100ms dashboard (https://dashboard.100ms.live/) 1. On the dashboard, create a new app using the Live Streaming Starter Kit 1. Specify a subdomain and region to deploy the app   Understand roles   Live Streaming roles (/docs/docs/v2/live-streaming-roles.png) This starter kit configures your new app with  two roles (/docs/javascript/v2/foundation/templates-and-roles):   broadcaster : This role represents a streamer who publishes their audio/video. There can be multiple peers who join as broadcasters   hls-viewer : This role represents a circle 3 audience, who subscribes to the composite live stream and can interact using messaging   Go live   Go live (/docs/docs/v2/live-streaming-go-live.gif) 1. To go live for the first time, join the room as a broadcaster and start the live stream 2. Once the stream has started, join the room as an hls-viewer and you should be able to see the ongoing live stream 3. Use chat messages to interact between the viewer and the broadcaster   Customize the stream   Go live (/docs/docs/v2/live-streaming-customise.png) By default, the live stream is composed in landscape mode for desktop viewers (with an aspect ratio of 16:9). You can customise the live stream for viewers on mobile or to support multiple broadcaster tiles. 1. On the 100ms dashboard, click the gear icon on your app to open configuration settings 2. Go to \"destinations\" and scroll down to find live stream (HLS) configuration 3. Update the configuration based on your needs:    If your viewers are on mobile, change the video aspect ratio to 9:16    If you have multiple broadcasters joining in, choose grid or active speaker based on your needs    In case of grid layout, choose the tile size that fits your use-case. For example, a stream with 2 streamers looks better with 1:1 tiles. </StepsContainer>   Integrate in your app To integrate 100ms Interactive Live Streaming in your app, follow these steps: 1.  Enable live streaming destination ( enable-destination) 2.  Integrate the 100ms SDK in your app ( sdk-integration) 3.  Integrate live stream playback ( live-stream-playback) 4.  Video on demand (VOD) use cases ( video-on-demand-vod-use-cases)   Enable destination   Enable HLS (/docs/docs/v2/live-streaming-enable.gif) If your app is based on the Live Streaming Starter Kit (as shown above), the live streaming destination is enabled out-of-the-box. For custom apps, you can enable the live streaming destination manually: 1. Open configuration for your existing app using the 100ms dashboard 1. In the \"destinations\" tab, enable \"Live Streaming with HLS\" 1. Ensure that you have roles for the broadcaster (who can publish their audio/video) and the viewer (who cannot publish audio/video)   SDK integration Use the 100ms client-side SDKs to integrate streaming in your application. See code snippets for the client-side SDK  here (/docs/javascript/v2/features/hls).   Live stream playback Using our client-side SDKs, you can enable live stream playback and add interactive experiences like chat, raise hand and other functionalities to your app using  peer metadata (/docs/javascript/v2/advanced-features/peer-metadata). The process is so simple: 1. Once you  start ( step-3-go-live) live streaming, you will get an HLS URL (M3U8 URL) which you can use for playback. 2. You can use the  client-side SDK (/docs/javascript/v2/features/hls) to get the HLS URL by checking the  current state (/docs/javascript/v2/features/hls current-room-status) of the room and start playback. If you need to only enable HLS playback and don't need interactivity, you can follow one of the below approaches to get the HLS URL:     Webhook:   You can listen to hls.started.success  webhook event (/docs/server-side/v2/introduction/webhook hls-started-success) and get the HLS URL from the url field. Please check the  webhooks guide (/docs/server-side/v2/introduction/webhook) to learn more about webhooks.     Static URL:   This configuration will help you get a static URL for playback. You can enable the Static playback URLs in your template from the  dashboard (https://dashboard.100ms.live/dashboard). You can go to Destination > enable \"Live streaming with HLS\" > under \"Customise stream video output\" section > enable \"Static playback URLs.\"     Enable Static URL (/docs/docs/v2/enable-static-url.png)     _Format_: https://cdn.100ms.live/beam/<customer_id>/<room_id>/master.m3u8     customer_id : replace this placeholder with your customer_id from  developer section (https://dashboard.100ms.live/developer) on your dashboard.     room_id : replace this placeholder with the room_id of the respective room from which the stream will be broadcasted.   Video on Demand (VOD) use cases If you wish to replay your HLS stream for Video on demand (VOD) use case, 100ms provides the capability to record the HLS stream which will be posted to your webhook as a ZIP file of M3U8 format (same playback format as HLS) with all the chunks once the stream ends. You can start recording a live stream using the  client-side SDK (/docs/javascript/v2/features/hls) or using the  server API (/docs/server-side/v2/Destinations/rtmp-streaming-and-browser-recording start-streaming-recording). Once the HLS recording is completed, you will get the details of recording as a callback to the webhook configured in your account. Check hls.recording.success  webhook event (/docs/server-side/v2/introduction/webhook hls-recording-success) for more information.  Use the 100ms client-side SDKs to integrate streaming in your application. See code snippets for the web SDK  here (/javascript/v2/features/hls). "
    },
    {
        "title": "SFU Recording",
        "link": "/server-side/v2/Destinations/recording",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/Destinations/recording",
        "keywords": [],
        "content": "    Introduction SFU recording can be used to save audio/video calls for offline viewing. SFU recording supports both individual and composite recordings.   Types of SFU recording   Individual recording Media for each peer is provided as a separate mp4 file. This file will have both audio and video of the peer. These files can be used for offline review or in implementing custom composition.   Composite recording Audio and video of all peers is composed as per their joining/leaving the meeting and provided as a single mp4. This file can be used for offline viewing of the meeting.   How to enable SFU recording   Using Dashboard There are two ways to enable SFU recording for a room.    First way (recommended) This requires two steps 1. Provide upload info in the template and enable the SFU recording.     Enable recording from a template (/docs/v2/recording.png) 2. Choose Auto for recording while creating a room.     Choose auto recording while creating a room (/docs/v2/create-a-room-recording-auto.png)   >   Note  : Choosing Auto for recording will make room's recording settings depends on template.   > That means    >   >  Enabling/Disabling SFU recording in template will also enable/disable it for the room.   >  Changes made in upload settings in template, will also be reflected for the room.    Second way Another way to enable SFU recording for a room is to choose Enabled for recording settings while creating it.   Enable recording when creating a room (/docs/v2/create-a-room-recording-enabled.png) >   Note  : Choosing Enabled/Disabled explicitly for recording while creating a room, will make the recording settings in room not depend on template. > That means > >   Enabling/Disabling SFU recording in template will have no effect this template. >   Room will have its own upload info. By default, they will be uploaded to 100ms storage. More on this below.   Using API    See detailed API (/server-side/v2/Rooms/create-via-api)   Uploading destination configuration By default recordings will be uploaded to 100ms storage and a pre-signed URL for the same will be provided to customers via a webhook. The pre-signed URL will expire in 12 hours. Customers can also configure the recordings to be stored in their cloud storage. Following are the configurations for the same.  Name     Type    Description                                                     Required   :    :    :                              :     type      string  Upload destination type. Currently, s3 (AWS), gs (Google Cloud Storage), oss (Alibaba Cloud) are supported.  Yes     location    string  Name of the storage bucket in which you want to store all recordings                        Yes     prefix     string  Upload prefix path                                                 No      options    object  Additional configurations of type Options to be used for uploading                        No      credentials  object  Object of type Credentials . This is used to share credentials to access the storage bucket specified       No     Where Options is  Name   Type    Description                                Required   :   :    :                    :     region  string  Region of the account hosting the storage bucket for storing recordings.  No     Where Credentials is  Name   Type    Description                                   Required   :   :    :                     :     key    string  Access Key for the account hosting your storage bucket for storing recordings  Yes     secret  string  Secret for the account hosting the storage bucket for storing recordings    Yes     Composite mp4 recording will be placed at: <type>://<location>/<prefix>/<room_id>/<start_date>/<session_id>/Rec-<session_id>-<epoch>.mp4  Example s3://test-bucket/test-prefix/61101349d0f56e684b608c08/20210721/5fccb7dc72909272bf999014/Rec-5fccb7dc72909272bf999014-1626898422000.mp4  Individual mp4 recordings will be placed at: <type>://<location>/<prefix>/<room_id>/<start_date>/<session_id>/<peer_id>/<stream-id>/<stream-id>.mp4  Example s3://test-bucket/test-prefix/61101349d0f56e684b608c08/20210721/5fccb7dc72909272bf999014/76539d6b-d10d-4790-bdd2-ab57fa7facb0/9aeec8db-524d-4129-874d-7a71d340bdec/9aeec8db-524d-4129-874d-7a71d340bdec.mp4 "
    },
    {
        "title": "RTMP Streaming & Browser Recording",
        "link": "/server-side/v2/Destinations/rtmp-streaming-and-browser-recording",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/Destinations/rtmp-streaming-and-browser-recording",
        "keywords": [],
        "content": "    Introduction RTMP Streaming can be used to live stream your video conferencing apps to platforms like YouTube, Twitch, Facebook, MUX, etc. Browser Recording can be used to record your video conferencing apps to a file, for subsequent access. You can start either RTMP Streaming or Browser Recording or both for any room   Pre-requisites 1. Management token     Click here to see how to generate management token (/server-side/v2/introduction/authentication-and-tokens) 2. Single click meeting URL to directly enter a room 3. 100ms SDK logs should be logged to console for the web-app URL provided above. The SDK logs will be analysed to detect any room join failures and retries will be attempted to fix the same. If SDK logs are not enabled then the server will stop recording/streaming and return error (after fixed number of retries).   APIs   Start Streaming/Recording <PostRequest title=\"https://prod-in2.100ms.live/api/v2/beam\"   <Request id=\"req-0\">   bash curl    location    request POST 'https://prod-in2.100ms.live/api/v2/beam'    header 'Content-Type: application/json'    header 'Authorization: Bearer <management_token>'    data-raw '      \"operation\": \"start\",     \"room_id\": <rooom_id>,     \"meeting_url\": <meeting_url>,     \"rtmp_urls\":  <rtmp_url_1>, <rtmp_url_2> ,     \"record\": true,     \"resolution\" :  \"width\": 1280, \"height\": 720     '   </Request> <ResponseBox id=\"res-0\" status=\"200 OK\" resText=\"Beam has started successfully\"   >   Note  : Please use https://prod-us2.100ms.live/api/v2/beam or https://prod-eu2.100ms.live/api/v2/beam as API endpoint if your room's region is US or EU. <div id=\"start-0\">    Headers  Name         Value             Required   :      :        :     Content-type <br   application/json       Yes     Authorization <br   Bearer <management token>  Yes       Body Parameters  Name        Type    Description                                                                                 Required   :      :    :                                            :     operation <br    string  Must be \"start\"                                                                               Yes     room_id <br     string  The room id of the room which we need to start streaming/recording                                                     Yes     meeting_url <br   string  Single click meeting URL which we need to start streaming/recording (see below for more details)                                      Yes     rtmp_urls <br    array   List of RTMP output URLs to which the meeting will be streamed. <br  Required when RTMP streaming needs to be started. <br  Supports upto 3 rtmp:// / rtmps:// urls  No      record <br      bool   Flag to enable browser recording                                                                      No      resolution <br    object  Video resolution for streaming/recording                                                                  No        meeting_url This is a URL which lands into the meeting directly. If you are using 100ms apps then this will be like  https://<domain>.app.100ms.live/preview/<room_id>/<role>?skip_preview=true . There are also a few additional query params which can be used for the 100ms app link    auth_token=token1234 : this can be used to pass auth token generated on your side to the recording web-app   ui_mode=activespeaker : this can be used to open active speaker mode by default for recording. In active speaker mode,   the current speaker will show up in center, and rest of the peers will show up in the sidebar. Example  https://myname.app.100ms.live/preview/61101349d0f56e684b608c08/__internal_recorder?skip_preview=true&auth_token=token1234&ui_mode=activespeaker     rtmp_urls This URL is the combination of RTMP server URL and stream key. You can add upto 3 RTMP URLs (as array of strings) for this argument.   Format: rtmp://server.com/app/STREAM_KEY   Example: rtmp://a.rtmp.youtube.com/live2/k0jv-329m-1y7f-ktth-ck48     \"rtmp://a.rtmp.youtube.com/live2/\"  RTMP stream URL.     \"k0jv-329m-1y7f-ktth-ck48\"  RTMP stream key.  Please refer to the platform specific documentation for details on how to obtain the stream URL and stream key. Here are some examples:      YouTube (https://support.google.com/youtube/answer/2907883?hl=en&ref_topic=9257892)      Facebook (https://www.facebook.com/help/587160588142067)      Instagram (https://about.instagram.com/blog/tips-and-tricks/instagram-live-producer)      Twitch (https://help.twitch.tv/s/article/twitch-stream-key-faq?language=en_US)      LinkedIn (https://www.linkedin.com/help/linkedin/answer/a564446/go-live-using-a-custom-stream-rtmp)    resolution Minimum Resolution supported from 500x480 (in landscape mode) and 500x500 (in portrait mode) Maximum Resolution supported upto 1920x1080 (in landscape mode) and 1080x1920 (in portrait mode)  Name      Type  Description                                                Required   :     :   :                           :     width <br    int  Width of the video. <br  Range: 500  1920 . If height>1080 then max width=1080. <br Default: 1280  Yes     height <br   int  Height of the video. <br  Range: 480  1920 . If width>1080 then max height=1080. <br Default: 720  Yes     </div>     Stop Streaming/Recording <PostRequest title=\"https://prod-in2.100ms.live/api/v2/beam\"   <Request id=\"req-1\">   bash curl    location    request POST 'https://prod-in2.100ms.live/api/v2/beam'    header 'Content-Type: application/json'    header 'Authorization: Bearer <management_token>'    data-raw '      \"operation\": \"stop\",     \"room_id\": <rooom_id>    '   </Request> <ResponseBox id=\"res-1\" status=\"200 OK\" resText=\"Beam has stopped successfully\"   <div id=\"stop-0\">    Headers  Name         Value             Required   :      :        :     Content-type <br   application/json       Yes     Authorization <br   Bearer <management token>  Yes       Body Paramters  Name       Type    Description                               Required   :     :    :                   :     operation <br   string  Must be \"stop\"                             Yes     room_id <br    string  The room id of the room for which we need to stop streaming/recording  Yes     </div>   Error messages in RTMP API  Message                           Code  Description                         :                :   :                 Error: beam already started                 1   RTMP streaming or Recording already running for the room   Error: internal server error                2   server error                         Error: room_id missing                   3   room id is missing in the request payload          Error: room_id invalid                   4   Room id provided is invalid                 Error: rtmp_url and record missing             5   RTMP URLs or record flag is required             Error: meeting_url missing                 6   Meeting URL is missing in the request payload        Error: meeting_url invalid                 7   Meeting URL is invalid                    Error: operation invalid                  8   Operation provided is invalid                Error: operation missing                  9   Operation field is missing in the request payload      Error: no active beam found for the passed in identifiers  10   No Active beam running in the room              Error: Resolution not supported               11   Input Resolution not Supported                 Recording upload destination By default recordings will be uploaded to 100ms storage and a pre-signed URL for the same will be provided to customers via a webhook. The pre-signed URL will expire in 12hours. Customers can also configure the recordings to be stored in their cloud storage. The configuration for the same can be provided when creating the room.  Create Room (/server-side/v2/Rooms/create-via-api) Recording mp4 will be placed at  <upload_type>://<location>/<prefix>/beam/<room_id>/<start_date>/Rec-<room_id>-<epoch>.mp4 . Example s3://test-bucket/test-prefix/beam/61101349d0f56e684b608c08/20210721/Rec-61101349d0f56e684b608c08-1626898422000.mp4   Postman Collection <DownloadCollection type=\"rtmp\"   "
    },
    {
        "title": "Authentication and tokens",
        "link": "/server-side/v2/introduction/authentication-and-tokens",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/introduction/authentication-and-tokens",
        "keywords": [],
        "content": "  In 100ms, two types of tokens are used to authenticate requests coming from your Client apps and Backend application server into the 100ms platform.    App token ( app-token) : Used to authenticate and allow end-users (peers) to join 100ms rooms. An App Token controls Peer identity and Room permissions in your real-time or Interactive live-streaming video application.    Management token ( management-token) : Used to authenticate all the requests to 100ms REST API. You can set the expiry to both these tokens; if you follow the code samples from this guide, the expiry will be set as 24 hours. However, a best practice is to set the expiry as short as feasible for your application. You must host your server to generate these tokens while transitioning your app to production.   App Token 100ms _client-side SDKs_ use App Tokens to authenticate a peer (participant) while  joining a room (/javascript/v2/features/join). Generate this token on the server side and make it available for your client-side apps that use the 100ms SDKs. To create an App Token, you need to use app_access_key , app_secret , room_id , and user_id .   You can get the app_access_key and app_secret from the  developer section (https://dashboard.100ms.live/developer) in your 100ms dashboard.     room_id  : This is the unique identifier for your room. You can get it from the  rooms page (https://dashboard.100ms.live/rooms) in your dashboard or in the response payload of the  create room server-side API (./../Rooms/create-via-api).     user_id  : This identifier can be used to map a 100ms peer to your own internal user object for business logic. Specify your internal user identifier as the peer's user_id. If not available, use any random string.   How to use? You can get App tokens using a couple of approaches based on your app's lifecycle stage. Please check the below sections for more information:    Set up your own authentication endpoint ( set-up-your-own-authentication-endpoint)    Get app tokens from the token endpoint ( get-app-tokens-from-the-token-endpoint)    Get app tokens from the dashboard ( get-app-tokens-from-the-dashboard)    Set up your own authentication endpoint When you have completed your integration with 100ms, and while transitioning your app to production, we recommend you create your backend service for app token generation; use the code snippet below and set up the token generation service as per your preferred programming language.    Code sample: Generate app token  s id=\"client-code-token\" items=  'Node.js', 'Python', 'Java', 'Ruby', 'PHP'       id='client-code-token-0'>   javascript var jwt = require('jsonwebtoken'); var uuid4 = require('uuid4'); var app_access_key = '<app_access_key>'; var app_secret = '<app_secret>'; var payload =     access_key: app_access_key,   room_id: '<room_id>',   user_id: '<user_id>',   role: '<role>',   type: 'app',   version: 2,   iat: Math.floor(Date.now() / 1000),   nbf: Math.floor(Date.now() / 1000)  ; jwt.sign(   payload,   app_secret,         algorithm: 'HS256',     expiresIn: '24h',     jwtid: uuid4()    ,   function (err, token)       console.log(token);     );   </Tab>   id='client-code-token-1'>   python  /usr/bin/env python3 import jwt import uuid import datetime import sys app_access_key = \"<app_access_key>\" app_secret = \"<app_secret>\" def generate(room_id, user_id, role):   expires = expires or 24   3600   now = datetime.datetime.utcnow()   exp = now+ datetime.timedelta(seconds=expires)   return jwt.encode(payload=          \"access_key\": app_access_key,         \"type\":\"app\",         \"version\":2,         \"room_id\": room_id,         \"user_id\": user_id,         \"role\":role,         \"jti\": str(uuid.uuid4()),         \"exp\": exp,         \"iat\": now,         \"nbf\": now,          , key=app_secret) if __name__ == \"__main__\":   if len(sys.argv) == 3:     room_id = sys.argv 0      user_id = sys.argv 1      role = sys.argv 2    print(generate(room_id=room_id, user_id=user_id, role=role))   </Tab>   id='client-code-token-2' >   java import java.time.Instant; import java.util.Date; import java.util.HashMap; import java.util.Map; import java.util.UUID; import io.jsonwebtoken.Jwts; import io.jsonwebtoken.SignatureAlgorithm; private void generateHmsClientToken()     Map<String, Object> payload = new HashMap<>();   payload.put(\"access_key\", \"<app_access_key>\");   payload.put(\"room_id\", \"<room_id>\");   payload.put(\"user_id\", \"<user_id>\");   payload.put(\"role\", \"<role>\");   payload.put(\"type\", \"app\");   payload.put(\"version\", 2);   String token = Jwts.builder().setClaims(payload).setId(UUID.randomUUID().toString())     .setExpiration(new Date(System.currentTimeMillis() + 86400   1000))     .setIssuedAt(Date.from(Instant.ofEpochMilli(System.currentTimeMillis()  60000)))     .setNotBefore(new Date(System.currentTimeMillis()))     .signWith(SignatureAlgorithm.HS256, \"<app_secret>\".getBytes()).compact();      </Tab>   id='client-code-token-3' >   ruby require 'jwt' require 'securerandom'  $app_access_key = \"<app_access_key>\" $app_secret = \"app_secret\" def generateAppToken(room_id, user_id, role)   now = Time.now   exp = now + 86400   payload =       access_key: $app_access_key,     room_id: room_id,     user_id: user_id,     role: role,     type: \"app\",     jti: SecureRandom.uuid,     version: 2,     iat: now.to_i,     nbf: now.to_i,     exp: exp.to_i       token = JWT.encode(payload, $app_secret, 'HS256') end puts generateAppToken \"<room_id>\", \"<user_id>\", \"<role>\"    </Tab>   id='client-code-token-4'>   php <?php use Firebase JWT JWT; use Ramsey Uuid Uuid; $issuedAt = new DateTimeImmutable(); $expire  = $issuedAt->modify('+24 hours')->getTimestamp(); $accessKey = \"<app_access_key>\"; $secret = \"<app_secret>\"; $version  = 2; $type   = \"app\"; $role   = \"<role>\"; $roomId  = \"<room_id>\"; $userId  = \"<user_id>\"; $payload =     'iat' => $issuedAt->getTimestamp(),   'nbf' => $issuedAt->getTimestamp(),   'exp' => $expire,   'access_key' => $accessKey,   'type' => \"app\",   'jti' => Uuid::uuid4()->toString()   'version' => 2,   'role' => $role,   'room_id' => $roomId,   'user_id' => $userId  ; $token = JWT::encode(   $payload,   $secret,   'HS256' );   </Tab> <Note type=\"warning\">   We strictly advise you not to post <i>app_access_key</i> and <i>app_secret</i> publicly; if you   need to store them in <strong>Git</strong>, please change the repository from public to private.   <br     <br   You cannot use an <strong>App token</strong> to trigger server API requests. </Note>    Get app tokens from the token endpoint If you are building integration with 100ms, you can get app tokens from the 100ms token endpoint without hosting a token generation backend service. Refer to  this guide (/javascript/v2/guides/token-endpoint get-a-token-using-100ms-token-endpoint) for more information.    Get app tokens from the dashboard If you are building your first app by following one of our  quickstart guides (/javascript/v2/guides/javascript-quickstart), you can get the app token directly from 100ms dashboard to join a room for the first time. Refer to  this guide (/javascript/v2/guides/token get-a-temporary-token-from-100ms-dashboard) for more information.   Management Token 100ms uses management tokens to authenticate REST APIs. If you're evaluating 100ms  server APIs (./basics), you can use our public  Postman collection (./postman-guide fork-the-collection), which doesn't require you to create a management token as we've managed it using a  pre-request script (./postman-guide simplified-token-generation) within the collection. If you're transitioning your app to production, we recommend you create your backend service for management token generation. You must use the app_access_key and app_secret from the  developer section (https://dashboard.100ms.live/developer) in your 100ms dashboard to create the management token.    Code sample: Generate management token  s id=\"test-code\" items=  'Node.js', 'Python', 'Java', 'Ruby', 'PHP'       id='test-code-0'>   js var jwt = require('jsonwebtoken'); var uuid4 = require('uuid4'); var app_access_key = '<app_access_key>'; var app_secret = '<app_secret>'; jwt.sign(         access_key: app_access_key,     type: 'management',     version: 2,     iat: Math.floor(Date.now() / 1000),     nbf: Math.floor(Date.now() / 1000)    ,   app_secret,         algorithm: 'HS256',     expiresIn: '24h',     jwtid: uuid4()    ,   function (err, token)       console.log(token);     );   </Tab>   id='test-code-1'>   py  /usr/bin/env python3 import jwt import uuid import datetime app_access_key = '<app_access_key>' app_secret = '<app_secret>'  def generateManagementToken():   expires = 24   3600   now = datetime.datetime.utcnow()   exp = now + datetime.timedelta(seconds=expires)   return jwt.encode(payload=      'access_key': app_access_key,     'type': 'management',     'version': 2,     'jti': str(uuid.uuid4()),     'iat': now,     'exp': exp,     'nbf': now      , key=app_secret) if __name__ == '__main__':   print(generateManagementToken())   </Tab>   id=\"test-code-2\">   java import java.time.Instant; import java.util.Date; import java.util.HashMap; import java.util.Map; import java.util.UUID; import io.jsonwebtoken.Jwts; import io.jsonwebtoken.SignatureAlgorithm; private void generateManagementToken()     Map<String, Object> payload = new HashMap<>();   payload.put(\"access_key\", \"<app_access_key>\");   payload.put(\"type\", \"management\");   payload.put(\"version\", 2);   String token = Jwts.builder().setClaims(payload).setId(UUID.randomUUID().toString())     .setExpiration(new Date(System.currentTimeMillis() + 86400   1000))     .setIssuedAt(Date.from(Instant.ofEpochMilli(System.currentTimeMillis()  60000)))     .setNotBefore(new Date(System.currentTimeMillis()))     .signWith(SignatureAlgorithm.HS256, \"<app_secret>\".getBytes()).compact();      </Tab>   id=\"test-code-3\">   ruby require 'jwt' require 'securerandom' $app_access_key = \"<app_access_key>\" $app_secret = \"<app_secret>\" def generateManagementToken()   now = Time.now   exp = now + 86400   payload =     access_key: $app_access_key,   type: \"management\",   version: 2,   jti: SecureRandom.uuid,   iat: now.to_i,   nbf: now.to_i,   exp: exp.to_i   token = JWT.encode(payload, $app_secret, 'HS256') return token end puts generateManagementToken   </Tab>   id=\"test-code-4\">   php <?php use Firebase JWT JWT; use Ramsey Uuid Uuid; $app_access_key = \"<app_access_key>\"; $app_secret = \"<app_secret>\"; $issuedAt  = new DateTimeImmutable(); $expire   = $issuedAt->modify('+24 hours')->getTimestamp(); $payload =     'access_key' => $app_access_key,   'type' => 'management',   'version' => 2,   'jti' => Uuid::uuid4()->toString(),   'iat' => $issuedAt->getTimestamp(),   'nbf' => $issuedAt->getTimestamp(),   'exp' => $expire,  ; $token = JWT::encode($payload, $app_secret, 'HS256'); ?>   </Tab> <Note type=\"warning\">   We strictly advise you not to post <i>app_access_key</i> and <i>app_secret</i> publicly; if you   need to store them in <strong>Git</strong>, please change the repository from public to private.   <br     <br     You cannot authenticate room join requests from your client-side apps with a <strong>     Management token   </strong>. </Note> "
    },
    {
        "title": "Basics\r",
        "link": "/server-side/v2/introduction/basics",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/introduction/basics",
        "keywords": [],
        "content": " -\r \r  100ms API collection\r \r TL;DR  You can directly use our Postman collection to start exploring 100ms APIs. \r \r    Run in Postman (https://run.pstmn.io/button.svg) (https://god.gw.postman.com/run-collection/22726679-47dcd974-29d5-4965-a35b-bf9b74a8b25a?action=collection%2Ffork&collection-url=entityId%3D22726679-47dcd974-29d5-4965-a35b-bf9b74a8b25a%26entityType%3Dcollection%26workspaceId%3Dd9145dd6-337b-4761-81d6-21a30b4147a2)\r \r Refer to the  Postman guide (/server-side/v2/introduction/postman-guide) to get started with 100ms API collection. \r \r   Authentication\r 100ms uses management tokens to authenticate REST APIs. The service required to generate this token should be hosted on your server. You must use the app_access_key and app_secret from the  developer section (https://dashboard.100ms.live/developer) in your 100ms dashboard to create the management token. Refer to the  authentication and tokens guide (/server-side/v2/introduction/authentication-and-tokens code-sample-generate-management-token) for more information on generating the management token.\r \r  Architecture 100ms is a cloud platform that allows developers to add video and audio conferencing to Web, Android and iOS applications. The platform provides REST APIs, SDKs, and a dashboard that makes it simple to capture, distribute, record, and render live interactive audio, video. Any application built using 100ms' SDK has 2 components.     Client:   Use 100ms android, iOS, Web SDKs to manage connections, room states, render audio/video.     Server:   Use 100ms' APIs or dashboard to create rooms, setup room templates, trigger recording or RTMP streaming, access events.   Architecture (/docs/docs/v2/arch.png)   Basic Concepts   Room A room is the basic object that 100ms SDKs return on successful connection. This contains references to peers, tracks and everything you need to render a live a/v or live streaming app.   Peer A peer is the object returned by 100ms SDKs that contains all information about a user  name, role, video track etc.   Session A session depicts activity in a room. A session is created when one or more peers join a room to communicate with each other. A single room can have multiple sessions; a unique ID will be assigned to each session. The maximum allowed duration for a session on the 100ms platform is 12 hours.   Track A track is a segment of media (audio/video) captured from the peer's camera and microphone. Peers in a session publish local tracks and subscribe to remote tracks from other peers.   Role A role defines who can a peer see/hear, the quality at which they publish their video, whether they have permissions to publish video/screenshare, mute someone, change someone's role.   Template A template is a collection of roles, room settings, recording and RTMP settings (if used), that are used by the SDK to decide which geography to connect to, which tracks to return to the client, whether to turn on recording when a room is created, etc. Each room is associated with a template.   Destinations Destinations is used to save audio/video calls for offline viewing. 100ms supports 2 kinds of recording   SFU recording (/docs/javascript/v2/foundation/recordings sfu-recording-advanced) and  Browser recording (/docs/javascript/v2/foundation/recordings browser-recording-recommended). Also, HLS enabled configuration will allow you to live stream your room over HLS.   RTMP RTMP streaming is used to live stream your video conferencing apps to platforms like YouTube, Twitch, Facebook, MUX, etc.   Webhooks Webhook is an HTTP(S) endpoint used for pushing the notifications to your application. It will be invoked by 100ms servers to notify events of your room.   Workspace A workspace is an isolated environment which contains account data like templates, rooms, room and session history, etc. You can use workspaces to represent environments like “Production” and “Development” and invite team members to a workspace.   What are the steps to build a live app with 100ms? 1. Sign up on 100ms using the   Try For Free   button in the top navbar.   Signup for 100ms account (/docs/docs/v2/signup.png) 2. Once you're logged in to the dashboard, click on Create Your First App    Signup for 100ms account (/docs/docs/v2/create-your-first-app.png) 3.   Hover   on one of the Starter Kits. Deploy one of them. (We will use the   Video Conference Starter Kit   for this example)   Dashboard _ 100ms.png (/docs/docs/v2/select-starter-kit.png) 4. Select your account type and fill in the details   Dashboard _ 100ms.png (/docs/docs/v2/personal-details.png) 5. Choose a deployment option. This could be 100ms or Vercel (based on the Starter Kit you are deploying)   Video Conferencing Starter Kit (/docs/docs/v2/choose-your-deployment.png) 6. Enter a subdomain of your choice. Please avoid entering https/http/www or dots while entering the subdomain. Select a region closest to you and hit Continue.   choose subdomain (/docs/docs/v2/choose-subdomain.png) 7. Join or Invite someone to your deployed app with one of the roles:    join or invite (/docs/docs/v2/demo-your-app.png)   Where should I start? \r \r   Quickstart\r \r If you want to see 100ms' SDKs in action in under 5 minutes, run one of our quickstart  apps (/javascript/v2/guides/javascript-quickstart)\r \r  API Overview\r \r Create rooms, configure and maintain room policies and templates, start recording or RTMP streaming, and access events via 100ms' APIs. These APIs use standard HTTP verbs and HTTP status codes to make it simple to incorporate video into your code. We serve our API through HTTPS to secure your requests to our servers.\r \r <BaseRequest\r   title=\"https://api.100ms.live/ version /\"\r  \r \r >   Note  : our current version is v2.\r \r    Run in Postman (https://run.pstmn.io/button.svg) (https://god.gw.postman.com/run-collection/22726679-47dcd974-29d5-4965-a35b-bf9b74a8b25a?action=collection%2Ffork&collection-url=entityId%3D22726679-47dcd974-29d5-4965-a35b-bf9b74a8b25a%26entityType%3Dcollection%26workspaceId%3Dd9145dd6-337b-4761-81d6-21a30b4147a2)\r \r Refer to the  Postman guide (/server-side/v2/introduction/postman-guide) to get started with 100ms API collection. "
    },
    {
        "title": "Domain and Port Whitelisting",
        "link": "/server-side/v2/introduction/firewall-and-ports",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/introduction/firewall-and-ports",
        "keywords": [],
        "content": "tags:    domains    ports    firewall    whitelisting  For smooth call experience add following domains and ports to your firewall whitelist   Domains   .100ms.live   Ports   Minimum requirement  Ports   Protocol  Description         :   :    :         80, 443  TCP    API, WebSocket, TURN/TLS    Requirements for best experience  Ports          Protocol  Description           :       :    :          80, 443, 3478      TCP    API, WebSocket, TURN/TLS     443, 3478, 41000-65535  UDP    TURN/UDP, ICE, Media traffic    NAT gateway IP address whitelisting for webhooks Please refer to  how to secure webhooks section (/server-side/v2/introduction/webhook how-to-secure-webhooks) in webhooks guide fore more details. "
    },
    {
        "title": "Postman Guide",
        "link": "/server-side/v2/introduction/postman-guide",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/introduction/postman-guide",
        "keywords": [],
        "content": "  100ms collection includes API for rooms, sessions, policies, etc., packaged and ready to use with Postman, along with documentation for each API to help you learn about them. You can import/fork these APIs and start using them instantly. The API collection comes with predefined collection variables and a pre-request script that generates the management token to help you get started immediately.   Fork the collection    Run in Postman (https://run.pstmn.io/button.svg) (https://god.gw.postman.com/run-collection/22726679-47dcd974-29d5-4965-a35b-bf9b74a8b25a?action=collection%2Ffork&collection-url=entityId%3D22726679-47dcd974-29d5-4965-a35b-bf9b74a8b25a%26entityType%3Dcollection%26workspaceId%3Dd9145dd6-337b-4761-81d6-21a30b4147a2) You can click on this button to fork the 100ms API collection.   Authentication 100ms uses management tokens to authenticate REST APIs. To create the management token, you must use the App Access Key and App Secret from the  developer section (https://dashboard.100ms.live/developer) in your 100ms dashboard. This key and secret differ across workspaces so please ensure you are in the intended workspace before copying these credentials.   Copy app secret and key (/docs/v2/app-key-secret.png)   Simplified token generation We have simplified the process of management token generation for you within this collection:   All you have to do is update the   collection variables   APP_ACCESS_KEY and APP_SECRET ;   We have added a pre-request script within the collection, which will generate the management token and set the same as a collection variable (TOKEN) in the forked Postman collection.   Automatically, this will be used as authentication for all the APIs in the 100ms collection.   Add-token (/docs/v2/token.png) >   Note:   Refer to the  authentication and tokens guide (https://100ms.live/docs/server-side/v2/introduction/authentication-and-tokens code-sample-generate-management-token) for more generating the management token in your preferred programming language for your development/production environment. <Note type=\"warning\">   We strictly advise you not to post <i>APP_ACCESS_KEY</i> and <i>APP_SECRET</i> publicly; if you   need to store them in <strong>Git</strong>, please change the repository from public to private. </Note>   Manage authentication for multiple 100ms workspaces in Postman You can use the environment variables in Postman to maintain authentication for multiple 100ms workspaces. An example use case is managing API authentication for Development, QA, and Production workspaces. You can learn more about environment variables in Postman  here (https://learning.postman.com/docs/sending-requests/variables/). You can setup environment variables using the steps below: 1. Hover over to Environments in the Postman sidebar. Click on Create an Environment . 2. Name your Environment (for example, Dev Env) and add two variables APP_ACCESS_KEY and APP_SECRET . 3. Go to the  Developer section (https://dashboard.100ms.live/developer) of your workspace, copy APP_ACCESS_KEY and APP_SECRET , paste those values in the INITIAL VALUE column, and Save your Environment. 4. Head to your \"100ms API collection\" and switch your Environment to Dev Env to send API requests for the respective 100ms workspace. 5. Similarly, you create other environments (for example, QA and Production) for different workspaces to maintain ease of authentication access. <video loop=\"true\" autoplay=\"autoPlay\" controls=\"controls\" id=\"vid\" muted>   <source src=\"/docs/guides/env-vars.mp4\" type=\"video/mp4\"   </video> <br   >   Note:   These environments exist across your Postman workspaces and would be visible in the dropdown for other collections.   Example: Create a room Let's try to create a room to see how the 100ms collection works. From the forked REST API collection, select the folder named   Room APIs   and select the request to create room . Replace the placeholders like name, description, template_id, etc with the desired values. Click the Send button once done.   API Reference You can refer to the detailed  API reference (/server-side/v2/Rooms/object) to understand the request/response structure better, request arguments, response variables, webhooks, etc.   Stay up to date Similar to how the forked repository on GitHub works, you can stay up to date with the 100ms master collection by pulling the changes. In the forked collection of your Postman instance, Click on the 'three-dots(...) menu' next to the 'Save' button in the top right section and select Pull changes from the drop-down menu.   pulling the changes (/docs/v2/pull-changes.png) "
    },
    {
        "title": "Recordings",
        "link": "/server-side/v2/introduction/recordings",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/introduction/recordings",
        "keywords": [],
        "content": "  Recordings are an important part of the live video stack as they convert live, ephemeral content into a long-term asset. But the use of this asset varies from business to business depending on their respective use case. For example, one of the common use cases for recording is for archival purposes versus, for some, its content to be publicized. Based on your end goal, you can choose one of the recording types and its implementation. You can understand some key differences using the comparison table below.   Recording types   Recording types ( recording-types)    Quick Comparison ( quick-comparison)    Browser Recording  Recommended   ( browser-recording-recommended)    SFU Recording  Advanced   ( sfu-recording-advanced)    Recordings for Live Streaming Use-cases ( recordings-for-live-streaming-use-cases)     Video-on-demand Recording ( video-on-demand-recording)     Multiresolution Recording ( multiresolution-recording)   Configure storage ( configure-storage)    How to configure recording storage? ( how-to-configure-recording-storage)   Quick Comparison  Recording Features           Browser Recording  Recommended   SFU Recording  Advanced                                      Resolution               Upto 1080p            Only 720p              Participant-level Audio/Video Tracks  Not Available           Available              Portrait/Landscape Mode        Available             Not Available            Start/Stop Recording          On-demand             Auto start/stop with the session   Custom Layout             Available             Not Available            Role-Specific Recording        Available             Not Available            Recording Output            MP4                MP4, WebM                Browser Recording  Recommended  Browser recording is built to give users a participant-first recording experience. When enabled, our browser-based bot Beam joins a room to record the viewport like any other participant. The output is an MP4 file that captures the room's published audio/video tracks together into one single file. This option removes the complexity of syncing various audio/video tracks and offers an intuitive, participant-first recording experience. An example use case is to record a sales meeting for later usage.   Resources     How to implement Browser Recording (https://www.100ms.live/docs/server-side/v2/Destinations/rtmp-streaming-and-browser-recording)   SFU Recording  Advanced  SFU recording is built for advanced use cases, which require individual audio and video tracks for each participant. This recording option allows you to isolate recording at a participant level. Track recording allows you to record audio and video streams separately, making it easier to edit, layer, or reuse each of them. An example use case is to record a live podcast and later edit it for publishing. You can get track recordings in two forms:   Individual: Media for each peer is provided as a separate mp4 file. This file will have both audio and video of the peer. These files can be used for offline review or in implementing custom composition.   Composite  currently in beta : Audio and video of all peers are composed as per their joining/leaving the meeting and provided as a single mp4. This file can be used for offline viewing of the meeting.   Resources      How to implement SFU Recording (https://www.100ms.live/docs/server-side/v2/Destinations/recording)   Recordings for Live Streaming Use-cases These are the types of live streaming recordings:    Video-on-demand Recording Video-on-demand recording is available for our Interactive Live Streaming capability. This recording will be a file with an M3U8 file (same playback format as HLS), which can be used for replaying your HLS stream. This option is more suitable for Video-on-Demand use cases. For the implementation of this type of recording, please  contact us (https://www.100ms.live/contact).    Multiresolution Recording A multi-resolution recording is available for Interactive Live Streaming capability. This type of recording will have a multi-file structure for all available resolutions of the stream. The output will be multiple MP4 files with these resolutions: 240p, 480p, 720p, and 1080p. For an implementation of this type of recording, please  contact us (https://www.100ms.live/contact).   Configure storage You can specify a cloud storage location for your recording files in your template. Our current offering allows you to store your recordings in Amazon S3 buckets. Once you configure the S3 config of your bucket in a template, all respective recordings of sessions created via those templates will be sent to your configured bucket. This holds true for all types of aforementioned recordings.   How to configure recording storage? 1. Generate your credentials; for this example, you can check out a  guide from AWS (https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html). You can skip this step if you already have credentials. Please note that if you are running a Browser recording, you need to give upload permission to your key, but if you are running an SFU recording, you need to give both upload and download permission. 2. Go to 100ms Dashboard and go to template   configuration by selecting the configure icon  .   Create your first app (/docs/docs/v2/recording-storage-settings-step2.png) 3. Head over to the   Destinations   tab.   Destinations (/docs/docs/v2/recording-storage-settings-step3.png) 1. Key in your credentials (using an example of an S3 bucket here):    Access Key: Access Key generated from AWS IAM Console    Secret Key: Secret Key generated from AWS IAM Console    Bucket: Name of the bucket in S3    Region: Name of the region, for example, ap-south1    Prefix for Upload Path: Define the directory name (optional)   Destinations (/docs/docs/v2/recording-storage-settings-step4.png) 5. Use the   Validate Config   button to test your storage setup.   Destinations (/docs/docs/v2/recording-storage-settings-step5.png) 6. You will see a message that the AWS   configuration was successfully validated  .   Destinations (/docs/docs/v2/recording-storage-settings-step6.png) The above message ensures that your configuration is successful now, and all your recordings will start collecting in your configured destination. "
    },
    {
        "title": "Request/Response",
        "link": "/server-side/v2/introduction/request-and-response",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/introduction/request-and-response",
        "keywords": [],
        "content": "    Request Our REST APIs enable you to access various capabilities to complete your integration based on the requirements for your use case. <BaseRequest title=\"https://api.100ms.live/v2/\"   You can leverage these requests using the common HTTP methods: 1.   GET    retrieves data from 100ms server.   Example  : Retrieve details of a specific room 2.   POST    sends new data to 100ms server.   Example  : Create a room 3.   DELETE    removes existing data from 100ms server.   Example  : Delete a role from a template.   Content type The input of the request should be of the type application/json. The payload for all POST requests should be passed as a JSON object with the Content-Type as application/json . Most of the GET request arguments must be passed as a path param whereas some of the APIs accept it as a query param. DELETE request arguments should be passed as a path param.    Headers  Name         Value             Required   :      :        :     Content-type <br   application/json       Yes     Authorization <br   Bearer <management token>  Yes      Response 100ms API responds to all requests with a response in JSON format along with an HTTP status code to indicate whether the request was successful.   HTTP status codes All requests return one of these standard HTTP status codes  Code         Description                                                       200  OK       The request has succeeded.                      204  No Content   Resource deleted.                           401  Unauthorized  No valid API management token provided.                403  Forbidden   The API management token provided doesn't have enough permission.   404  Not Found   The request resource was not found.                  500  Server Error  Something went wrong with 100ms servers. (These are rare).        Errors   API responses with 4XX and 5XX HTTP status codes indicate that the request has failed.   4XX : This can be either 401 or 403 or 404. You can refer to the  status codes section ( http-status-codes) for more information. Also, the response body will contain the error message that will help to resolve the error and resend the request.      Example:     json         \"code\": 404,     \"message\": \"Error: Room not found\",     \"data\":         \"description\": \"Error: Room not found\"                 5XX : Server errors. This happens when something is wrong with 100ms servers(these are rare).   Pagination All API resources support bulk fetches via “list” API methods (For example, list all rooms). These APIs use query parameters  limit, start, and last  to support pagination. To fetch the first page of 10 objects, set limit=10 as a query param. This API will respond with a list of objects in key data . The response also has key last with the unique identifier for the last object in data . To fetch the second page of 10 objects (11 to 20), pass the previous last value as the start query param. Set limit = 10 and start = &lt;last value from previous response&gt; .   List API response variables  Name     Type     Description                                                        :    :    :                                limit <br   int     Number of objects the API tried to find. If returned array has lesser number of results, only those number were found.   data <br    array    Array of objects.                                                     last <br    ObjectID  The last object in the list of objects in the response. Can be used as start for subsequent request.            Params used for pagination  Name  Type    Description                                                :   :    :                            limit  int    A limit on the number of objects to be returned.<br  Default value : 10<br  Allowed range : 10-100   start  object  Determines the starting point in the list of objects to be returned in the response.           "
    },
    {
        "title": "Webhook",
        "link": "/server-side/v2/introduction/webhook",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/introduction/webhook",
        "keywords": [],
        "content": "    Introduction Webhook is an HTTP(S) endpoint used for pushing the notifications to your application. Event-based webhooks/user-defined HTTP webhooks, can be used to track the events in your room and build business logic, allowing you to create analytics on your traffic. Create an attendance system, for instance. Every session, 100ms sends a list of events to the URL you specify in the webhook configuration. For analytics and/or debugging, you can store these data on your server. To handle a webhook, you must create a listener to accept these HTTP requests from 100ms.   Requirements   Webhook MUST implement POST HTTP method for receiving notifications. 100ms notifications are in JSON format so it should be handled accordingly.   To handle webhooks in your app, your endpoint should:     Capture HTTP requests     Respond to the requests   When 100ms sends the HTTP request callbacks to the webhook during an event, you should capture the request and respond with a 200 OK response. You can store the webhook data in your database for later processing.   >   Note:   100ms automatically retries webhooks three times if your server does not return an HTTP 200 status code.   >   >   First at 1 second after the original attempt.   >   Second at 3 seconds after the first retry attempt.   >   Third at 10 seconds after the second retry attempt.   How to secure webhooks You can secure webhooks in two ways: 1. Whitelisting 100ms NAT gateway IP addresses, 2. Specifying unique headers while configuring webhook endpoint in 100ms.   IP whitelisting Your infrastructure might be secured by a firewall that monitors and filters incoming requests. To ensure that your firewall does not block the event-based webhooks sent by 100ms, allow traffic from these 100ms NAT gateway IP addresses to your webhook endpoint.   34.100.213.146/32 35.200.143.211/32 34.100.191.162/32 34.100.132.35/32 34.93.93.114/32 34.131.109.150/32 34.131.52.47/32 34.131.200.41/32 34.131.13.182/32 34.131.24.136/32 34.138.143.222/32 35.242.196.203/32 35.200.222.156/32 34.93.74.33/32 34.93.142.55/32 34.93.176.177/32 34.93.210.177/32 34.93.175.47/32 129.154.236.15/32     Specifying headers Additionally, you can specify headers that will be passed transparently to your webhook endpoint. This can be used for securing or tracing the origin of the request.   How to configure Webhook Webhook can be configured using  Developer (https://dashboard.100ms.live/developer) section of  100ms Dashboard (https://dashboard.100ms.live/). Each workspace can have its own webhook configuration.   Events Event is a JSON dictionary which has the following keys.   Event Attributes  Name     Type          Description                                                     :    :       :                               version    string        Version of the event <br <br  Example: 2.0                                     id       string        Id of the event <br <br  Example: bd0c76fd-1ab1-4d7d-ab8d-bbfa74b620c4                       app_id     string        App ID from which this event is generated <br <br  Example: 5ff5881b80b66969e1fb35f6                account_id   string        Customer ID from which this event is generated <br <br  Example: 5ff5881b80b66969e1fb35f4             template_id  string        Template ID of the room <br <br  Example: 66112497abcd52312556c4gg                         timestamp   timestamp (in UTC)  Timestamp of the event <br <br  Example: 2020-11-11T16:32:17Z                           type      string        Type of the event <br <br  Example: peer.join.success                               data      dict         Event data. Its keys will be different for different type of event.<br <br  Example: < See description below >    Sample Event Payload   json     \"version\": \"2.0\",   \"id\": \"6c75de35-b778-4fb0-88ae-114c28dc285f\",   \"account_id\": \"60b8e13a6eb86d8101b57354\",   \"timestamp\": \"2021-08-08T07:04:15Z\",   \"type\": \"peer.join.success\",   \"data\":       \"joined_at\": \"2021-08-08T07:04:15.001380432Z\",     \"peer_id\": \"83b869e1-9a4b-4037-84b2-913cf76e4392\",     \"role\": \"host\",     \"room_id\": \"60b8e13a6eb86d8101b57354\",     \"room_name\": \"test room\",     \"session_id\": \"610f81ee870dde099a249948\",     \"template_id\": \"66112497abcd52312556c4gg\",     \"user_id\": \"user.001\",     \"user_name\": \"test user\"           List of events Here's the list of events available on the 100ms platform.  Event name              Description                                                                                                                                           :          :                                                                          session               Triggered during the start and end of a session. <br <br   Events:    session.open.success ( session-open-success),  session.close.success ( session-close-success)                                                              peer                 Triggered when a peer join/leave succeeds/fails. <br <br   Events:    peer.join.success ( peer-join-success),  peer.leave.success ( peer-leave-success),  peer.join.failure ( peer-join-failure),  peer.leave.failure ( peer-leave-failure)                          room                 Triggered when a room ends. <br <br   Events:    room.end.success ( room-end-success)                                                                                                     SFU recording            Triggered during the start, end, and failure of a SFU recording. <br <br   Events:    recording.success ( recording-success),  recording.failed ( recording-failed)                                                              RTMP Streaming & Browser Recording  Triggered during the start, end, and failure of RTMP streaming and/or browser recording.<br <br   Events:    beam.started.success ( beam-started-success),  beam.stopped.success ( beam-stopped-success),  beam.recording.success ( beam-recording-success),  beam.failure ( beam-failure)   HLS Streaming Events         Triggered during the start, end, and failure of HLS streaming and/or HLS recording.<br <br   Events:    hls.started.success ( hls-started-success),  hls.stopped.success ( hls-stopped-success),  hls.recording.success ( hls-recording-success),  hls.failure ( hls-failure)         Role change Events          Triggered when a role is updated. <br <br   Events:    role.change.success ( role-change-success)                                                                                                Session Events   session.open.success This event will be sent when session opens successfully.    Attributes  Name         Type          Description                                        :      :       :                        room_id        string        100ms assigned room id <br <br  Example: 5f9edc6ac238215aec2312df            room_name       string        Room name provided when creating the room <br <br  Example: Test Room          session_id      string        100ms assigned id to identify the session <br <br  Example: 5f9edc6bd238215aec7700df   template_id      string        Template ID of the room <br <br  Example: 66112497abcd52312556c4gg            session_started_at  timestamp (in UTC)  Timestamp when session started <br <br  Example: 2020-11-11T16:32:17Z            Sample session.open.success event   json     \"version\": \"2.0\",   \"id\": \"        -    -    -    -           \",   \"account_id\": \"                        \",   \"app_id\": \"                        \",   \"timestamp\": \"2021-11-30T12:58:49Z\",   \"type\": \"session.open.success\",   \"data\":       \"room_id\": \"                        \",     \"room_name\": \"          \",     \"session_id\": \"                        \",     \"template_id\": \"                        \",     \"session_started_at\": \"2021-11-30T12:58:49.97291247Z\"           session.close.success This event will be sent when session closes successfully.    Attributes  Name         Type          Description                                        :      :       :                        room_id        string        100ms assigned room id <br <br  Example: 5f9edc6ac238215aec2312df            room_name       string        Room name provided when creating the room <br <br  Example: Test Room          session_id      string        100ms assigned id to identify the session <br <br  Example: 5f9edc6bd238215aec7700df   template_id      string        Template ID of the room <br <br  Example: 66112497abcd52312556c4gg            session_duration   int          Duration the user spent in the room in seconds <br <br  Example: 36000          session_started_at  timestamp (in UTC)  Timestamp when session started <br <br  Example: 2020-11-11T16:32:17Z          session_stopped_at  timestamp (in UTC)  Timestamp when session ended <br <br  Example: 2020-11-11T16:32:17Z             Sample session.close.success event   json     \"version\": \"2.0\",   \"id\": \"        -    -    -    -           \",   \"account_id\": \"                        \",   \"app_id\": \"                        \",   \"timestamp\": \"2021-11-30T12:58:49Z\",   \"type\": \"session.close.success\",   \"data\":       \"room_id\": \"                        \",     \"room_name\": \"          \",     \"session_id\": \"                        \",     \"template_id\": \"                        \",     \"session_duration\": 600,     \"session_started_at\": \"2021-11-30T12:48:49.97291247Z\",     \"session_stopped_at\": \"2021-11-30T12:58:49.97291247Z\"           Peer Events   peer.join.success This event will be sent when any peer joins the room successfully    Attributes  Name         Type          Description                                                 :      :       :                            room_id        string        100ms assigned room id <br <br  Example: 5f9edc6ac238215aec2312df                     room_name       string        Room name provided when creating the room <br <br  Example: Test Room                   session_id      string        100ms assigned id to identify the session <br <br  Example: 5f9edc6bd238215aec7700df           peer_id        string        100ms assigned id to identify the joining user <br <br  Example: bd0c76fd-1ab1-4d7d-ab8d-bbfa74b620c4   user_id        string        User id assigned by the customer <br <br  Example: user.001                        template_id      string        Template ID of the room <br <br  Example: 66112497abcd52312556c4gg                    user_name       string        User name of the joining user <br <br  Example: Test user                         user_data       string        User data of the joining user <br <br  Example:  \"isHandRaised\":true                    role         string        Role of the joining user <br <br  Example: host                              joined_at       timestamp (in UTC)  Timestamp when user joined <br <br  Example: 2020-11-11T16:32:17Z                     session_started_at  timestamp (in UTC)  Timestamp when session started <br <br  Example: 2020-11-11T16:32:17Z                     Sample peer.join.success event   json     \"version\": \"2.0\",   \"id\": \"        -    -    -    -           \",   \"account_id\": \"                        \",   \"timestamp\": \"2021-11-30T12:58:49Z\",   \"type\": \"peer.join.success\",   \"data\":       \"joined_at\": \"2021-11-30T12:58:49.97291247Z\",     \"peer_id\": \"        -    -    -    -           \",     \"role\": \"        \",     \"room_id\": \"                        \",     \"room_name\": \"          \",     \"session_id\": \"                        \",     \"template_id\": \"                        \",     \"user_id\": \"                        \",     \"user_name\": \"        \",     \"user_data\": \"\",     \"session_started_at\": \"2021-11-30T12:58:49.97291247Z\"           peer.leave.success This event will be sent when peer leaves the room    Attributes  Name         Type          Description                                                        :      :       :                                room_id        string        100ms assigned room id <br <br  Example: 5f9edc6ac238215aec2312df                            room_name       string        Room name provided when creating the room <br <br  Example: Test Room                          session_id      string        100ms assigned id to identify the session <br <br  Example: 5f9edc6bd238215aec7700df                  peer_id        string        100ms assigned id to identify the joining user <br <br  Example: bd0c76fd-1ab1-4d7d-ab8d-bbfa74b620c4          user_id        string        User id assigned by the customer <br <br  Example: user.001                               template_id      string        Template ID of the room <br <br  Example: 66112497abcd52312556c4gg                           user_name       string        User name of the user <br <br  Example: Test user                                    user_data       string        User data of the user <br <br  Example:  \"isHandRaised\":true                               role         string        Role of the user <br <br  Example: host                                         left_at        timestamp (in UTC)  Timestamp when user left <br <br  Example: 2020-11-11T17:32:17Z                             duration       int          Duration the user spent in the room in seconds <br <br  Example: 36000                         reason        string        Reason for the peer leaving, see more details below <br <br  Example: client request                  message        string        Reason specified while kicking peer out of room, see more details below <br <br  Example: removed due to misconduct   joined_at       timestamp (in UTC)  Timestamp when user joined <br <br  Example: 2020-11-11T16:32:17Z                            session_started_at  timestamp (in UTC)  Timestamp when session started <br <br  Example: 2020-11-11T16:32:17Z                            Peer Leave Reason 1. client request : if the peer leaves the room (leave request is sent from the app/client-side).    _Example scenarios:_      If the peer clicks the Leave/End button.      If the peer refreshes or closes the tab while using one of 100ms demo links ( https://<YOUR_SUB_DOMAIN>.app.100ms.live/preview/nnd-eke-tpp ) 2. peer kicked : if the peer is removed by someone else from an active room using the  Client SDK (/javascript/v2/features/remove-peer) or the  Server API (/server-side/v2/active-rooms/remove-peers).    _Example scenarios:_      If the peer is removed by a role which has Can remove participant from the room  permissions (./../policy/create-template-via-dashboard permissions) enabled in the template.      If the peer is removed using the  remove peer API (./../active-rooms/remove-peers) from the application server. <br           Note  : If you remove a peer from an  active room (./../active-rooms/overview) using the       client SDK (/javascript/v2/features/remove-peer) or the  server API (/server-side/v2/active-rooms/remove-peers),      you can pass the reason as a message and the peer.leave.success event will contain the      same in the message field. 3. websocket closed : network issues    _Example scenario:_      If there's an abrupt network disconnection from the client side (peer) without any further attempts to reconnect, the 100ms server will close the WebSocket connection after 60 seconds. This can happen if the app is forcefully closed or the network is disconnected. 4. ice connection state closed : network issues    _Example scenario:_      Suppose there's an abrupt network disconnection at the media transport level from the client side (peer) without any further attempts to reconnect. In that case, the 100ms server will close the WebSocket connection after 60 seconds. This can happen if network config changes prohibit media packets from being sent/received by the client. Possible causes could be firewall changes or connection overwhelming because of higher downloads than the network can handle. 5. server removed peer : network issues    _Example scenario:_      If there are any server-side anomalies, like a server failure mid-way into the call, it will remove peers with this error. The occurrence of this is rare.    Sample peer.leave.success event   json     \"version\": \"2.0\",   \"id\": \"        -    -    -    -           \",   \"account_id\": \"                        \",   \"timestamp\": \"2021-11-30T12:58:58Z\",   \"type\": \"peer.leave.success\",   \"data\":       \"duration\": 40,     \"joined_at\": \"2021-11-30T12:58:49.97291247Z\",     \"left_at\": \"2021-11-30T12:58:58.500451704Z\",     \"peer_id\": \"        -    -    -    -           \",     \"reason\": \"client request\",     \"message\": \"removed due to misconduct\",     \"role\": \"        \",     \"room_id\": \"                        \",     \"room_name\": \"          \",     \"session_id\": \"                        \",     \"template_id\": \"                        \",     \"user_id\": \"                        \",     \"user_name\": \"        \",     \"user_data\": \"\",     \"session_started_at\": \"2021-11-30T12:58:49.97291247Z\"           peer.join.failure This event will be sent when a peer fails to join a room. This can occur when, 1. Server is overloaded and results in timeout 2. Network disconnection    Attributes  Name      Type          Description                                                 :     :       :                            room_id     string        100ms assigned room id <br <br  Example: 5f9edc6ac238215aec2312df                     room_name    string        Room name provided when creating the room <br <br  Example: Test Room                   peer_id     string        100ms assigned id to identify the joining user <br <br  Example: bd0c76fd-1ab1-4d7d-ab8d-bbfa74b620c4   user_id     string        User id assigned by the customer <br <br  Example: user.001                        template_id   string        Template ID of the room <br <br  Example: 66112497abcd52312556c4gg                    user_name    string        User name of the user <br <br  Example: Test user                             user_data    string        User data of the user <br <br  Example:  \"isHandRaised\":true                        role       string        Role of the user <br <br  Example: host                                  joined_at    timestamp (in UTC)  Timestamp when user joined <br <br  Example: 2020-11-11T16:32:17Z                     error_message  string        Reason for failure <br <br  Example: Peer not joined                             Peer join failure Reason 1. role not allowed 2. network disconnection 3. duplicate peer id    Sample peer.join.failure event   json     \"version\": \"2.0\",   \"id\": \"        -    -    -    -           \",   \"account_id\": \"                        \",   \"timestamp\": \"2021-11-25T09:10:35Z\",   \"type\": \"peer.join.failure\",   \"data\":       \"joined_at\": \"0001-01-01T00:00:00Z\",     \"peer_id\": \"        -    -    -    -           \",     \"role\": \"        \",     \"room_id\": \"                        \",     \"room_name\": \"          \",     \"user_id\": \"                        \",     \"template_id\": \"                        \",     \"user_name\": \"        \",     \"user_data\": \"\",     \"error_message\": \"role not allowed\"           peer.leave.failure This event will be sent when the peer leave fails. This can occur when, 1. peer.leave is called twice 2. peer.leave is called before peer.join or if peer.join has failed 3. peer.leave is called after peer is kicked out of room / room has ended    Attributes  Name      Type          Description                                                 :     :       :                            room_id     string        100ms assigned room id <br <br  Example: 5f9edc6ac238215aec2312df                     room_name    string        Room name provided when creating the room <br <br  Example: Test Room                   peer_id     string        100ms assigned id to identify the joining user <br <br  Example: bd0c76fd-1ab1-4d7d-ab8d-bbfa74b620c4   user_id     string        User id assigned by the customer <br <br  Example: user.001                        template_id   string        Template ID of the room <br <br  Example: 66112497abcd52312556c4gg                    user_name    string        User name of the user <br <br  Example: Test user                             user_data    string        User data of the user <br <br  Example:  \"isHandRaised\":true                        role       string        Role of the user <br <br  Example: host                                  left_at     timestamp (in UTC)  Timestamp when user left <br <br  Example: 2020-11-11T17:32:17Z                      duration     int          Duration the user spent in the room in seconds <br <br  Example: 36000                  error_message  string        Reason for failure <br <br  Example: Peer not joined                           joined_at    timestamp (in UTC)  Timestamp when user joined <br <br  Example: 2020-11-11T16:32:17Z                       Peer leave failure Reason 1. peer not joined    Sample peer.leave.failure event   json     \"version\": \"2.0\",   \"id\": \"        -    -    -    -           \",   \"account_id\": \"                        \",   \"timestamp\": \"2021-11-30T12:55:51Z\",   \"type\": \"peer.leave.failure\",   \"data\":       \"duration\": 0,     \"left_at\": \"0001-01-01T00:00:00Z\",     \"joined_at\": \"2021-11-30T12:58:49.97291247Z\",     \"peer_id\": \"        -    -    -    -           \",     \"role\": \"        \",     \"room_id\": \"                        \",     \"room_name\": \"          \",     \"user_id\": \"                        \",     \"template_id\": \"                        \",     \"user_name\": \"        \",     \"user_data\": \"\",     \"error_message\": \"Peer not joined\"           Room Events   room.end.success This event will be sent when room end is called and then it is successful.    Attributes  Name         Type          Description                                                 :      :       :                            room_id        string        100ms assigned room id <br <br  Example: 5f9edc6ac238215aec2312df                     room_name       string        Room name provided when creating the room <br <br  Example: Test Room                   session_id      string        100ms assigned id to identify the session <br <br  Example: 5f9edc6bd238215aec7700df           peer_id        string        100ms assigned id to identify the joining user <br <br  Example: bd0c76fd-1ab1-4d7d-ab8d-bbfa74b620c4   user_id        string        User id assigned by the customer <br <br  Example: user.001                        template_id      string        Template ID of the room <br <br  Example: 66112497abcd52312556c4gg                    user_name       string        User name of the user <br <br  Example: Test user                             role         string        Role of the user <br <br  Example: host                                  lock_room       bool         Flag to indicate if the room was lock <br <br  Example: false                       reason        string        Reason specified with end room call <br <br  Example: End Room by admin                  source        string        Source of end room api call <br <br  Example: peer                            session_started_at  timestamp (in UTC)  Timestamp when session started <br <br  Example: 2020-11-11T16:32:17Z                     Sample room.end.success event   json     \"version\": \"2.0\",   \"id\": \"        -    -    -    -           \",   \"account_id\": \"                        \",   \"timestamp\": \"2021-11-25T09:10:35Z\",   \"type\": \"room.end.success\",   \"data\":       \"peer_id\": \"        -    -    -    -           \",     \"role\": \"        \",     \"room_id\": \"                        \",     \"room_name\": \"          \",     \"session_id\": \"                        \",     \"template_id\": \"                        \",     \"user_id\": \"                        \",     \"user_name\": \"        \",     \"lock_room\": false,     \"reason\": \"                \",     \"source\": \"peer\",     \"session_started_at\": \"2021-11-30T12:58:49.97291247Z\"           room.end.failure This event will be sent when room end is called and the it is unsuccessful.    Attributes  Name         Type          Description                                                 :      :       :                            room_id        string        100ms assigned room id <br <br  Example: 5f9edc6ac238215aec2312df                     room_name       string        Room name provided when creating the room <br <br  Example: Test Room                   session_id      string        100ms assigned id to identify the session <br <br  Example: 5f9edc6bd238215aec7700df           peer_id        string        100ms assigned id to identify the joining user <br <br  Example: bd0c76fd-1ab1-4d7d-ab8d-bbfa74b620c4   user_id        string        User id assigned by the customer <br <br  Example: user.001                        template_id      string        Template ID of the room <br <br  Example: 66112497abcd52312556c4gg                    user_name       string        User name of the user <br <br  Example: Test user                             role         string        Role of the user <br <br  Example: host                                  lock_room       bool         Flag to indicate if the room was lock <br <br  Example: false                       reason        string        Reason specified with end room call <br <br  Example: End Room by admin                  source        string        Source of end room api call <br <br  Example: peer                            session_started_at  timestamp (in UTC)  Timestamp when session started <br <br  Example: 2020-11-11T16:32:17Z                   error_message     string        Error message to indicate the reason of failure <br <br  Example: session not found            error_code      int          Error code for failure <br <br  Example: 404                                 Sample room.end.success event   json     \"version\": \"2.0\",   \"id\": \"        -    -    -    -           \",   \"account_id\": \"                        \",   \"timestamp\": \"2021-11-25T09:10:35Z\",   \"type\": \"room.end.success\",   \"data\":       \"peer_id\": \"        -    -    -    -           \",     \"role\": \"        \",     \"room_id\": \"                        \",     \"room_name\": \"          \",     \"session_id\": \"                        \",     \"template_id\": \"                        \",     \"user_id\": \"                        \",     \"user_name\": \"        \",     \"lock_room\": false,     \"reason\": \"                \",     \"source\": \"peer\",     \"session_started_at\": \"2021-11-30T12:58:49.97291247Z\",     \"error_code\": 404,     \"error_message\": \"session not found\"           Role change Events   role.change.success This event will be sent when the role change for a peer is successful.    Attributes  Name         Type          Description                                                 :      :       :                            room_id        string        100ms assigned room id <br <br  Example: 5f9edc6ac238215aec2312df                     room_name       string        Room name provided when creating the room <br <br  Example: Test Room                   session_id      string        100ms assigned id to identify the session <br <br  Example: 5f9edc6bd238215aec7700df           template_id      string        Template ID of the room <br <br  Example: 66112497abcd52312556c4gg                    peer_id        string        100ms assigned id to identify the joining user <br <br  Example: bd0c76fd-1ab1-4d7d-ab8d-bbfa74b620c4   user_id        string        User id assigned by the customer <br <br  Example: user.001                        user_name       string        User name of the user <br <br  Example: Test user                             user_data       string        User data of the user <br <br  Example:  \"isHandRaised\":true                        previous_role     string        Previous role of the peer <br <br  Example: host                             role         string        New role of the user <br <br  Example: host                                joined_at       timestamp (in UTC)  Timestamp when user joined <br <br  Example: 2020-11-11T16:32:17Z                     session_started_at  timestamp (in UTC)  Timestamp when session started <br <br  Example: 2020-11-11T16:32:17Z                   role_changed_at    timestamp (in UTC)  Timestamp when role changed <br <br  Example: 2020-10-11T16:32:17Z                      Sample role.change.success event   json     \"version\": \"2.0\",   \"id\": \"        -    -    -    -           \",   \"account_id\": \"                        \",   \"app_id\": \"                        \",   \"timestamp\": \"2021-11-30T12:58:49Z\",   \"type\": \"role.change.success\",   \"data\":       \"joined_at\": \"2021-11-30T12:58:49.97291247Z\",     \"peer_id\": \"        -    -    -    -           \",     \"role\": \"        \",     \"room_id\": \"                        \",     \"room_name\": \"          \",     \"session_id\": \"                        \",     \"template_id\": \"                        \",     \"user_id\": \"                        \",     \"user_name\": \"        \",     \"user_data\": \"\",     \"previous_role\": \"        \",     \"session_started_at\": \"2021-11-30T12:48:49.97291247Z\",     \"role_changed_at\": \"2021-11-30T12:58:49.97291247Z\"           role.change.failure This event will be sent when the role change for a peer fails. For example: 1. When there's an attempt to change peer's role to a specific role whose limit has already been reached.    Attributes  Name         Type          Description                                                 :      :       :                            room_id        string        100ms assigned room id <br <br  Example: 5f9edc6ac238215aec2312df                     room_name       string        Room name provided when creating the room <br <br  Example: Test Room                   session_id      string        100ms assigned id to identify the session <br <br  Example: 5f9edc6bd238215aec7700df           template_id      string        Template ID of the room <br <br  Example: 66112497abcd52312556c4gg                    peer_id        string        100ms assigned id to identify the joining user <br <br  Example: bd0c76fd-1ab1-4d7d-ab8d-bbfa74b620c4   user_id        string        User id assigned by the customer <br <br  Example: user.001                        user_name       string        User name of the user <br <br  Example: Test user                             user_data       string        User data of the user <br <br  Example:  \"isHandRaised\":true                        role         string        New role of the user <br <br  Example: host                                joined_at       timestamp (in UTC)  Timestamp when user joined <br <br  Example: 2020-11-11T16:32:17Z                     session_started_at  timestamp (in UTC)  Timestamp when session started <br <br  Example: 2020-11-11T16:32:17Z                   error_message     string        Role change error message (description) <br <br  Example: role limit reached                 Sample role.change.failure event   json     \"version\": \"2.0\",   \"id\": \"        -    -    -    -           \",   \"account_id\": \"                        \",   \"app_id\": \"                        \",   \"timestamp\": \"2021-11-30T12:58:49Z\",   \"type\": \"role.change.failure\",   \"data\":       \"peer_id\": \"        -    -    -    -           \",     \"role\": \"        \",     \"room_id\": \"                        \",     \"room_name\": \"          \",     \"session_id\": \"                        \",     \"template_id\": \"                        \",     \"user_id\": \"                        \",     \"user_name\": \"        \",     \"user_data\": \"\",     \"session_started_at\": \"2021-11-30T12:48:49.97291247Z\",     \"joined_at\": \"2021-11-30T12:58:49.97291247Z\",     \"error_message\": \"role limit reached\"           SFU Recording Events   recording.success This event will be sent when final composed recording is generated and uploaded to the destination    Attributes  Name           Type          Description                                                                             :       :       :                                          room_id          string        100ms assigned room id <br <br  Example: 5f9edc6ac238215aec2312dff                                                room_name         string        Room name assigned by you when creating room <br <br  Example: class-9-batch-2                                          session_id         string        100ms assigned id to identify the session <br <br  Example: 5f9edc6bd238215aec7700df                                       template_id        string        Template ID of the room <br <br  Example: 66112497abcd52312556c4gg                                                location          string        (Deprecated, use recording_path) URI of the recorded video along with the storage type<br <br  Example: s3://bucket/prefix/ac.mp4                 location          string        (Deprecated, use recording_presigned_url) HTTPS url to recorded session file on storage bucket <br <br  Example: https://upload-location/bucket/beam/ac.mp4     duration          int          Duration the user spent in the room in seconds <br <br  Example: 3600                                               recording_path       string        Upload path of the recorded video such as s3 URI <br <br  Example: s3://bucket/prefix/ac.mp4                                   recording_presigned_url  string        Presigned URL for the recorded video, for download. Valid for 24 hours <br <br  Example: https://upload-location/bucket/ac.mp4                  size            int          Size of the recorded video (in bytes) <br <br  Example: 10024                                                   session_started_at     timestamp (in UTC)  Timestamp when session started <br <br  Example: 2020-11-11T16:32:17Z                                               session_stopped_at     timestamp (in UTC)  Timestamp when session ended <br <br  Example: 2020-11-11T16:32:17Z                                                  Sample recording.success event   json     \"version\": \"1.0\",   \"id\": \"        -    -    -    -           \",   \"app_id\": \"                        \",   \"account_id\": \"                        \",   \"timestamp\": \"2021-11-30T20:12:35Z\",   \"type\": \"recording.success\",   \"data\":       \"duration\": 600,     \"room_id\": \"                        \",     \"room_name\": \"TestRoom1\",     \"session_id\": \"                        \",     \"template_id\": \"                        \",     \"recording_path\": \"s3://<file-bucket-address>.mp4\",     \"recording_presigned_url\": \"https://<file-access-url>?<signature>\",     \"size\": 13933649,     \"session_started_at\": \"2021-11-30T12:48:49.97291247Z\",     \"session_stopped_at\": \"2021-11-30T12:58:49.97291247Z\"           recording.failed This event will be sent when failure occurs during final recording composition or upload to storage    Attributes  Name         Type          Description                                        :      :       :                        room_id        string        100ms assigned room id <br <br  Example: 5f9edc6ac238215aec2312df            room_name       string        Room name assigned by you when creating room <br <br  Example: class-9-batch-2      session_id      string        100ms assigned id to identify the session <br <br  Example: 5f9edc6bd238215aec7700df   template_id      string        Template ID of the room <br <br  Example: 66112497abcd52312556c4gg            error         string        Error message <br <br  Example: Upload Failure                      session_started_at  timestamp (in UTC)  Timestamp when session started <br <br  Example: 2020-11-11T16:32:17Z          session_stopped_at  timestamp (in UTC)  Timestamp when session ended <br <br  Example: 2020-11-11T16:32:17Z             Sample recording.failed event   json     \"version\": \"1.0\",   \"id\": \"        -    -    -    -           \",   \"app_id\": \"                        \",   \"account_id\": \"                        \",   \"timestamp\": \"2021-11-23T09:04:25Z\",   \"type\": \"recording.failed\",   \"data\":       \"error\": \"Upload Failure\",     \"room_id\": \"                        \",     \"room_name\": \"TestRoom1\",     \"session_id\": \"                        \",     \"template_id\": \"                        \",     \"session_started_at\": \"2021-11-30T12:48:49.97291247Z\",     \"session_stopped_at\": \"2021-11-30T12:58:49.97291247Z\"           RTMP Streaming & Browser Recording Events   beam.started.success This event is sent when RTMP streaming and/or browser recording is successfully triggered    Attributes  Name         Type          Description                                                 :      :       :                            beam_id        string        Unique beam id <br <br  Example: 61d3def54b616982bd80ed83                         job_id        string        Beam job id <br <br  Example: 60b8e1d96eb86d8101b57359                          recording_enabled   bool         Indicates whether recording is enabled or not <br <br  Example: true                   room_id        string        100ms assigned room id <br <br  Example: 5f9edc6ac238215aec2312df                     peer_id        string        100ms assigned id to identify the joining user <br <br  Example: bd0c76fd-1ab1-4d7d-ab8d-bbfa74b620c4   session_id      string        100ms assigned id to identify the session <br <br  Example: 5f9edc6bd238215aec7700df           metadata_id      string        Webhook metadata ID <br <br  Example: 14f350f5-18c4-46ca-8a33-71cbcc836600                template_id      string        Template ID of the room <br <br  Example: 66112497abcd52312556c4gg                    metadata_timestamp  timestamp (in UTC)  Webhook message creation timestamp <br <br  Example: 2020-11-11T17:32:17Z                 start_delay      int          Time taken by beam to start recording <br <br  Example: 12                        state_name      string        Beam state identifier <br <br  Example: Started                              state_timestamp    timestamp (in UTC)  Timestamp at which beam state changed <br <br  Example: 2020-11-11T17:32:18Z               max_width       int          Maximum width of the screen supported for streaming / recording in pixels <br <br  Example: 1280     max_height      int          Maximum height of the screen supported for streaming / recording in pixels <br <br  Example: 720     meeting_url      string        meeting_url provided at rtmp start <br <br  Example: \"https://app.100ms.live/room_id\"           rtmp         array         List of RTMP objects provided at rtmp start <br <br  Example:   \"url\": \"http://test.com\"          session_started_at  timestamp (in UTC)  Timestamp when session started <br <br  Example: 2020-11-11T16:32:17Z                     Sample beam.started.success event   json     \"version\": \"2.0\",   \"id\": \"        -    -    -    -           \",   \"account_id\": \"                        \",   \"timestamp\": \"2021-11-30T12:58:46Z\",   \"type\": \"beam.started.success\",   \"data\":       \"beam_id\": \"                        \",     \"job_id\": \"                        \",     \"metadata_id\": \"        -    -    -    -           \",     \"metadata_timestamp\": \"2021-11-30T12:58:46.400759043Z\",     \"peer_id\": \"        -    -    -    -           \",     \"recording_enabled\": true,     \"room_id\": \"                        \",     \"session_id\": \"                        \",     \"template_id\": \"                        \",     \"start_delay\": 12,     \"state_name\": \"Started\",     \"state_timestamp\": \"2021-11-30T12:58:46.385555859Z\",     \"max_width\": 1280,     \"max_height\": 720,     \"meeting_url\": \"https://app.100ms.live/preview/rpe-pwl-akt?token=beam_recording\",     \"rtmp\":    \"url\": \"http://test.com\"   ,     \"session_started_at\": \"2021-11-30T12:58:49.97291247Z\"           beam.stopped.success This event is sent when RTMP streaming and/or browser recording is successfully stopped    Attributes  Name         Type          Description                                                 :      :       :                            beam_id        string        Unique beam id <br <br  Example: 61d3def54b616982bd80ed83                         job_id        string        Beam job id <br <br  Example: 60b8e1d96eb86d8101b57359                          recording_enabled   bool         Indicates whether recording is enabled or not <br <br  Example: true                   room_id        string        100ms assigned room id <br <br  Example: 5f9edc6ac238215aec2312df                     peer_id        string        100ms assigned id to identify the joining user <br <br  Example: bd0c76fd-1ab1-4d7d-ab8d-bbfa74b620c4   session_id      string        100ms assigned id to identify the session <br <br  Example: 5f9edc6bd238215aec7700df           metadata_id      string        Webhook metadata ID <br <br  Example: 14f350f5-18c4-46ca-8a33-71cbcc836600                template_id      string        Template ID of the room <br <br  Example: 66112497abcd52312556c4gg                    metadata_timestamp  timestamp (in UTC)  Webhook message creation timestamp <br <br  Example: 2020-11-11T17:32:17Z                 state_name      string        Beam state identifier <br <br  Example: Stopped                              state_timestamp    timestamp (in UTC)  Timestamp at which beam state changed <br <br  Example: 2020-11-11T17:32:18Z               duration       int          Duration of RTMP streaming / recording in seconds <br <br  Example: 12                  max_width       int          Maximum width of the screen supported for streaming / recording in pixels <br <br  Example: 1280     max_height      int          Maximum height of the screen supported for streaming / recording in pixels <br <br  Example: 720     meeting_url      string        meeting_url provided at rtmp start <br <br  Example: \"https://app.100ms.live/room_id\"           rtmp         array         List of RTMP objects provided at rtmp start <br <br  Example:   \"url\": \"http://test.com\"          session_started_at  timestamp (in UTC)  Timestamp when session started <br <br  Example: 2020-11-11T16:32:17Z                     Sample beam.stopped.success event   json     \"version\": \"2.0\",   \"id\": \"        -    -    -    -           \",   \"account_id\": \"                        \",   \"timestamp\": \"2021-11-30T12:59:57Z\",   \"type\": \"beam.stopped.success\",   \"data\":       \"beam_id\": \"                        \",     \"job_id\": \"                        \",     \"metadata_id\": \"        -    -    -    -           \",     \"metadata_timestamp\": \"2021-11-30T12:59:57.797972469Z\",     \"peer_id\": \"        -    -    -    -           \",     \"recording_enabled\": true,     \"room_id\": \"                        \",     \"session_id\": \"                        \",     \"template_id\": \"                        \",     \"state_name\": \"Stopped\",     \"state_timestamp\": \"2021-11-30T12:59:57.685503281Z\",     \"duration\": 56,     \"max_height\": 720,     \"max_width\": 1280,     \"meeting_url\": \"https://app.100ms.live/preview/rpe-pwl-akt?token=beam_recording\",     \"rtmp\":    \"url\": \"http://test.com\"   ,     \"session_started_at\": \"2021-11-30T12:58:49.97291247Z\"           beam.recording.success This event is sent when beam successfully records the room and uploads the video to storage    Attributes  Name           Type          Description                                                                             :       :       :                                          beam_id          string        Unique beam id <br <br  Example: 61d3def54b616982bd80ed83                                                     room_id          string        100ms assigned room id <br <br  Example: 5f9edc6ac238215aec2312df                                                 peer_id          string        100ms assigned id to identify the joining user <br <br  Example: bd0c76fd-1ab1-4d7d-ab8d-bbfa74b620c4                               metadata_id        string        Webhook metadata ID <br <br  Example: 14f350f5-18c4-46ca-8a33-71cbcc836600                                            metadata_timestamp     timestamp (in UTC)  Webhook message creation timestamp <br <br  Example: 2020-11-11T17:32:17Z                                             session_id         string        100ms assigned id to identify the session <br <br  Example: 5f9edc6bd238215aec7700df                                       template_id        string        Template ID of the room <br <br  Example: 66112497abcd52312556c4gg                                                created_at         timestamp (in UTC)  Timestamp at which recording was created <br <br  Example: 2020-11-11T17:12:17Z                                          duration          int          Duration of beam recording (seconds) <br <br  Example: 79                                                     location          string        (Deprecated, use recording_presigned_url) HTTPS url to recorded session file on storage bucket <br <br  Example: https://upload-location/bucket/beam/ac.mp4     started_at         timestamp (in UTC)  Beam recording started at <br <br  Example: 2020-11-11T17:12:27Z                                                 stopped_at         timestamp (in UTC)  Beam recording stopped at <br <br  Example: 2020-11-11T17:32:15Z                                                 max_width         int          Maximum width of the screen supported for recording in pixels <br <br  Example: 1280                                       max_height         int          Maximum height of the screen supported for recording in pixels <br <br  Example: 720                                       recording_path       string        Upload path of the recorded video such as s3 URI <br <br  Example: s3://bucket/prefix/ac.mp4                                   recording_presigned_url  string        Presigned URL for the recorded video, for download <br <br  Example: https://upload-location/bucket/ac.mp4                            meeting_url        string        meeting_url provided at rtmp start <br <br  Example: \"https://app.100ms.live/room_id\"                                       rtmp            array         List of RTMP objects provided at rtmp start <br <br  Example:   \"url\": \"http://test.com\"                                      session_started_at     timestamp (in UTC)  Timestamp when session started <br <br  Example: 2020-11-11T16:32:17Z                                               size            int          Size of the recording (in bytes) <br <br  Example: 10024                                                       Sample beam.recording.success event   json     \"version\": \"2.0\",   \"id\": \"        -    -    -    -           \",   \"account_id\": \"                        \",   \"timestamp\": \"2021-11-30T12:59:57Z\",   \"type\": \"beam.recording.success\",   \"data\":       \"beam_id\": \"                        \",     \"created_at\": \"2021-11-30T12:59:57.672493658Z\",     \"duration\": 79,     \"metadata_id\": \"        -    -    -    -           \",     \"metadata_timestamp\": \"2021-11-30T12:59:57.679491494Z\",     \"peer_id\": \"        -    -    -    -           \",     \"recording_path\": \"s3://bucket/prefix/ac.mp4\",     \"recording_presigned_url\": \"https://<file access URL>\",     \"room_id\": \"                        \",     \"started_at\": \"2021-11-30T12:58:34.051Z\",     \"stopped_at\": \"2021-11-30T12:59:56.778Z\",     \"session_id\": \"                        \",     \"template_id\": \"                        \",     \"max_height\": 720,     \"max_width\": 1280,     \"meeting_url\": \"https://app.100ms.live/preview/rpe-pwl-akt?token=beam_recording\",     \"rtmp\":    \"url\": \"http://test.com\"   ,     \"session_started_at\": \"2021-11-30T12:58:49.97291247Z\",     \"size\": 10024           beam.failure This event will be sent when there are failures in RTMP streaming and/or browser recording. This can occur when, 1. Invalid RTMP URL 2. Browser failed to load the web-app 3. Incorrect storage (S3) credentials 4. Other unexpected errors Please check the below table for possible error types, messages and description.    Error types  Type         Message                                              Description                                                  :      :                          :                             INTERNAL_ERROR    Internal error                                           Internal errors in beam.                                           MAX_RETRIES_REACHED  Beam couldn't connect to Meeting URL after <max_count> retries. Check if meeting URL is correct.  Sent when Beam cannot join meeting URL after predefined number of max retries.                UPLOAD_ERROR     Failed to upload recordings. Check if upload config is correct.                  Sent when there are failures in uploading recordings. This is most likely due to incorrect upload config.   BEAM_STOPPED     Beam stopped too early                                       Beam takes a few seconds to start, if a stop request is sent before that, we send BEAM_STOPPED error.       Attributes  Name         Type          Description                                                       :      :       :                                beam_id        string        Unique beam id <br <br  Example: 61d3def54b616982bd80ed83                               job_id        string        Beam job id <br <br  Example: 60b8e1d96eb86d8101b57359                                 recording_enabled   bool         Indicates whether recording is enabled or not <br <br  Example: true                          room_id        string        100ms assigned room id <br <br  Example: 5f9edc6ac238215aec2312df                           peer_id        string        100ms assigned id to identify the joining user <br <br  Example: bd0c76fd-1ab1-4d7d-ab8d-bbfa74b620c4         session_id      string        100ms assigned id to identify the session <br <br  Example: 5f9edc6bd238215aec7700df                  template_id      string        Template ID of the room <br <br  Example: 66112497abcd52312556c4gg                           error_code      int          Beam error code <br <br  Example: 1 <br  (Deprecated)                                 error_message     string        Beam error message (description) <br <br  Example: Failed to upload recordings. Check if upload config is correct.   error_type      string        Beam error type <br <br  Example: UPLOAD_ERROR                                     metadata_id      string        Webhook metadata ID <br <br  Example: 14f350f5-18c4-46ca-8a33-71cbcc836600                       metadata_timestamp  timestamp (in UTC)  Webhook message creation timestamp <br <br  Example: 2020-11-11T17:32:17Z                       state_name      string        Beam state identifier <br <br  Example: Failed <br  (Deprecated)                           state_timestamp    timestamp (in UTC)  Timestamp at which beam state changed <br <br  Example: 2020-11-11T17:32:18Z <br  (Deprecated)            duration       int          Duration of RTMP streaming / recording in seconds <br <br  Example: 12                         max_width       int          Maximum width of the screen supported for streaming / recording in pixels <br <br  Example: 1280            max_height      int          Maximum height of the screen supported for streaming / recording in pixels <br <br  Example: 720            meeting_url      string        meeting_url provided at rtmp start <br <br  Example: \"https://app.100ms.live/room_id\"                 rtmp         array         List of RTMP objects provided at rtmp start <br <br  Example:   \"url\": \"http://test.com\"                 session_started_at  timestamp (in UTC)  Timestamp when session started <br <br  Example: 2020-11-11T16:32:17Z                           Sample beam.failure event   json     \"version\": \"2.0\",   \"id\": \"        -    -    -    -           \",   \"account_id\": \"                        \",   \"timestamp\": \"2021-11-30T20:36:55Z\",   \"type\": \"beam.failure\",   \"data\":       \"beam_id\": \"                        \",     \"error_code\": 1,     \"error_message\": \"Failed to upload recordings. Check if upload config is correct.\",     \"error_type\": \"UPLOAD_ERROR\",     \"job_id\": \"                        \",     \"metadata_id\": \"        -    -    -    -           \",     \"metadata_timestamp\": \"2021-11-30T20:36:55.807155801Z\",     \"peer_id\": \"        -    -    -    -           \",     \"recording_enabled\": true,     \"room_id\": \"                        \",     \"session_id\": \"                        \",     \"template_id\": \"                        \",     \"state_message\": \" tcp @ 0x55f6cc5f5780  Failed to resolve hostname rvs-isr: Name or service not known\",     \"state_name\": \"Failed\",     \"state_timestamp\": \"2021-11-30T20:36:55.799192444Z\",     \"max_height\": 720,     \"max_width\": 1280,     \"meeting_url\": \"https://app.100ms.live/preview/rpe-pwl-akt?token=beam_recording\",     \"rtmp\":    \"url\": \"http://test.com\"   ,     \"duration\": 56,     \"session_started_at\": \"2021-11-30T12:58:49.97291247Z\"           HLS Streaming Events   hls.started.success This event is sent when HLS streaming is successfully triggered    Attributes  Name                   Type          Description                                                                                       :           :       :                                               beam_id                  string        Unique beam id <br <br  Example: 61d3def54b616982bd80ed83                                                               job_id                   string        Beam job id <br <br  Example: 60b8e1d96eb86d8101b57359                                                                recording_single_file_per_layer_enabled  bool         Indicates whether HLS recording should be available as separate files for various dimensions supported. Output will be Individual mp4 file per HLS layer. <br <br  Example: false   recording_vod_playlist_enabled       bool         Indicates whether Video on Demand is enabled or not. Output will be a ZIP file of m3u8 format with all the chunks. <br <br  Example: false                      room_id                  string        100ms assigned room id <br <br  Example: 5f9edc6ac238215aec2312df                                                           peer_id                  string        100ms assigned id to identify the joining user <br <br  Example: bd0c76fd-1ab1-4d7d-ab8d-bbfa74b620c4                                         session_id                 string        100ms assigned id to identify the session <br <br  Example: 5f9edc6bd238215aec7700df                                                 template_id                string        Template ID of the room <br <br  Example: 66112497abcd52312556c4gg                                                          meeting_url                string        meeting_url provided at HLS start <br <br  Example: \"https://app.100ms.live/room_id\"                                                 metadata_id                string        Webhook metadata ID <br <br  Example: 14f350f5-18c4-46ca-8a33-71cbcc836600                                                      metadata_timestamp             timestamp (in UTC)  Webhook message creation timestamp <br <br  Example: 2020-11-11T17:32:17Z                                                       start_delay                int          Time taken by beam to start streaming <br <br  Example: 12                                                              state_name                 string        Beam state identifier <br <br  Example: HLSStarted                                                                  state_timestamp              timestamp (in UTC)  Timestamp at which beam state changed <br <br  Example: 2020-11-11T17:32:18Z                                                     max_width                 int          Maximum width of the screen supported for streaming / recording in pixels <br <br  Example: 1280                                           max_height                 int          Maximum height of the screen supported for streaming / recording in pixels <br <br  Example: 720                                           url                    string        HLS live streaming url <br <br  Example: https://100ms-live.m3u8                                                           started_at                 timestamp (in UTC)  Timestamp at which HLS started <br <br  Example: 2020-11-11T17:32:18Z                                                         session_started_at             timestamp (in UTC)  Timestamp when session started <br <br  Example: 2020-11-11T16:32:17Z                                                           Sample hls.started.success event   json     \"version\": \"2.0\",   \"id\": \"        -    -    -    -           \",   \"account_id\": \"                        \",   \"timestamp\": \"2021-11-30T12:58:46Z\",   \"type\": \"hls.started.success\",   \"data\":       \"beam_id\": \"                        \",     \"job_id\": \"                        \",     \"metadata_id\": \"        -    -    -    -           \",     \"metadata_timestamp\": \"2021-11-30T12:58:46.400759043Z\",     \"peer_id\": \"        -    -    -    -           \",     \"recording_single_file_per_layer_enabled\": false,     \"recording_vod_playlist_enabled\": false,     \"room_id\": \"                        \",     \"meeting_url\": \"https://app.100ms.live/preview/rpe-pwl-akt?token=beam_recording\",     \"session_id\": \"                        \",     \"template_id\": \"                        \",     \"start_delay\": 12,     \"state_name\": \"HLSStarted\",     \"state_timestamp\": \"2021-11-30T12:58:46.385555859Z\",     \"max_width\": 1280,     \"max_height\": 720,     \"url\": \"https://100ms-live.m3u8\",     \"started_at\": \"2021-11-30T12:58:46.400759043Z\",     \"session_started_at\": \"2021-11-30T12:58:49.97291247Z\"           hls.stopped.success This event is sent when HLS streaming is successfully stopped    Attributes  Name                   Type          Description                                                                                       :           :       :                                               beam_id                  string        Unique beam id <br <br  Example: 61d3def54b616982bd80ed83                                                               job_id                   string        Beam job id <br <br  Example: 60b8e1d96eb86d8101b57359                                                                recording_single_file_per_layer_enabled  bool         Indicates whether HLS recording should be available as separate files for various dimensions supported. Output will be Individual mp4 file per HLS layer. <br <br  Example: false   recording_vod_playlist_enabled       bool         Indicates whether Video on Demand is enabled or not. Output will be a ZIP file of m3u8 format with all the chunks. <br <br  Example: false                      room_id                  string        100ms assigned room id <br <br  Example: 5f9edc6ac238215aec2312df                                                           peer_id                  string        100ms assigned id to identify the joining user <br <br  Example: bd0c76fd-1ab1-4d7d-ab8d-bbfa74b620c4                                         session_id                 string        100ms assigned id to identify the session <br <br  Example: 5f9edc6bd238215aec7700df                                                 template_id                string        Template ID of the room <br <br  Example: 66112497abcd52312556c4gg                                                          max_width                 int          Maximum width of the screen supported for HLS streaming / recording in pixels <br <br  Example: 1280                                         max_height                 int          Maximum height of the screen supported for HLS streaming / recording in pixels <br <br  Example: 720                                         meeting_url                string        meeting_url provided at HLS start <br <br  Example: \"https://app.100ms.live/room_id\"                                                 metadata_id                string        Webhook metadata ID <br <br  Example: 14f350f5-18c4-46ca-8a33-71cbcc836600                                                      metadata_timestamp             timestamp (in UTC)  Webhook message creation timestamp <br <br  Example: 2020-11-11T17:32:17Z                                                       state_name                 string        Beam state identifier <br <br  Example: HLSStopped                                                                  state_timestamp              timestamp (in UTC)  Timestamp at which beam state changed <br <br  Example: 2020-11-11T17:32:18Z                                                     url                    string        HLS live streaming url <br <br  Example: https://100ms-live.m3u8                                                           duration                  int          Duration of HLS streaming in seconds <br <br  Example: 12                                                               started_at                 timestamp (in UTC)  Timestamp at which HLS started <br <br  Example: 2020-11-11T17:32:18Z                                                         stopped_at                 timestamp (in UTC)  Timestamp at which HLS stopped <br <br  Example: 2020-11-11T17:32:18Z                                                         session_started_at             timestamp (in UTC)  Timestamp when session started <br <br  Example: 2020-11-11T16:32:17Z                                                           Sample hls.stopped.success event   json     \"version\": \"2.0\",   \"id\": \"        -    -    -    -           \",   \"account_id\": \"                        \",   \"timestamp\": \"2021-11-30T12:59:57Z\",   \"type\": \"hls.stopped.success\",   \"data\":       \"beam_id\": \"                        \",     \"job_id\": \"                        \",     \"metadata_id\": \"        -    -    -    -           \",     \"metadata_timestamp\": \"2021-11-30T12:59:57.797972469Z\",     \"peer_id\": \"        -    -    -    -           \",     \"recording_single_file_per_layer_enabled\": false,     \"recording_vod_playlist_enabled\": false,     \"room_id\": \"                        \",     \"max_height\": 720,     \"max_width\": 1280,     \"meeting_url\": \"https://app.100ms.live/preview/rpe-pwl-akt?token=beam_recording\",     \"session_id\": \"                        \",     \"template_id\": \"                        \",     \"state_name\": \"HLSStopped\",     \"state_timestamp\": \"2021-11-30T12:59:57.685503281Z\",     \"duration\": 56,     \"started_at\": \"2021-11-30T12:59:57.797972469Z\",     \"stopped_at\": \"2021-11-30T12:59:57.797972469Z\",     \"session_started_at\": \"2021-11-30T12:58:49.97291247Z\"           hls.failure This event will be sent when there are failures in HLS streaming and/or recording. This can occur when, 1. The meeting_url is invalid 2. Other unexpected errors with HLS streaming / recording. Please check the below table for possible error types, messages and description.    Error types  Type         Message                                              Description                                                  :      :                          :                             INTERNAL_ERROR    Internal error                                           Internal errors in beam.                                           MAX_RETRIES_REACHED  Beam couldn't connect to Meeting URL after <max_count> retries. Check if meeting URL is correct.  Sent when Beam cannot join meeting URL after predefined number of max retries.                UPLOAD_ERROR     Failed to upload recordings. Check if upload config is correct.                  Sent when there are failures in uploading recordings. This is most likely due to incorrect upload config.   BEAM_STOPPED     Beam stopped too early                                       Beam takes a few seconds to start, if a stop request is sent before that, we send BEAM_STOPPED error.       Attributes  Name                   Type          Description                                                                                       :           :       :                                               beam_id                  string        Unique beam id <br <br  Example: 61d3def54b616982bd80ed83                                                               job_id                   string        Beam job id <br <br  Example: 60b8e1d96eb86d8101b57359                                                                recording_single_file_per_layer_enabled  bool         Indicates whether HLS recording should be available as separate files for various dimensions supported. Output will be Individual mp4 file per HLS layer. <br <br  Example: false   recording_vod_playlist_enabled       bool         Indicates whether Video on Demand is enabled or not. Output will be a ZIP file of m3u8 format with all the chunks. <br <br  Example: false                      room_id                  string        100ms assigned room id <br <br  Example: 5f9edc6ac238215aec2312df                                                           peer_id                  string        100ms assigned id to identify the joining user <br <br  Example: bd0c76fd-1ab1-4d7d-ab8d-bbfa74b620c4                                         session_id                 string        100ms assigned id to identify the session <br <br  Example: 5f9edc6bd238215aec7700df                                                 template_id                string        Template ID of the room <br <br  Example: 66112497abcd52312556c4gg                                                          max_width                 int          Maximum width of the screen supported for HLS streaming / recording in pixels <br <br  Example: 1280                                         max_height                 int          Maximum height of the screen supported for HLS streaming / recording in pixels <br <br  Example: 720                                         meeting_url                string        meeting_url provided at HLS start <br <br  Example: \"https://app.100ms.live/room_id\"                                                 error_code                 int          Beam error code <br <br  Example: 1 <br  (Deprecated)                                                                error_message               string        Beam error message (description) <br <br  Example: Failed to upload recordings. Check if upload config is correct.                                  error_type                 string        Beam error type <br <br  Example: UPLOAD_ERROR                                                                    metadata_id                string        Webhook metadata ID <br <br  Example: 14f350f5-18c4-46ca-8a33-71cbcc836600                                                      metadata_timestamp             timestamp (in UTC)  Webhook message creation timestamp <br <br  Example: 2020-11-11T17:32:17Z                                                       state_name                 string        Beam state identifier <br <br  Example: HLSFailed <br  (Deprecated)                                                         state_timestamp              timestamp (in UTC)  Timestamp at which beam state changed <br <br  Example: 2020-11-11T17:32:18Z <br  (Deprecated)                                            duration                  int          Duration of HLS streaming in seconds <br <br  Example: 12                                                               session_started_at             timestamp (in UTC)  Timestamp when session started <br <br  Example: 2020-11-11T16:32:17Z                                                           Sample hls.failure event   json     \"version\": \"2.0\",   \"id\": \"        -    -    -    -           \",   \"account_id\": \"                        \",   \"timestamp\": \"2021-11-30T20:36:55Z\",   \"type\": \"hls.failure\",   \"data\":       \"beam_id\": \"                        \",     \"error_code\": 1,     \"error_message\": \"Failed to upload recordings. Check if upload config is correct.\",     \"error_type\": \"UPLOAD_ERROR\",     \"job_id\": \"                        \",     \"metadata_id\": \"        -    -    -    -           \",     \"metadata_timestamp\": \"2021-11-30T20:36:55.807155801Z\",     \"peer_id\": \"        -    -    -    -           \",     \"recording_single_file_per_layer_enabled\": false,     \"recording_vod_playlist_enabled\": false,     \"room_id\": \"                        \",     \"max_height\": 720,     \"max_width\": 1280,     \"meeting_url\": \"https://app.100ms.live/preview/rpe-pwl-akt?token=beam_recording\",     \"session_id\": \"                        \",     \"template_id\": \"                        \",     \"state_name\": \"HLSFailed\",     \"state_timestamp\": \"2021-11-30T20:36:55.799192444Z\",     \"duration\": 56,     \"session_started_at\": \"2021-11-30T12:58:49.97291247Z\"           hls.recording.success This event will be sent when HLS recordings are successful and uploaded to the storage    Attributes  Name                 Type          Description                                                            :          :       :                                  beam_id                string        Unique beam id <br <br  Example: 61d3def54b616982bd80ed83                                    room_id                string        100ms assigned room id <br <br  Example: 5f9edc6ac238215aec2312df                                metadata_id              string        Webhook metadata ID <br <br  Example: 14f350f5-18c4-46ca-8a33-71cbcc836600                            metadata_timestamp           timestamp (in UTC)  Webhook message creation timestamp <br <br  Example: 2020-11-11T17:32:17Z                            duration                int          Duration of HLS recording in seconds <br <br  Example: 12                                    session_id               string        100ms assigned id to identify the session <br <br  Example: 5f9edc6bd238215aec7700df                       template_id              string        Template ID of the room <br <br  Example: 66112497abcd52312556c4gg                                hls_vod_recording_path         string        Upload path of the HLS vod playlist such as s3 URI <br <br  Example: s3://bucket/prefix/ac.mp4                  hls_vod_recording_presigned_url    string        Pre signed url for HLS vod playlist url <br <br  Example: https://upload-location/bucket/hls-vod.zip               hls_vod_recording_size         int          Size of the HLS vod recording (in bytes) <br <br  Example: 10024                                 max_width               int          Maximum width of the screen supported for HLS recording in pixels <br <br  Example: 1280                     max_height               int          Maximum height of the screen supported for HLS recording in pixels <br <br  Example: 720                     meeting_url              string        meeting_url provided at HLS start <br <br  Example: \"https://app.100ms.live/room_id\"                       recording_created_at          timestamp (in UTC)  Timestamp at which recording was created <br <br  Example: 2020-11-11T17:12:17Z                         started_at               timestamp (in UTC)  Timestamp at which HLS started <br <br  Example: 2020-11-11T17:32:18Z                              stopped_at               timestamp (in UTC)  Timestamp at which HLS stopped <br <br  Example: 2020-11-11T17:32:18Z                              recording_single_files         array         List of recording details per layer. layer=\"0\" is the highest quality layer <br <br  Example: < see below >           recording_hls_vod_playlist_location  string        (Deprecated, use hls_vod_recording_presigned_url) Pre signed url for HLS vod playlist url. <br <br  Example: < see below >     recording_thumbnails          array         List of thumbnails generated <br <br  Example: < see below >                                   session_started_at           timestamp (in UTC)  Timestamp when session started <br <br  Example: 2020-11-11T16:32:17Z                                Sample hls.recording.success event   json     \"version\": \"2.0\",   \"id\": \"        -    -    -    -           \",   \"account_id\": \"                        \",   \"timestamp\": \"2021-11-30T20:36:55Z\",   \"type\": \"hls.recording.success\",   \"data\":       \"beam_id\": \"                        \",     \"session_id\": \"                        \",     \"template_id\": \"                        \",     \"metadata_id\": \"        -    -    -    -           \",     \"metadata_timestamp\": \"2021-11-30T20:36:55.807155801Z\",     \"room_id\": \"                        \",     \"duration\": 56,     \"max_height\": 720,     \"max_width\": 1280,     \"meeting_url\": \"https://app.100ms.live/preview/rpe-pwl-akt?token=beam_recording\",     \"hls_vod_recording_path\": \"s3://<hls-vod-bucket-address>.zip\",     \"hls_vod_recording_presigned_url\": \"https://<hls-vod-access-url>?<signature>\",     \"hls_vod_recording_size\": 10024,     \"recording_single_files\":                   \"layer\": \"0\",         \"recording_path\": \"s3://<file-0-bucket-address>.mp4\",         \"recording_presigned_url\": \"https://<file-0-access-url>?<signature-0>\",         \"size\": 10024        ,                 \"layer\": \"1\",         \"recording_path\": \"s3://<file-1-bucket-address>.mp4\",         \"recording_presigned_url\": \"https://<file-1-access-url>?<signature-1>\",         \"size\": 5012        ,                 \"layer\": \"2\",         \"recording_path\": \"s3://<file-2-bucket-address>.mp4\",         \"recording_presigned_url\": \"https://<file-2-access-url>?<signature-2>\",         \"size\": 2506        ,                 \"layer\": \"3\",         \"recording_path\": \"s3://<file-3-bucket-address>.mp4\",         \"recording_presigned_url\": \"https://<file-3-access-url>?<signature-3>\",         \"size\": 1024              ,     \"recording_thumbnails\":                   \"width\": 1280,         \"height\": 720,         \"offset\": 60,         \"location\": \"s3://<thumbnail-file-bucket-address>.png\",         \"url\": \"https://<thumbnail-access-url>?<signature3>\"              ,     \"recording_created_at\": \"2021-11-30T12:59:57.672493658Z\",     \"started_at\": \"2021-11-30T12:59:57.797972469Z\",     \"stopped_at\": \"2021-11-30T12:59:57.797972469Z\",     \"session_started_at\": \"2021-11-30T12:58:49.97291247Z\"         "
    },
    {
        "title": "Room",
        "link": "/server-side/v2/legacy-api%20(v1)/room",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/legacy-api%20(v1)/room",
        "keywords": [],
        "content": "  >   Note  : The Legacy API (V1) will remain fully functional. However, we recommend you use our  new API (/server-side/v2/Rooms/object) to leverage the new functionalities and features and stay up to date with future releases; you can continue to check the docs for the Legacy API (V1)s  here (/server-side/v2/deprecated/templates-and-roles).   Create Room using API You can create room using API. This provides a more scalable way of creating room. <PostRequest   title=\"Create Room API\"   desp=\"This endpoint is used to create room. \"   url=\"https://prod-in2.100ms.live/api/v2/rooms\"    s items=  'Request', 'Response'   id=\"req\"     id=\"req-0\">   bash curl https://prod-in2.100ms.live/api/v2/rooms -H 'Authorization: Bearer <management_token>' -X POST -H 'Content-Type: application/json' -d ' \"name\": \"test-room\", \"description\": \"This is a test room\", \"recording_info\":  \"enabled\": true, \"upload_info\":  \"type\": \"s3\", \"location\": \"test-bucket\", \"prefix\": \"test-prefix\", \"options\":  \"region\": \"ap-south-1\" , \"credentials\":  \"key\": \"aws-access-key\", \"secret\": \"aws-secret-key\"    '   </Tab>   id=\"req-1\"> <Response res=\"200: OK\"     json     \"id\": \"61101349d0f56e684b608c08\",   \"name\": \"test-room\",   \"description\": \"This is a test room\",   \"active\": true,   \"recording_info\":       \"enabled\": true,     \"upload_info\":         \"type\": \"s3\",       \"location\": \"test-bucket\",       \"prefix\": \"test-prefix\",       \"credentials\":           \"key\": \"aws-access-key\",         \"secret\": \"aws-secret-key\"        ,       \"options\":           \"region\": \"ap-south-1\"                  ,   \"user\": \"60b8e13a6eb86d8101b57353\",   \"customer\": \"60b8e13a6eb86d8101b57354\",   \"created_at\": \"2021-08-08T17:24:25.682362823Z\",   \"updated_at\": \"0001-01-01T00:00:00Z\"     </Tab>   Headers  Name         Value             Required   :      :        :     Content-type <br   application/json       Yes     Authorization <br   Bearer <management token>  Yes       Body Paramters  Name          Type    Description                                                                                                                                                                                                                          Required   :       :    :                                                                                                                :     name <br        string  A unique identifier you can assign to 100ms rooms. This is case-insensitive . Accepted characters are a-z, A-Z, 0-9, and .  : _ . If not provided, this is generated automatically. <br <br    Note  : <br <br  (1) If create room is called with an existing room name, then the corresponding room ID is returned <br <br  (2) Providing room name will be helpful if you want to disable the room later (disable room API requires room name)  No      description <br     string  A string to describe your room's usage. E.g. \"9PM English Class Batch 2\"                                                                                                                                                                                           No      template <br      string   Template (../foundation/templates-and-roles) of the room. You can get template name from room details section on dashboard                                                                                                                                                                  No      recording_info <br   object  Object of type recording_info . This object contains information for enabling recording/setting storage location for recordings.                                                                                                                                                               No      region <br       string  Region in which you want to create room. <br <br  in  India <br <br  us  United States <br <br  eu  European Union <br <br  auto Automatic region selection (default, and in case this parameter is not provided)                                                                                                             No         recording_info  Name     Type    Description                                                                                            Required   :    :    :                                                 :     enabled    boolean  Enable SFU recording. Disabled by default                                                                             No      upload_info  object   Object of type upload_info . This object contains information on recordings storage location. If you want to store recording with 100ms, and not use your own s3, don't add this to the object  No      recording_info in room acts as an atomic property. Following describes the way recording_info depends on recording settings defined in a template.   If recording_info key is   not   provided in the api, the room will fill it with recording settings from template. And the response body will include recording_source_template: true . This means, whenever recording info is changed in template, they will also be reflected in room's recording settings.   If recording_info key is provided in the api, the room's recording settings will not depend on template at all. To know more about recording please visit  Recording (./recording)   upload_info  Name     Type    Description                                               Required   :    :    :                           :     type      string  Upload Destination type. Currently, only s3 is supported                        Yes     location    string  Name of the AWS s3 bucket in which you want to store all recordings                   Yes     prefix     string  Upload prefix path                                            No      options    object  Additional configurations of type Options to be used for uploading                   No      credentials  object  Object of type Credentials . This is used to share AWS credentials to access the s3 bucket specified.  No       Options  Name   Type    Description                                                        Required   :   :    :                                :     region  string  Region of the AWS account hosting the s3 bucket for storing recordings. If not provided it is assumed to be ap-south-1  No       Credentials  Name   Type    Description                                      Required   :   :    :                       :     key    string   access key ID for the AWS account hosting the s3 bucket for storing recordings    Yes     secret  string   secret access key for the AWS account hosting the s3 bucket for storing recordings  Yes     The access keys should have read(GetObject) and write(PutObject) permissions for the s3 bucket. For more details check  https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html   Disable Room This API can be used to disable a room from further use <PostRequest   title=\"Disable Room API\"   desp=\"This endpoint is used to disable a Room. \"   url=\"https://prod-in2.100ms.live/api/v2/rooms\"    s items=  'Request', 'Response'   id=\"dis\"     id=\"dis-0\">   bash curl https://prod-in2.100ms.live/api/v2/rooms -H 'Authorization: Bearer <management_token>' -X POST -H 'Content-Type: application/json' -d ' \"name\": \"test-room\", \"active\": false '   </Tab>   id=\"dis-1\"> <Response res=\"200: OK\"     json     \"id\": \"61101349d0f56e684b608c08\",   \"name\": \"test-room\",   \"description\": \"This is a test room\",   \"active\": false,   \"recording_info\":       \"enabled\": true,     \"upload_info\":         \"type\": \"s3\",       \"location\": \"test-bucket\",       \"prefix\": \"test-prefix\",       \"credentials\":           \"key\": \"aws-access-key\",         \"secret\": \"aws-secret-key\"        ,       \"options\":           \"region\": \"ap-south-1\"                  ,   \"user\": \"60b8e13a6eb86d8101b57353\",   \"customer\": \"60b8e13a6eb86d8101b57354\",   \"created_at\": \"2021-08-08T17:24:25.682362823Z\",   \"updated_at\": \"0001-01-01T00:00:00Z\"     </Tab>   Headers  Name         Value             Required   :      :        :     Content-type <br   application/json       Yes     Authorization <br   Bearer <management token>  Yes       Body Paramters  Name      Type    Description                            Required   :     :    :                  :     name <br    string  Room name as given during creation of the room           Yes     active <br   bool   Flag to indicate disabling of the room, should be set to false  Yes    "
    },
    {
        "title": "Session",
        "link": "/server-side/v2/legacy-api%20(v1)/session",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/legacy-api%20(v1)/session",
        "keywords": [],
        "content": "  >   Note  : The Legacy API (V1) will remain fully functional. However, we recommend you use our  new API (/server-side/v2/Sessions/object) to leverage the new functionalities and features and stay up to date with future releases; you can continue to check the docs for the Legacy API (V1)s  here (/server-side/v2/deprecated/templates-and-roles).   Overview Session is a single continuous call in a  room (room). To allow users to join a 100ms video conferencing session inside your app, you first need to create a room . A single room can have multiple sessions. You will need a  management token (../foundation/authentication-and-tokens) as a bearer token for your requests.   Get Session Data using API You can query session data using API. <GetRequest   title=\"Get Session API\"   desp=\"This endpoint is used to get sessions. \"   url=\"https://api.100ms.live/v2/sessions\"    s items=  'Request', 'Response'   id=\"req\"     id=\"req-0\">   bash curl location request GET 'https://api.100ms.live/v2/sessions?room_id=61652b404e556e35f2179759' -H 'Authorization: Bearer   management_token  '   </Tab>   id=\"req-1\"> <Response res=\"200: OK\"     json     \"limit\": 10,   \"data\":               \"id\": \"6205e9826d83bc36e6e761e2\",       \"room_id\": \"61652b404e556e35f2179759\",       \"customer_id\": \"60b8e1d96eb86d8101b5735a\",       \"user_id\": \"60b8e1d96eb86d8101b57359\",       \"active\": true,       \"peers\":           \"860aa563-6f18-4a04-b7ad-a2cb41ba883e\":             \"name\": \"Sagar\",           \"role\": \"host\",           \"joined_at\": \"2022-02-11T04:43:46.306Z\"                  ,       \"created_at\": \"2022-02-11T04:43:46.294Z\",       \"updated_at\": \"2022-02-11T04:43:46.75Z\"      ,             \"id\": \"6205e9376d83bc36e6e761e1\",       \"room_id\": \"6204fea00c108fa49d599d79\",       \"customer_id\": \"60c1ea9a6eb86d8101b57570\",       \"user_id\": \"60c1ea9a6eb86d8101b5756f\",       \"active\": false,       \"peers\":           \"210e5175-4816-4c65-a985-07b366f571cc\":             \"name\": \"Deep2\",           \"role\": \"teacher\",           \"joined_at\": \"2022-02-11T04:42:36.595Z\",           \"left_at\": \"2022-02-11T04:42:42.126Z\"          ,         \"2d3ab4d3-cc04-426e-b7aa-6f7952169509\":             \"name\": \"Deep2\",           \"role\": \"teacher\",           \"joined_at\": \"2022-02-11T04:42:31.027Z\",           \"left_at\": \"2022-02-11T04:42:42.738Z\"                  ,       \"created_at\": \"2022-02-11T04:42:31.018Z\",       \"updated_at\": \"2022-02-11T04:42:42.785Z\"          ,   \"last\": \"6205e9376d83bc36e6e761e1\"     </Tab>   Headers  Name         Value             Required   :      :        :     Authorization <br   Bearer <management token>  Yes       URL Parameters  Name      Type    Description                                                        Required   :     :    :                               :     id <br      string  ID of the session if known.                                                No      room_id <br   string  Room ID of the session if known.                                             No      active <br    bool   Session is currently going on or not.                                           No      before <br    time   Timestamp before(inclusive) which session is created in RFC3339 format e.g. 2022-02-09T05:53:23.375Z .          No      after <br    time   Timestamp after(inclusive) which session is created in RFC3339 format e.g. 2022-02-09T05:53:23.375Z .          No      limit <br    int    Number of sessions returned by response. Range 10  20                                  No      start <br    string  ID of the session after which to start the response. Sessions are returned in decreasing IDs or created_at timestamp.  No       Response  Name     Type     Description                                                            Required   :    :    :                                  :     limit <br   int     Number of sessions the API tried to find. If returned array has lesser number of results, only those number were found.      No      data <br    array    Array of sessions or null if no session was found.                                        No      last <br    ObjectID  Sessions are returned in decreasing IDs or created_at timestamp. Last represent the earliest session returned by the response.  No       How to calculate session duration You can calculate additional information from session data using given information. Redundant information like peer duration/ session duration is not provided since they can be calculated from provided response. You can calculate peer duration by subtracting joined_at timestamp from left_at for a given peer. Similarly you can calculate session duration by adding all peers' duration.   Postman Collection <DownloadCollection type=\"rooms\"   "
    },
    {
        "title": "Templates and Roles",
        "link": "/server-side/v2/legacy-api%20(v1)/templates-and-roles",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/legacy-api%20(v1)/templates-and-roles",
        "keywords": [],
        "content": "  >   Note  : The Legacy API (V1) will remain fully functional. However, we recommend you use our  new API (/server-side/v2/policy/template-object) to leverage the new functionalities and features and stay up to date with future releases; you can continue to check the docs for the Legacy API (V1)s  here (/server-side/v2/deprecated/templates-and-roles).   Create Template API <PostRequest title=\"/policy/v1/templates\"   <Request id=\"req-comp-0\">   bash curl 'https://prod-policy.100ms.live/policy/v1/templates' -H 'Authorization: Bearer   management_token  ' -X POST -H 'Content-Type: application/json' -d '  \"name\": \"test-template\", \"default\": true, \"roles\":  \"student\":   , \"teacher\":   , \"admin\":       '   </Request> <ResponseBox id=\"resp-0\" status=\"200 OK\">   json     \"_id\": \"615e61b0ba65e77a89ba75c1\",   \"name\": \"test-template\",   \"customer\": \"60f7cc95f88117b9e47bf618\",   \"default\": true,   \"roles\":       \"admin\":         \"name\": \"admin\",       \"publishParams\":           \"allowed\":  \"video\", \"screen\", \"audio\" ,         \"audio\":             \"bitRate\": 32,           \"codec\": \"opus\"          ,         \"video\":             \"bitRate\": 250,           \"codec\": \"vp8\",           \"frameRate\": 30,           \"width\": 480,           \"height\": 270          ,         \"screen\":             \"codec\": \"vp8\",           \"frameRate\": 10,           \"width\": 1920,           \"height\": 1080                  ,       \"subscribeParams\":           \"subscribeToRoles\":  \"guest\" ,         \"maxSubsBitRate\": 3200,         \"subscribeDegradation\":           ,       \"permissions\":         ,     \"student\":         \"name\": \"student\",       \"publishParams\":           \"allowed\":  \"video\", \"screen\", \"audio\" ,         \"audio\":             \"bitRate\": 32,           \"codec\": \"opus\"          ,         \"video\":             \"bitRate\": 250,           \"codec\": \"vp8\",           \"frameRate\": 30,           \"width\": 480,           \"height\": 270          ,         \"screen\":             \"codec\": \"vp8\",           \"frameRate\": 10,           \"width\": 1920,           \"height\": 1080                  ,       \"subscribeParams\":           \"subscribeToRoles\":  \"guest\" ,         \"maxSubsBitRate\": 3200,         \"subscribeDegradation\":           ,       \"permissions\":         ,     \"teacher\":         \"name\": \"teacher\",       \"publishParams\":           \"allowed\":  \"video\", \"screen\", \"audio\" ,         \"audio\":             \"bitRate\": 32,           \"codec\": \"opus\"          ,         \"video\":             \"bitRate\": 250,           \"codec\": \"vp8\",           \"frameRate\": 30,           \"width\": 480,           \"height\": 270          ,         \"screen\":             \"codec\": \"vp8\",           \"frameRate\": 10,           \"width\": 1920,           \"height\": 1080                  ,       \"subscribeParams\":           \"subscribeToRoles\":  \"guest\" ,         \"maxSubsBitRate\": 3200,         \"subscribeDegradation\":           ,       \"permissions\":                  </ResponseBox>    Body Parameters  Name       Type    Description                                                                                                                                                                 Required   :     :    :                                                                                    :     name <br     string   A unique name you can assign to 100ms templates. Accepted characters are a-z, A-Z, 0-9, space, hyphen  , dot . , underscore _ and colon : . If not provided, this is generated automatically. <br <br    Notes  : <br If create template is called with an existing template name, then that template will be overwritten.<br   No      default <br    boolean  Whether this is default template for a customer. Default template for a customer is used when no template is provided in create room api.                                                                                                 No      roles <br     object   Map of  roleName:roleObject  . Some sane defaults are set for values that are not provided in the roles object.                                                                                                               No      settings <br   object   Global settings for this template.                                                                                                                                                      No        roleObject  Name       Type    Description                               Required   :     :    :                   :     publishParams   object  Publish parameters for this role.                    No      subscribeParams  object  Subscribe parameters for this role.                   No      permissions    object  Permissions for this role.                       No      priority      int    Priority of the role for subscription/publishing.<br Range: 1  10  No        publishParams  Name   Type    Description                         Required   :   :    :                :     allowed  array   Allowed tracks. Subarray of   audio , video , screen  .  No      audio   object  Audio publish parameters.                  No      video   object  Video publish parameters.                  No      screen   object  Screenshare publish parameters.               No        audio  Name   Type    Description                                     Required   :   :    :                      :     bitRate  int    Audio Max bitrate of audio track in kbps.<br Range: 16  128 <br Default: 32  No      codec   string  Codec for the audio track.<br Options:   opus   <br Default: opus        No        video  Name    Type    Description                                   Required   :    :    :                     :     bitRate   int    Max bitrate of video track in kbps.<br Range: 30  2000 <br Default: 256  No      codec    string  Codec for the video track.<br Options:   vp8  <br Default: vp8        No      frameRate  int    Max number of video frames per second.<br Range: 1  30 <br Default: 25   No      height    int    Height of the video track.<br Range: 50  1080 <br Default: 180       No      width    int    Width of the video track.<br Range: 50  1920 <br Default: 320       No        screen  Name    Type    Description                                  Required   :    :    :                     :     bitRate   int    Max bitrate of screen track in kbps.<br Range: >=500 <br Default: 1024  No      codec    string  Codec for the screen track.<br Options:   vp8  <br Default: vp8       No      frameRate  int    Max number of screen frames per second<br Range: 1  30 <br Default: 10  No      height    int    Height of the screen track.<br Range: 270  1080 <br Default: 720     No      width    int    Width of the screen track.<br Range: 480  1920 <br Default: 1280     No        subscribeParams  Name          Type    Description                                Required   :       :    :                    :     maxSubsBitRate     int    Maximum bitrate (in kbps) that can subscribed.               No      subscribeToRoles    array   List of roles which can be subscribed to.                 No      subscribeDegradation  object  Subscribe degradation parameters (adding this enables screen simulcast).  No        subscribeDegradation  Name            Type  Description                                Required   :        :   :                   :     packetLossThreshold     int  Threshold for packet loss.<br Range: 1  100 <br Default: 50     No      degradeGracePeriodSeconds  int  Degrade grace period (in seconds).<br Range: 1  10 <br Default: 1  No      recoverGracePeriodSeconds  int  Recover grace period (in seconds).<br Range: 1  10 <br Default: 4  No        permissions  Name      Type    Description                     Required   :     :    :              :     endRoom     boolean  Permission to end room for all.           No      removeOthers  boolean  Permission to remove others from the room.      No      mute      boolean  Permission to request others to mute them.      No      unmute     boolean  Permission to request others to unmute them.     No      changeRole   boolean  Permission to request others to change their role.  No        settings  Name    Type    Description                                                               Required   :    :    :                                   :     region    string  Region in which the room will be hosted by default. Possible values could be in , us , eu or auto (automatic region selection).  No      recording  object  Object of type recording . This object contains information for enabling recording/setting storage location for recordings.      No        recording  Name   Type    Description                                                                                         Required   :   :    :                                                :     enabled  boolean  Enable the SFU recording for its rooms. Disabled by default.                                                                 No      upload   object   Object of type upload . This object contains information on recordings storage location. If you want to store recording with 100ms, and not use your own s3, don't add this to the object  No        upload  Name     Type    Description                                               Required   :    :    :                           :     type      string  Upload Destination type. Currently, only s3 is supported.                        Yes     location    string  Name of the AWS s3 bucket in which you want to store all recordings                   Yes     prefix     string  Upload prefix path                                            Yes     options    object  Additional configurations of type Options to be used for uploading                    No      credentials  object  Object of type Credentials . This is used to share AWS credentials to access the s3 bucket specified.  No        options  Name   Type    Description                                             Required   :   :    :                          :     region  string  Region of the AWS account hosting the s3 bucket. If not provided it is assumed to be ap-south-1  No        Credentials  Name   Type    Description                                      Required   :   :    :                       :     key    string   access key ID for the AWS account hosting the s3 bucket for storing recordings    Yes     secret  string   secret access key for the AWS account hosting the s3 bucket for storing recordings  Yes      Create/Modify Role API <PostRequest title=\"/policy/v1/templates/:id/roles/:name\"   <Request id=\"req-comp-1\">   bash curl 'https://prod-policy.100ms.live/policy/v1/templates/  templateID  /roles/  roleName  ' -H 'Authorization: Bearer   management_token  ' -X POST -H 'Content-Type: application/json' -d '  \"name\": \"listener3\", \"publishParams\":   \"allowed\":   \"video\", \"screen\", \"audio\"  , \"audio\":   \"bitRate\": 40, \"codec\": \"opus\"  , \"video\":   \"bitRate\": 400, \"codec\": \"vp8\", \"frameRate\": 30, \"width\": 480, \"height\": 360  , \"screen\":   \"codec\": \"vp8\", \"frameRate\": 10, \"width\": 1920, \"height\": 1080    , \"subscribeParams\":   \"subscribeToRoles\":   \"speaker\", \"moderator\", \"new-role-9880\"  , \"maxSubsBitRate\": 3200, \"subscribeDegradation\":     , \"permissions\":   , \"priority\": 1  '   </Request> <ResponseBox id=\"resp-1\" status=\"200 OK\">   json     \"name\": \"listener3\",   \"publishParams\":       \"allowed\":  \"video\", \"screen\", \"audio\" ,     \"audio\":         \"bitRate\": 40,       \"codec\": \"opus\"      ,     \"video\":         \"bitRate\": 400,       \"codec\": \"vp8\",       \"frameRate\": 30,       \"width\": 480,       \"height\": 360      ,     \"screen\":         \"codec\": \"vp8\",       \"frameRate\": 10,       \"width\": 1920,       \"height\": 1080          ,   \"subscribeParams\":       \"subscribeToRoles\":  \"speaker\", \"moderator\", \"new-role-9880\" ,     \"maxSubsBitRate\": 3200,     \"subscribeDegradation\":       ,   \"permissions\":   ,   \"priority\": 1     </ResponseBox>    Body Parameters  Name        Type    Description                                                            Required   :      :    :                                 :     roleObject <br   object  Role object correspoding to the given role.                                            Yes     name <br      string  Providing a name renames the role to the provided name. If role with same name already exists, it gives an overwrite error.  No       Update Settings API <PostRequest title=\"/policy/v1/templates/:id/settings\"   <Request id=\"req-comp-2\">   bash curl 'https://prod-policy.100ms.live/policy/v1/templates/  templateID  /settings' -H 'Authorization: Bearer   management_token  ' -X POST -H 'Content-Type: application/json' -d '  \"region\": \"us\"  '   </Request> <ResponseBox id=\"resp-2\" status=\"200 OK\">   json     \"region\": \"us\"     </ResponseBox>    Body Parameters  Name      Type    Description                                    Required   :     :    :                      :     region <br   string  Region in which the room will be hosted by default  in , eu , us or auto .  No       Get Templates API <GetRequest title=\"/policy/v1/templates?id=ID\"   <Request id=\"req-3\">   bash curl 'https://prod-policy.100ms.live/policy/v1/templates?id=  templateID  ' -H 'Authorization: Bearer   management_token  '   </Request> <ResponseBox id=\"res-3\" status=\"200 OK\">   json     \"limit\": 10,   \"data\":               \"_id\": \"615dbca1ba65e77a89a8e2b8\",       \"name\": \"test-template10\",       \"customer\": \"60f7cc95f88117b9e47bf618\",       \"roles\":           \"student\":             \"name\": \"student\",           \"publishParams\":               \"allowed\":  \"screen\", \"audio\" ,             \"audio\":                 \"bitRate\": 42,               \"codec\": \"opus\"              ,             \"video\":                 \"bitRate\": 250,               \"codec\": \"vp8\",               \"frameRate\": 30,               \"width\": 480,               \"height\": 270              ,             \"screen\":                 \"codec\": \"vp8\",               \"frameRate\": 10,               \"width\": 1920,               \"height\": 1086                          ,           \"subscribeParams\":               \"subscribeToRoles\":  \"teacher\", \"student\" ,             \"maxSubsBitRate\": 2000,             \"subscribeDegradation\":               ,           \"permissions\":   ,           \"priority\": 1                  ,       \"settings\":           \"region\": \"in\"                  ,   \"last\": \"615dbca1ba65e77a89a8e2b8\"     </ResponseBox>    URL Parameters  Name      Type    Description                                                             Required   :     :    :                                  :     name <br     string   Name of the template.                                                        No      id <br      string   ID of the template.                                                         No      start <br    string   Returned dataset is sorted by decreasing order of IDs. Provide the ID of the last result set to start in reverse order from there.  No      limit <br    int    Number of templates to return (max 20 ).                                              No      default <br   boolean  Whether template is default or not.                                                 No       Get Role API <GetRequest title=\"/policy/v1/templates/:id/roles/:name\"   <Request id=\"req-4\">   bash curl 'https://prod-policy.100ms.live/policy/v1/templates/  templateID  /roles/  roleName  ' -H 'Authorization: Bearer   management_token  '   </Request> <ResponseBox id=\"res-4\" status=\"200 OK\">   json     \"name\": \"student\",   \"publishParams\":       \"allowed\":  \"screen\", \"audio\" ,     \"audio\":         \"bitRate\": 42,       \"codec\": \"opus\"      ,     \"video\":         \"bitRate\": 250,       \"codec\": \"vp8\",       \"frameRate\": 30,       \"width\": 480,       \"height\": 270      ,     \"screen\":         \"codec\": \"vp8\",       \"frameRate\": 10,       \"width\": 1920,       \"height\": 1086          ,   \"subscribeParams\":       \"subscribeToRoles\":  \"teacher\", \"student\" ,     \"maxSubsBitRate\": 2000,     \"subscribeDegradation\":       ,   \"permissions\":   ,   \"priority\": 1     </ResponseBox>   Get Settings API <GetRequest title=\"/policy/v1/templates/:id/settings\"   <Request id=\"req-5\">   bash curl 'https://prod-policy.100ms.live/policy/v1/templates/  templateID  /settings' -H 'Authorization: Bearer   management_token  '   </Request> <ResponseBox id=\"res-5\" status=\"200 OK\">   json     \"region\": \"in\"     </ResponseBox>   Modify Template API <PostRequest   title=\"/policy/v1/templates?id=ID\"   desp=\"This endpoint is used to modify a Template. You can also use the URL parameter name instead of id to modify a template by name. Providing the same name as a previous template overrides that template. You need to provide all options for roles and settings which you want to apply.\"   url=\"https://prod-policy.100ms.live/policy/v1/templates?id=  tempalateID  \"   <Request id=\"req-6\">   bash curl 'https://prod-policy.100ms.live/policy/v1/templates?id=  templateID  ' -H 'Authorization: Bearer   management_token  ' -X POST -H 'Content-Type: application/json' -d '  \"roles\":  \"student\":  \"name\":\"student\", \"publishParams\":  \"allowed\":  \"screen\", \"audio\"  , \"audio\":  \"bitRate\":42, \"codec\":\"opus\"  , \"video\":  \"bitRate\":250, \"codec\":\"vp8\", \"frameRate\":30, \"width\":480, \"height\":270  , \"screen\":  \"codec\":\"vp8\", \"frameRate\":10, \"width\":1920, \"height\":1086    , \"subscribeParams\":  \"subscribeToRoles\":  \"teacher\", \"student\"  , \"maxSubsBitRate\":2000  , \"priority\":1    , \"settings\":  \"region\":\"eu\"    '   </Request> <ResponseBox id=\"res-6\" status=\"200 OK\">   json     \"_id\": \"615dbca1ba65e77a89a8e2b8\",   \"name\": \"test-template10\",   \"customer\": \"60f7cc95f88117b9e47bf618\",   \"roles\":       \"student\":         \"name\": \"student\",       \"publishParams\":           \"allowed\":  \"screen\", \"audio\" ,         \"audio\":             \"bitRate\": 42,           \"codec\": \"opus\"          ,         \"video\":             \"bitRate\": 250,           \"codec\": \"vp8\",           \"frameRate\": 30,           \"width\": 480,           \"height\": 270          ,         \"screen\":             \"codec\": \"vp8\",           \"frameRate\": 10,           \"width\": 1920,           \"height\": 1086                  ,       \"subscribeParams\":           \"subscribeToRoles\":  \"teacher\", \"student\" ,         \"maxSubsBitRate\": 2000,         \"subscribeDegradation\":           ,       \"permissions\":   ,       \"priority\": 1          ,   \"settings\":       \"region\": \"eu\"         </ResponseBox>    Body Parameters  Name       Type    Description                                                                               Required   :     :    :                                           :     name <br     string   If you don't provide the name , earlier provided name is used. Trying to modify a template's name to match another existing template's name produces overwrite error.  No      default <br    boolean  Whether this is default template for customer. The default template for a customer is used when no template is provided in create room API.               No      roles <br     object   Modified map of  roleName:roleObject  . Since this is a POST request, you need to provide the whole roles object to modify/persist/override it.             Yes     settings <br   object   Modified settings. Since this is a POST request, you need to provide the whole settings object to modify/persist/override it.                      Yes      Delete Role API <DeleteRequest   title=\"/policy/v1/templates/:id/roles/:name\"   desp=\"This endpoint is used to delete a role. Subsequent requests for deleting role which does not exist gives 404.\"   url=\"https://prod-policy.100ms.live/policy/v1/templates/  templateID  /roles/  roleName  \"   <Request id=\"req-7\">   bash curl 'https://prod-policy.100ms.live/policy/v1/templates/  templateID  /roles/  roleName  ' -H 'Authorization: Bearer   management_token  ' -X DELETE -H 'Content-Type: application/json'   </Request> <ResponseBox id=\"res-7\" status=\"204 No Content\"></ResponseBox> "
    },
    {
        "title": "Create a template via API",
        "link": "/server-side/v2/policy/create-template-via-api",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/policy/create-template-via-api",
        "keywords": [],
        "content": "  This endpoint is used to create a template. A recommended way is to provide a list of roles with empty objects to get default values for various options corresponding to those roles, then update those roles by modifying them using create/update role API.   Understand sub-objects hierarchy   Template sub-objects hierarchy (/docs/v2/template-object-hierarchy.png) <PostRequest title=\"https://api.100ms.live/v2/templates\"   <Request id=\"req-0\">   bash curl location request POST 'https://api.100ms.live/v2/templates'  header 'Authorization: Bearer <management_token>'  header 'Content-Type: application/json'  data-raw '    \"name\": \"new-template  $timestamp  \",   \"default\": false,   \"roles\":       \"guest\":         \"name\": \"guest\",       \"publishParams\":           \"allowed\":             \"audio\",           \"video\",           \"screen\"          ,         \"audio\":             \"bitRate\": 32,           \"codec\": \"opus\"          ,         \"video\":             \"bitRate\": 300,           \"codec\": \"vp8\",           \"frameRate\": 30,           \"width\": 480,           \"height\": 360          ,         \"screen\":             \"codec\": \"vp8\",           \"frameRate\": 10,           \"width\": 1920,           \"height\": 1080          ,         \"videoSimulcastLayers\":   ,         \"screenSimulcastLayers\":           ,       \"subscribeParams\":           \"subscribeToRoles\":             \"host\",           \"guest\"          ,         \"maxSubsBitRate\": 3200,         \"subscribeDegradation\":             \"packetLossThreshold\": 25,           \"degradeGracePeriodSeconds\": 1,           \"recoverGracePeriodSeconds\": 4                  ,       \"permissions\":           \"sendRoomState\": false        ,       \"priority\": 1,       \"maxPeerCount\": 0      ,     \"host\":         \"name\": \"host\",       \"publishParams\":           \"allowed\":             \"audio\",           \"video\",           \"screen\"          ,         \"audio\":             \"bitRate\": 32,           \"codec\": \"opus\"          ,         \"video\":             \"bitRate\": 300,           \"codec\": \"vp8\",           \"frameRate\": 30,           \"width\": 480,           \"height\": 360          ,         \"screen\":             \"codec\": \"vp8\",           \"frameRate\": 10,           \"width\": 1920,           \"height\": 1080          ,         \"videoSimulcastLayers\":   ,         \"screenSimulcastLayers\":           ,       \"subscribeParams\":           \"subscribeToRoles\":             \"guest\",           \"host\"          ,         \"maxSubsBitRate\": 3200,         \"subscribeDegradation\":             \"packetLossThreshold\": 25,           \"degradeGracePeriodSeconds\": 1,           \"recoverGracePeriodSeconds\": 4                  ,       \"permissions\":           \"endRoom\": true,         \"removeOthers\": true,         \"mute\": true,         \"unmute\": true,         \"changeRole\": true,         \"sendRoomState\": false        ,       \"priority\": 1,       \"maxPeerCount\": 0          ,   \"settings\":       \"region\": \"in\",     \"subscribeDegradation\":   ,     \"recording\":         \"enabled\": true,       \"upload\":           \"type\": \"<upload type: supported are s3, gs, oss>\",         \"location\": \"<Name of the storage bucket>\",         \"prefix\": \"<Upload prefix path>\",         \"options\":             \"region\": \"<region of the storage bucket>\"          ,         \"credentials\":             \"key\": \"<access key ID for accessing the storage bucket>\",           \"secretKey\": \"<secret access key for accessing the storage bucket>\"                        ,     \"screenSimulcastLayers\":   ,     \"videoSimulcastLayers\":   ,     \"roomState\":         \"messageInterval\": 5,       \"sendPeerList\": false,       \"stopRoomStateOnJoin\": true,       \"enabled\": false          ,   \"destinations\":       \"browserRecordings\":         \"test\":           \"name\": \"test\",         \"width\": 1920,         \"height\": 1080,         \"maxDuration\": 1800,         \"thumbnails\":             \"width\": 0,           \"height\": 0          ,         \"presignDuration\": 3600              ,     \"rtmpDestinations\":         \"test\":           \"name\": \"test\",         \"width\": 1080,         \"height\": 1920,         \"maxDuration\": 1800,         \"rtmpUrls\":             \"url\",           \"ashdas\",           \"ddsfsdf\"          ,         \"recordingEnabled\": true              ,     \"hlsDestinations\":         \"test\":           \"name\": \"test\",         \"maxDuration\": 28800,         \"layers\":                           \"width\": 1080,             \"height\": 720,             \"videoBitrate\": 1100,             \"audioBitrate\": 32            ,                         \"width\": 720,             \"height\": 720,             \"videoBitrate\": 600            ,                       ,         \"playlistType\": \"event\",         \"numPlaylistSegments\": 12,         \"videoFrameRate\": 25,         \"enableMetadataInsertion\": true,         \"enableStaticUrl\": true,         \"recording\":             \"hlsVod\": true,           \"singleFilePerLayer\": true,           \"layers\":                               \"width\": 720,               \"height\": 1920,               \"videoBitrate\": 2000,               \"audioBitrate\": 64                          ,           \"thumbnails\":               \"enabled\": true,             \"width\": 720,             \"height\": 1920,             \"offsets\":                 2              ,             \"fps\": 60            ,           \"presignDuration\": 604800                              '   </Request> <ResponseBox id=\"res-0\" status=\"200 OK\">   json     \"id\": \"6324661c4da877930beaecaa\",   \"name\": \"new-template-1663329820\",   \"customerId\": \"627cda54ff688c037a39291b\",   \"roles\":       \"guest\":         \"name\": \"guest\",       \"publishParams\":           \"allowed\":  \"audio\", \"video\", \"screen\" ,         \"audio\":             \"bitRate\": 32,           \"codec\": \"opus\"          ,         \"video\":             \"bitRate\": 300,           \"codec\": \"vp8\",           \"frameRate\": 30,           \"width\": 480,           \"height\": 360          ,         \"screen\":             \"codec\": \"vp8\",           \"frameRate\": 10,           \"width\": 1920,           \"height\": 1080          ,         \"videoSimulcastLayers\":   ,         \"screenSimulcastLayers\":           ,       \"subscribeParams\":           \"subscribeToRoles\":  \"host\", \"guest\" ,         \"maxSubsBitRate\": 3200,         \"subscribeDegradation\":             \"packetLossThreshold\": 25,           \"degradeGracePeriodSeconds\": 1,           \"recoverGracePeriodSeconds\": 4                  ,       \"permissions\":           \"rtmpStreaming\": true,         \"hlsStreaming\": true,         \"browserRecording\": true,         \"sendRoomState\": false        ,       \"priority\": 1,       \"maxPeerCount\": 0      ,     \"host\":         \"name\": \"host\",       \"publishParams\":           \"allowed\":  \"audio\", \"video\", \"screen\" ,         \"audio\":             \"bitRate\": 32,           \"codec\": \"opus\"          ,         \"video\":             \"bitRate\": 300,           \"codec\": \"vp8\",           \"frameRate\": 30,           \"width\": 480,           \"height\": 360          ,         \"screen\":             \"codec\": \"vp8\",           \"frameRate\": 10,           \"width\": 1920,           \"height\": 1080          ,         \"videoSimulcastLayers\":   ,         \"screenSimulcastLayers\":           ,       \"subscribeParams\":           \"subscribeToRoles\":  \"guest\", \"host\" ,         \"maxSubsBitRate\": 3200,         \"subscribeDegradation\":             \"packetLossThreshold\": 25,           \"degradeGracePeriodSeconds\": 1,           \"recoverGracePeriodSeconds\": 4                  ,       \"permissions\":           \"endRoom\": true,         \"removeOthers\": true,         \"mute\": true,         \"unmute\": true,         \"changeRole\": true,         \"rtmpStreaming\": true,         \"hlsStreaming\": true,         \"browserRecording\": true,         \"sendRoomState\": false        ,       \"priority\": 1,       \"maxPeerCount\": 0          ,   \"settings\":       \"region\": \"in\",     \"subscribeDegradation\":   ,     \"recording\":         \"enabled\": true,       \"upload\":           \"location\": \"<Name of the storage bucket>\",         \"type\": \"<upload type: Supported are s3, gs, oss>\",         \"prefix\": \"<Upload prefix path>\",         \"credentials\":             \"key\": \"<access key ID for accessing the storage bucket>\",           \"secretKey\": \"<secret access key for accessing the storage bucket>\"          ,         \"options\":             \"region\": \"<region of the storage bucket>\"                        ,     \"roomState\":         \"messageInterval\": 5,       \"sendPeerList\": false,       \"stopRoomStateOnJoin\": true,       \"enabled\": false      ,     \"retry\":       ,   \"destinations\":       \"browserRecordings\":         \"test\":           \"name\": \"test\",         \"width\": 1920,         \"height\": 1080,         \"maxDuration\": 1800,         \"thumbnails\":             \"width\": 0,           \"height\": 0          ,         \"presignDuration\": 3600              ,     \"rtmpDestinations\":         \"test\":           \"name\": \"test\",         \"width\": 1080,         \"height\": 1920,         \"maxDuration\": 1800,         \"rtmpUrls\":  \"url\", \"ashdas\", \"ddsfsdf\" ,         \"recordingEnabled\": true              ,     \"hlsDestinations\":         \"test\":           \"name\": \"test\",         \"maxDuration\": 28800,         \"layers\":                           \"width\": 1080,             \"height\": 720,             \"videoBitrate\": 1100,             \"audioBitrate\": 32            ,                         \"width\": 720,             \"height\": 720,             \"videoBitrate\": 600            ,                       ,         \"playlistType\": \"event\",         \"numPlaylistSegments\": 12,         \"videoFrameRate\": 25,         \"enableMetadataInsertion\": true,         \"enableStaticUrl\": true,         \"recording\":             \"hlsVod\": true,           \"singleFilePerLayer\": true,           \"layers\":                               \"width\": 720,               \"height\": 1920,               \"videoBitrate\": 2000,               \"audioBitrate\": 64                          ,           \"thumbnails\":               \"enabled\": true,             \"width\": 720,             \"height\": 1920,             \"offsets\":  2 ,             \"fps\": 60            ,           \"presignDuration\": 604800                            ,   \"createdAt\": \"2022-09-16T12:03:40.068Z\",   \"updatedAt\": \"2022-09-16T12:03:40.068Z\",   \"_id\": \"6324661c4da877930beaecaa\",   \"customer\": \"627cda54ff688c037a39291b\"     </ResponseBox>   Main arguments  Name         Type    Description                                                                                                                                                                 Required   :      :    :                                                                                    :     name <br       string   A unique name you can assign to 100ms templates. Accepted characters are a-z, A-Z, 0-9, space, hyphen  , dot . , underscore _ and colon : . If not provided, this is generated automatically. <br <br    Notes  : <br If create template is called with an existing template name, then that template will be overwritten.<br   No      default <br      boolean  Whether this is default template for a customer. Default template for a customer is used when no template is provided in  create room API (/server-side/v2/Rooms/create-via-api).                                                                              No      roles <br       object   Map of  roleName:roleObject  . Some sane defaults are set for values that are not provided in the roles object.                                                                                                               No      settings <br     object   Global settings for this template.                                                                                                                                                      No      destinations <br   object   Object of type destinations . This object contains information for enabling/configuring features like browserRecordings , rtmpDestinations , & hlsDestinations .                                                                                     No       roles object  Name       Type    Description                                                                                                                           Required        :                                                                     :     name        string  Alias for the role associated with a particular template.                                                                                                    No      publishParams   object  Publish parameters for this role. <br <br Object of type  publishParams ( publishparams-object) with the collection of objects such as allowed , audio , video , screen , etc                                       No      subscribeParams  object  Subscribe parameters for this role. <br <br Object of type  subscribeParams ( subscribe-object) with a mix of arguments and objects like subscribeToRoles , maxSubsBitRate , & subscribeDegradation                             No      permissions    object  Permissions for this role. <br <br Object of type  permission ( permissions-object) with collection of objects like browserRecordings , rtmpDestinations , & hlsDestinations .                                       No      priority      int    Priority of the role for subscribing/publishing. <br <br Priority will determine the order in which the roles will be degraded.<br <br    Allowed values:   1 to 5. <br <br <ul><li>1: highest priority.</li><li>5: lowest priority.</li></ul> <br     No      maxPeerCount    int    The number of peers allowed for a particular role in a session. <br <br  <ul><li>  0  : unlimited (default)</li><li>  n  : \"n\" number of peers are allowed to join with that role.</li><li>  -1  : no peer will be allowed to join with that role.</li></ul>  No       publishParams object  Name   Type    Description                         Required   :   :    :                :     allowed  array   Allowed tracks. Subarray of   audio , video , screen  .  No      audio   object  Audio publish parameters.                  No      video   object  Video publish parameters.                  No      screen   object  Screenshare publish parameters.               No        audio object  Name   Type    Description                            Default  Required   :   :    :                 :   :     bitRate  int    Audio Max bitrate of audio track in kbps.<br Range: 16  128  32    No      codec   string  Codec for the audio track.<br Options:   opus           opus   No        video object  Name    Type    Description                         Default  Required   :    :    :                :   :     bitRate   int    Max bitrate of video track in kbps.<br Range: 30  2000  256    No      codec    string  Codec for the video track.<br Options:   vp8         vp8    No      frameRate  int    Max number of video frames per second.<br Range: 1  30  25    No      height    int    Height of the video track.<br Range: 50  1080       180    No      width    int    Width of the video track.<br Range: 50  1920       320    No        screen object  Name    Type    Description                         Default  Required   :    :    :                :   :     bitRate   int    Max bitrate of screen track in kbps.<br Range: >=500    1024   No      codec    string  Codec for the screen track.<br Options:   vp8         vp8    No      frameRate  int    Max number of screen frames per second<br Range: 1  30  10    No      height    int    Height of the screen track.<br Range: 270  1080      720    No      width    int    Width of the screen track.<br Range: 480  1920      1280   No       subscribeParams object  Name          Type    Description                                Required   :       :    :                    :     maxSubsBitRate     int    Maximum bitrate (in kbps) that can subscribed.               No      subscribeToRoles    array   List of roles which can be subscribed to.                 No      subscribeDegradation  object  Subscribe degradation parameters (adding this enables screen simulcast).  No        subscribeDegradation object  Name            Type  Description                       Default  Required   :        :   :               :   :     packetLossThreshold     int  Threshold for packet loss.<br Range: 1  100      50    No      degradeGracePeriodSeconds  int  Degrade grace period (in seconds).<br Range: 1  10  1     No      recoverGracePeriodSeconds  int  Recover grace period (in seconds).<br Range: 1  10  4     No       permissions object  Name        Type    Description                                                                                  Required  Default   :      :    :                                            :    :    endRoom       boolean  Permission to end room for all.                                                                        No     false   removeOthers    boolean  Permission to remove others from the room.                                                                  No     false   mute        boolean  Permission to request others to mute them.                                                                  No     false   unmute       boolean  Permission to request others to unmute them.                                                                 No     false   changeRole     boolean  Permission to request others to change their role.                                                              No     false   sendRoomState    boolean  Permission to receive room state like peer-count and peer-list                                                        No     false   browserRecording  boolean  Permission to start/stop browser (beam) recording. <br <br  This is also required to start/stop RTMP + browser recording in conjunction with rtmpStreaming permissions.  No     true    rtmpStreaming    boolean  Permission to start/stop RTMP streaming. <br <br  This is also required to start/stop RTMP + browser recording in conjunction with browserRecording permissions.     No     true    hlsStreaming    boolean  Permission to start/stop HLS streaming (with or without recording). <br <br This is also required to  pause/unpause HLS recording or insert metadata in HLS stream.    No     true     settings object  Name    Type    Description                                                                                                                         Required   :    :    :                                                                :     region    string  Region in which the room will be hosted by default. Possible values could be in , us , eu or auto (automatic region selection).                                                            No      recording  object  Object of type recording . This object contains information for enabling recording/setting storage location for recordings.                                                                No      roomState  object  Object of type roomState . This object will help you to define strategy of sending state updates to client SDKs. <br <br Also, enables you to build a \"preview\" screen which shows the state of the room before joining by providing the list of peers  No       recording object  Name   Type    Description                                                                                                  Required   :   :    :                                                    :     enabled  boolean  Enable the SFU recording for its rooms. Disabled by default.                                                                         No      upload   object   Object of type upload . This object contains information on recordings storage location. If you want to store recording with 100ms, and not use your own storage (s3/gs/oss), don't add this to the object  No        upload object  Name     Type    Description                                                     Required   :    :    :                              :     type      string  Upload destination type. Currently, s3 (AWS), gs (Google Cloud Storage), oss (Alibaba Cloud) are supported.  Yes     location    string  Name of the storage bucket in which you want to store all recordings                        Yes     prefix     string  Upload prefix path                                                 Yes     options    object  Additional configurations of type Options to be used for uploading                         No      credentials  object  Object of type credentials . This is used to share the credentials to access the storage bucket specified.     No        options object  Name   Type    Description                                Required   :   :    :                    :     region  string  Region of the account hosting the storage bucket for storing recordings.  No        credentials object  Name    Type    Description                                   Required   :    :    :                     :     key     string  Access Key for the account hosting your storage bucket for storing recordings  Yes     secretKey  string  Secret for the account hosting the storage bucket for storing recordings    Yes       roomState object  Name       Type    Description                                                                                                                               Required   :     :    :                                                                   :     messageInterval  int    Room-state data will be sent over a regular interval of these many seconds. <br <br Consequently, the room state displayed on the preview screen will refresh accordingly. <br <br This value must be a multiple of 5, between 5 and 3600 seconds, both inclusive.  No      sendPeerList    boolean  Enabling this will send peer-list info of the room. If disabled, only the peer count is sent.                                                                                      No      enabled      boolean  This is the list of all the roles which will get the room-state data. You can also individually toggle these settings in the Roles tab under the Permissions section.                                                  No       destinations object  Name        Type    Description                                                                       Required   :      :    :                                       :     browserRecordings  object  Object of type browserRecordings . This can be used to record your sessions to a file, for subsequent access.                      No      rtmpDestinations   object  Object of type rtmpDestinations . This can be used to live stream your video conferencing apps to platforms like YouTube, Twitch, Facebook, MUX, etc.  No      hlsDestinations   object  Object of type hlsDestinations . This can be used to configure arguments for your HLS live streaming                          No       browserRecordings object  Name       Type    Description                                                Required   :     :    :                           :     name        string  The name you can assign to identify the browser recording                         No      width       int    Indicates the width of the screen to be recorded in pixels. For example, 1280 pixels           No      height       int    Indicates the height of the screen to be recorded in pixels. For example, 720 pixels           No      maxDuration    int    Sets the maximum duration, in seconds, of the recording. For example, 1800 seconds(30 mins)        No      thumbnails     object  Object of type thumbnails . This can be used to configure thumbnail's dimension             No      presignDuration  int    Indicates the expiry of the pre-signed URLs—the duration for which the pre-signed URL should be signed.  No       rtmpDestinations object  Name        Type    Description                                                     Required   :      :    :                              :     name        string   The name you can assign to identify the RTMP destination                               No      width        int    Indicates the width of the screen to be recorded in pixels. For example, 1280 pixels                 No      height       int    Indicates the height of the screen to be recorded in pixels. For example, 720 pixels                 No      maxDuration     int    Sets the maximum duration, in seconds, of the recording. For example, 1800 seconds(30 mins)             No      rtmpUrls      object   Object of type rtmpUrls . This can be used to configure the RTMP URLs(max 3) you wish to broadcast the stream to.  No      recordingEnabled  boolean  If recording is required this can be set as true. This value has no effect on streaming.               No       hlsDestinations object  Name           Type    Description                                                         Required   :       :    :                                :     name            string   The name you can assign to identify the HLS destination                                   No      maxDuration        int    Sets the maximum duration, in seconds, of the recording. For example, 1800 seconds(30 mins)                 No      layers           object   Object of type layers . This can be used to configure the various dimensions to be supported for HLS streaming.       No      playlistType        string   Indicates the type of playlist. Can be of type event or live .                              No      numPlaylistSegments    int    Indicates the number of media segments in the playlist. Applicable only for live playlistType. Min/max/default = 3/12/5.  No      videoFrameRate       int    Set this argument to configure the frames per second (or FPS) for your stream. For example, 25                No      enableMetadataInsertion  boolean  Set this to true to enable metadata insertion along with the stream.                             No      enableStaticUrl      boolean  If recording is required this can be set as true. This value has no effect on streaming.                   No      recording         object   Object of type recording . This can be used to enable HLS recording and configure the layers, thumbnails, etc.       No        HLS  recording object  Name         Type    Description                                                                           Required   :      :    :                                         :     hlsVod        boolean  Set this to True to enabled Video on Demand. Output will be a ZIP file of m3u8 format with all the chunks.                          No      singleFilePerLayer  boolean  Set this to True if HLS recording should be available as separate files for various dimensions supported. Output will be Individual mp4 file per HLS layer.  No      layers        object   Object of type layers . This can be used to configure the various dimensions to be supported for HLS recording.                        No      thumbnails      object   Object of type thumbnails . This can be used to configure thumbnail's dimension                                        No      presignDuration    int    Indicates the expiry of the pre-signed URLs—the duration for which the pre-signed URL should be signed.                             No     "
    },
    {
        "title": "Create a template via dashboard",
        "link": "/server-side/v2/policy/create-template-via-dashboard",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/policy/create-template-via-dashboard",
        "keywords": [],
        "content": "    Introduction A template is a collection of roles, room settings, recording and RTMP settings (if used), that are used by the SDK to decide which geography to connect to, which tracks to return to the client, whether to turn on recording when a room is created, etc. Each room is associated with a template.   Create a template via dashboard Users can see or modify the templates by visiting  Templates on Dashboard (https://dashboard.100ms.live/templates) or via API(see below). After updating a template or some part of its like permissions, you need to rejoin or restart the session for the template updates to take place.   Template (/docs/v2/template.png)   Roles Role is a collection of permissions that allows you to perform certain set of operations while being part of the room. It has the following attributes:   Name Every role has a name that should be unique inside a template. This name will be used while generating app tokens and referencing inside a template.   Priority Priority will determine the order in which the roles will be degraded. A lower number represents a higher priority.   Publish Strategies Publish strategies will be used to determine the tracks and their quality which can be published by this role.  Strategy       Description                                                                                                                                                                                                                            Can share audio    Whether the role is allowed to publish the audio track or not.                                                                                                                  Can share video    Whether the role is allowed to publish the video track or not                                                                                                                   Can share screen   Whether the role is allowed to do screen share or not                                                                                                                       Video quality     Quality of the video track which is going to be published by the role. Currently, 6 video qualities 1080p , 720p , 480p , 360p , 240p and 120p are predefined and the user can select one out of these values. This option will be visible only if the   Can share video   is enabled.   Screenshare quality  Quality of the screen which is going to be shared by the role. Currently, 2 video qualities 720p and 1080p are predefined and the user can select one out of these values. This option will be visible only if the   Can share screen   is enabled.                       Subscribe Strategies Subscribe strategies will be used to determine what all roles, this role can subscribe to.  Strategy        Description                                                                                                                                                                                                      Subscribe to      You can select all the roles of the template which this role will subscribe                                                                                             Subscribe Degradation  When this flag is turned on, one or more remote video tracks will be muted automatically when the network condition worsens. Such tracks will be marked as degraded . When the network condition improves, the degraded tracks will automatically be unmuted.    Permissions Permissions will contain a list of additional privileges that this role will have.  Permission               Description                                                                                                                                              Can change any participant's role   With this permission, user will be able to change the role of the other participant's who are present in the room                                  Can mute any participant        With this permission, user will be able to mute any participant's audio and/or video.                                                Can ask participant to unmute     With this permission, user will be able to ask any participant to unmute their audio and/or video.                                         Can remove participant from the room  With this permission, user will be able to remove any participant from the current session of the room.                                       Can end current session of the room  With this permission, user will be able to end the current session of the room.                                                   Can receive room state         With this permission, user will be able to receive room state like peer-count and peer-list on the preview screen.                                 Can start/stop RTMP livestream     With this permission, user will be able to publish live audio/video livestream externally to social media and custom platforms (e.g Youtube/Facebook/Twitter).           Can start/stop HLS livestream     With this permission, user will be able to publish audio/video livestream in the HLS format.                                            Can start/stop Beam Recording     With this permission, user will be able to record meeting/livestream via the browser recording approach where a bot will join the room and record the meeting/livestream as is.    Advanced Settings As the name suggests, Advanced Settings section contains more settings and controls for the advanced user.   Template (/docs/v2/advanced-settings.png)   Preview room state Preview room state enables you to build a \"preview\" screen which shows the state of the room before joining. This room state includes a list of peers, which can be used to show who is in the room. Preview room state also includes recording and streaming state. Preview room state settings define strategy of sending state updates to client SDKs.  Setting                  Description                                                                                                                                                                                                 Room-state Message Interval (in seconds)  Room-state data will be sent over a regular interval of these many seconds. Consequently, the room state displayed on the preview screen will refresh accordingly. This value must be a multiple of 5, between 5 and 3600 seconds, both inclusive.   Send Peer List in Room-state        Enabling this will send peer-list info of the room. If disabled, only the peer count is sent.                                                                             Enable Room-State             If enabled, room-state data will be sent to the preview screen. If disabled, no such room-state data will be sent.                                                                   Roles with room-state permission      This is the list of all the roles which will get the room-state data. You can also individually toggle these settings in the Roles tab under the Permissions section.                                        "
    },
    {
        "title": "Create/update a role",
        "link": "/server-side/v2/policy/create-update-role",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/policy/create-update-role",
        "keywords": [],
        "content": "  This endpoint is used to create/modify a role. <PostRequest title=\"https://api.100ms.live/v2/templates/<template_id>/roles/<role_name>\"   <Request id=\"req-0\">   bash curl location request POST 'https://api.100ms.live/v2/templates/<template_id>/roles/<role_name>'  header 'Authorization: Bearer <management_token>'  header 'Content-Type: application/json'  data-raw '    \"publishParams\":       \"allowed\":         \"audio\",       \"video\",       \"screen\"      ,     \"audio\":         \"bitRate\": 32,       \"codec\": \"opus\"      ,     \"video\":         \"bitRate\": 310,       \"codec\": \"vp8\",       \"frameRate\": 30,       \"width\": 480,       \"height\": 360      ,     \"screen\":         \"codec\": \"vp8\",       \"frameRate\": 10,       \"width\": 1920,       \"height\": 1080      ,     \"videoSimulcastLayers\":   ,     \"screenSimulcastLayers\":       ,   \"subscribeParams\":       \"subscribeToRoles\":         \"guest\",       \"host\"      ,     \"maxSubsBitRate\": 3200,     \"subscribeDegradation\":         \"packetLossThreshold\": 25,       \"degradeGracePeriodSeconds\": 1,       \"recoverGracePeriodSeconds\": 4          ,   \"permissions\":       \"endRoom\": true,     \"removeOthers\": true,     \"mute\": true,     \"unmute\": true,     \"changeRole\": true,     \"sendRoomState\": false    ,   \"priority\": 1,   \"maxPeerCount\": 0  '   </Request> <ResponseBox id=\"res-0\" status=\"200 OK\">   json     \"name\": \"testurlpath\",   \"publishParams\":       \"allowed\":  \"audio\", \"video\", \"screen\" ,     \"audio\":         \"bitRate\": 32,       \"codec\": \"opus\"      ,     \"video\":         \"bitRate\": 310,       \"codec\": \"vp8\",       \"frameRate\": 30,       \"width\": 480,       \"height\": 360      ,     \"screen\":         \"codec\": \"vp8\",       \"frameRate\": 10,       \"width\": 1920,       \"height\": 1080      ,     \"videoSimulcastLayers\":   ,     \"screenSimulcastLayers\":       ,   \"subscribeParams\":       \"subscribeToRoles\":  \"guest\", \"host\" ,     \"maxSubsBitRate\": 3200,     \"subscribeDegradation\":         \"packetLossThreshold\": 25,       \"degradeGracePeriodSeconds\": 1,       \"recoverGracePeriodSeconds\": 4          ,   \"permissions\":       \"endRoom\": true,     \"removeOthers\": true,     \"mute\": true,     \"unmute\": true,     \"changeRole\": true,     \"rtmpStreaming\": true,     \"hlsStreaming\": true,     \"browserRecording\": true,     \"sendRoomState\": false    ,   \"priority\": 1,   \"maxPeerCount\": 0     </ResponseBox>   Arguments  Name               Type    Description                                                                                                  Required   :         :    :                                                     :     roleObject <br          object  Role object corresponding to the given role. <br <br check the  roles object arguments section (/server-side/v2/policy/create-template-via-api roles-object) for the list of arguments supported  Yes     name <br  — path param      string  Providing a name renames the role to the provided name. If a role with the same name already exists, it gives an overwrite error.                                       Yes     template_id <br  — path param  string  Unique identifier of the template you wish to associate the role. <br <br   Example  : 627ce5b637300e55a72ba7ed                                               Yes    "
    },
    {
        "title": "Delete a role",
        "link": "/server-side/v2/policy/delete-a-role",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/policy/delete-a-role",
        "keywords": [],
        "content": "  This endpoint is used to delete a role. Subsequent requests for deleting role which does not exist gives 404. <DeleteRequest title=\"https://api.100ms.live/v2/templates/<template_id>/roles/<role_name>\"   <Request id=\"req-0\">   bash curl location request DELETE 'https://api.100ms.live/v2/templates/<template_id>/roles/<role_name>'  header 'Authorization: Bearer <management_token>'   </Request> <ResponseBox id=\"res-0\" status=\"204 No Content\"></ResponseBox>   Arguments  Name               Type    Description                                                     Required   :         :    :                              :     name <br  — path param      string  Alias of the role you wish to delete.                                        Yes     template_id <br  — path param  string  Unique identifier of the template to which the role is associated. <br <br   Example  : 627ce5b637300e55a72ba7ed  Yes    "
    },
    {
        "title": "List all templates",
        "link": "/server-side/v2/policy/list-templates",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/policy/list-templates",
        "keywords": [],
        "content": "  This API retrieves details of all the templates in your account. <GetRequest title=\"https://api.100ms.live/v2/templates\"   <Request id=\"req-0\">   bash curl location request GET 'https://api.100ms.live/v2/templates'  header 'Authorization: Bearer <management_token>'   </Request> <ResponseBox id=\"res-0\" status=\"200 OK\">   json     \"limit\": 10,   \"data\":               \"id\": \"63188115d11d6db790c73c60\",       \"name\": \"new-template-1662550293\",       \"customerId\": \"627cdddff2e4e30487862ad1\",       \"roles\":           \"guest\":             \"name\": \"guest\",           \"publishParams\":               \"allowed\":                 \"audio\",               \"video\",               \"screen\"              ,             \"audio\":                 \"bitRate\": 32,               \"codec\": \"opus\"              ,             \"video\":                 \"bitRate\": 300,               \"codec\": \"vp8\",               \"frameRate\": 30,               \"width\": 480,               \"height\": 360              ,             \"screen\":                 \"codec\": \"vp8\",               \"frameRate\": 10,               \"width\": 1920,               \"height\": 1080              ,             \"videoSimulcastLayers\":   ,             \"screenSimulcastLayers\":               ,           \"subscribeParams\":               \"subscribeToRoles\":                 \"host\",               \"guest\"              ,             \"maxSubsBitRate\": 3200,             \"subscribeDegradation\":                 \"packetLossThreshold\": 25,               \"degradeGracePeriodSeconds\": 1,               \"recoverGracePeriodSeconds\": 4                          ,           \"permissions\":               \"rtmpStreaming\": true,             \"hlsStreaming\": true,             \"browserRecording\": true,             \"sendRoomState\": false            ,           \"priority\": 1,           \"maxPeerCount\": 0          ,         \"host\":             \"name\": \"host\",           \"publishParams\":               \"allowed\":                 \"audio\",               \"video\",               \"screen\"              ,             \"audio\":                 \"bitRate\": 32,               \"codec\": \"opus\"              ,             \"video\":                 \"bitRate\": 300,               \"codec\": \"vp8\",               \"frameRate\": 30,               \"width\": 480,               \"height\": 360              ,             \"screen\":                 \"codec\": \"vp8\",               \"frameRate\": 10,               \"width\": 1920,               \"height\": 1080              ,             \"videoSimulcastLayers\":   ,             \"screenSimulcastLayers\":               ,           \"subscribeParams\":               \"subscribeToRoles\":                 \"guest\",               \"host\"              ,             \"maxSubsBitRate\": 3200,             \"subscribeDegradation\":                 \"packetLossThreshold\": 25,               \"degradeGracePeriodSeconds\": 1,               \"recoverGracePeriodSeconds\": 4                          ,           \"permissions\":               \"endRoom\": true,             \"removeOthers\": true,             \"mute\": true,             \"unmute\": true,             \"changeRole\": true,             \"rtmpStreaming\": true,             \"hlsStreaming\": true,             \"browserRecording\": true,             \"sendRoomState\": false            ,           \"priority\": 1,           \"maxPeerCount\": 0                  ,       \"settings\":           \"region\": \"in\",         \"subscribeDegradation\":   ,         \"screenSimulcastLayers\":   ,         \"videoSimulcastLayers\":   ,         \"recording\": null,         \"roomState\":             \"messageInterval\": 5,           \"sendPeerList\": false,           \"stopRoomStateOnJoin\": true,           \"enabled\": false          ,         \"retry\":           ,       \"destinations\":   ,       \"createdAt\": \"2022-09-07T11:31:33.618Z\",       \"updatedAt\": \"2022-09-07T11:31:33.618Z\",       \"_id\": \"63188115d11d6db790c73c60\",       \"customer\": \"627cdddff2e4e30487862ad1\"      ,     .....     .....     .....     .....     .....             \"id\": \"627ce39e37300e55a72b6339\",       \"name\": \"nixon_edtech_93d47c52-d1fd-43ce-bed1-5b2df2cdabe0\",       \"customerId\": \"627cdddff2e4e30487862ad1\",       \"roles\":           \"student\":             \"name\": \"student\",           \"publishParams\":               \"allowed\":                 \"audio\",               \"video\"              ,             \"audio\":                 \"bitRate\": 32,               \"codec\": \"opus\"              ,             \"video\":                 \"bitRate\": 300,               \"codec\": \"vp8\",               \"frameRate\": 30,               \"width\": 480,               \"height\": 360              ,             \"screen\":                 \"codec\": \"vp8\",               \"frameRate\": 10,               \"width\": 1920,               \"height\": 1080              ,             \"videoSimulcastLayers\":   ,             \"screenSimulcastLayers\":               ,           \"subscribeParams\":               \"subscribeToRoles\":                 \"teacher\",               \"student\"              ,             \"maxSubsBitRate\": 3200,             \"subscribeDegradation\":                 \"packetLossThreshold\": 25,               \"degradeGracePeriodSeconds\": 1,               \"recoverGracePeriodSeconds\": 4                          ,           \"permissions\":               \"rtmpStreaming\": true,             \"hlsStreaming\": true,             \"browserRecording\": true,             \"sendRoomState\": false            ,           \"priority\": 1,           \"maxPeerCount\": 0          ,         \"teacher\":             \"name\": \"teacher\",           \"publishParams\":               \"allowed\":                 \"audio\",               \"video\",               \"screen\"              ,             \"audio\":                 \"bitRate\": 32,               \"codec\": \"opus\"              ,             \"video\":                 \"bitRate\": 300,               \"codec\": \"vp8\",               \"frameRate\": 30,               \"width\": 480,               \"height\": 360              ,             \"screen\":                 \"codec\": \"vp8\",               \"frameRate\": 10,               \"width\": 1920,               \"height\": 1080              ,             \"videoSimulcastLayers\":   ,             \"screenSimulcastLayers\":               ,           \"subscribeParams\":               \"subscribeToRoles\":                 \"student\",               \"teacher\"              ,             \"maxSubsBitRate\": 3200,             \"subscribeDegradation\":                 \"packetLossThreshold\": 25,               \"degradeGracePeriodSeconds\": 1,               \"recoverGracePeriodSeconds\": 4                          ,           \"permissions\":               \"endRoom\": true,             \"removeOthers\": true,             \"mute\": true,             \"unmute\": true,             \"changeRole\": true,             \"rtmpStreaming\": true,             \"hlsStreaming\": true,             \"browserRecording\": true,             \"sendRoomState\": false            ,           \"priority\": 1,           \"maxPeerCount\": 0                  ,       \"settings\":           \"region\": \"in\",         \"subscribeDegradation\":   ,         \"screenSimulcastLayers\":   ,         \"videoSimulcastLayers\":   ,         \"recording\": null,         \"roomState\":             \"messageInterval\": 5,           \"sendPeerList\": false,           \"stopRoomStateOnJoin\": true,           \"enabled\": false          ,         \"retry\":           ,       \"destinations\":   ,       \"createdAt\": \"2022-05-12T10:38:22.193Z\",       \"updatedAt\": \"2022-05-12T10:38:22.23Z\",       \"_id\": \"627ce39e37300e55a72b6339\",       \"customer\": \"627cdddff2e4e30487862ad1\"          ,   \"last\": \"627ce39e37300e55a72b6339\"     </ResponseBox>   Arguments  Name     Type    Description                                                                          Required   :    :    :                                         :     limit <br   int    Determines the number of template objects to be included in response. <br <br  Default : 10, <br  Allowed values :   Min  : 10,   Max  : 100         No      start <br   string  Determines the starting point. <br \"last\" field of the response can be used as start for the subsequent request. <br <br For example \"627cda81ab4f3b56a077dc33\".  No    "
    },
    {
        "title": "Retrieve a specific role",
        "link": "/server-side/v2/policy/retrieve-a-role",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/policy/retrieve-a-role",
        "keywords": [],
        "content": "  Retrieves the details of a specific role. 100ms provides options to retrieve details of a role with their name. <GetRequest title=\"https://api.100ms.live/v2/templates/<template_id>/roles/<role_name>\"   <Request id=\"req-0\">   bash curl location request GET 'https://api.100ms.live/v2/templates/<template_id>/roles/<role_name>'  header 'Authorization: Bearer <management_token>'   </Request> <ResponseBox id=\"res-0\" status=\"200 OK\">   json     \"name\": \"testurlpath\",   \"publishParams\":       \"allowed\":  \"audio\", \"video\", \"screen\" ,     \"audio\":         \"bitRate\": 32,       \"codec\": \"opus\"      ,     \"video\":         \"bitRate\": 310,       \"codec\": \"vp8\",       \"frameRate\": 30,       \"width\": 480,       \"height\": 360      ,     \"screen\":         \"codec\": \"vp8\",       \"frameRate\": 10,       \"width\": 1920,       \"height\": 1080      ,     \"videoSimulcastLayers\":   ,     \"screenSimulcastLayers\":       ,   \"subscribeParams\":       \"subscribeToRoles\":  \"guest\", \"host\" ,     \"maxSubsBitRate\": 3200,     \"subscribeDegradation\":         \"packetLossThreshold\": 25,       \"degradeGracePeriodSeconds\": 1,       \"recoverGracePeriodSeconds\": 4          ,   \"permissions\":       \"endRoom\": true,     \"removeOthers\": true,     \"mute\": true,     \"unmute\": true,     \"changeRole\": true,     \"rtmpStreaming\": true,     \"hlsStreaming\": true,     \"browserRecording\": true,     \"sendRoomState\": false    ,   \"priority\": 1,   \"maxPeerCount\": 0     </ResponseBox>   Arguments  Name               Type    Description                                                     Required   :         :    :                              :     name <br  — path param      string  Alias of the role you wish to fetch details for.                                   Yes     template_id <br  — path param  string  Unique identifier of the template to which the role is associated. <br <br   Example  : 627ce5b637300e55a72ba7ed  Yes    "
    },
    {
        "title": "Retrieve a specific template",
        "link": "/server-side/v2/policy/retrieve-a-template",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/policy/retrieve-a-template",
        "keywords": [],
        "content": "  Retrieves the details of a specific template. 100ms provides options to retrieve details of a room with their ID. <GetRequest title=\"https://api.100ms.live/v2/templates/<template_id>\"   <Request id=\"req-0\">   bash curl location request GET 'https://api.100ms.live/v2/templates/<template_id>'  header 'Authorization: Bearer <management_token>'   </Request> <ResponseBox id=\"res-0\" status=\"200 OK\">   json     \"id\": \"627ce5b637300e55a72ba7ed\",   \"name\": \"nixon-samuel_eventapp_bf337846-259e-40c1-97bd-52ac0e876d55\",   \"customerId\": \"627cdddff2e4e30487862ad1\",   \"roles\":       \"backstage\":         \"name\": \"backstage\",       \"publishParams\":           \"allowed\":  \"audio\", \"video\", \"screen\" ,         \"audio\":             \"bitRate\": 32,           \"codec\": \"opus\"          ,         \"video\":             \"bitRate\": 400,           \"codec\": \"vp8\",           \"frameRate\": 30,           \"width\": 640,           \"height\": 480          ,         \"screen\":             \"codec\": \"vp8\",           \"frameRate\": 10,           \"width\": 1920,           \"height\": 1080          ,         \"videoSimulcastLayers\":   ,         \"screenSimulcastLayers\":           ,       \"subscribeParams\":           \"subscribeToRoles\":  \"stage\", \"backstage\", \"invitee\", \"viewer\" ,         \"maxSubsBitRate\": 5200,         \"subscribeDegradation\":             \"packetLossThreshold\": 25,           \"degradeGracePeriodSeconds\": 1,           \"recoverGracePeriodSeconds\": 4                  ,       \"permissions\":           \"endRoom\": true,         \"removeOthers\": true,         \"mute\": true,         \"unmute\": true,         \"changeRole\": true,         \"rtmpStreaming\": true,         \"hlsStreaming\": true,         \"browserRecording\": true,         \"sendRoomState\": false        ,       \"priority\": 1,       \"maxPeerCount\": 0      ,     \"invitee\":         \"name\": \"invitee\",       \"publishParams\":           \"allowed\":  \"audio\", \"video\", \"screen\" ,         \"audio\":             \"bitRate\": 32,           \"codec\": \"opus\"          ,         \"video\":             \"bitRate\": 400,           \"codec\": \"vp8\",           \"frameRate\": 30,           \"width\": 640,           \"height\": 480          ,         \"screen\":             \"codec\": \"vp8\",           \"frameRate\": 10,           \"width\": 1920,           \"height\": 1080          ,         \"videoSimulcastLayers\":   ,         \"screenSimulcastLayers\":           ,       \"subscribeParams\":           \"subscribeToRoles\":  \"stage\", \"invitee\", \"backstage\" ,         \"maxSubsBitRate\": 5200,         \"subscribeDegradation\":             \"packetLossThreshold\": 25,           \"degradeGracePeriodSeconds\": 1,           \"recoverGracePeriodSeconds\": 4                  ,       \"permissions\":           \"rtmpStreaming\": true,         \"hlsStreaming\": true,         \"browserRecording\": true,         \"sendRoomState\": false        ,       \"priority\": 1,       \"maxPeerCount\": 0      ,     \"stage\":         \"name\": \"stage\",       \"publishParams\":           \"allowed\":  \"audio\", \"video\", \"screen\" ,         \"audio\":             \"bitRate\": 32,           \"codec\": \"opus\"          ,         \"video\":             \"bitRate\": 400,           \"codec\": \"vp8\",           \"frameRate\": 30,           \"width\": 640,           \"height\": 480          ,         \"screen\":             \"codec\": \"vp8\",           \"frameRate\": 10,           \"width\": 1920,           \"height\": 1080          ,         \"videoSimulcastLayers\":   ,         \"screenSimulcastLayers\":           ,       \"subscribeParams\":           \"subscribeToRoles\":  \"stage\", \"invitee\", \"backstage\" ,         \"maxSubsBitRate\": 5200,         \"subscribeDegradation\":             \"packetLossThreshold\": 25,           \"degradeGracePeriodSeconds\": 1,           \"recoverGracePeriodSeconds\": 4                  ,       \"permissions\":           \"endRoom\": true,         \"changeRole\": true,         \"rtmpStreaming\": true,         \"hlsStreaming\": true,         \"browserRecording\": true,         \"sendRoomState\": false        ,       \"priority\": 1,       \"maxPeerCount\": 0      ,     \"viewer\":         \"name\": \"viewer\",       \"publishParams\":           \"audio\":             \"bitRate\": 32,           \"codec\": \"opus\"          ,         \"video\":             \"bitRate\": 400,           \"codec\": \"vp8\",           \"frameRate\": 30,           \"width\": 640,           \"height\": 480          ,         \"screen\":             \"codec\": \"vp8\",           \"frameRate\": 10,           \"width\": 1920,           \"height\": 1080          ,         \"videoSimulcastLayers\":   ,         \"screenSimulcastLayers\":           ,       \"subscribeParams\":           \"subscribeToRoles\":  \"stage\", \"invitee\", \"backstage\" ,         \"maxSubsBitRate\": 5200,         \"subscribeDegradation\":             \"packetLossThreshold\": 25,           \"degradeGracePeriodSeconds\": 1,           \"recoverGracePeriodSeconds\": 4                  ,       \"permissions\":           \"rtmpStreaming\": true,         \"hlsStreaming\": true,         \"browserRecording\": true,         \"sendRoomState\": false        ,       \"priority\": 1,       \"maxPeerCount\": 0          ,   \"settings\":       \"region\": \"in\",     \"subscribeDegradation\":   ,     \"screenSimulcastLayers\":   ,     \"videoSimulcastLayers\":   ,     \"recording\": null,     \"roomState\":         \"messageInterval\": 5,       \"sendPeerList\": false,       \"stopRoomStateOnJoin\": true,       \"enabled\": false      ,     \"retry\":       ,   \"destinations\":       \"hlsDestinations\":         \"40c39240-4ab6-4ba3-b406-231b1d6f8bc3\":           \"name\": \"40c39240-4ab6-4ba3-b406-231b1d6f8bc3\",         \"role\": \"__internal_recorder\",         \"layers\":                           \"width\": 1280,             \"height\": 720,             \"videoBitrate\": 2048,             \"audioBitrate\": 64            ,                         \"width\": 960,             \"height\": 540,             \"videoBitrate\": 1228,             \"audioBitrate\": 64            ,                         \"width\": 852,             \"height\": 480,             \"videoBitrate\": 850,             \"audioBitrate\": 64            ,                         \"width\": 640,             \"height\": 360,             \"videoBitrate\": 500,             \"audioBitrate\": 64                      ,         \"recording\":             \"thumbnails\":               \"width\": 0,             \"height\": 0                                        ,   \"createdAt\": \"2022-05-12T10:47:18.588Z\",   \"updatedAt\": \"2022-05-12T10:47:18.588Z\",   \"_id\": \"627ce5b637300e55a72ba7ed\",   \"customer\": \"627cdddff2e4e30487862ad1\"     </ResponseBox>   Arguments  Name               Type    Description                                                    Required   :         :    :                              :     template_id <br  — path param  string  Unique identifier of the template you wish to fetch details for. <br <br   Example  : 627ce5b637300e55a72ba7ed  Yes    "
    },
    {
        "title": "Retrieve settings",
        "link": "/server-side/v2/policy/retrieve-settings",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/policy/retrieve-settings",
        "keywords": [],
        "content": "  This endpoint is used to fetch settings. <GetRequest title=\"https://api.100ms.live/v2/templates/<template_id>/settings\"   <Request id=\"req-0\">   bash curl location request GET 'https://api.100ms.live/v2/templates/<template_id>/settings'  header 'Authorization: Bearer <management_token>'   </Request> <ResponseBox id=\"res-0\" status=\"200 OK\">   json     \"region\": \"in\",   \"subscribeDegradation\":   ,   \"screenSimulcastLayers\":   ,   \"videoSimulcastLayers\":   ,   \"recording\": null,   \"roomState\":       \"messageInterval\": 5,     \"sendPeerList\": false,     \"stopRoomStateOnJoin\": true,     \"enabled\": false    ,   \"retry\":        </ResponseBox>   Arguments  Name               Type    Description                                                       Required   :         :    :                               :     template_id <br  — path param  string  Unique identifier of the template you wish to fetch settings details for. <br    Example:   6316d5ca5a2635a5849a0216  Yes    "
    },
    {
        "title": "Template object",
        "link": "/server-side/v2/policy/template-object",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/policy/template-object",
        "keywords": [],
        "content": "    Overview Template is the blueprint of the room. It defines the settings of the room along with the behavior of users who are part of it. Room will inherit the properties from a template that you have specified while creating it. If you have not specified any template then it will pick the default template. Each template will be identified by its id or name. For example default_videoconf_7e450ffc-8ef1-4572-ab28-b32474107b89  <EndpointRequest title=\"https://api.100ms.live/v2/templates\"     Template object  Argument    Description                                                                                                                                                                                       id       Unique identifier for the template. <br <br    Deprecation notice  : _id argument which was available in the previous version has been deprecated.                                              name      Alias for the template                                                                                                               customer_id  Unique identifier for your account. <br <br    Deprecation notice  : customer argument which was available in the previous version has been deprecated.                                           roles     Array of  roles objects (/server-side/v2/policy/create-template-via-api roles-object) with a collection of arguments and objects such as name , publishParams , subscribeParams , permissions , priority , maxPeerCount , etc.   settings    Object of type  settings (/server-side/v2/policy/create-template-via-api settings-object) with the collection of arguments and objects such as region , subscribeDegradation , recording , roomState , etc.             destinations  Object of type destinations . This object contains information for enabling/configuring features like browserRecordings , rtmpDestinations , & hlsDestinations .                                        default    Whether this is default template for a customer. Default template for a customer is used when no template is provided in  create room API (/server-side/v2/Rooms/create-via-api).                                  Understand sub-objects hierarchy   Template sub-objects hierarchy (/docs/v2/template-object-hierarchy.png) Refer to the sub-objects attributes  here (/server-side/v2/policy/create-template-via-api main-arguments)   Postman collection You can use our Postman collection to start exploring 100ms APIs.    Run in Postman (https://run.pstmn.io/button.svg) (https://god.gw.postman.com/run-collection/22726679-47dcd974-29d5-4965-a35b-bf9b74a8b25a?action=collection%2Ffork&collection-url=entityId%3D22726679-47dcd974-29d5-4965-a35b-bf9b74a8b25a%26entityType%3Dcollection%26workspaceId%3Dd9145dd6-337b-4761-81d6-21a30b4147a2) Refer to the  Postman guide (/server-side/v2/introduction/postman-guide) to get started with 100ms API collection. "
    },
    {
        "title": "Update a template",
        "link": "/server-side/v2/policy/update-a-template",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/policy/update-a-template",
        "keywords": [],
        "content": "  This endpoint is used to modify a Template. You need to provide all options for roles and settings to which you want to apply.   Understand sub-objects hierarchy   Template sub-objects hierarchy (/docs/v2/template-object-hierarchy.png) Refer to the sub-objects attributes  here (/server-side/v2/policy/create-template-via-api main-arguments) <PostRequest title=\"https://api.100ms.live/v2/templates/<template_id>\"   <Request id=\"req-0\">   bash curl location request POST 'https://api.100ms.live/v2/templates/6316d5ca5a2635a5849a0216'  header 'Authorization: Bearer <management_token>'  header 'Content-Type: application/json'  data-raw '    \"name\": \"new-template-1662552998\",   \"default\": false,   \"roles\":       \"guest\":         \"name\": \"guest\",       \"publishParams\":   ,       \"subscribeParams\":   ,       \"permissions\":           \"sendRoomState\": true        ,       \"priority\": 1,       \"maxPeerCount\": 0      ,     \"host\":       ,   \"settings\":     '   </Request> <ResponseBox id=\"res-0\" status=\"200 OK\">   json     \"id\": \"6316d5ca5a2635a5849a0216\",   \"name\": \"new-template-1662552988\",   \"customerId\": \"627cdddff2e4e30487862ad1\",   \"roles\":       \"guest\":         \"name\": \"guest\",       \"publishParams\":           \"allowed\":  \"audio\", \"video\", \"screen\" ,         \"audio\":             \"bitRate\": 32,           \"codec\": \"opus\"          ,         \"video\":             \"bitRate\": 300,           \"codec\": \"vp8\",           \"frameRate\": 30,           \"width\": 480,           \"height\": 360          ,         \"screen\":             \"codec\": \"vp8\",           \"frameRate\": 10,           \"width\": 1920,           \"height\": 1080          ,         \"videoSimulcastLayers\":   ,         \"screenSimulcastLayers\":           ,       \"subscribeParams\":           \"subscribeToRoles\":  \"host\", \"guest\" ,         \"maxSubsBitRate\": 3200,         \"subscribeDegradation\":             \"packetLossThreshold\": 25,           \"degradeGracePeriodSeconds\": 1,           \"recoverGracePeriodSeconds\": 4                  ,       \"permissions\":           \"rtmpStreaming\": true,         \"hlsStreaming\": true,         \"browserRecording\": true,         \"sendRoomState\": false        ,       \"priority\": 1,       \"maxPeerCount\": 0      ,     \"host\":         \"name\": \"host\",       \"publishParams\":           \"allowed\":  \"audio\", \"video\", \"screen\" ,         \"audio\":             \"bitRate\": 32,           \"codec\": \"opus\"          ,         \"video\":             \"bitRate\": 300,           \"codec\": \"vp8\",           \"frameRate\": 30,           \"width\": 480,           \"height\": 360          ,         \"screen\":             \"codec\": \"vp8\",           \"frameRate\": 10,           \"width\": 1920,           \"height\": 1080          ,         \"videoSimulcastLayers\":   ,         \"screenSimulcastLayers\":           ,       \"subscribeParams\":           \"subscribeToRoles\":  \"guest\", \"host\" ,         \"maxSubsBitRate\": 3200,         \"subscribeDegradation\":             \"packetLossThreshold\": 25,           \"degradeGracePeriodSeconds\": 1,           \"recoverGracePeriodSeconds\": 4                  ,       \"permissions\":           \"endRoom\": true,         \"removeOthers\": true,         \"mute\": true,         \"unmute\": true,         \"changeRole\": true,         \"rtmpStreaming\": true,         \"hlsStreaming\": true,         \"browserRecording\": true,         \"sendRoomState\": false        ,       \"priority\": 1,       \"maxPeerCount\": 0          ,   \"settings\":       \"region\": \"in\",     \"subscribeDegradation\":   ,     \"screenSimulcastLayers\":   ,     \"videoSimulcastLayers\":   ,     \"recording\": null,     \"roomState\":         \"messageInterval\": 5,       \"sendPeerList\": false,       \"stopRoomStateOnJoin\": true,       \"enabled\": false      ,     \"retry\":       ,   \"destinations\":   ,   \"createdAt\": \"0001-01-01T00:00:00Z\",   \"updatedAt\": \"0001-01-01T00:00:00Z\",   \"_id\": \"6316d5ca5a2635a5849a0216\",   \"customer\": \"627cdddff2e4e30487862ad1\"     </ResponseBox>   Arguments  Name               Type    Description                                                                                                                                               Required   :         :    :                                                                           :     template_id <br  — path param  string   Unique identifier of the template you wish to update. <br <br   Example  : 627ce5b637300e55a72ba7ed                                                                                                  Yes     roles <br             object   Modified map of  roleName:roleObject  . Since this is a POST request, you need to provide the whole roles object to modify/persist/override it. <br <br check the  roles object arguments section (/server-side/v2/policy/create-template-via-api roles-object) for the list of arguments supported  Yes     settings <br           object   Modified settings. Since this is a POST request, you need to provide the whole settings object to modify/persist/override it. <br <br check the  settings object arguments section (/server-side/v2/policy/create-template-via-api settings-object) for the list of arguments supported        Yes     name <br             string   If you don't provide the name , earlier provided name is used. Trying to modify a template's name to match another existing template's name produces overwrite error.                                                                  No      default <br            boolean  Whether this is default template for customer. The default template for a customer is used when no template is provided in create room API.                                                                               No     "
    },
    {
        "title": "Update settings",
        "link": "/server-side/v2/policy/update-settings",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/policy/update-settings",
        "keywords": [],
        "content": "  This endpoint is used to update settings. <PostRequest title=\"https://api.100ms.live/v2/templates/<template_id>/settings\"   <Request id=\"req-0\">   bash curl location request POST 'https://api.100ms.live/v2/<template_id>/settings'  header 'Authorization: Bearer <management_token>'  header 'Content-Type: application/json'  data-raw '    \"region\": \"eu\",   \"subscribeDegradation\":   ,   \"recording\":       \"enabled\": true,     \"upload\":         \"type\": \"<upload type: supported are s3, gs, oss>\",       \"location\": \"<Name of the storage bucket>\",       \"prefix\": \"<Upload prefix path>\",       \"options\":           \"region\": \"<region of the storage bucket>\"        ,       \"credentials\":           \"key\": \"<access key ID for accessing the storage bucket>\",         \"secretKey\": \"<secret access key for accessing the storage bucket>\"                  ,   \"screenSimulcastLayers\":   ,   \"videoSimulcastLayers\":   ,   \"roomState\":       \"messageInterval\": 5,     \"sendPeerList\": false,     \"stopRoomStateOnJoin\": true,     \"enabled\": false      '   </Request> <ResponseBox id=\"res-0\" status=\"200 OK\">   json     \"region\": \"eu\",   \"subscribeDegradation\":   ,   \"screenSimulcastLayers\":   ,   \"videoSimulcastLayers\":   ,   \"recording\":       \"enabled\": true,     \"upload\":         \"location\": \"<Name of the storage bucket>\",       \"type\": \"<upload type: supported are s3, gs, oss>\",       \"prefix\": \"<Upload prefix path>\",       \"credentials\":           \"key\": \"<access key ID for accessing the storage bucket>\",         \"secret\": \"<secret access key for accessing the storage bucket>\"        ,       \"options\":           \"region\": \"<region of the storage bucket>\"                  ,   \"roomState\":       \"messageInterval\": 5,     \"sendPeerList\": false,     \"stopRoomStateOnJoin\": true,     \"enabled\": false    ,   \"retry\":        </ResponseBox>   Arguments  Name               Type    Description                                                                                                      Required   :         :    :                                                      :     template_id <br  — path param  string  Unique identifier of the template you wish to update settings details for. <br    Example:   6316d5ca5a2635a5849a0216                                                Yes     settingsObject <br        object  Settings object corresponding to the given template_id. <br <br check the  settings object arguments section (/server-side/v2/policy/create-template-via-api settings-object) for the list of arguments supported  Yes    "
    },
    {
        "title": "Create via API",
        "link": "/server-side/v2/Rooms/create-via-api",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/Rooms/create-via-api",
        "keywords": [],
        "content": "  Creates a new room. This provides a more scalable way of creating a room. <PostRequest title=\"https://api.100ms.live/v2/rooms\"   <Request id=\"req-comp-0\">   bash curl location request POST 'https://api.100ms.live/v2/rooms'  header 'Authorization: Bearer <management_token>'  header 'Content-Type: application/json'  data-raw '    \"name\": \"new-room-1662723668\",   \"description\": \"This is a sample description for the room\",   \"template_id\": \"<template_id of the template you wish to associate with the room>\",   \"region\": \"us\"  '   </Request> <ResponseBox id=\"resp-0\" status=\"200 OK\">   json     \"id\": \"631b2654f771854d9bf633df\",   \"name\": \"new-room-1662723668\",   \"enabled\": true,   \"description\": \"This is a sample description for the room\",   \"customer\": \"627cdddff2e4e30487862ad1\",   \"recording_info\":       \"enabled\": false    ,   \"template_id\": \"63188115d11d6db790c73c60\",   \"template\": \"new-template-1662550293\",   \"region\": \"us\",   \"created_at\": \"2022-09-09T11:41:08.082Z\",   \"updated_at\": \"2022-09-09T11:41:08.074Z\"     </ResponseBox>   Main Arguments  Name          Type    Description                                                                                                                                                                           Required   :       :    :                                                                                         :     name <br        string  An alias you can assign to 100ms rooms. This is case-insensitive . <br <br Accepted characters are a-z, A-Z, 0-9, and .  : _ . If not provided, this is generated automatically. <br <br   Note  : If Create room request is triggered with an existing room name, then the corresponding room ID is returned.                     No      description <br     string  A string to describe your room's usage. For example \"9PM English Class Batch 2\"                                                                                                                                         No      template_id <br     string  Template ID of template you wish to associate with the room. You can get template ID either from the templates section on the  dashboard (https://dashboard.100ms.live/dashboard) or use the  Template API (../policy/retrieve-a-template). <br <br   Note  : default template will be assigned if template is not specified in the create room request .  No      recording_info <br   object  Object of type recording_info . This object contains information for enabling recording/setting storage location for recordings. <br <br Check the  recording_info arguments ( recording-info-arguments) below for more information.                                                             No      region <br       string  Region in which you want to create room. <br <br  in  India <br  us  United States <br  eu  European Union <br  auto  Automatic region selection (default, and in case this parameter is not provided)                                                                     No     >   Warning:   If you create a room with the name of an existing room, the same room will be updated with the new configuration passed in the request payload. <br  For example, if the existing room was assigned to template-ABC earlier and in the request payload you've used template-DEF, then template-DEF will be assigned to the existing room.   recording_info arguments >   Note  : This object enables recording and configuring storage during room creation. But we recommend configuring it at a template level through the  Dashboard (https://dashboard.100ms.live/dashboard), where the config validator can help with validating inputs proactively.  Name     Type    Description                                                                                                                                                        Required   :    :    :                                                                                :     enabled    boolean  Enable SFU recording. Disabled by default. <br <br    Note:   This argument is only applicable to enable/disable  SFU recording (/server-side/v2/Destinations/recording). Refer to  RTMP Streaming & Browser Recording (/server-side/v2/Destinations/rtmp-streaming-and-browser-recording) guide for other options.   No      upload_info  object   Object of type upload_info . This object contains information on recordings storage location. <br <br If you want to store recording with 100ms, and not use your own storage (s3/gs/oss), don't add this to the object. <br <br Check the  upload_info object ( upload-info-arguments) below for more information.  No      recording_info in the room acts as an atomic property. It depends on the recording settings defined in a template as below:   If the recording_info key is not provided in the API, the room will fill it with recording settings from the template. And the response body will include recording_source_template: true . This implies that whenever recording info is modified in the template, the recording settings for the room will also be updated.   If the recording_info key is provided in the API, the recording settings of the room won't be affected by the template at all. To know more about recording please visit  Recording (/server-side/v2/Destinations/recording).   upload_info arguments  Name     Type    Description                                                                Required   :    :    :                                   :     type      string  Upload destination type. Currently, s3 (AWS), gs (Google Cloud Storage), oss (Alibaba Cloud) are supported.             Yes     location    string  Name of the storage bucket in which you want to store all recordings                                   Yes     prefix     string  Upload prefix path                                                            No      options    object  Additional configurations of type Options to be used for uploading. <br <br Check the options arguments below for more information.  No      credentials  object  Object of type credentials . This is used to share the credentials to access the storage bucket specified.                No        Options arguments  Name   Type    Description                                Required   :   :    :                    :     region  string  Region of the account hosting the storage bucket for storing recordings.  No        Credentials arguments  Name   Type    Description                                   Required   :   :    :                     :     key    string  Access Key for the account hosting your storage bucket for storing recordings  Yes     secret  string  Secret for the account hosting the storage bucket for storing recordings    Yes     The access keys should have read(GetObject) and write(PutObject) permissions for the storage bucket. For more details check  https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html "
    },
    {
        "title": "Create via dashboard",
        "link": "/server-side/v2/Rooms/create-via-dashboard",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/Rooms/create-via-dashboard",
        "keywords": [],
        "content": "    Introduction Room is a virtual space which holds conferencing of the people. To allow users to join a 100ms video conferencing session inside your app, you first need to create a room .   Create Room via Dashboard You can create room on  100ms Dashboard (https://dashboard.100ms.live/create-room). While creating a room you can specify it's name, template, region and/or enable SFU recording. Room creation via dashboard is helpful in doing quick POC or creating smaller number of rooms.   Create a room (/docs/v2/create-a-room.png) >   Warning:   If you create a room with the name of an existing room, the same room will be updated with the new configuration passed in the request payload. <br  For example, if the existing room was assigned to template-ABC earlier and in the request payload you've used template-DEF, then template-DEF will be assigned to the existing room. You can add or modify templates on  100ms Dashboard (https://dashboard.100ms.live/templates).   Room details on dashboard (/docs/v2/role-dashboard.png) "
    },
    {
        "title": "Disable/Enable a room",
        "link": "/server-side/v2/Rooms/disable-or-enable",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/Rooms/disable-or-enable",
        "keywords": [],
        "content": "  Trigger this request to disable a specific room. >   Note:   If you disable a room, that room can be used for only the current session and users will not be able to join the room later. You can use this API with the enabled param set to true to enable the room again. <PostRequest title=\"https://api.100ms.live/v2/rooms/<room_id>\"   <Request id=\"req-0\">   bash curl location request POST 'https://api.100ms.live/v2/rooms/<room_id>'  header 'Authorization: Bearer <management_token>'  header 'Content-Type: application/json'  data-raw '    \"enabled\": false  '   </Request> <ResponseBox id=\"res-0\" status=\"200 OK\">   json     \"id\": \"6316d5cfae8c4271df5d0554\",   \"name\": \"new-room-1662440912\",   \"enabled\": false,   \"description\": \"\",   \"customer_id\": \"627cdddff2e4e30487862ad1\",   \"recording_source_template\": true,   \"enabled_source_template\": true,   \"recording\": null,   \"template_id\": \"\",   \"template\": \"\",   \"region\": \"\",   \"created_at\": \"2022-09-06T05:08:31.935Z\",   \"updated_at\": \"2022-09-08T14:58:25.629819038Z\"     </ResponseBox>   Arguments  Name             Type    Description                                                      Required   :        :    :                              :     room_id <br  — path param  string   Unique identifier of the room you wish to disable. <br <br   Example  : 627cda81ab4f3b56a077dc33          Yes     enabled <br  — body param  boolean  Status of the room to be set. <br  Allowed values : true, false. <br Should be set to false to disable the room  Yes    "
    },
    {
        "title": "List rooms",
        "link": "/server-side/v2/Rooms/list-rooms",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/Rooms/list-rooms",
        "keywords": [],
        "content": "    Overview This API retrieves details of a list of rooms in your account. 100ms provides options to   retrieve the complete list of rooms or   rooms that are enabled/disabled or   rooms based on their creation time-range or   combinations of these.   List all rooms This API retrieves details of all the rooms in your account. <GetRequest title=\"https://api.100ms.live/v2/rooms\"   <Request id=\"req-0\">   bash curl location request GET 'https://api.100ms.live/v2/rooms'  header 'Authorization: Bearer <management_token>'   </Request> <ResponseBox id=\"res-0\" status=\"200 OK\">   json     \"limit\": 10,   \"data\":               \"id\": \"62bbc84        0a6aa1a\",       \"name\": \"test-room\",       \"enabled\": true,       \"description\": \"This is a test room\",       \"customer\": \"627cda        a39291b\",       \"recording_source_template\": true,       \"template_id\": \"627cd        784227d3\",       \"template\": \"default_createown_e319635a-    -    -    -ae9e95891e31\",       \"region\": \"in\",       \"created_at\": \"2022-06-29T03:34:30.194Z\",       \"key\": \"627           a39291b:test-room\",       \"updated_at\": \"0001-01-01T00:00:00Z\"      ,             \"id\": \"627cda81        6b077bc33\",       \"name\": \"4b2705aa-    -    -    -f470c06d0c03\",       \"enabled\": true,       \"description\": \"This is a test room\",       \"customer\": \"627cd        037a39291b\",       \"recording_source_template\": false,       \"recording\":           \"enabled\": false        ,       \"template_id\": \"627cda        84227d3\",       \"template\": \"default_createown_e319635a-    -    -    -ae9e95891e31\",       \"region\": \"in\",       \"created_at\": \"2022-05-12T09:59:29.343Z\",       \"key\": \"627cda        037a39291b:4b2705aa-    -    -    -f470c06d0c03\",       \"updated_at\": \"0001-01-01T00:00:00Z\"          ,   \"last\": \"627cda81bd4f3b56b077bc33\"     </ResponseBox>    Allowed filters  Name      Type    Description                                                                            Required   :     :    :                                          :     enabled <br   boolean  Flag to indicate if the room is enabled. <br <br  Default : true, <br  Allowed values : true, false                               No      after <br    string   Check for rooms created with a timestamp greater than or equal to \"after\" <br <br  Format : YYYY-MM-DDTHH:mm:ss.s+z.                       No      before <br    string   Check for rooms created with a timestamp less than or equal to \"after\" <br <br  Format : YYYY-MM-DDTHH:mm:ss.s+z.                        No      limit <br    int    Determines the number of room objects to be included in response. <br <br  Default : 10, <br  Allowed values :   Min  : 10,   Max  : 100             No      start <br    string   Determines the starting point. <br  last field of the response can be used as start for the subsequent request. <br <br   For example   \"627cda81ab4f3b56a077dc33\".  No     "
    },
    {
        "title": "Room object",
        "link": "/server-side/v2/Rooms/object",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/Rooms/object",
        "keywords": [],
        "content": "  <EndpointRequest title=\"https://api.100ms.live/v2/rooms\"     Object  Argument     Description                                                                                                                                                                                                                                                                                                                      id        Unique identifier for the room                                                                                                                                                                                               name       Alias for the room                                                                                                                                                                                                     enabled     Indicates whether the room is enabled or disabled. <br <br    Note:   If you disable a room, that room can be used for only the current session and users will not be able to join the room later. You can use the  Disable/Enable a room API (/server-side/v2/Rooms/disable-or-enable) with the enabled param set to true to enable the room again.                                 description   Description for the room                                                                                                                                                                                                  customer     Unique identifier for your account                                                                                                                                                                                             recording_info  Indicates whether the recording is enabled or disabled. <br <br    Note:   enabled  argument (/server-side/v2/Rooms/create-via-api recording-info-arguments) in this object is only applicable to enable/disable  SFU recording (/server-side/v2/Destinations/recording). Refer to  RTMP Streaming & Browser Recording (/server-side/v2/Destinations/rtmp-streaming-and-browser-recording) guide for other options.   template_id   Identifier for the template used for this room                                                                                                                                                                                       template     Name for the template                                                                                                                                                                                                    region      The region in which the room has to be created                                                                                                                                                                                        Postman collection You can use our Postman collection to start exploring 100ms APIs.    Run in Postman (https://run.pstmn.io/button.svg) (https://god.gw.postman.com/run-collection/22726679-47dcd974-29d5-4965-a35b-bf9b74a8b25a?action=collection%2Ffork&collection-url=entityId%3D22726679-47dcd974-29d5-4965-a35b-bf9b74a8b25a%26entityType%3Dcollection%26workspaceId%3Dd9145dd6-337b-4761-81d6-21a30b4147a2) Refer to the  Postman guide (/server-side/v2/introduction/postman-guide) to get started with 100ms API collection. "
    },
    {
        "title": "Overview",
        "link": "/server-side/v2/Rooms/overview",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/Rooms/overview",
        "keywords": [],
        "content": "    What is a room? A room is a virtual space where one or more peers communicate with each other. It is a central entity that provides real-time communication capabilities for any video conferencing or live streaming session. To allow users to join a 100ms video conferencing or a live streaming session inside your app, you first need to create a room.   Room (/docs/v2/room.png) The above diagram illustrates how a room acts as the central entity in the lifecycle of an interactive audio/video session and the transition of the room from one state to another based on the configuration of the room and the peer's presence.   Room APIs Room APIs will enable you to script actions from the server side based on your business logic and requirements, actions such as:    creating a room (/server-side/v2/Rooms/create-via-api),    updating room properties (/server-side/v2/Rooms/update-a-room),    retrieving room details (/server-side/v2/Rooms/retrieve-room), and    disabling/enabling a room (/server-side/v2/Rooms/disable-or-enable). You can control peer behavior and build interactivity scenarios in an active room with the help of  active room APIs (/server-side/v2/active-rooms/overview).   Postman collection You can use our Postman collection to start exploring 100ms APIs.    Run in Postman (https://run.pstmn.io/button.svg) (https://god.gw.postman.com/run-collection/22726679-47dcd974-29d5-4965-a35b-bf9b74a8b25a?action=collection%2Ffork&collection-url=entityId%3D22726679-47dcd974-29d5-4965-a35b-bf9b74a8b25a%26entityType%3Dcollection%26workspaceId%3Dd9145dd6-337b-4761-81d6-21a30b4147a2) Refer to the  Postman guide (/server-side/v2/introduction/postman-guide) to get started with 100ms API collection. "
    },
    {
        "title": "Retrieve a specific room",
        "link": "/server-side/v2/Rooms/retrieve-room",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/Rooms/retrieve-room",
        "keywords": [],
        "content": "  Retrieves the details of a specific room. 100ms provides options to retrieve details of a room with their ID or name. <GetRequest title=\"https://api.100ms.live/v2/rooms/<room_id>\"   <Request id=\"req-0\">   bash curl location request GET 'https://api.100ms.live/v2/rooms/<room_id>'  header 'Authorization: Bearer <management_token>'   </Request> <ResponseBox id=\"res-0\" status=\"200 OK\">   json     \"id\": \"631a05390e6ffae22efa610b\",   \"name\": \"sampleroom\",   \"enabled\": true,   \"description\": \"This is sample description for room\",   \"customer\": \"627cdddff2e4e30487862ad1\",   \"recording_source_template\": false,   \"recording_info\":       \"enabled\": true,     \"upload_info\":         \"type\": \"s3\",       \"location\": \"brytecam-test-bucket-ap-south-1\",       \"prefix\": \"dev/627cdddff2e4e30487862ad1\"          ,   \"template_id\": \"63188115d11d6db790c73c60\",   \"template\": \"new-template-1662550293\",   \"region\": \"in\",   \"created_at\": \"2022-09-08T15:07:37.83Z\",   \"updated_at\": \"2022-09-08T15:07:37.83Z\"     </ResponseBox>   Arguments  Name             Type    Description                                                  Required   :        :    :                             :     room_id <br  — path param  string  Unique identifier of the room you wish to fetch details for. <br <br   Example  : 627cda81ab4f3b56a077dc33  Yes     name <br  — query param   string  Alias of the room you wish to fetch details for. <br <br   Example  : test-room               No     "
    },
    {
        "title": "Update a room",
        "link": "/server-side/v2/Rooms/update-a-room",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/Rooms/update-a-room",
        "keywords": [],
        "content": "  You can use this API to update various details of the room such as name, description, recording_info and region. >   Note  :  recording_info (./create-via-api recording-info-arguments) object enables recording and configuring storage during room creation. But we recommend configuring it at a template level through the  Dashboard (https://dashboard.100ms.live/dashboard), where the config validator can help with validating inputs proactively. <PostRequest title=\"https://api.100ms.live/v2/rooms/<room_id>\"   <Request id=\"req-0\">   bash curl location request POST 'https://api.100ms.live/v2/rooms/<room_id>'  header 'Authorization: Bearer <management_token>'  header 'Content-Type: application/json'  data-raw '    \"name\": \"update-room-name\",   \"description\": \"Updated description for the room\",   \"recording_info\":       \"enabled\": true,     \"upload_info\":         \"type\": \"<Updated upload type (supported are s3, gs, oss)>\",       \"location\": \"<Updated Name of the storage bucket>\",       \"prefix\": \"<Updated Upload prefix path>\",       \"options\":           \"region\": \"<Updated region of the storage bucket, example ap-south-1 >\"        ,       \"credentials\":           \"key\": \"<Updated access key ID for accessing the storage bucket>\",         \"secret\": \"<Updated secret access key for accessing the storage bucket>\"                  ,   \"region\": \"us\"  '   </Request> <ResponseBox id=\"res-0\" status=\"200 OK\">   json     \"id\": \"63215ea14354ff09328378c6\",   \"name\": \"update-room-name\",   \"enabled\": true,   \"description\": \"Updated description for the room\",   \"customer_id\": \"627cda54ff688c037a39291b\",   \"recording_source_template\": false,   \"enabled_source_template\": false,   \"recording_info\":       \"enabled\": true,     \"upload_info\":         \"type\": \"s3\",       \"location\": \"<Updated Name of the AWS s3 bucket>\",       \"prefix\": \"<Upload prefix path>\",       \"credentials\":           \"key\": \"<Updated access key ID for the AWS account/S3 bucket>\",         \"secret\": \"<Updated secret access key for the AWS account/S3 bucket>\"        ,       \"options\":           \"region\": \"<Updated region of the s3 bucket, example ap-south-1 >\"                  ,   \"template_id\": \"6311c9b3ba3f6f0133f899ee\",   \"template\": \"new-template-1662110132\",   \"region\": \"us\",   \"created_at\": \"2022-09-14T04:54:57.707Z\",   \"updated_at\": \"2022-09-26T12:17:48.37Z\",   \"customer\": \"627cda54ff688c037a39291b\"     </ResponseBox>   Arguments  Name          Type    Description                                                                                                                                                                                                                                                                       Required   :       :    :                                                                                                                                       :     name <br        string  An alias you can assign to 100ms rooms. This is case-insensitive . <br <br Accepted characters are a-z, A-Z, 0-9, and .  : _ . If not provided, this is generated automatically. <br <br   Note  : Update room request is triggered with an existing room name, then the corresponding room ID is returned.                                                                                                                   No      description <br     string  A string to describe your room's usage. For example \"9PM English Class Batch 2\"                                                                                                                                                                                                                                     No      recording_info <br   object  Object of type recording_info . This object contains information for enabling recording/setting storage location for recordings. <br <br Check the  recording_info arguments (/server-side/v2/Rooms/create-via-api recording-info-arguments) below for more information. <br <br    Note:   This argument is only applicable to enable/disable  SFU recording (/server-side/v2/Destinations/recording). Refer to  RTMP Streaming & Browser Recording (/server-side/v2/Destinations/rtmp-streaming-and-browser-recording) guide for other options.  No      region <br       string  Region in which you want to create room. <br <br  in  India <br  us  United States <br  eu  European Union <br  auto  Automatic region selection (default, and in case this parameter is not provided)                                                                                                                                                                 No     "
    },
    {
        "title": "Example - Build attendance",
        "link": "/server-side/v2/Sessions/example-build-attendance",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/Sessions/example-build-attendance",
        "keywords": [],
        "content": "    Introduction This guide will help you understand various approaches available to calculate total session duration, aggregated participants' duration, and individual participants' duration for a specific session. We recommend two approaches: 1. Use the  Retrieve a specific session API (/server-side/v2/Sessions/retrieve-a-session) to get the individual participants' duration, total session duration, and aggregated participants' duration. 2. Use the data export option available on  Sessions page (https://dashboard.100ms.live/sessions) in your 100ms dashboard to get the total session duration and aggregated participants' duration. Please follow the below sections for more details with examples.   Using Sessions API Use this API to build the attendance system to gather information about peers for a session, such as join/leave timestamps, duration, peer_id, user_id, etc. Let's take a quick look at the overall flow of how we can build this: <StepsToc   parentId=\"using-sessions-api\"   descriptions=       'Listen to the session.close.success webhook event to monitor the end of a session.',     'Fetch details of peers using \"Retrieve a specific session API\".',     \"Find the difference between the joined_at and left_at fields for the respective peer to calculate the individual participants' duration.\",     \"Find the sum of duration of all the peers to calculate aggregated participants' duration.\",     'Find the difference between created_at and updated_at timestamps to calculate the total session duration.',     'Look at the example output to understand how you can store it in your database.'        <StepsContainer id=\"using-sessions-api\">   Listen to session.close.success webhook event Listen to the session.close.success  webhook event (/server-side/v2/introduction/webhook session-close-success), which indicates that the particular session has ended.   javascript // Build attendance app.post('/webhooks', async function (req, res)     var eventType = req.body.type;   var sessionId = req.body.data 'session_id' ;   // Listen to the session.close.success webhook event   if (eventType === 'session.close.success')       console.log('Session has ended');     try         console.log(sessionId);       catch (err)         console.log(err.data);           else console.log('Some other event');  );     Fetch details of peers  Sessions API Once you receive the webhook, fetch session_id and trigger the  Retrieve a specific session API (/server-side/v2/Sessions/retrieve-a-session) to fetch details of all the peers who participated in the session.   javascript // Trigger Retrieve a session API to fetch details of all peers in the session. var response = await axios(    method: 'get',   url: baseUrl + '/sessions/' + sessionId,   headers:   Authorization: Bearer $ managementToken     ); var parsedData = response.data;     Calculate the individual participants' duration   Calculate the individual participants' duration by finding the difference between the joined_at and left_at fields for the respective peer     Map the duration against peer_id or user_id (your internal user identifier used while creating the  app token (/server-side/v2/introduction/authentication-and-tokens app-token) for peers). >   Note  : > >   100ms generates a peer_id (unique identifier) every time a peer joins a room. If a peer leaves and re-joins the room, they will be assigned a new peer_id. >   A user_id will be unique to a peer as it's associated with the  app token (/server-side/v2/introduction/authentication-and-tokens app-token) they use to join the room. The below example uses user_id to avoid duplicate entries for a single peer.   javascript // Calculate individual participants' duration const peers = Object.values(parsedData.peers); const durationByUser = peers.reduce((acc, peer) =>     const duration = moment.duration(moment(peer.left_at).diff(moment(peer.joined_at))).asMinutes();   const roundedDuration = Math.round(duration   100) / 100;   acc peer.user_id  = (acc peer.user_id   0) + roundedDuration;   return acc;  ,   ); const result = Object.entries(durationByUser).map(( user_id, duration ) => (    user_id,   duration  )); console.log(result);     Calculate aggregated participants' duration Calculate aggregated participants' duration by adding the duration of all the peers.   javascript // Calculate aggregated participants' duration const totalDuration = Object.values(durationByUser)   .reduce((a, b) => a + b)   .toFixed(2); console.log( Total duration for all peers: $ totalDuration  minutes );     Calculate total session duration Calculate total session duration by finding the difference between created_at and updated_at timestamps.   javascript // Calculate total session duration const sessionDuration = moment   .duration(moment(parsedData.updated_at).diff(moment(parsedData.created_at)))   .asMinutes()   .toFixed(2); console.log( Session duration is: $ sessionDuration  minutes );     Example output Check the below example to understand how you can store the data in your preferred database for later review.   json      user_id: 'user1', duration: 2.42  ,    user_id: 'user2', duration: 2.37     Total duration for all peers: 4.79 minutes Total session duration is: 2.42 minutes   </StepsContainer> Check the complete code sample  below ( complete-code-example-using-sessions-api).   Using data export  100ms dashboard 100ms provides a simple way to extract the Sessions data to create reports using the exported data. Please follow the below steps:   Go to the  Sessions page (https://dashboard.100ms.live/sessions) on your dashboard.   Click on the download icon next to \"Your Sessions\" at the top of the page.   This will initiate a data export request that will be shared as a CSV file via email to your account email address. <video loop=\"true\" autoplay=\"autoplay\" controls=\"controls\" id=\"vid\" muted>   <source src=\"/docs/guides/export-csv.mp4\" type=\"video/mp4\"   </video> <br     Example CSV The exported data will contain data for fields such as:   session_id,   room_id,   created_at (session start time),   recording_enabled (recording status),   total_peers,   participant duration (aggregated duration for all peers),   session_duration, and   recording_duration (only available for  SFU recording (/server-side/v2/Destinations/recording) at the moment).   Room (/docs/v2/exported-data.png)   Other possible methods   You can use the peer.leave.success  webhook event (/server-side/v2/introduction/webhook peer-leave-success) for a specific session to calculate individual participants' duration and aggregated participants' duration.     From the webhook events fetch details of joined_at and left_at fields for each peer and find the difference to calculate individual participants' duration.     Find the sum of the duration for all the peers to calculate the aggregated participants' duration.   If you need to get the duration of an ongoing session, you can use the below logic:     Use  Retrieve a specific session API (/server-side/v2/Sessions/retrieve-a-session) to fetch details of individual peers currently active in the session.     Find the difference between the value for joined_at and current_time (use some library to get the current time) to get the current duration.   Complete code example  Using sessions API   javascript var express = require('express'); var app = express(); var bodyParser = require('body-parser'); var axios = require('axios'); var jwt = require('jsonwebtoken'); var moment = require('moment'); var uuid4 = require('uuid4'); var baseUrl = 'https://api.100ms.live/v2'; var app_access_key = '<YOUR_APP_ACCESS_KEY>'; var app_secret = '<YOUR_APP_SECRET>'; app.use(bodyParser.urlencoded(  extended: false  )); app.use(bodyParser.json()); var managementToken = jwt.sign(         access_key: app_access_key,     type: 'management',     version: 2,     iat: Math.floor(Date.now() / 1000),     nbf: Math.floor(Date.now() / 1000)    ,   app_secret,     algorithm: 'HS256', expiresIn: '24h', jwtid: uuid4()   ); // Build attendance app.post('/webhooks', async function (req, res)     var eventType = req.body.type;   var sessionId = req.body.data 'session_id' ;   // Listen to the session.close.success webhook event   if (eventType === 'session.close.success')       console.log('Session has ended');     try         // Trigger Retrieve a session API to fetch details of all peers in the session.       var response = await axios(          method: 'get',         url: baseUrl + '/sessions/' + sessionId,         headers:   Authorization: Bearer $ managementToken           );       var parsedData = response.data;       // Calculate individual participants' duration       const peers = Object.values(parsedData.peers);       const durationByUser = peers.reduce((acc, peer) =>           const duration = moment           .duration(moment(peer.left_at).diff(moment(peer.joined_at)))           .asMinutes();         const roundedDuration = Math.round(duration   100) / 100;         acc peer.user_id  = (acc peer.user_id   0) + roundedDuration;         return acc;        ,   );       const result = Object.entries(durationByUser).map(( user_id, duration ) => (          user_id,         duration        ));       console.log(result);       // Calculate aggregated participants' duration       const totalDuration = Object.values(durationByUser)         .reduce((a, b) => a + b)         .toFixed(2);       console.log( Total duration for all peers: $ totalDuration  minutes );       // Calculate total session duration       const sessionDuration = moment         .duration(moment(parsedData.updated_at).diff(moment(parsedData.created_at)))         .asMinutes()         .toFixed(2);       console.log( Session duration is: $ sessionDuration  minutes );       catch (err)         console.log(err.data);           else console.log('Some other event');  ); app.set('port', process.env.PORT  2400); app.listen(app.get('port'), function ()     console.log('Node app is running on port', app.get('port'));  );   "
    },
    {
        "title": "List sessions",
        "link": "/server-side/v2/Sessions/list-sessions",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/Sessions/list-sessions",
        "keywords": [],
        "content": "  This API retrieves details of a list of sessions in your account. <GetRequest title=\"https://api.100ms.live/v2/sessions/\"   <Request id=\"req-0\">   bash curl location request GET 'https://api.100ms.live/v2/sessions'  header 'Authorization: Bearer <management_token>'   </Request> <ResponseBox id=\"res-0\" status=\"200 OK\">   json     \"limit\": 10,   \"data\":               \"id\": \"631a0fa23e3d9fdb00832596\",       \"room_id\": \"631a0f990e6ffae22efa610c\",       \"customer_id\": \"627cdddff2e4e30487862ad1\",       \"active\": false,       \"peers\":           \"a24687bb-de68-4276-9758-e1efa5e7e4d8\":             \"id\": \"a24687bb-de68-4276-9758-e1efa5e7e4d8\",           \"session_id\": \"631a0fa23e3d9fdb00832596\",           \"name\": \"asdas\",           \"role\": \"host\",           \"user_id\": \"Batch1-student1\",           \"joined_at\": \"2022-09-08T15:52:02.194Z\",           \"left_at\": \"2022-09-08T15:52:04.009Z\"                  ,       \"created_at\": \"2022-09-08T15:52:02.197Z\",       \"updated_at\": \"2022-09-08T15:52:04.015Z\"      ,     ......     ......     ......     ......             \"id\": \"63172c2d3e3d9fdb008322e1\",       \"room_id\": \"627ce5b8f2e4e30487862adb\",       \"customer_id\": \"627cdddff2e4e30487862ad1\",       \"active\": false,       \"peers\":           \"2d873b0a-c2c9-41dc-ac72-c6ea9b2f2cc3\":             \"id\": \"2d873b0a-c2c9-41dc-ac72-c6ea9b2f2cc3\",           \"session_id\": \"63172c2d3e3d9fdb008322e1\",           \"name\": \"Nixon K\",           \"role\": \"backstage\",           \"user_id\": \"Batch1-student10\",           \"joined_at\": \"2022-09-06T11:17:01.816Z\",           \"left_at\": \"2022-09-06T11:17:03.643Z\"                  ,       \"created_at\": \"2022-09-06T11:17:01.817Z\",       \"updated_at\": \"2022-09-06T11:17:03.65Z\"          ,   \"last\": \"63172c2d3e3d9fdb008322e1\"     </ResponseBox>   Allowed filters  Name      Type    Description                                                                                Required   :     :    :                                           :     active <br    boolean  Flag to fetch the list of active room sessions. <br <br  Default : true, <br  Allowed values : true, false                               No      room_id <br   string   Fetch the list of sessions created in the room specified. <br <br   Example  : 627cda81ab4f3b56a077dc33                                No      after <br    string   Check for sessions started with a timestamp greater than or equal to \"after\" <br <br  Format : YYYY-MM-DDTHH:mm:ss.s+z.                         No      before <br    string   Check for sessions started with a timestamp less than or equal to \"after\" <br <br  Format : YYYY-MM-DDTHH:mm:ss.s+z.                          No      limit <br    int    Determines the number of session objects to be included in response. <br <br  Default : 10, <br  Allowed values :   Min  : 10,   Max  : 100              No      start <br    string   Determines the starting point. <br  last field of the response can be used as start for the subsequent request. <br <br   For example   \"63172c2d3e3d9fdb008322e1\".  No     "
    },
    {
        "title": "Session object",
        "link": "/server-side/v2/Sessions/object",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/Sessions/object",
        "keywords": [],
        "content": "    Overview Session is a single continuous call in a  room (/server-side/v2/Rooms/create-via-api). To allow users to join a 100ms video conferencing session inside your app, you first need to create a room. A single room can have multiple sessions. You will need a  management token (/server-side/v2/introduction/authentication-and-tokens) as a bearer token for your requests. This API retrieves details of a list of sessions in your account. 100ms provides actions to 1. retrieve a specific session 2. retrieve a list of sessions based on filters like:    status(active/inactive)    room_id    started time-range >   Note  : The maximum allowed duration for a session on the 100ms platform is 12 hours. Do check the  Build attendance example (./example-build-attendance) to see how you can use this API to get the attendance data of a particular session. <EndpointRequest title=\"https://api.100ms.live/v2/sessions\"     Session object  Argument   Description                                                                                            id      Unique identifier for the session.                                            room_id    Unique identifier for the room used for the particular session.                              customer_id  Unique identifier for your account                                            active    Indicates whether the session is active or inactive                                    peers     Array of peer objects with arguments such as id , session_id , name , role , user , joined_at , left_at , etc.    Peer object  Argument      Description                                                                                                                   id         Unique identifier for the peer.                                                            session_id     Unique identifier for the session.                                                           name        Alias/name of the peer used to join the session.                                                    role        Indicates the name of the role. Example: Host, Guest, Backstage, etc.                                         user_id       Internal user identifier or random string specified for peer during  app token (./../introduction/authentication-and-tokens app-token) generation.   user (Deprecated)  Unique identifier for the peer generated by 100ms                                                   joined_at      Timestamp when the peer joined the session.                                                      left_at       Timestamp when the peer left the session.                                                        Postman collection You can use our Postman collection to start exploring 100ms APIs.    Run in Postman (https://run.pstmn.io/button.svg) (https://god.gw.postman.com/run-collection/22726679-47dcd974-29d5-4965-a35b-bf9b74a8b25a?action=collection%2Ffork&collection-url=entityId%3D22726679-47dcd974-29d5-4965-a35b-bf9b74a8b25a%26entityType%3Dcollection%26workspaceId%3Dd9145dd6-337b-4761-81d6-21a30b4147a2) Refer to the  Postman guide (/server-side/v2/introduction/postman-guide) to get started with 100ms API collection. "
    },
    {
        "title": "Retrieve a specific session",
        "link": "/server-side/v2/Sessions/retrieve-a-session",
        "platformName": "Server-side",
        "objectID": "/server-side/v2/Sessions/retrieve-a-session",
        "keywords": [],
        "content": "  Retrieves the details of a specific session. 100ms provides options to retrieve details of a session with their ID. <GetRequest title=\"https://api.100ms.live/v2/sessions/<session_id>\"   <Request id=\"req-0\">   bash curl location request GET 'https://api.100ms.live/v2/sessions/<session_id>'  header 'Authorization: Bearer <management_token>'   </Request> <ResponseBox id=\"res-0\" status=\"200 OK\">   json     \"id\": \"63172c2d3e3d9fdb008322e1\",   \"room_id\": \"627ce5b8f2e4e30487862adb\",   \"customer_id\": \"627cdddff2e4e30487862ad1\",   \"active\": false,   \"peers\":       \"2d873b0a-c2c9-41dc-ac72-c6ea9b2f2cc3\":         \"id\": \"2d873b0a-c2c9-41dc-ac72-c6ea9b2f2cc3\",       \"session_id\": \"63172c2d3e3d9fdb008322e1\",       \"name\": \"Nixon K\",       \"role\": \"backstage\",       \"user_id\": \"Batch1-student\",       \"joined_at\": \"2022-09-06T11:17:01.816Z\",       \"left_at\": \"2022-09-06T11:17:03.643Z\"          ,   \"created_at\": \"2022-09-06T11:17:01.817Z\",   \"updated_at\": \"2022-09-06T11:17:03.65Z\"     </ResponseBox>   Arguments  Name              Type    Description                                                    Required   :         :    :                             :     session_id <br  — path param  string  Unique identifier of the session you wish to fetch details for. <br <br   Example  : 63172c2d3e3d9fdb008322e1  Yes      How to calculate session duration You can calculate peer duration by subtracting joined_at timestamp from left_at for a given peer. Similarly, you can calculate session duration by adding all peers' duration. Please check  Build attendance example (./example-build-attendance) to see how you can use this API to get the attendance data of a particular session. "
    }
]
