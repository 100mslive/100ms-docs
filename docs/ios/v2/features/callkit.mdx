---
title: Callkit Integration (Beta)
nav: 3.984
---

iOS SDK works well with Apple Callkit framework in your app. With Callkit, your apps can be integrated into iOS native call experience.

## Minimum Requirements

-   No minimum requirements, all our sdk versions work with Apple Callkit framework.

## How Apple Callkit works?

Callkit is generally used to support receiving voip calls in your audio/video app. When one person call other person, your backend can push a silent voip notification to your iOS app using APNS (Apple Push Notification Service).
You iOS voip app registers to listen for this notitification using Apple's Pushkit. Upon receiving this notification, iOS shows native incoming call screen for your app. Once use accepts the call, iOS calls your registered delegate to handle the call.
You handle the call by joining the the meeting mentioned in the notification payload.

When you make a call, you report Callkit about the call's status so that the system will show the appropriate native UI feedback to the user about the status of the current call. Callkit also activates it's audio session exclusive to your app when call is in progress.

### Does 100ms SDK provide special API's for Callkit?

There is no need for special APIs that needs to be used on 100ms SDK. You can use the sdk with callkit as ususal, while keeping some things in mind about how callkit's audio session works.

### What to keep in mind when using 100ms SDK with Callkit?

Callkit

1. You create an instance of AVPictureInPictureVideoCallViewController. And add the view that you want to show in PiP window as subview:

```swift
    let pipVideoCallViewController = AVPictureInPictureVideoCallViewController()
    pipVideoCallViewController.view.addSubview(...)
  ```

2. Next, you create a content source from pipVideoCallViewController, passing the target view that PiP window will use as anchor view to animate from (you can optionally set PiP preferred size):

```swift
    let pipContentSource = AVPictureInPictureController.ContentSource(
            activeVideoCallSourceView: targetView,
            contentViewController: pipVideoCallViewController)
            
    // Optionally set the target frame as preferred content size for PiP window
    pipVideoCallViewController.preferredContentSize = targetView.frame.size
  ```
  
3. Then you create AVPictureInPictureController with the content source:

```swift
    let pipController = AVPictureInPictureController(contentSource: pipContentSource)
  ```
  
4. To start the PiP mode, you set pipController to automatically start PiP when the app goes to background or you can use startPictureInPicture function to start PiP manually:

```swift
    // To start PiP automatically when app goes to background
    pipController.canStartPictureInPictureAutomaticallyFromInline = true
            
    // Or you can start PiP manually
    pipController.startPictureInPicture()
  ```
  
### How to display participant's video in PiP

AVPictureInPictureController requires source content to use AVSampleBufferDisplayLayer on it's subview. HMSVideoView uses Metal for rendering video of peers on the call. But because Metal is currently unsupported by AVPictureInPictureController, you can't directly use HMSVideoView to draw participants' video in PiP window. You need to use HMSSampleBufferDisplayView instead. HMSSampleBufferDisplayView is an UIImageView that uses AVSampleBufferDisplayLayer for drawing.

1. You create an instance of HMSSampleBufferDisplayView and set the track to display. You add this as subview to pipVideoCallViewController view:

```swift
    let trackVideoView = HMSSampleBufferDisplayView(frame: .zero)
    trackVideoView.track = track
    
    // Optionally set preferredSize and contentMode
    trackVideoView.preferredSize = CGSize(width: 640.0, height: 480.0)
    trackVideoView.contentMode = .scaleAspectFill
    
    ...
    // As in step 1 in 'How to add PiP support'
    pipVideoCallViewController.view.addSubview(trackVideoView)
  ```
  
2. Set trackVideoView to beging drawing by making it enabled when PiP window is shown (Make it false when PiP window is closed to save resources)

```swift
    trackVideoView.isEnabled = true
    ...
    
    // When PiP window is hidden
    trackVideoView.isEnabled = false
  ```
  
### CPU budget in the background
HMSSampleBufferDisplayView updates it's frame every 0.25 seconds (4 frames per second) to save CPU cycles. This is done to not exceed CPU budget assinged to a background app on iOS. You can experiment and change this update frequency using 'updateEvery' property on HMSSampleBufferDisplayView:

```swift
    // 10 frames per second
    trackVideoView.updateEvery = 0.1
  ```

ðŸ‘€ To see an example iOS Picuture in Picture implementation using 100ms SDK, checkout [our example project](https://github.com/100mslive/100ms-ios-sdk/tree/main/Example).
