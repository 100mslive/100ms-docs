---
title: Enable Transcription and Summarisation
nav: 6
---

This is a guide to enable 100ms **post call transcription** with **speaker labels** and **AI-generated summary**. The feature is currently in `Beta`.
In case you're looking to enable **live transcription**, refer to this [documentation](/server-side/v2/how-to-guides/live-transcription-hls).

<StepsToc
    parentId="transcription-flow"
    descriptions={[
        "Understanding how 100ms' post call transcription works and its pre-requisites.",
        'Enabling speaker labelled transcription and AI-generated summary through various methods.',
        "Understanding the generated output files.",
        "Accessing and consuming the generated output."
    ]}
/>

<StepsContainer id="transcription-flow">

### Getting Started

Since recording is a pre-requisite to having the transcript, you will need to start the recording either through the SDK during the session, or through the [Recording API](/server-side/v2/api-reference/recordings/start-recording-for-room) or [Live Stream API](/server-side/v2/api-reference/live-streams/start-live-stream-for-room) with recording configured. You can use auto-start in Recordings to auto-transcribe all room sessions condition to transcription being enabled.

![Flow](/docs/v2/transcription-flow.png)
The above flowchart shows the entire workflow of transcript and summary generation and their consumption. 

### Enabling Transcription and Summarisation

<Tabs id="implementation" items={['Dashboard Implementation', 'Recording API', 'Live Stream API', 'Policy API']} />
<Tab id='implementation-0'>

#### **Method 1: Dashboard Implementation**

You can enable transcription for all the rooms under a particular template.

1. Access an existing template via the sidebar.
2. Navigate to the `Transcription (Beta)` tab in the template configuration.
3. In the second card which says `Post Call Transcription`, enable the `Transcribe Recordings` toggle.
4. Enabling `Post Call Transcription` will expose an extra configuration called `Output Modes` just below the toggle. File format of the transcript output can be set using this. Following file formats are offered:
    - Text (.txt)
    - Subtitle (.srt)
    - Structured (.json)
  The example output for the above can be seen [here](#example-output-files).

5. In the same card, enable the `Summarise Transcripts` toggle. This will take the default settings for summary.
6. Save the configuration.
7. Join a room to initiate a session. Start recording (or live stream with recording enabled) using the SDK or API. If it's your first time joining a 100ms room, you'll find the option to `Start Recording` in the created room. For more information on creating room templates, refer to [this documentation](/server-side/v2/api-reference/policy/create-template-via-dashboard).

<video loop="true" autoplay="autoPlay" controls="false" id="vid" muted>
    <source src="/docs/docs/v2/transcription-dashboard-config.mov" type="video/mp4" />
</video>

**Advanced Transcription Settings**

When you enable the `Transcribe Recordings` toggle, you will observe an new card pop up below with `Advanced Transcription Configuration` as its heading. This will contain advanced settings which will be applicable for both `Live Transcription (HLS)` as well as `Post Call Transcription`.

![Advanced settings](/docs/v2/advanced-transcription-settings.png)

- `Custom Vocabulary`: Add non-dictionary words which are expected to be spoken to enhance recognition. Useful for names, abbreviations, slang, technical jargon, and more.
- `Language`: Configure the primary spoken language that has to be transcribed. This will hint the AI model to perform transcription more accurately.  Currently, only **English** is supported, which is the default. Support for more languages will follow soon.

> **Note - Dashboard Implementation Default Values**
>
> The following are the default values used in the template for transcription and summary. If you want to understand more about these and use custom values, you can refer to our [Policy API](https://www.100ms.live/docs/server-side/v2/api-reference/policy/create-template-via-api).
>
> ``` json
>{
>  "transcription": {
>    "outputModes": ["txt", "srt","json"],
>    "customVocabulary": [],
>    "summary": {
>      "enabled": true,
>      "context": "",
>      "sections": [
>        {
>          "title": "Agenda",
>          "format": "bullets"
>        },
>        {
>          "title": "Key Points Discussed",
>          "format": "bullets"
>        },
>        {
>          "title": "Follow Up Action Items",
>          "format": "bullets"
>        },
>        {
>          "title": "Short Summary",
>          "format": "paragraph"
>        }
>      ],
>      "temperature": 0.5
>    }
>  }
>}
> ```

</Tab>
<Tab id='implementation-1'>

#### **Method 2: Recording API**

You can refer to our [Recording API documentation](/server-side/v2/api-reference/recordings/start-recording-for-room) to start recording.

1. You are required to mark the transcription enable flag in the request as `true` to transcribe unless you’ve already enabled transcription for the given room template. Refer Method 1.
2. You can override the transcription and summary settings given in the policy when starting the recording through the Recording API.
3. If no input is given, the API will pick up the transcription configuration from the template policy.

</Tab>
<Tab id='implementation-2'>

#### **Method 3: Live Stream API**

You can refer to our [Live Stream API documentation](/server-side/v2/api-reference/live-streams/start-live-stream-for-room) to start stream with recording enabled.

1. You are required to mark the transcription enable flag in the request as `true` to transcribe unless you’ve already enabled transcription for the given room template. Refer Method 1.
2. You can override the transcription and summary settings given in the policy when starting the stream.
3. If no input is given, the API will pick up the transcription configuration from the template policy.

</Tab>
<Tab id='implementation-3'>

#### **Method 4: Policy API**

You can refer to our [Policy API documentation](/server-side/v2/api-reference/policy/template-object) to work with various aspects of creating and updating the template.

> **Note for Policy API**:
>
> - You will have to create and configure a `transcriptions` *destinations* object as well as either a `browserRecordings` or `hlsDestinations` *destinations* object.
> - The role given in the `transcriptions` *destinations* object and `browserRecordings` *destinations* object needs to be same and subscribed to the roles you want to record and transcribe.
> - Refer to [this documentation](/server-side/v2/api-reference/recordings/start-recording-for-room) to understand more about starting the recording.
</Tab>

### Example Output Files

Transcripts can be generated in the form of a txt, srt or a json file. Summaries are generated in the json file format only. Following are example outputs for reference:

<Tabs id="example" items={['transcript.txt', 'transcript.srt', 'transcript.json','summary.json']} />
<Tab id='example-0'>
```
John: Hello, hello, hello! How's your day been?

Sarah: Hey, long time no see! What have you been up to?
```
</Tab>
<Tab id='example-1'>
```
1
00:00:01,010 --> 00:00:12,870
John: Hello, hello, hello! How's your day been?

2
00:00:14,230 --> 00:00:17,390
Sarah: Hey, long time no see! What have you been up to?
```
</Tab>
<Tab id='example-2'>

```json
{
  "words": [
    {
      "word": "Hello",
      "name": "John",
      "peer_id": "cf348192-f1d5-4cdb-9eb3-b62c2649e419",
      "start_time": "1010",
      "end_time": "1570"
    },
	.
	.
    {
      "word": "Hey",
      "name": "Sarah",
      "peer_id": "05292829-3603-4a64-be91-5a46abdbd5af",
      "start_time": "14230",
      "end_time": "14430"
    },
	.
	.
  ],
  "sentences": [
    {
      "sentence": "Hello, hello, hello! How's your day been?",
      "name": "John",
      "peer_id": "cf348192-f1d5-4cdb-9eb3-b62c2649e419",
      "start_time": "1010",
      "end_time": "12870"
    },
    {
      "sentence": "Hey, long time no see! What have you been up to?",
      "name": "Sarah",
      "peer_id": "05292829-3603-4a64-be91-5a46abdbd5af",
      "start_time": "14230",
      "end_time": "17390"
    }
  ]
}
```
</Tab>
<Tab id='example-3'>

```json
{
  "summary": {
    "sections": [
      {
        "title": "Speakers",
        "format": "bullets",
        "bullets": [ "John", "Sarah" ],
        "paragraph": ""
      },
      {
        "title": "Agenda",
        "format": "bullets",
        "bullets": [ "Agenda item 1", "Agenda item 2" ],
        "paragraph": ""
      },
      {
        "title": "Key Points Discussed",
        "format": "bullets",
        "bullets": [ "Key Point 1", "Key Point 2" ],
        "paragraph": ""
      },
      {
        "title": "Follow-up Action Items",
        "format": "bullets",
        "bullets": [ "Follow-up Action Item 1", "Follow-up Action Item 2"],
        "paragraph": ""
      },
      {
        "title": "Short Summary",
        "format": "paragraph",
        "bullets": [],
        "paragraph": "Short summary of the meeting here"
      }
    ]
  }
}
```
</Tab>

### Consuming Transcripts and Summaries

The transcripts and summaries will be saved as *[Recording Assets](/server-side/v2/api-reference/recording-assets/overview)*. If you’ve configured storage on 100ms, they’ll be stored in your cloud bucket. Otherwise, they’ll be stored in 100ms’ storage for the same duration as the recording. 

There are three ways to consume the generated transcripts and summaries. 

<Tabs id="consumption" items={['Dashboard','Webhooks','Recording Assets API']}/>
<Tab id='consumption-0'>

#### **Method 1: Dashboard**

Once you’ve recorded a session with transcription and summary enabled, you can expect recording assets to be ready within 20% of the recording duration time period.

To access transcripts and summaries on the dashboard:

1. Navigate to the `Sessions` tab in the sidebar to view previous sessions.
2. Locate the session with transcription enabled. The `Recording Status` column will indicate the status of the recording.
3. Click on the `Completed` status of the chosen session ID.
4. This will open the Session Details page. Access the `Recording Log` to find the available assets and view them.
5. Click on `View Assets` to open a pop-up with pre-signed URLs for the recording, chat, transcripts, and summary.

![transcription-consumption](/docs/v2/transcription-dashboard-consumption.gif)

</Tab>
<Tab id='consumption-1'>

#### **Method 2: Webhooks**

As soon as the transcription is completed, the webhook containing the asset type, id, path and the pre-signed URL to the asset itself (transcript, summary) will be sent to the configured webhook URL. 

1. Learn to configure the webhook using [this documentation](/server-side/v2/how-to-guides/configure-webhooks/overview).
2. Transcription specific webhook events can be found [here](/server-side/v2/how-to-guides/configure-webhooks/webhook#transcriptionstartedsuccess).

</Tab>
<Tab id='consumption-2'>

#### **Method 3: Recording Assets API**

You can always use 100ms’ Recording Assets API to access the transcripts and summaries files. Refer to [this documentation](/server-side/v2/api-reference/recording-assets/overview).

</Tab>

</StepsContainer>

## Limitations

1. The transcription and summary won't be available immediately. The processing and delivery will take a minimum of 5 minutes and 20% of the recording duration to be generated.
2. This feature does not work with SFU Recordings.
3. Presently, you can only input a maximum of 6 sections in the summary through the API.
4. There are chances of incomplete summary generation in case of recordings which are longer than 90 minutes.
5. This feature only supports the **English** language as of now.


### Frequently Asked Questions (FAQs)


1. **How many languages are supported?**
    
  Presently, only English language is supported. But support for other popular languages like French, Portuguese, Spanish and more is coming soon.

2. **What happens if multiple languages are being spoken in the live stream?**
    
  The bits which are spoken in the selected language will be transcribed. There might be some gibberish text though due to hallucinations by the AI.

3. **Can multiple summaries be generated or re-generated with different prompts for the same transcript?**

  This is not possible at this point of time. But we intend to bring the functionality of re-running transcription and summarization functions to enable users to test and build their own summaries.

4. **What can be done if the speaker label is not working?**
    
  There are two possible options here:
  1. If you are using an older webSDK version, please update to the latest. Refer to our web documentation [here](/javascript/v2/release-notes/release-notes).
  2. Add the following snippet to enable speaker logging.
    - Create a new file `src/components/AudioLevel/BeamSpeakerLabelsLogging.jsx` with below code.
    ``` jsx
    import { useEffect } from "react";
    import { useHMSActions } from "@100mslive/react-sdk";
    import { useIsHeadless } from "../AppData/useUISettings";

    export function BeamSpeakerLabelsLogging() {
      const hmsActions = useHMSActions();
      const isHeadless = useIsHeadless();

      useEffect(() => {
        if (isHeadless) {
          hmsActions.enableBeamSpeakerLabelsLogging();
        }
      }, [hmsActions, isHeadless]);
      return null;
    }
    ```
    - Register `<BeamSpeakerLabelsLogging />` in Approutes along with import statement
    `import { BeamSpeakerLabelsLogging } from "./components/AudioLevel/BeamSpeakerLabelsLogging";`



